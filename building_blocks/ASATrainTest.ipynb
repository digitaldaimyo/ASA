{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/digitaldaimyo/ASA/blob/main/building_blocks/ASATrainTest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QekhwaCNThxw"
      },
      "source": [
        "# Addressed State Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyUvgie9Tl1f"
      },
      "source": [
        "## Define"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "iaYUl2jSTpN-"
      },
      "outputs": [],
      "source": [
        "#@title Addressed State Attention\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Optional, Dict, Tuple\n",
        "\n",
        "# -------------------------\n",
        "# RoPE helper (rotate-half)\n",
        "# -------------------------\n",
        "def _rotate_half(x: torch.Tensor) -> torch.Tensor:\n",
        "    x1 = x[..., ::2]\n",
        "    x2 = x[..., 1::2]\n",
        "    return torch.stack((-x2, x1), dim=-1).flatten(-2)\n",
        "\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    def __init__(self, dim: int, base: float = 10000.0):\n",
        "        super().__init__()\n",
        "        assert dim % 2 == 0, \"RoPE requires even dim\"\n",
        "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
        "        self._cos_cached = None\n",
        "        self._sin_cached = None\n",
        "        self._t_cached = None\n",
        "        self._device_cached = None\n",
        "\n",
        "    def get_cos_sin(self, T: int, device, dtype):\n",
        "        if (\n",
        "            self._t_cached == T\n",
        "            and self._cos_cached is not None\n",
        "            and self._device_cached == device\n",
        "        ):\n",
        "            return self._cos_cached.to(dtype=dtype), self._sin_cached.to(dtype=dtype)\n",
        "\n",
        "        t = torch.arange(T, device=device, dtype=self.inv_freq.dtype)\n",
        "        freqs = torch.einsum(\"t,f->tf\", t, self.inv_freq)  # [T, d/2]\n",
        "        emb = torch.cat([freqs, freqs], dim=-1)            # [T, d]\n",
        "        cos = emb.cos()[None, None, :, :]                  # [1,1,T,d]\n",
        "        sin = emb.sin()[None, None, :, :]                  # [1,1,T,d]\n",
        "\n",
        "        self._t_cached = T\n",
        "        self._device_cached = device\n",
        "        self._cos_cached = cos\n",
        "        self._sin_cached = sin\n",
        "        return cos.to(dtype=dtype), sin.to(dtype=dtype)\n",
        "\n",
        "def apply_rope(x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> torch.Tensor:\n",
        "    return (x * cos) + (_rotate_half(x) * sin)\n",
        "\n",
        "# -------------------------\n",
        "# ALiBi slopes helper\n",
        "# -------------------------\n",
        "def alibi_slopes(num_heads: int, device=None, dtype=torch.float32) -> torch.Tensor:\n",
        "    def get_slopes(n):\n",
        "        def power_of_2_slopes(n):\n",
        "            start = 2.0 ** (-(2.0 ** -(math.log2(n) - 3)))\n",
        "            ratio = start\n",
        "            return [start * (ratio ** i) for i in range(n)]\n",
        "        if math.log2(n).is_integer():\n",
        "            return power_of_2_slopes(n)\n",
        "        closest = 2 ** math.floor(math.log2(n))\n",
        "        return power_of_2_slopes(closest) + get_slopes(2 * closest)[0::2][: n - closest]\n",
        "    return torch.tensor(get_slopes(num_heads), device=device, dtype=dtype)  # [H]\n",
        "\n",
        "# -------------------------\n",
        "# softplus init helpers\n",
        "# -------------------------\n",
        "def _inv_softplus(y: torch.Tensor) -> torch.Tensor:\n",
        "    return torch.log(torch.expm1(y))\n",
        "\n",
        "# -------------------------\n",
        "# Linear attention feature map (Performer-style)\n",
        "# -------------------------\n",
        "def phi(x: torch.Tensor) -> torch.Tensor:\n",
        "    return F.elu(x) + 1.0\n",
        "\n",
        "\n",
        "class AddressedStateAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Addressed State Attention (ASA):\n",
        "      - prefix-softmax WRITE into slots (O(T))\n",
        "      - READ routing from tokens -> slots (softmax over slots)\n",
        "      - optional content-conditioned READ term (gamma)\n",
        "      - RoPE on write keys (geometry)\n",
        "      - ALiBi bias on write logits (prefix-friendly)\n",
        "\n",
        "    Optional slot-space refinement (formerly \"k-space\"):\n",
        "      - causal linear attention in a low-dim slot-address coordinate space\n",
        "      - produces per-token signed weights over slots\n",
        "      - decoded through the same streaming slot-state basis\n",
        "      - gated by learnable slotspace_gate (softplus)\n",
        "\n",
        "    PERF (behavior-preserving):\n",
        "      - Streaming prefix write states in chunks (no [B,H,K,T,d] materialization)\n",
        "      - Slot-space prefix scan is chunked (exact)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int,\n",
        "        num_heads: int = 8,\n",
        "        num_slots: int = 8,\n",
        "        dropout: float = 0.1,\n",
        "\n",
        "        # temperatures / numerics\n",
        "        read_temperature: float = 1.0,\n",
        "        write_temperature: float = 1.0,\n",
        "        state_fp32: bool = True,\n",
        "        slot_dropout: float = 0.0,\n",
        "        normalize_k: bool = False,\n",
        "\n",
        "        # positions (write geometry)\n",
        "        use_rope_keys: bool = True,\n",
        "        rope_base: float = 10000.0,\n",
        "\n",
        "        # write bias (ALiBi)\n",
        "        use_alibi_write: bool = True,\n",
        "        alibi_strength_init: float = 0.1,\n",
        "        learn_alibi_strength: bool = True,\n",
        "        min_strength: float = 0.0,\n",
        "\n",
        "        # content-conditioned read term\n",
        "        use_content_read: bool = True,\n",
        "        content_read_init: float = -4.0,\n",
        "        content_read_max_gamma: float = 3.0,\n",
        "\n",
        "        # optional slot-space refinement\n",
        "        use_slotspace_refine: bool = True,\n",
        "        slotspace_dim: int = 32,\n",
        "        slotspace_gate_init: float = -4.0,\n",
        "        slotspace_dropout: float = 0.05,\n",
        "        slotspace_signed_weights: bool = True,\n",
        "\n",
        "        # RoPE in slot-space matcher (Q/K only)\n",
        "        use_rope_slotspace: bool = True,\n",
        "        rope_base_slotspace: float = 100000.0,\n",
        "\n",
        "        # perf knobs (no behavior change)\n",
        "        write_chunk_size: int = 128,\n",
        "        slotspace_chunk_size: int = 128,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.num_slots = num_slots\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.read_temperature = float(read_temperature)\n",
        "        self.write_temperature = float(write_temperature)\n",
        "        self.state_fp32 = bool(state_fp32)\n",
        "        self.slot_dropout = float(slot_dropout)\n",
        "        self.normalize_k = bool(normalize_k)\n",
        "        self.routing_override = None\n",
        "\n",
        "        self.use_rope_keys = bool(use_rope_keys)\n",
        "        self.use_alibi_write = bool(use_alibi_write)\n",
        "        self.learn_alibi_strength = bool(learn_alibi_strength)\n",
        "        self.min_strength = float(min_strength)\n",
        "\n",
        "        self.use_content_read = bool(use_content_read)\n",
        "        self.content_read_max_gamma = float(content_read_max_gamma)\n",
        "\n",
        "        self.use_slotspace_refine = bool(use_slotspace_refine)\n",
        "        self.slotspace_dim = int(slotspace_dim)\n",
        "        self.slotspace_dropout = nn.Dropout(float(slotspace_dropout))\n",
        "        self.slotspace_signed_weights = bool(slotspace_signed_weights)\n",
        "\n",
        "        self.write_chunk_size = int(write_chunk_size)\n",
        "        self.slotspace_chunk_size = int(slotspace_chunk_size)\n",
        "\n",
        "        # Learned slot keys per head: [H,K,d]\n",
        "        self.slot_keys = nn.Parameter(\n",
        "            torch.randn(num_heads, num_slots, self.head_dim) / math.sqrt(self.head_dim)\n",
        "        )\n",
        "\n",
        "        # Projections\n",
        "        self.Wk_write = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.Wv_write = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.Wq_read  = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "\n",
        "        # RoPE (write geometry)\n",
        "        self.rope = RotaryEmbedding(self.head_dim, base=rope_base) if self.use_rope_keys else None\n",
        "\n",
        "        # ALiBi slopes (buffer)\n",
        "        if self.use_alibi_write:\n",
        "            self.register_buffer(\"_alibi_slopes\", alibi_slopes(num_heads), persistent=False)  # [H]\n",
        "        else:\n",
        "            self.register_buffer(\"_alibi_slopes\", torch.zeros(num_heads), persistent=False)\n",
        "\n",
        "        # Learnable ALiBi strength (positive via softplus)\n",
        "        if self.use_alibi_write and self.learn_alibi_strength:\n",
        "            init = torch.tensor(float(alibi_strength_init) - self.min_strength).clamp_min(1e-8)\n",
        "            self._alibi_strength_param = nn.Parameter(_inv_softplus(init))\n",
        "        else:\n",
        "            self._alibi_strength_param = None\n",
        "            self.alibi_strength = float(alibi_strength_init)\n",
        "\n",
        "        # Content read gamma (>=0 via softplus)\n",
        "        if self.use_content_read:\n",
        "            self._content_read_gamma_raw = nn.Parameter(torch.tensor(float(content_read_init)))\n",
        "        else:\n",
        "            self._content_read_gamma_raw = None\n",
        "\n",
        "        # -------------------------\n",
        "        # Optional slot-space refinement\n",
        "        # -------------------------\n",
        "        self.use_rope_slotspace = bool(use_rope_slotspace) and bool(self.use_slotspace_refine)\n",
        "        if self.use_slotspace_refine:\n",
        "            self.slot_in  = nn.Linear(num_slots, self.slotspace_dim, bias=False)\n",
        "            self.slot_q   = nn.Linear(self.slotspace_dim, self.slotspace_dim, bias=False)\n",
        "            self.slot_k   = nn.Linear(self.slotspace_dim, self.slotspace_dim, bias=False)\n",
        "            self.slot_v   = nn.Linear(self.slotspace_dim, self.slotspace_dim, bias=False)\n",
        "            self.slot_out = nn.Linear(self.slotspace_dim, num_slots, bias=False)\n",
        "            self._slotspace_gate_raw = nn.Parameter(torch.tensor(float(slotspace_gate_init)))\n",
        "\n",
        "            if self.use_rope_slotspace:\n",
        "                assert (self.slotspace_dim % 2) == 0, \"use_rope_slotspace requires even slotspace_dim\"\n",
        "                self.rope_slotspace = RotaryEmbedding(self.slotspace_dim, base=float(rope_base_slotspace))\n",
        "            else:\n",
        "                self.rope_slotspace = None\n",
        "        else:\n",
        "            self.slot_in = None\n",
        "            self.slot_q = self.slot_k = self.slot_v = None\n",
        "            self.slot_out = None\n",
        "            self._slotspace_gate_raw = None\n",
        "            self.rope_slotspace = None\n",
        "\n",
        "    def _alibi_strength(self, dtype, device) -> torch.Tensor:\n",
        "        if not (self.use_alibi_write and self.learn_alibi_strength):\n",
        "            return torch.tensor(self.alibi_strength, dtype=dtype, device=device)\n",
        "        return (F.softplus(self._alibi_strength_param) + self.min_strength).to(dtype=dtype, device=device)\n",
        "\n",
        "    def _content_read_gamma(self, dtype, device) -> torch.Tensor:\n",
        "        if not self.use_content_read:\n",
        "            return torch.tensor(0.0, dtype=dtype, device=device)\n",
        "        g = F.softplus(self._content_read_gamma_raw)  # >= 0\n",
        "        if self.content_read_max_gamma is not None and self.content_read_max_gamma > 0:\n",
        "            g = g.clamp(max=self.content_read_max_gamma)\n",
        "        return g.to(dtype=dtype, device=device)\n",
        "\n",
        "    def _slotspace_gate(self, dtype, device) -> torch.Tensor:\n",
        "        if not self.use_slotspace_refine:\n",
        "            return torch.tensor(0.0, dtype=dtype, device=device)\n",
        "        return F.softplus(self._slotspace_gate_raw).to(dtype=dtype, device=device)\n",
        "\n",
        "    @staticmethod\n",
        "    def _safe_exp_sub_max(s: torch.Tensor, m: torch.Tensor) -> torch.Tensor:\n",
        "        diff = s - m\n",
        "        diff = diff.masked_fill(~torch.isfinite(m), float(\"-inf\"))\n",
        "        return torch.exp(diff)\n",
        "\n",
        "    @torch.compile\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        return_info: bool = False,\n",
        "        routing_mode: str = \"softmax\",           # \"softmax\" | \"top1\" | \"topk\" | \"external\"\n",
        "        routing_topk: int = 2,                   # used if routing_mode==\"topk\"\n",
        "        read_weights_override: Optional[torch.Tensor] = None,  # [B,H,T,K] or [B,H,L,K]\n",
        "        routing_noise: Optional[str] = None,     # None | \"gumbel\" | \"gaussian\"\n",
        "        routing_noise_scale: float = 1.0,\n",
        "\n",
        "    ) -> Tuple[torch.Tensor, Optional[Dict[str, torch.Tensor]]]:\n",
        "        B, T, C = x.shape\n",
        "        H, K, d = self.num_heads, self.num_slots, self.head_dim\n",
        "\n",
        "        # Project (write K/V, read Q)\n",
        "        k_write = self.Wk_write(x).view(B, T, H, d).transpose(1, 2)  # [B,H,T,d]\n",
        "        v_write = self.Wv_write(x).view(B, T, H, d).transpose(1, 2)  # [B,H,T,d]\n",
        "        q_read  = self.Wq_read(x).view(B, T, H, d).transpose(1, 2)   # [B,H,T,d]\n",
        "\n",
        "        if self.normalize_k:\n",
        "            k_write = F.normalize(k_write, dim=-1, eps=1e-8)\n",
        "\n",
        "        # RoPE on write keys\n",
        "        if self.use_rope_keys:\n",
        "            cos, sin = self.rope.get_cos_sin(T, device=x.device, dtype=k_write.dtype)\n",
        "            k_write = apply_rope(k_write, cos, sin)\n",
        "\n",
        "        # Slot dropout\n",
        "        slot_keys = self.slot_keys\n",
        "        if self.training and self.slot_dropout > 0.0:\n",
        "            drop = (torch.rand((H, K), device=x.device) < self.slot_dropout)\n",
        "            slot_keys = slot_keys * (~drop).to(slot_keys.dtype).unsqueeze(-1)\n",
        "\n",
        "        # WRITE logits: [B,H,K,T]\n",
        "        write_logits_raw = torch.einsum(\"hkd,bhtd->bhkt\", slot_keys, k_write) / math.sqrt(d)\n",
        "\n",
        "        # Stable dtype for prefix-softmax math\n",
        "        state_dtype = torch.float32 if (self.state_fp32 and x.dtype != torch.float32) else x.dtype\n",
        "        write_logits = write_logits_raw.to(state_dtype)\n",
        "\n",
        "        # Write temperature\n",
        "        wtemp = max(1e-6, self.write_temperature)\n",
        "        write_logits = write_logits / wtemp\n",
        "\n",
        "        # ALiBi distance bias (prefix-friendly)\n",
        "        alibi_bias_applied = None\n",
        "        if self.use_alibi_write:\n",
        "            strength = self._alibi_strength(dtype=state_dtype, device=x.device)  # scalar\n",
        "            slopes = self._alibi_slopes.to(device=x.device, dtype=state_dtype) * strength  # [H]\n",
        "            pos_i = torch.arange(T, device=x.device, dtype=state_dtype)  # [T]\n",
        "            alibi_bias = slopes.view(1, H, 1, 1) * pos_i.view(1, 1, 1, T) # [1,H,1,T]\n",
        "            write_logits = write_logits + alibi_bias\n",
        "            alibi_bias_applied = alibi_bias\n",
        "\n",
        "        # Key padding mask (mask positions that are padding)\n",
        "        if attention_mask is not None:\n",
        "            valid = attention_mask.to(dtype=torch.bool)\n",
        "            write_logits = write_logits.masked_fill(~valid.view(B, 1, 1, T), float(\"-inf\"))\n",
        "        else:\n",
        "            valid = None\n",
        "\n",
        "        # =====================================================\n",
        "        # STREAMING WRITE + READ (no [B,H,K,T,d] slot states)\n",
        "        # =====================================================\n",
        "        content_read_gamma = self._content_read_gamma(dtype=q_read.dtype, device=x.device)\n",
        "        rtemp = max(1e-6, self.read_temperature)\n",
        "\n",
        "        out_h = torch.empty((B, H, T, d), device=x.device, dtype=state_dtype)\n",
        "        read_weights = torch.empty((B, H, T, K), device=x.device, dtype=q_read.dtype)\n",
        "\n",
        "        # Optional analytics: [B,H,T,K] (later permuted to [B,H,K,T])\n",
        "        slot_state_norm_t = torch.empty((B, H, T, K), device=x.device, dtype=torch.float32) if return_info else None\n",
        "\n",
        "        denom_state = torch.zeros((B, H, K), device=x.device, dtype=state_dtype)\n",
        "        numer_state = torch.zeros((B, H, K, d), device=x.device, dtype=state_dtype)\n",
        "        m_state = torch.full((B, H, K), float(\"-inf\"), device=x.device, dtype=state_dtype)\n",
        "\n",
        "        WRITE_CHUNK = self.write_chunk_size\n",
        "\n",
        "        for t0 in range(0, T, WRITE_CHUNK):\n",
        "            t1 = min(T, t0 + WRITE_CHUNK)\n",
        "            L = t1 - t0\n",
        "\n",
        "            wlog_c = write_logits[:, :, :, t0:t1]  # [B,H,K,L]\n",
        "\n",
        "            # streaming cummax\n",
        "            m_c, _ = torch.cummax(wlog_c, dim=-1)  # [B,H,K,L]\n",
        "            m_new = torch.maximum(m_state.unsqueeze(-1), m_c)  # [B,H,K,L]\n",
        "\n",
        "            # rescale old prefix state to new max reference\n",
        "            scale = torch.exp(m_state.unsqueeze(-1) - m_new)  # [B,H,K,L] (exp(-inf)=0)\n",
        "\n",
        "            denom_c = denom_state.unsqueeze(-1) * scale                  # [B,H,K,L]\n",
        "            numer_c = numer_state.unsqueeze(-2) * scale.unsqueeze(-1)    # [B,H,K,L,d]\n",
        "\n",
        "            # new weights\n",
        "            w_new = self._safe_exp_sub_max(wlog_c, m_new)  # [B,H,K,L]\n",
        "\n",
        "            # accumulate within chunk\n",
        "            denom_c = denom_c + torch.cumsum(w_new, dim=-1)  # [B,H,K,L]\n",
        "            v_c = v_write[:, :, t0:t1, :].to(state_dtype)    # [B,H,L,d]\n",
        "            add = torch.cumsum(w_new.unsqueeze(-1) * v_c.unsqueeze(2), dim=-2)  # [B,H,K,L,d]\n",
        "            numer_c = numer_c + add\n",
        "\n",
        "            # per-token slot state for this chunk only\n",
        "            slot_state_c = numer_c / denom_c.clamp_min(1e-8).unsqueeze(-1)  # [B,H,K,L,d]\n",
        "            slot_state_t = slot_state_c.permute(0, 1, 3, 2, 4).contiguous() # [B,H,L,K,d]\n",
        "\n",
        "            # READ routing logits\n",
        "            q_read_c = q_read[:, :, t0:t1, :]  # [B,H,L,d]\n",
        "\n",
        "            # base (key) term\n",
        "            read_logits_key = torch.einsum(\"bhld,hkd->bhlk\", q_read_c, slot_keys) / math.sqrt(d)\n",
        "\n",
        "            # optional content term\n",
        "            read_logits_content = None\n",
        "            read_logits = read_logits_key\n",
        "            if self.use_content_read:\n",
        "                read_logits_content = torch.einsum(\n",
        "                    \"bhld,bhlkd->bhlk\",\n",
        "                    q_read_c,\n",
        "                    slot_state_t.to(q_read_c.dtype)\n",
        "                ) / math.sqrt(d)\n",
        "                read_logits = read_logits + content_read_gamma.to(read_logits.dtype) * read_logits_content\n",
        "\n",
        "            # Optional: noise on logits to probe routing stability (off by default)\n",
        "            # You can plumb these as forward() kwargs; see signature snippet below.\n",
        "            if routing_noise is not None:\n",
        "                if routing_noise == \"gumbel\":\n",
        "                    # gumbel(0,1) noise; scale by routing_noise_scale\n",
        "                    u = torch.rand_like(read_logits)\n",
        "                    g = -torch.log(-torch.log(u.clamp_min(1e-8)).clamp_min(1e-8))\n",
        "                    read_logits = read_logits + routing_noise_scale * g\n",
        "                elif routing_noise == \"gaussian\":\n",
        "                    read_logits = read_logits + routing_noise_scale * torch.randn_like(read_logits)\n",
        "                else:\n",
        "                    raise ValueError(f\"Unknown routing_noise={routing_noise}\")\n",
        "\n",
        "\n",
        "            if self.routing_override is not None:\n",
        "                if callable(self.routing_override):\n",
        "                    ctx = {\n",
        "                        \"t0\": t0,\n",
        "                        \"t1\": t1,\n",
        "                        \"B\": B, \"H\": H, \"T\": T, \"K\": K, \"d\": d,\n",
        "                        \"rtemp\": rtemp,\n",
        "                        \"state_dtype\": state_dtype,\n",
        "                        \"q_read_c\": q_read_c,          # [B,H,L,d]\n",
        "                        \"slot_keys\": slot_keys,        # [H,K,d]\n",
        "                        \"slot_state_t\": slot_state_t,  # [B,H,L,K,d] (current prefix slot states)\n",
        "                        \"valid\": valid,                # [B,T] or None\n",
        "                    }\n",
        "\n",
        "\n",
        "\n",
        "                    # must return [B,H,L,K]\n",
        "                    read_w_c = self.routing_override(\n",
        "                        t0, t1, read_logits,    # [B,H,L,K] full (key + content + noise if applied),\n",
        "                        read_logits_key,        # [B,H,L,K] key-only\n",
        "                        read_logits_content,    # [B,H,L,K] or None\n",
        "                        ctx,\n",
        "                    )\n",
        "                else:\n",
        "                    # tensor override: [B,H,T,K]\n",
        "                    read_w_c = self.routing_override[:, :, t0:t1, :].to(read_logits.dtype)\n",
        "\n",
        "                # safety: ensure finite + normalize\n",
        "                read_w_c = torch.nan_to_num(read_w_c, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "                read_w_c = read_w_c.clamp_min(0.0)\n",
        "                read_w_c = read_w_c / read_w_c.sum(dim=-1, keepdim=True).clamp_min(1e-8)\n",
        "\n",
        "            else:\n",
        "\n",
        "                # Routing mode\n",
        "                if routing_mode == \"softmax\":\n",
        "                    read_w_c = torch.softmax(read_logits / rtemp, dim=-1)  # [B,H,L,K]\n",
        "\n",
        "                elif routing_mode == \"top1\":\n",
        "                    # hard one-hot on argmax\n",
        "                    top = read_logits.argmax(dim=-1)  # [B,H,L]\n",
        "                    read_w_c = F.one_hot(top, num_classes=K).to(read_logits.dtype)\n",
        "\n",
        "                elif routing_mode == \"topk\":\n",
        "                    kk = int(routing_topk)\n",
        "                    kk = max(1, min(K, kk))\n",
        "                    # mask out everything except top-k then renormalize with softmax-like\n",
        "                    vals, idx = torch.topk(read_logits, k=kk, dim=-1)\n",
        "                    masked = torch.full_like(read_logits, float(\"-inf\"))\n",
        "                    masked.scatter_(-1, idx, vals)\n",
        "                    read_w_c = torch.softmax(masked / rtemp, dim=-1)\n",
        "\n",
        "                elif routing_mode == \"external\":\n",
        "                    if read_weights_override is None:\n",
        "                        raise ValueError(\"routing_mode='external' requires read_weights_override\")\n",
        "                    # accept either full [B,H,T,K] or chunk [B,H,L,K]\n",
        "                    if read_weights_override.shape[-2] == T:\n",
        "                        read_w_c = read_weights_override[:, :, t0:t1, :]\n",
        "                    else:\n",
        "                        read_w_c = read_weights_override\n",
        "                    # safety: renormalize\n",
        "                    read_w_c = read_w_c / read_w_c.sum(dim=-1, keepdim=True).clamp_min(1e-8)\n",
        "\n",
        "                else:\n",
        "                    raise ValueError(f\"Unknown routing_mode={routing_mode}\")\n",
        "\n",
        "            read_weights[:, :, t0:t1, :] = read_w_c\n",
        "\n",
        "            # token output\n",
        "            out_h[:, :, t0:t1, :] = torch.einsum(\n",
        "                \"bhlk,bhlkd->bhld\",\n",
        "                read_w_c.to(state_dtype),\n",
        "                slot_state_t.to(state_dtype),\n",
        "            )\n",
        "\n",
        "            if return_info:\n",
        "                slot_state_norm_t[:, :, t0:t1, :] = slot_state_t.to(torch.float32).norm(dim=-1)\n",
        "\n",
        "            # update running states to end-of-chunk\n",
        "            m_state = m_new[:, :, :, -1]\n",
        "            denom_state = denom_c[:, :, :, -1]\n",
        "            numer_state = numer_c[:, :, :, -1, :]\n",
        "\n",
        "        # =====================================================\n",
        "        # Optional: causal linear attention in slot-space (CHUNKED prefix scan)\n",
        "        # =====================================================\n",
        "        slotspace_delta_norm_mean = None\n",
        "        if self.use_slotspace_refine:\n",
        "            slotspace_dtype = state_dtype\n",
        "            M = self.slotspace_dim\n",
        "\n",
        "            # Encode read weights into slot-space coordinates\n",
        "            u = self.slot_in(read_weights.to(slotspace_dtype))  # [B,H,T,M]\n",
        "            q_s  = self.slot_q(u)\n",
        "            k_s  = self.slot_k(u)\n",
        "            v_s  = self.slot_v(u)\n",
        "\n",
        "            # RoPE in slot-space matcher (Q/K only)\n",
        "            if self.use_rope_slotspace:\n",
        "                cos_s, sin_s = self.rope_slotspace.get_cos_sin(T, device=x.device, dtype=q_s.dtype)\n",
        "                q_s = apply_rope(q_s, cos_s, sin_s)\n",
        "                k_s = apply_rope(k_s, cos_s, sin_s)\n",
        "\n",
        "            qf = phi(q_s)\n",
        "            kf = phi(k_s)\n",
        "\n",
        "            if valid is not None:\n",
        "                mask = valid.view(B, 1, T, 1).to(slotspace_dtype)\n",
        "                qf = qf * mask\n",
        "                kf = kf * mask\n",
        "                v_s = v_s * mask\n",
        "\n",
        "            u2 = torch.empty((B, H, T, M), device=x.device, dtype=slotspace_dtype)\n",
        "\n",
        "            S_state = torch.zeros((B, H, M, M), device=x.device, dtype=slotspace_dtype)\n",
        "            Z_state = torch.zeros((B, H, M), device=x.device, dtype=slotspace_dtype)\n",
        "\n",
        "            SS_CHUNK = self.slotspace_chunk_size\n",
        "\n",
        "            for t0 in range(0, T, SS_CHUNK):\n",
        "                t1 = min(T, t0 + SS_CHUNK)\n",
        "                L = t1 - t0\n",
        "\n",
        "                qf_c = qf[:, :, t0:t1, :]   # [B,H,L,M]\n",
        "                kf_c = kf[:, :, t0:t1, :]   # [B,H,L,M]\n",
        "                v_c  = v_s[:, :, t0:t1, :]  # [B,H,L,M]\n",
        "\n",
        "                kv = torch.einsum(\"bhlm,bhln->bhlmn\", kf_c, v_c)  # [B,H,L,M,M]\n",
        "                S_c = torch.cumsum(kv, dim=2)\n",
        "                Z_c = torch.cumsum(kf_c, dim=2)\n",
        "\n",
        "                S_c = S_c + S_state.unsqueeze(2)\n",
        "                Z_c = (Z_c + Z_state.unsqueeze(2)).clamp_min(1e-8)\n",
        "\n",
        "                num = torch.einsum(\"bhlm,bhlmn->bhln\", qf_c, S_c)\n",
        "                den = torch.einsum(\"bhlm,bhlm->bhl\", qf_c, Z_c).unsqueeze(-1).clamp_min(1e-8)\n",
        "                u2[:, :, t0:t1, :] = num / den\n",
        "\n",
        "                S_state = S_c[:, :, -1, :, :]\n",
        "                Z_state = Z_c[:, :, -1, :]\n",
        "\n",
        "            u2 = self.slotspace_dropout(u2)\n",
        "\n",
        "            # Decode slot weights per token\n",
        "            slot_w = self.slot_out(u2)  # [B,H,T,K]\n",
        "            if self.slotspace_signed_weights:\n",
        "                slot_w = torch.tanh(slot_w)\n",
        "            else:\n",
        "                slot_w = torch.softmax(slot_w, dim=-1)\n",
        "\n",
        "            # Second streaming pass to decode slotspace contribution through slot states\n",
        "            gate = self._slotspace_gate(dtype=state_dtype, device=x.device).to(state_dtype)\n",
        "\n",
        "            denom_state = torch.zeros((B, H, K), device=x.device, dtype=state_dtype)\n",
        "            numer_state = torch.zeros((B, H, K, d), device=x.device, dtype=state_dtype)\n",
        "            m_state = torch.full((B, H, K), float(\"-inf\"), device=x.device, dtype=state_dtype)\n",
        "\n",
        "            delta_norm_sum = torch.zeros((), device=x.device, dtype=torch.float32)\n",
        "            delta_norm_count = 0\n",
        "\n",
        "            for t0 in range(0, T, WRITE_CHUNK):\n",
        "                t1 = min(T, t0 + WRITE_CHUNK)\n",
        "                L = t1 - t0\n",
        "\n",
        "                wlog_c = write_logits[:, :, :, t0:t1]  # [B,H,K,L]\n",
        "                m_c, _ = torch.cummax(wlog_c, dim=-1)\n",
        "                m_new = torch.maximum(m_state.unsqueeze(-1), m_c)\n",
        "\n",
        "                scale = torch.exp(m_state.unsqueeze(-1) - m_new)\n",
        "                denom_c = denom_state.unsqueeze(-1) * scale\n",
        "                numer_c = numer_state.unsqueeze(-2) * scale.unsqueeze(-1)\n",
        "\n",
        "                w_new = self._safe_exp_sub_max(wlog_c, m_new)\n",
        "                denom_c = denom_c + torch.cumsum(w_new, dim=-1)\n",
        "\n",
        "                v_c = v_write[:, :, t0:t1, :].to(state_dtype)  # [B,H,L,d]\n",
        "                add = torch.cumsum(w_new.unsqueeze(-1) * v_c.unsqueeze(2), dim=-2)\n",
        "                numer_c = numer_c + add\n",
        "\n",
        "                slot_state_c = numer_c / denom_c.clamp_min(1e-8).unsqueeze(-1)  # [B,H,K,L,d]\n",
        "                slot_state_t = slot_state_c.permute(0, 1, 3, 2, 4).contiguous() # [B,H,L,K,d]\n",
        "\n",
        "                slot_w_c = slot_w[:, :, t0:t1, :].to(state_dtype)  # [B,H,L,K]\n",
        "                delta_c = torch.einsum(\"bhlk,bhlkd->bhld\", slot_w_c, slot_state_t.to(state_dtype))  # [B,H,L,d]\n",
        "\n",
        "                out_h[:, :, t0:t1, :] = out_h[:, :, t0:t1, :] + gate * delta_c\n",
        "\n",
        "                delta_norm_sum = delta_norm_sum + delta_c.detach().to(torch.float32).norm(dim=-1).sum()\n",
        "                delta_norm_count += (B * H * L)\n",
        "\n",
        "                m_state = m_new[:, :, :, -1]\n",
        "                denom_state = denom_c[:, :, :, -1]\n",
        "                numer_state = numer_c[:, :, :, -1, :]\n",
        "\n",
        "            slotspace_delta_norm_mean = (delta_norm_sum / max(1, delta_norm_count)).detach().cpu()\n",
        "\n",
        "        # Finish\n",
        "        out = out_h.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        out = self.out_proj(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        info = None\n",
        "        if return_info:\n",
        "            info = {\n",
        "                \"write_logits_raw\": write_logits_raw.detach(),\n",
        "                \"write_logits\": write_logits.detach().to(torch.float32),\n",
        "                \"read_weights\": read_weights.detach(),\n",
        "                # [B,H,K,T]\n",
        "                \"slot_state_norm\": slot_state_norm_t.detach().permute(0, 1, 3, 2).contiguous() if slot_state_norm_t is not None else None,\n",
        "                \"content_read_gamma\": content_read_gamma.detach().to(torch.float32).cpu(),\n",
        "            }\n",
        "            if alibi_bias_applied is not None:\n",
        "                info[\"alibi_bias_applied\"] = alibi_bias_applied.detach().to(torch.float32)\n",
        "            if self.use_alibi_write and self.learn_alibi_strength:\n",
        "                info[\"alibi_strength\"] = self._alibi_strength(dtype=torch.float32, device=x.device).detach().cpu()\n",
        "            if self.use_slotspace_refine:\n",
        "                info[\"slotspace_gate\"] = self._slotspace_gate(dtype=torch.float32, device=x.device).detach().cpu()\n",
        "                info[\"use_rope_slotspace\"] = torch.tensor(bool(self.use_rope_slotspace))\n",
        "                if slotspace_delta_norm_mean is not None:\n",
        "                    info[\"slotspace_delta_norm\"] = slotspace_delta_norm_mean\n",
        "\n",
        "            info[\"read_logits\"] = read_logits.detach().to(torch.float32)\n",
        "            info[\"read_logits_key\"] = read_logits_key.detach().to(torch.float32)\n",
        "            if read_logits_content is not None:\n",
        "                info[\"read_logits_content\"] = read_logits_content.detach().to(torch.float32)\n",
        "            info[\"routing_mode\"] = routing_mode\n",
        "\n",
        "        return out, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WT6jflhoTyhN"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Addressed State Attention (training-focused efficient version) â€” ONLINE slotspace scan, single pass, chunkwise dropout\n",
        "# Drop-in replacement for your current \"training-focused efficient version\".\n",
        "# Key change:\n",
        "#   - Slotspace refine is computed ONLINE inside the same write/read chunk loop.\n",
        "#   - No [B,H,T,K] read_weights buffer, no [B,H,T,M] u2 buffer, no second streaming decode pass.\n",
        "#   - Slotspace dropout is applied chunkwise to u2_c (as requested).\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Optional, Dict, Tuple\n",
        "\n",
        "# -------------------------\n",
        "# RoPE helper (rotate-half)\n",
        "# -------------------------\n",
        "def _rotate_half(x: torch.Tensor) -> torch.Tensor:\n",
        "    x1 = x[..., ::2]\n",
        "    x2 = x[..., 1::2]\n",
        "    return torch.stack((-x2, x1), dim=-1).flatten(-2)\n",
        "\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    def __init__(self, dim: int, base: float = 10000.0):\n",
        "        super().__init__()\n",
        "        assert dim % 2 == 0, \"RoPE requires even dim\"\n",
        "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
        "        self._cos_cached = None\n",
        "        self._sin_cached = None\n",
        "        self._t_cached = None\n",
        "        self._device_cached = None\n",
        "\n",
        "    def get_cos_sin(self, T: int, device, dtype):\n",
        "        if (\n",
        "            self._t_cached == T\n",
        "            and self._cos_cached is not None\n",
        "            and self._device_cached == device\n",
        "        ):\n",
        "            return self._cos_cached.to(dtype=dtype), self._sin_cached.to(dtype=dtype)\n",
        "\n",
        "        t = torch.arange(T, device=device, dtype=self.inv_freq.dtype)\n",
        "        freqs = torch.einsum(\"t,f->tf\", t, self.inv_freq)  # [T, d/2]\n",
        "        emb = torch.cat([freqs, freqs], dim=-1)            # [T, d]\n",
        "        cos = emb.cos()[None, None, :, :]                  # [1,1,T,d]\n",
        "        sin = emb.sin()[None, None, :, :]                  # [1,1,T,d]\n",
        "\n",
        "        self._t_cached = T\n",
        "        self._device_cached = device\n",
        "        self._cos_cached = cos\n",
        "        self._sin_cached = sin\n",
        "        return cos.to(dtype=dtype), sin.to(dtype=dtype)\n",
        "\n",
        "def apply_rope(x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> torch.Tensor:\n",
        "    return (x * cos) + (_rotate_half(x) * sin)\n",
        "\n",
        "# -------------------------\n",
        "# ALiBi slopes helper\n",
        "# -------------------------\n",
        "def alibi_slopes(num_heads: int, device=None, dtype=torch.float32) -> torch.Tensor:\n",
        "    def get_slopes(n):\n",
        "        def power_of_2_slopes(n):\n",
        "            start = 2.0 ** (-(2.0 ** -(math.log2(n) - 3)))\n",
        "            ratio = start\n",
        "            return [start * (ratio ** i) for i in range(n)]\n",
        "        if math.log2(n).is_integer():\n",
        "            return power_of_2_slopes(n)\n",
        "        closest = 2 ** math.floor(math.log2(n))\n",
        "        return power_of_2_slopes(closest) + get_slopes(2 * closest)[0::2][: n - closest]\n",
        "    return torch.tensor(get_slopes(num_heads), device=device, dtype=dtype)  # [H]\n",
        "\n",
        "def _inv_softplus(y: torch.Tensor) -> torch.Tensor:\n",
        "    return torch.log(torch.expm1(y))\n",
        "\n",
        "def phi(x: torch.Tensor) -> torch.Tensor:\n",
        "    return F.elu(x) + 1.0\n",
        "\n",
        "\n",
        "class AddressedStateAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Training-focused ASA (ONLINE slotspace scan, single pass):\n",
        "      - streaming write + read (no [B,H,K,T,d])\n",
        "      - optional slotspace refine computed ONLINE inside the same chunk loop:\n",
        "          * causal linear-attn scan state (S_state, Z_state) carried across chunks\n",
        "          * decode slot_w_c for the chunk only, apply via SAME slot_state_t (no second pass)\n",
        "      - minimal allocations by default\n",
        "      - optional lightweight stats for monitoring\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int,\n",
        "        num_heads: int = 8,\n",
        "        num_slots: int = 8,\n",
        "        dropout: float = 0.1,\n",
        "\n",
        "        # temperatures / numerics\n",
        "        read_temperature: float = 1.0,\n",
        "        write_temperature: float = 1.0,\n",
        "        state_fp32: bool = True,\n",
        "        slot_dropout: float = 0.0,\n",
        "        normalize_k: bool = False,\n",
        "\n",
        "        # positions (write geometry)\n",
        "        use_rope_keys: bool = True,\n",
        "        rope_base: float = 10000.0,\n",
        "\n",
        "        # write bias (ALiBi)\n",
        "        use_alibi_write: bool = True,\n",
        "        alibi_strength_init: float = 0.1,\n",
        "        learn_alibi_strength: bool = True,\n",
        "        min_strength: float = 0.0,\n",
        "\n",
        "        # content-conditioned read term\n",
        "        use_content_read: bool = True,\n",
        "        content_read_init: float = -4.0,\n",
        "        content_read_max_gamma: float = 3.0,\n",
        "\n",
        "        # optional slot-space refinement (2nd order primary path)\n",
        "        use_slotspace_refine: bool = True,\n",
        "        slotspace_dim: int = 32,\n",
        "        slotspace_gate_init: float = -4.0,\n",
        "        slotspace_dropout: float = 0.05,\n",
        "        slotspace_signed_weights: bool = True,\n",
        "\n",
        "        # RoPE in slot-space matcher (Q/K only)\n",
        "        use_rope_slotspace: bool = True,\n",
        "        rope_base_slotspace: float = 100000.0,\n",
        "\n",
        "        # perf knobs\n",
        "        write_chunk_size: int = 128,\n",
        "        enable_compiled: bool = False,\n",
        "\n",
        "        # training diag knobs\n",
        "        return_light_stats_default: bool = False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.num_slots = num_slots\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.read_temperature = float(read_temperature)\n",
        "        self.write_temperature = float(write_temperature)\n",
        "        self.state_fp32 = bool(state_fp32)\n",
        "        self.slot_dropout = float(slot_dropout)\n",
        "        self.normalize_k = bool(normalize_k)\n",
        "\n",
        "        self.use_rope_keys = bool(use_rope_keys)\n",
        "        self.use_alibi_write = bool(use_alibi_write)\n",
        "        self.learn_alibi_strength = bool(learn_alibi_strength)\n",
        "        self.min_strength = float(min_strength)\n",
        "\n",
        "        self.use_content_read = bool(use_content_read)\n",
        "        self.content_read_max_gamma = float(content_read_max_gamma)\n",
        "\n",
        "        self.use_slotspace_refine = bool(use_slotspace_refine)\n",
        "        self.slotspace_dim = int(slotspace_dim)\n",
        "        self.slotspace_dropout = nn.Dropout(float(slotspace_dropout))\n",
        "        self.slotspace_signed_weights = bool(slotspace_signed_weights)\n",
        "\n",
        "        self.use_rope_slotspace = bool(use_rope_slotspace) and bool(self.use_slotspace_refine)\n",
        "\n",
        "        self.write_chunk_size = int(write_chunk_size)\n",
        "        self.return_light_stats_default = bool(return_light_stats_default)\n",
        "\n",
        "        H, K, d = self.num_heads, self.num_slots, self.head_dim\n",
        "\n",
        "        # Learned slot keys per head: [H,K,d]\n",
        "        self.slot_keys = nn.Parameter(torch.randn(H, K, d) / math.sqrt(d))\n",
        "\n",
        "        # Projections\n",
        "        self.Wk_write = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.Wv_write = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.Wq_read  = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "\n",
        "        # RoPE (write geometry)\n",
        "        self.rope = RotaryEmbedding(d, base=rope_base) if self.use_rope_keys else None\n",
        "\n",
        "        # ALiBi slopes\n",
        "        if self.use_alibi_write:\n",
        "            self.register_buffer(\"_alibi_slopes\", alibi_slopes(H), persistent=False)  # [H]\n",
        "        else:\n",
        "            self.register_buffer(\"_alibi_slopes\", torch.zeros(H), persistent=False)\n",
        "\n",
        "        if self.use_alibi_write and self.learn_alibi_strength:\n",
        "            init = torch.tensor(float(alibi_strength_init) - self.min_strength).clamp_min(1e-8)\n",
        "            self._alibi_strength_param = nn.Parameter(_inv_softplus(init))\n",
        "        else:\n",
        "            self._alibi_strength_param = None\n",
        "            self.alibi_strength = float(alibi_strength_init)\n",
        "\n",
        "        # Content read gamma\n",
        "        if self.use_content_read:\n",
        "            self._content_read_gamma_raw = nn.Parameter(torch.tensor(float(content_read_init)))\n",
        "        else:\n",
        "            self._content_read_gamma_raw = None\n",
        "\n",
        "        # Slotspace refine stack\n",
        "        if self.use_slotspace_refine:\n",
        "            self.slot_in  = nn.Linear(K, self.slotspace_dim, bias=False)\n",
        "            self.slot_q   = nn.Linear(self.slotspace_dim, self.slotspace_dim, bias=False)\n",
        "            self.slot_k   = nn.Linear(self.slotspace_dim, self.slotspace_dim, bias=False)\n",
        "            self.slot_v   = nn.Linear(self.slotspace_dim, self.slotspace_dim, bias=False)\n",
        "            self.slot_out = nn.Linear(self.slotspace_dim, K, bias=False)\n",
        "            self._slotspace_gate_raw = nn.Parameter(torch.tensor(float(slotspace_gate_init)))\n",
        "\n",
        "            if self.use_rope_slotspace:\n",
        "                assert (self.slotspace_dim % 2) == 0\n",
        "                self.rope_slotspace = RotaryEmbedding(self.slotspace_dim, base=float(rope_base_slotspace))\n",
        "            else:\n",
        "                self.rope_slotspace = None\n",
        "        else:\n",
        "            self.slot_in = None\n",
        "            self.slot_q = self.slot_k = self.slot_v = None\n",
        "            self.slot_out = None\n",
        "            self._slotspace_gate_raw = None\n",
        "            self.rope_slotspace = None\n",
        "\n",
        "        # Compile only the inner chunk kernel if desired.\n",
        "        self._compiled = None\n",
        "        if enable_compiled:\n",
        "            self.enable_compiled_kernel()\n",
        "\n",
        "    def enable_compiled_kernel(self):\n",
        "        if self._compiled is None:\n",
        "            self._compiled = torch.compile(self._write_read_chunk, fullgraph=False)\n",
        "\n",
        "    def _alibi_strength(self, dtype, device) -> torch.Tensor:\n",
        "        if not (self.use_alibi_write and self.learn_alibi_strength):\n",
        "            return torch.tensor(getattr(self, \"alibi_strength\", 0.0), dtype=dtype, device=device)\n",
        "        return (F.softplus(self._alibi_strength_param) + self.min_strength).to(dtype=dtype, device=device)\n",
        "\n",
        "    def _content_read_gamma(self, dtype, device) -> torch.Tensor:\n",
        "        if not self.use_content_read:\n",
        "            return torch.tensor(0.0, dtype=dtype, device=device)\n",
        "        g = F.softplus(self._content_read_gamma_raw)\n",
        "        if self.content_read_max_gamma is not None and self.content_read_max_gamma > 0:\n",
        "            g = g.clamp(max=self.content_read_max_gamma)\n",
        "        return g.to(dtype=dtype, device=device)\n",
        "\n",
        "    def _slotspace_gate(self, dtype, device) -> torch.Tensor:\n",
        "        if not self.use_slotspace_refine:\n",
        "            return torch.tensor(0.0, dtype=dtype, device=device)\n",
        "        return F.softplus(self._slotspace_gate_raw).to(dtype=dtype, device=device)\n",
        "\n",
        "    @staticmethod\n",
        "    def _safe_exp_sub_max(s: torch.Tensor, m: torch.Tensor) -> torch.Tensor:\n",
        "        diff = s - m\n",
        "        diff = diff.masked_fill(~torch.isfinite(m), float(\"-inf\"))\n",
        "        return torch.exp(diff)\n",
        "\n",
        "    def _write_read_chunk(\n",
        "        self,\n",
        "        wlog_c: torch.Tensor,       # [B,H,K,L] state_dtype\n",
        "        v_c: torch.Tensor,          # [B,H,L,d] state_dtype\n",
        "        q_read_c: torch.Tensor,     # [B,H,L,d] q dtype\n",
        "        slot_keys: torch.Tensor,    # [H,K,d]\n",
        "        content_gamma: torch.Tensor,# scalar tensor (q dtype)\n",
        "        rtemp: float,\n",
        "        m_state: torch.Tensor,      # [B,H,K]\n",
        "        denom_state: torch.Tensor,  # [B,H,K]\n",
        "        numer_state: torch.Tensor,  # [B,H,K,d]\n",
        "    ) -> Tuple[\n",
        "        torch.Tensor,  # out_base_c [B,H,L,d] (state_dtype)\n",
        "        torch.Tensor,  # read_w_c   [B,H,L,K] (q dtype)\n",
        "        torch.Tensor,  # slot_state_t [B,H,L,K,d] (state_dtype)  <-- needed for online 2nd order\n",
        "        torch.Tensor,  # m_state_new\n",
        "        torch.Tensor,  # denom_state_new\n",
        "        torch.Tensor,  # numer_state_new\n",
        "    ]:\n",
        "        B, H, K, L = wlog_c.shape\n",
        "        d = numer_state.shape[-1]\n",
        "        state_dtype = numer_state.dtype\n",
        "\n",
        "        # prefix-softmax streaming within chunk\n",
        "        m_c, _ = torch.cummax(wlog_c, dim=-1)              # [B,H,K,L]\n",
        "        m_new = torch.maximum(m_state.unsqueeze(-1), m_c)  # [B,H,K,L]\n",
        "        scale = torch.exp(m_state.unsqueeze(-1) - m_new)   # [B,H,K,L]\n",
        "\n",
        "        denom_c = denom_state.unsqueeze(-1) * scale\n",
        "        numer_c = numer_state.unsqueeze(-2) * scale.unsqueeze(-1)\n",
        "\n",
        "        w_new = self._safe_exp_sub_max(wlog_c, m_new)\n",
        "        denom_c = denom_c + torch.cumsum(w_new, dim=-1)\n",
        "\n",
        "        add = torch.cumsum(w_new.unsqueeze(-1) * v_c.unsqueeze(2), dim=-2)\n",
        "        numer_c = numer_c + add\n",
        "\n",
        "        slot_state_c = numer_c / denom_c.clamp_min(1e-8).unsqueeze(-1)      # [B,H,K,L,d]\n",
        "        slot_state_t = slot_state_c.permute(0, 1, 3, 2, 4).contiguous()     # [B,H,L,K,d]\n",
        "\n",
        "        # routing\n",
        "        read_logits_key = torch.einsum(\"bhld,hkd->bhlk\", q_read_c, slot_keys) / math.sqrt(d)\n",
        "        if self.use_content_read:\n",
        "            read_logits_content = torch.einsum(\"bhld,bhlkd->bhlk\", q_read_c, slot_state_t.to(q_read_c.dtype)) / math.sqrt(d)\n",
        "            read_logits = read_logits_key + content_gamma.to(read_logits_key.dtype) * read_logits_content\n",
        "        else:\n",
        "            read_logits = read_logits_key\n",
        "\n",
        "        read_w_c = torch.softmax(read_logits / rtemp, dim=-1)\n",
        "\n",
        "        # base output\n",
        "        out_base_c = torch.einsum(\"bhlk,bhlkd->bhld\", read_w_c.to(state_dtype), slot_state_t)\n",
        "\n",
        "        # update prefix state\n",
        "        m_state_new = m_new[:, :, :, -1]\n",
        "        denom_state_new = denom_c[:, :, :, -1]\n",
        "        numer_state_new = numer_c[:, :, :, -1, :]\n",
        "\n",
        "        return out_base_c, read_w_c, slot_state_t, m_state_new, denom_state_new, numer_state_new\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        return_info: bool = False,\n",
        "        return_light_stats: Optional[bool] = None,\n",
        "    ) -> Tuple[torch.Tensor, Optional[Dict[str, torch.Tensor]]]:\n",
        "\n",
        "        if return_light_stats is None:\n",
        "            return_light_stats = self.return_light_stats_default\n",
        "\n",
        "        B, T, C = x.shape\n",
        "        H, K, d = self.num_heads, self.num_slots, self.head_dim\n",
        "\n",
        "        # projections\n",
        "        k_write = self.Wk_write(x).view(B, T, H, d).transpose(1, 2)  # [B,H,T,d]\n",
        "        v_write = self.Wv_write(x).view(B, T, H, d).transpose(1, 2)  # [B,H,T,d]\n",
        "        q_read  = self.Wq_read(x).view(B, T, H, d).transpose(1, 2)   # [B,H,T,d]\n",
        "\n",
        "        if self.normalize_k:\n",
        "            k_write = F.normalize(k_write, dim=-1, eps=1e-8)\n",
        "\n",
        "        if self.use_rope_keys:\n",
        "            cos, sin = self.rope.get_cos_sin(T, device=x.device, dtype=k_write.dtype)\n",
        "            k_write = apply_rope(k_write, cos, sin)\n",
        "\n",
        "        # slot dropout\n",
        "        slot_keys = self.slot_keys\n",
        "        if self.training and self.slot_dropout > 0.0:\n",
        "            drop = (torch.rand((H, K), device=x.device) < self.slot_dropout)\n",
        "            slot_keys = slot_keys * (~drop).to(slot_keys.dtype).unsqueeze(-1)\n",
        "\n",
        "        # write logits [B,H,K,T]\n",
        "        write_logits_raw = torch.einsum(\"hkd,bhtd->bhkt\", slot_keys, k_write) / math.sqrt(d)\n",
        "\n",
        "        state_dtype = torch.float32 if (self.state_fp32 and x.dtype != torch.float32) else x.dtype\n",
        "        write_logits = write_logits_raw.to(state_dtype)\n",
        "\n",
        "        wtemp = max(1e-6, self.write_temperature)\n",
        "        write_logits = write_logits / wtemp\n",
        "\n",
        "        if self.use_alibi_write:\n",
        "            strength = self._alibi_strength(dtype=state_dtype, device=x.device)\n",
        "            slopes = self._alibi_slopes.to(device=x.device, dtype=state_dtype) * strength\n",
        "            pos = torch.arange(T, device=x.device, dtype=state_dtype)\n",
        "            write_logits = write_logits + slopes.view(1, H, 1, 1) * pos.view(1, 1, 1, T)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            valid = attention_mask.to(dtype=torch.bool)\n",
        "            write_logits = write_logits.masked_fill(~valid.view(B, 1, 1, T), float(\"-inf\"))\n",
        "        else:\n",
        "            valid = None\n",
        "\n",
        "        content_gamma = self._content_read_gamma(dtype=q_read.dtype, device=x.device)\n",
        "        rtemp = max(1e-6, self.read_temperature)\n",
        "\n",
        "        out_h = torch.empty((B, H, T, d), device=x.device, dtype=state_dtype)\n",
        "\n",
        "        # write prefix state\n",
        "        denom_state = torch.zeros((B, H, K), device=x.device, dtype=state_dtype)\n",
        "        numer_state = torch.zeros((B, H, K, d), device=x.device, dtype=state_dtype)\n",
        "        m_state = torch.full((B, H, K), float(\"-inf\"), device=x.device, dtype=state_dtype)\n",
        "\n",
        "        # slotspace prefix scan state (ONLINE)\n",
        "        if self.use_slotspace_refine:\n",
        "            M = self.slotspace_dim\n",
        "            slotspace_dtype = state_dtype\n",
        "            S_state = torch.zeros((B, H, M, M), device=x.device, dtype=slotspace_dtype)\n",
        "            Z_state = torch.zeros((B, H, M), device=x.device, dtype=slotspace_dtype)\n",
        "            gate = self._slotspace_gate(dtype=state_dtype, device=x.device).to(state_dtype)\n",
        "\n",
        "            # precompute RoPE cos/sin for slotspace once, slice per chunk (cheap)\n",
        "            if self.use_rope_slotspace:\n",
        "                cos_s_full, sin_s_full = self.rope_slotspace.get_cos_sin(T, device=x.device, dtype=slotspace_dtype)\n",
        "            else:\n",
        "                cos_s_full = sin_s_full = None\n",
        "\n",
        "            if return_info and return_light_stats:\n",
        "                delta_norm_sum = torch.zeros((), device=x.device, dtype=torch.float32)\n",
        "                delta_norm_count = 0\n",
        "\n",
        "        if return_info and return_light_stats:\n",
        "            entropy_sum = torch.zeros((), device=x.device, dtype=torch.float32)\n",
        "            top1_sum = torch.zeros((), device=x.device, dtype=torch.float32)\n",
        "            stat_count = 0\n",
        "\n",
        "        WRITE_CHUNK = self.write_chunk_size\n",
        "        kernel = self._compiled if self._compiled is not None else self._write_read_chunk\n",
        "\n",
        "        for t0 in range(0, T, WRITE_CHUNK):\n",
        "            t1 = min(T, t0 + WRITE_CHUNK)\n",
        "            L = t1 - t0\n",
        "\n",
        "            wlog_c = write_logits[:, :, :, t0:t1]                    # [B,H,K,L]\n",
        "            v_c    = v_write[:, :, t0:t1, :].to(state_dtype)         # [B,H,L,d]\n",
        "            q_c    = q_read[:, :, t0:t1, :]                          # [B,H,L,d]\n",
        "\n",
        "            out_base_c, rw_c, slot_state_t, m_state, denom_state, numer_state = kernel(\n",
        "                wlog_c, v_c, q_c, slot_keys, content_gamma, rtemp,\n",
        "                m_state, denom_state, numer_state\n",
        "            )\n",
        "\n",
        "            out_c = out_base_c\n",
        "\n",
        "            # -------------------------\n",
        "            # ONLINE slotspace refine: scan update + decode + apply on THIS chunk's slot_state_t\n",
        "            # -------------------------\n",
        "            if self.use_slotspace_refine:\n",
        "                # encode read weights into slotspace coords\n",
        "                u_c = self.slot_in(rw_c.to(slotspace_dtype))      # [B,H,L,M]\n",
        "                q_s = self.slot_q(u_c)\n",
        "                k_s = self.slot_k(u_c)\n",
        "                v_s = self.slot_v(u_c)\n",
        "\n",
        "                # slotspace RoPE: slice [t0:t1] so positions remain absolute\n",
        "                if self.use_rope_slotspace:\n",
        "                    cos_s = cos_s_full[:, :, t0:t1, :]            # [1,1,L,M]\n",
        "                    sin_s = sin_s_full[:, :, t0:t1, :]\n",
        "                    q_s = apply_rope(q_s, cos_s, sin_s)\n",
        "                    k_s = apply_rope(k_s, cos_s, sin_s)\n",
        "\n",
        "                # optional mask\n",
        "                if valid is not None:\n",
        "                    mask_c = valid[:, t0:t1].view(B, 1, L, 1).to(slotspace_dtype)\n",
        "                    q_s = q_s * mask_c\n",
        "                    k_s = k_s * mask_c\n",
        "                    v_s = v_s * mask_c\n",
        "\n",
        "                qf_c = phi(q_s)\n",
        "                kf_c = phi(k_s)\n",
        "\n",
        "                # online prefix scan within chunk\n",
        "                kv_c = torch.einsum(\"bhlm,bhln->bhlmn\", kf_c, v_s.to(slotspace_dtype))   # [B,H,L,M,M]\n",
        "                S_c = torch.cumsum(kv_c, dim=2) + S_state.unsqueeze(2)                  # [B,H,L,M,M]\n",
        "                Z_c = torch.cumsum(kf_c, dim=2) + Z_state.unsqueeze(2)                  # [B,H,L,M]\n",
        "                Z_c = Z_c.clamp_min(1e-8)\n",
        "\n",
        "                num = torch.einsum(\"bhlm,bhlmn->bhln\", qf_c, S_c)                        # [B,H,L,M]\n",
        "                den = torch.einsum(\"bhlm,bhlm->bhl\", qf_c, Z_c).unsqueeze(-1)            # [B,H,L,1]\n",
        "                u2_c = num / den.clamp_min(1e-8)\n",
        "\n",
        "                # carry prefix state forward\n",
        "                S_state = S_c[:, :, -1, :, :]\n",
        "                Z_state = Z_c[:, :, -1, :]\n",
        "\n",
        "                # chunkwise dropout (requested)\n",
        "                u2_c = self.slotspace_dropout(u2_c)\n",
        "\n",
        "                # decode weights for this chunk only\n",
        "                slot_w_c = self.slot_out(u2_c)                                          # [B,H,L,K]\n",
        "                if self.slotspace_signed_weights:\n",
        "                    slot_w_c = torch.tanh(slot_w_c)\n",
        "                else:\n",
        "                    slot_w_c = torch.softmax(slot_w_c, dim=-1)\n",
        "\n",
        "                # second-order delta through *current* prefix slot state for this chunk\n",
        "                delta_c = torch.einsum(\n",
        "                    \"bhlk,bhlkd->bhld\",\n",
        "                    slot_w_c.to(state_dtype),\n",
        "                    slot_state_t,   # already state_dtype\n",
        "                )\n",
        "\n",
        "                out_c = out_c + gate * delta_c\n",
        "\n",
        "                if return_info and return_light_stats:\n",
        "                    delta_norm_sum += delta_c.detach().to(torch.float32).norm(dim=-1).sum()\n",
        "                    delta_norm_count += (B * H * L)\n",
        "\n",
        "            out_h[:, :, t0:t1, :] = out_c\n",
        "\n",
        "            if return_info and return_light_stats:\n",
        "                p = rw_c.clamp_min(1e-8)\n",
        "                ent = -(p * p.log()).sum(dim=-1).mean()\n",
        "                top = rw_c.argmax(dim=-1).reshape(-1)\n",
        "                hist = torch.bincount(top, minlength=K).float()\n",
        "                top1 = (hist.max() / hist.sum().clamp_min(1.0))\n",
        "                entropy_sum += ent.detach().to(torch.float32)\n",
        "                top1_sum += top1.detach().to(torch.float32)\n",
        "                stat_count += 1\n",
        "\n",
        "        # finish projection\n",
        "        out = out_h.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        out = self.out_proj(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        info = None\n",
        "        if return_info:\n",
        "            info = {}\n",
        "            if return_light_stats:\n",
        "                info[\"alibi_strength_mean\"] = self._alibi_strength(dtype=torch.float32, device=x.device).detach().cpu()\n",
        "                info[\"content_read_gamma_mean\"] = self._content_read_gamma(dtype=torch.float32, device=x.device).detach().cpu()\n",
        "                info[\"entropy_mean\"] = (entropy_sum / max(1, stat_count)).detach().cpu()\n",
        "                info[\"top1freq_mean\"] = (top1_sum / max(1, stat_count)).detach().cpu()\n",
        "                if self.use_slotspace_refine:\n",
        "                    info[\"slotspace_gate_mean\"] = self._slotspace_gate(dtype=torch.float32, device=x.device).detach().cpu()\n",
        "                    if \"delta_norm_sum\" in locals():\n",
        "                        info[\"slotspace_delta_norm\"] = (delta_norm_sum / max(1, delta_norm_count)).detach().cpu()\n",
        "            # heavy tensors intentionally omitted\n",
        "\n",
        "        return out, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RIsilgCRTpdP"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title LM and Config defs\n",
        "# ============================================================================\n",
        "# Addressed State Models (ASM): Config + Block + LM\n",
        "# - Naming aligned with paper: slots, read/write, slot-space refinement\n",
        "# - No compatibility layer (fresh public tooling)\n",
        "# - Assumes AddressedStateAttention is defined elsewhere (the primitive module)\n",
        "# ============================================================================\n",
        "\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Config\n",
        "# ============================================================================\n",
        "@dataclass\n",
        "class ASMTrainConfig:\n",
        "    # Data\n",
        "    dataset_name: str = \"wikitext\"\n",
        "    dataset_config: str = \"wikitext-103-raw-v1\"\n",
        "    tokenizer_name: str = \"gpt2\"\n",
        "\n",
        "    max_seq_len: int = 256\n",
        "    stride_frac_val: float = 0.50\n",
        "    seed: int = 1337\n",
        "\n",
        "    # Sample budgets\n",
        "    train_samples_target: int = 100_000_000\n",
        "    val_samples_target: int = 25_000\n",
        "\n",
        "    # Training\n",
        "    batch_size: int = 64\n",
        "    learning_rate: float = 3e-4\n",
        "    weight_decay: float = 0.01\n",
        "    betas: Tuple[float, float] = (0.9, 0.95)\n",
        "    grad_clip: float = 1.0\n",
        "    warmup_steps: int = 1_000\n",
        "    total_steps: int = 75_000\n",
        "    eval_interval: int = 1_000\n",
        "    log_interval: int = 100\n",
        "\n",
        "    # Model\n",
        "    vocab_size: int = 50257\n",
        "    embed_dim: int = 384\n",
        "    num_layers: int = 23\n",
        "    num_heads: int = 8\n",
        "    num_slots: int = 32\n",
        "    mlp_ratio: float = 4.0\n",
        "    dropout: float = 0.1\n",
        "    tie_weights: bool = True\n",
        "\n",
        "    # Addressed State Attention (ASA) / numerics\n",
        "    read_temperature: float = 1.0\n",
        "    write_temperature: float = 1.0\n",
        "    slot_dropout: float = 0.05\n",
        "    state_fp32: bool = True\n",
        "    normalize_k: bool = False\n",
        "\n",
        "    # Positions\n",
        "    use_abs_pos: bool = False\n",
        "    use_rope_keys: bool = True\n",
        "    rope_base: float = 10000.0\n",
        "    use_alibi_write: bool = True\n",
        "    alibi_strength_init: float = 0.1\n",
        "    learn_alibi_strength: bool = True\n",
        "    min_strength: float = 0.0\n",
        "\n",
        "    # Content-conditioned read term (gamma)\n",
        "    use_content_read: bool = True\n",
        "    content_read_init: float = -4.0\n",
        "    content_read_max_gamma: float = 3.0\n",
        "\n",
        "    # Optional slot-space refinement (formerly \"k-space\")\n",
        "    use_slotspace_refine: bool = True\n",
        "    slotspace_dim: int = 64\n",
        "    slotspace_gate_init: float = -4.0\n",
        "    slotspace_dropout: float = 0.05\n",
        "    slotspace_signed_weights: bool = True\n",
        "\n",
        "    # RoPE inside slot-space matcher (Q/K only)\n",
        "    use_rope_slotspace: bool = True\n",
        "    rope_base_slotspace: float = 100000.0\n",
        "\n",
        "    # Perf knobs (behavior-identical)\n",
        "    write_chunk_size: int = 128\n",
        "    slotspace_chunk_size: int = 128\n",
        "    enable_compiled: bool = False\n",
        "\n",
        "    # Analytics\n",
        "    eval_max_batches: int = 150\n",
        "    analytics_last_k: int = 4\n",
        "\n",
        "    # IO / caches\n",
        "    output_dir: str = \"./drive/MyDrive/asm_outputs\"\n",
        "    tag: str = \"asm_wikitext\"\n",
        "    cache_dir: str = \"./drive/MyDrive/asm_caches\"\n",
        "    val_windows_cache: str = \"./drive/MyDrive/asm_val_cache_windows_1024.pkl\"\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Block\n",
        "# ============================================================================\n",
        "class ASMBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int,\n",
        "        num_heads: int,\n",
        "        num_slots: int,\n",
        "        mlp_ratio: float = 4.0,\n",
        "        dropout: float = 0.1,\n",
        "\n",
        "        # temperatures / numerics\n",
        "        read_temperature: float = 1.0,\n",
        "        write_temperature: float = 1.0,\n",
        "        state_fp32: bool = True,\n",
        "        slot_dropout: float = 0.0,\n",
        "        normalize_k: bool = False,\n",
        "\n",
        "        # positions\n",
        "        use_rope_keys: bool = True,\n",
        "        rope_base: float = 10000.0,\n",
        "        use_alibi_write: bool = True,\n",
        "\n",
        "        # ALiBi params\n",
        "        alibi_strength_init: float = 0.1,\n",
        "        learn_alibi_strength: bool = True,\n",
        "        min_strength: float = 0.0,\n",
        "\n",
        "        # content-conditioned read (gamma)\n",
        "        use_content_read: bool = True,\n",
        "        content_read_init: float = -4.0,\n",
        "        content_read_max_gamma: float = 3.0,\n",
        "\n",
        "        # optional slot-space refinement\n",
        "        use_slotspace_refine: bool = True,\n",
        "        slotspace_dim: int = 32,\n",
        "        slotspace_gate_init: float = -10.0,\n",
        "        slotspace_dropout: float = 0.0,\n",
        "        slotspace_signed_weights: bool = True,\n",
        "\n",
        "        # RoPE inside slot-space matcher\n",
        "        use_rope_slotspace: bool = True,\n",
        "        rope_base_slotspace: float = 100000.0,\n",
        "\n",
        "        # chunk sizes\n",
        "        write_chunk_size: int = 128,\n",
        "        slotspace_chunk_size: int = 128,\n",
        "        enable_compiled: bool = False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        self.asa = AddressedStateAttention(\n",
        "            embed_dim=embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            num_slots=num_slots,\n",
        "            dropout=dropout,\n",
        "\n",
        "            read_temperature=read_temperature,\n",
        "            write_temperature=write_temperature,\n",
        "            state_fp32=state_fp32,\n",
        "            slot_dropout=slot_dropout,\n",
        "            normalize_k=normalize_k,\n",
        "\n",
        "            use_rope_keys=use_rope_keys,\n",
        "            rope_base=rope_base,\n",
        "            use_alibi_write=use_alibi_write,\n",
        "            alibi_strength_init=alibi_strength_init,\n",
        "            learn_alibi_strength=learn_alibi_strength,\n",
        "            min_strength=min_strength,\n",
        "\n",
        "            use_content_read=use_content_read,\n",
        "            content_read_init=content_read_init,\n",
        "            content_read_max_gamma=content_read_max_gamma,\n",
        "\n",
        "            use_slotspace_refine=use_slotspace_refine,\n",
        "            slotspace_dim=slotspace_dim,\n",
        "            slotspace_gate_init=slotspace_gate_init,\n",
        "            slotspace_dropout=slotspace_dropout,\n",
        "            slotspace_signed_weights=slotspace_signed_weights,\n",
        "\n",
        "            use_rope_slotspace=use_rope_slotspace,\n",
        "            rope_base_slotspace=rope_base_slotspace,\n",
        "\n",
        "            write_chunk_size=write_chunk_size,\n",
        "            slotspace_chunk_size=slotspace_chunk_size,\n",
        "            enable_compiled=enable_compiled,\n",
        "\n",
        "        )\n",
        "\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        hidden = int(embed_dim * mlp_ratio)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden, bias=False),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden, embed_dim, bias=False),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor, attention_mask: Optional[torch.Tensor] = None, return_info: bool = False):\n",
        "        a, info = self.asa(self.norm1(x), attention_mask=attention_mask, return_info=return_info)\n",
        "        x = x + a\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x, info\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# LM\n",
        "# ============================================================================\n",
        "class ASMLanguageModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        embed_dim: int = 384,\n",
        "        num_layers: int = 6,\n",
        "        num_heads: int = 8,\n",
        "        num_slots: int = 8,\n",
        "        max_seq_len: int = 1024,\n",
        "        mlp_ratio: float = 4.0,\n",
        "        dropout: float = 0.1,\n",
        "\n",
        "        # temperatures / numerics\n",
        "        read_temperature: float = 1.0,\n",
        "        write_temperature: float = 1.0,\n",
        "        state_fp32: bool = True,\n",
        "        slot_dropout: float = 0.05,\n",
        "        normalize_k: bool = False,\n",
        "\n",
        "        tie_weights: bool = True,\n",
        "\n",
        "        # LM-level abs pos\n",
        "        use_abs_pos: bool = False,\n",
        "\n",
        "        # positions\n",
        "        use_rope_keys: bool = True,\n",
        "        rope_base: float = 10000.0,\n",
        "        use_alibi_write: bool = True,\n",
        "\n",
        "        # ALiBi\n",
        "        alibi_strength_init: float = 0.1,\n",
        "        learn_alibi_strength: bool = True,\n",
        "        min_strength: float = 0.0,\n",
        "\n",
        "        # content-conditioned read (gamma)\n",
        "        use_content_read: bool = True,\n",
        "        content_read_init: float = -4.0,\n",
        "        content_read_max_gamma: float = 3.0,\n",
        "\n",
        "        # optional slot-space refinement\n",
        "        use_slotspace_refine: bool = True,\n",
        "        slotspace_dim: int = 32,\n",
        "        slotspace_gate_init: float = -10.0,\n",
        "        slotspace_dropout: float = 0.0,\n",
        "        slotspace_signed_weights: bool = True,\n",
        "\n",
        "        # RoPE inside slot-space matcher\n",
        "        use_rope_slotspace: bool = True,\n",
        "        rope_base_slotspace: float = 100000.0,\n",
        "\n",
        "        # chunk sizes\n",
        "        write_chunk_size: int = 128,\n",
        "        slotspace_chunk_size: int = 128,\n",
        "        enable_compile: bool = False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.use_abs_pos = bool(use_abs_pos)\n",
        "\n",
        "        self.tok = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos = nn.Embedding(max_seq_len, embed_dim) if self.use_abs_pos else None\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            ASMBlock(\n",
        "                embed_dim=embed_dim,\n",
        "                num_heads=num_heads,\n",
        "                num_slots=num_slots,\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                dropout=dropout,\n",
        "\n",
        "                read_temperature=read_temperature,\n",
        "                write_temperature=write_temperature,\n",
        "                state_fp32=state_fp32,\n",
        "                slot_dropout=slot_dropout,\n",
        "                normalize_k=normalize_k,\n",
        "\n",
        "                use_rope_keys=use_rope_keys,\n",
        "                rope_base=rope_base,\n",
        "                use_alibi_write=use_alibi_write,\n",
        "\n",
        "                alibi_strength_init=alibi_strength_init,\n",
        "                learn_alibi_strength=learn_alibi_strength,\n",
        "                min_strength=min_strength,\n",
        "\n",
        "                use_content_read=use_content_read,\n",
        "                content_read_init=content_read_init,\n",
        "                content_read_max_gamma=content_read_max_gamma,\n",
        "\n",
        "                use_slotspace_refine=use_slotspace_refine,\n",
        "                slotspace_dim=slotspace_dim,            slotspace_gate_init=slotspace_gate_init,\n",
        "                slotspace_dropout=slotspace_dropout,\n",
        "                slotspace_signed_weights=slotspace_signed_weights,\n",
        "                use_rope_slotspace=use_rope_slotspace,\n",
        "                rope_base_slotspace=rope_base_slotspace,\n",
        "\n",
        "                write_chunk_size=write_chunk_size,\n",
        "                slotspace_chunk_size=slotspace_chunk_size,\n",
        "                enable_compile=enable_compile,\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
        "        if tie_weights:\n",
        "            self.lm_head.weight = self.tok.weight\n",
        "\n",
        "        self.apply(self._init)\n",
        "\n",
        "    def _init(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.normal_(m.weight, std=0.02)\n",
        "        elif isinstance(m, nn.Embedding):\n",
        "            nn.init.normal_(m.weight, std=0.02)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.ones_(m.weight)\n",
        "            nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        return_info: bool = False,\n",
        "    ):\n",
        "        B, T = input_ids.shape\n",
        "        assert T <= self.max_seq_len, f\"T={T} exceeds max_seq_len={self.max_seq_len}\"\n",
        "\n",
        "        x = self.tok(input_ids)\n",
        "        if self.use_abs_pos:\n",
        "            pos = torch.arange(T, device=input_ids.device).unsqueeze(0).expand(B, -1)\n",
        "            x = x + self.pos(pos)\n",
        "\n",
        "        x = self.drop(x)\n",
        "\n",
        "        infos = []\n",
        "        for blk in self.blocks:\n",
        "            x, info = blk(x, attention_mask=attention_mask, return_info=return_info)\n",
        "            if return_info:\n",
        "                infos.append(info)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        logits = self.lm_head(x)\n",
        "        return (logits, infos) if return_info else logits\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Convenience: build model from config\n",
        "# ============================================================================\n",
        "def build_model_from_cfg(cfg: ASMTrainConfig) -> ASMLanguageModel:\n",
        "    return ASMLanguageModel(\n",
        "        vocab_size=cfg.vocab_size,\n",
        "        embed_dim=cfg.embed_dim,\n",
        "        num_layers=cfg.num_layers,\n",
        "        num_heads=cfg.num_heads,\n",
        "        num_slots=cfg.num_slots,\n",
        "        max_seq_len=cfg.max_seq_len,\n",
        "        mlp_ratio=cfg.mlp_ratio,\n",
        "        dropout=cfg.dropout,\n",
        "\n",
        "        read_temperature=cfg.read_temperature,\n",
        "        write_temperature=cfg.write_temperature,\n",
        "        state_fp32=cfg.state_fp32,\n",
        "        slot_dropout=cfg.slot_dropout,\n",
        "        normalize_k=cfg.normalize_k,\n",
        "\n",
        "        tie_weights=cfg.tie_weights,\n",
        "\n",
        "        use_abs_pos=cfg.use_abs_pos,\n",
        "        use_rope_keys=cfg.use_rope_keys,\n",
        "        rope_base=cfg.rope_base,\n",
        "        use_alibi_write=cfg.use_alibi_write,\n",
        "\n",
        "        alibi_strength_init=cfg.alibi_strength_init,\n",
        "        learn_alibi_strength=cfg.learn_alibi_strength,\n",
        "        min_strength=cfg.min_strength,\n",
        "\n",
        "        use_content_read=cfg.use_content_read,\n",
        "        content_read_init=cfg.content_read_init,\n",
        "        content_read_max_gamma=cfg.content_read_max_gamma,\n",
        "\n",
        "        use_slotspace_refine=cfg.use_slotspace_refine,\n",
        "        slotspace_dim=cfg.slotspace_dim,\n",
        "        slotspace_gate_init=cfg.slotspace_gate_init,\n",
        "        slotspace_dropout=cfg.slotspace_dropout,\n",
        "        slotspace_signed_weights=cfg.slotspace_signed_weights,\n",
        "        use_rope_slotspace=cfg.use_rope_slotspace,\n",
        "        rope_base_slotspace=cfg.rope_base_slotspace,\n",
        "\n",
        "        write_chunk_size=cfg.write_chunk_size,\n",
        "        slotspace_chunk_size=cfg.slotspace_chunk_size,\n",
        "        enable_compile=cfg.enable_compile,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-jwJA7lXULP8"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#@title LM and Config defs efficient\n",
        "# ============================================================================\n",
        "# Addressed State Models (ASM): Config + Block + LM\n",
        "# - Naming aligned with paper: slots, read/write, slot-space refinement\n",
        "# - No compatibility layer (fresh public tooling)\n",
        "# - Assumes AddressedStateAttention is defined elsewhere (the primitive module)\n",
        "# ============================================================================\n",
        "\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Config\n",
        "# ============================================================================\n",
        "@dataclass\n",
        "class ASMTrainConfig:\n",
        "    # Data\n",
        "    dataset_name: str = \"wikitext\"\n",
        "    dataset_config: str = \"wikitext-103-raw-v1\"\n",
        "    tokenizer_name: str = \"gpt2\"\n",
        "\n",
        "    max_seq_len: int = 256\n",
        "    stride_frac_val: float = 0.50\n",
        "    seed: int = 1337\n",
        "\n",
        "    # Sample budgets\n",
        "    train_samples_target: int = 100_000_000\n",
        "    val_samples_target: int = 25_000\n",
        "\n",
        "    # Training\n",
        "    batch_size: int = 64\n",
        "    learning_rate: float = 3e-4\n",
        "    weight_decay: float = 0.01\n",
        "    betas: Tuple[float, float] = (0.9, 0.95)\n",
        "    grad_clip: float = 1.0\n",
        "    warmup_steps: int = 1_000\n",
        "    total_steps: int = 75_000\n",
        "    eval_interval: int = 1_000\n",
        "    log_interval: int = 100\n",
        "\n",
        "    # Model\n",
        "    vocab_size: int = 50257\n",
        "    embed_dim: int = 384\n",
        "    num_layers: int = 23\n",
        "    num_heads: int = 8\n",
        "    num_slots: int = 32\n",
        "    mlp_ratio: float = 4.0\n",
        "    dropout: float = 0.1\n",
        "    tie_weights: bool = True\n",
        "\n",
        "    # Addressed State Attention (ASA) / numerics\n",
        "    read_temperature: float = 1.0\n",
        "    write_temperature: float = 1.0\n",
        "    slot_dropout: float = 0.05\n",
        "    state_fp32: bool = True\n",
        "    normalize_k: bool = False\n",
        "\n",
        "    # Positions\n",
        "    use_abs_pos: bool = False\n",
        "    use_rope_keys: bool = True\n",
        "    rope_base: float = 10000.0\n",
        "    use_alibi_write: bool = True\n",
        "    alibi_strength_init: float = 0.1\n",
        "    learn_alibi_strength: bool = True\n",
        "    min_strength: float = 0.0\n",
        "\n",
        "    # Content-conditioned read term (gamma)\n",
        "    use_content_read: bool = True\n",
        "    content_read_init: float = -4.0\n",
        "    content_read_max_gamma: float = 3.0\n",
        "\n",
        "    # Optional slot-space refinement (formerly \"k-space\")\n",
        "    use_slotspace_refine: bool = True\n",
        "    slotspace_dim: int = 64\n",
        "    slotspace_gate_init: float = -4.0\n",
        "    slotspace_dropout: float = 0.05\n",
        "    slotspace_signed_weights: bool = True\n",
        "\n",
        "    # RoPE inside slot-space matcher (Q/K only)\n",
        "    use_rope_slotspace: bool = True\n",
        "    rope_base_slotspace: float = 100000.0\n",
        "\n",
        "    # Perf knobs (behavior-identical)\n",
        "    write_chunk_size: int = 128\n",
        "    enable_compiled: bool = True\n",
        "\n",
        "    # Analytics\n",
        "    eval_max_batches: int = 150\n",
        "    analytics_last_k: int = 4\n",
        "\n",
        "    # IO / caches\n",
        "    output_dir: str = \"./drive/MyDrive/asm_outputs\"\n",
        "    tag: str = \"asm_wikitext\"\n",
        "    cache_dir: str = \"./drive/MyDrive/asm_caches\"\n",
        "    val_windows_cache: str = \"./drive/MyDrive/asm_val_cache_windows_1024.pkl\"\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Block\n",
        "# ============================================================================\n",
        "class ASMBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int,\n",
        "        num_heads: int,\n",
        "        num_slots: int,\n",
        "        mlp_ratio: float = 4.0,\n",
        "        dropout: float = 0.1,\n",
        "\n",
        "        # temperatures / numerics\n",
        "        read_temperature: float = 1.0,\n",
        "        write_temperature: float = 1.0,\n",
        "        state_fp32: bool = True,\n",
        "        slot_dropout: float = 0.0,\n",
        "        normalize_k: bool = False,\n",
        "\n",
        "        # positions\n",
        "        use_rope_keys: bool = True,\n",
        "        rope_base: float = 10000.0,\n",
        "        use_alibi_write: bool = True,\n",
        "\n",
        "        # ALiBi params\n",
        "        alibi_strength_init: float = 0.1,\n",
        "        learn_alibi_strength: bool = True,\n",
        "        min_strength: float = 0.0,\n",
        "\n",
        "        # content-conditioned read (gamma)\n",
        "        use_content_read: bool = True,\n",
        "        content_read_init: float = -4.0,\n",
        "        content_read_max_gamma: float = 3.0,\n",
        "\n",
        "        # optional slot-space refinement\n",
        "        use_slotspace_refine: bool = True,\n",
        "        slotspace_dim: int = 32,\n",
        "        slotspace_gate_init: float = -10.0,\n",
        "        slotspace_dropout: float = 0.0,\n",
        "        slotspace_signed_weights: bool = True,\n",
        "\n",
        "        # RoPE inside slot-space matcher\n",
        "        use_rope_slotspace: bool = True,\n",
        "        rope_base_slotspace: float = 100000.0,\n",
        "\n",
        "        # chunk sizes\n",
        "        write_chunk_size: int = 128,\n",
        "        enable_compiled: bool = False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        self.asa = AddressedStateAttention(\n",
        "            embed_dim=embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            num_slots=num_slots,\n",
        "            dropout=dropout,\n",
        "\n",
        "            read_temperature=read_temperature,\n",
        "            write_temperature=write_temperature,\n",
        "            state_fp32=state_fp32,\n",
        "            slot_dropout=slot_dropout,\n",
        "            normalize_k=normalize_k,\n",
        "\n",
        "            use_rope_keys=use_rope_keys,\n",
        "            rope_base=rope_base,\n",
        "            use_alibi_write=use_alibi_write,\n",
        "            alibi_strength_init=alibi_strength_init,\n",
        "            learn_alibi_strength=learn_alibi_strength,\n",
        "            min_strength=min_strength,\n",
        "\n",
        "            use_content_read=use_content_read,\n",
        "            content_read_init=content_read_init,\n",
        "            content_read_max_gamma=content_read_max_gamma,\n",
        "\n",
        "            use_slotspace_refine=use_slotspace_refine,\n",
        "            slotspace_dim=slotspace_dim,\n",
        "            slotspace_gate_init=slotspace_gate_init,\n",
        "            slotspace_dropout=slotspace_dropout,\n",
        "            slotspace_signed_weights=slotspace_signed_weights,\n",
        "\n",
        "            use_rope_slotspace=use_rope_slotspace,\n",
        "            rope_base_slotspace=rope_base_slotspace,\n",
        "\n",
        "            write_chunk_size=write_chunk_size,\n",
        "            enable_compiled=enable_compiled,\n",
        "\n",
        "        )\n",
        "\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        hidden = int(embed_dim * mlp_ratio)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden, bias=False),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden, embed_dim, bias=False),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor, attention_mask: Optional[torch.Tensor] = None, return_info: bool = False, return_light_stats: Optional[bool] = None):\n",
        "        a, info = self.asa(self.norm1(x), attention_mask=attention_mask, return_info=return_info, return_light_stats=return_light_stats)\n",
        "        x = x + a\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x, info\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# LM\n",
        "# ============================================================================\n",
        "class ASMLanguageModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        embed_dim: int = 384,\n",
        "        num_layers: int = 6,\n",
        "        num_heads: int = 8,\n",
        "        num_slots: int = 8,\n",
        "        max_seq_len: int = 1024,\n",
        "        mlp_ratio: float = 4.0,\n",
        "        dropout: float = 0.1,\n",
        "\n",
        "        # temperatures / numerics\n",
        "        read_temperature: float = 1.0,\n",
        "        write_temperature: float = 1.0,\n",
        "        state_fp32: bool = True,\n",
        "        slot_dropout: float = 0.05,\n",
        "        normalize_k: bool = False,\n",
        "\n",
        "        tie_weights: bool = True,\n",
        "\n",
        "        # LM-level abs pos\n",
        "        use_abs_pos: bool = False,\n",
        "\n",
        "        # positions\n",
        "        use_rope_keys: bool = True,\n",
        "        rope_base: float = 10000.0,\n",
        "        use_alibi_write: bool = True,\n",
        "\n",
        "        # ALiBi\n",
        "        alibi_strength_init: float = 0.1,\n",
        "        learn_alibi_strength: bool = True,\n",
        "        min_strength: float = 0.0,\n",
        "\n",
        "        # content-conditioned read (gamma)\n",
        "        use_content_read: bool = True,\n",
        "        content_read_init: float = -4.0,\n",
        "        content_read_max_gamma: float = 3.0,\n",
        "\n",
        "        # optional slot-space refinement\n",
        "        use_slotspace_refine: bool = True,\n",
        "        slotspace_dim: int = 32,\n",
        "        slotspace_gate_init: float = -10.0,\n",
        "        slotspace_dropout: float = 0.0,\n",
        "        slotspace_signed_weights: bool = True,\n",
        "\n",
        "        # RoPE inside slot-space matcher\n",
        "        use_rope_slotspace: bool = True,\n",
        "        rope_base_slotspace: float = 100000.0,\n",
        "\n",
        "        # chunk sizes\n",
        "        write_chunk_size: int = 128,\n",
        "        enable_compiled: bool = False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.use_abs_pos = bool(use_abs_pos)\n",
        "\n",
        "        self.tok = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos = nn.Embedding(max_seq_len, embed_dim) if self.use_abs_pos else None\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            ASMBlock(\n",
        "                embed_dim=embed_dim,\n",
        "                num_heads=num_heads,\n",
        "                num_slots=num_slots,\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                dropout=dropout,\n",
        "\n",
        "                read_temperature=read_temperature,\n",
        "                write_temperature=write_temperature,\n",
        "                state_fp32=state_fp32,\n",
        "                slot_dropout=slot_dropout,\n",
        "                normalize_k=normalize_k,\n",
        "\n",
        "                use_rope_keys=use_rope_keys,\n",
        "                rope_base=rope_base,\n",
        "                use_alibi_write=use_alibi_write,\n",
        "\n",
        "                alibi_strength_init=alibi_strength_init,\n",
        "                learn_alibi_strength=learn_alibi_strength,\n",
        "                min_strength=min_strength,\n",
        "\n",
        "                use_content_read=use_content_read,\n",
        "                content_read_init=content_read_init,\n",
        "                content_read_max_gamma=content_read_max_gamma,\n",
        "\n",
        "                use_slotspace_refine=use_slotspace_refine,\n",
        "                slotspace_dim=slotspace_dim,            slotspace_gate_init=slotspace_gate_init,\n",
        "                slotspace_dropout=slotspace_dropout,\n",
        "                slotspace_signed_weights=slotspace_signed_weights,\n",
        "                use_rope_slotspace=use_rope_slotspace,\n",
        "                rope_base_slotspace=rope_base_slotspace,\n",
        "\n",
        "                write_chunk_size=write_chunk_size,\n",
        "                enable_compiled=enable_compiled,\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
        "        if tie_weights:\n",
        "            self.lm_head.weight = self.tok.weight\n",
        "\n",
        "        self.apply(self._init)\n",
        "\n",
        "    def _init(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.normal_(m.weight, std=0.02)\n",
        "        elif isinstance(m, nn.Embedding):\n",
        "            nn.init.normal_(m.weight, std=0.02)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.ones_(m.weight)\n",
        "            nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        return_info: bool = False,\n",
        "        return_light_stats: Optional[bool] = None,\n",
        "    ):\n",
        "        B, T = input_ids.shape\n",
        "        assert T <= self.max_seq_len, f\"T={T} exceeds max_seq_len={self.max_seq_len}\"\n",
        "\n",
        "        x = self.tok(input_ids)\n",
        "        if self.use_abs_pos:\n",
        "            pos = torch.arange(T, device=input_ids.device).unsqueeze(0).expand(B, -1)\n",
        "            x = x + self.pos(pos)\n",
        "\n",
        "        x = self.drop(x)\n",
        "\n",
        "        infos = []\n",
        "        for blk in self.blocks:\n",
        "            x, info = blk(x, attention_mask=attention_mask, return_info=return_info, return_light_stats=return_light_stats)\n",
        "            if return_info:\n",
        "                infos.append(info)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        logits = self.lm_head(x)\n",
        "        return (logits, infos) if return_info else logits\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Convenience: build model from config\n",
        "# ============================================================================\n",
        "def build_model_from_cfg(cfg: ASMTrainConfig) -> ASMLanguageModel:\n",
        "    return ASMLanguageModel(\n",
        "        vocab_size=cfg.vocab_size,\n",
        "        embed_dim=cfg.embed_dim,\n",
        "        num_layers=cfg.num_layers,\n",
        "        num_heads=cfg.num_heads,\n",
        "        num_slots=cfg.num_slots,\n",
        "        max_seq_len=cfg.max_seq_len,\n",
        "        mlp_ratio=cfg.mlp_ratio,\n",
        "        dropout=cfg.dropout,\n",
        "\n",
        "        read_temperature=cfg.read_temperature,\n",
        "        write_temperature=cfg.write_temperature,\n",
        "        state_fp32=cfg.state_fp32,\n",
        "        slot_dropout=cfg.slot_dropout,\n",
        "        normalize_k=cfg.normalize_k,\n",
        "\n",
        "        tie_weights=cfg.tie_weights,\n",
        "\n",
        "        use_abs_pos=cfg.use_abs_pos,\n",
        "        use_rope_keys=cfg.use_rope_keys,\n",
        "        rope_base=cfg.rope_base,\n",
        "        use_alibi_write=cfg.use_alibi_write,\n",
        "\n",
        "        alibi_strength_init=cfg.alibi_strength_init,\n",
        "        learn_alibi_strength=cfg.learn_alibi_strength,\n",
        "        min_strength=cfg.min_strength,\n",
        "\n",
        "        use_content_read=cfg.use_content_read,\n",
        "        content_read_init=cfg.content_read_init,\n",
        "        content_read_max_gamma=cfg.content_read_max_gamma,\n",
        "\n",
        "        use_slotspace_refine=cfg.use_slotspace_refine,\n",
        "        slotspace_dim=cfg.slotspace_dim,\n",
        "        slotspace_gate_init=cfg.slotspace_gate_init,\n",
        "        slotspace_dropout=cfg.slotspace_dropout,\n",
        "        slotspace_signed_weights=cfg.slotspace_signed_weights,\n",
        "        use_rope_slotspace=cfg.use_rope_slotspace,\n",
        "        rope_base_slotspace=cfg.rope_base_slotspace,\n",
        "\n",
        "        write_chunk_size=cfg.write_chunk_size,\n",
        "        enable_compiled=cfg.enable_compiled,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KidrwBb7TpmU"
      },
      "outputs": [],
      "source": [
        "#@title Train, Eval Functions and Utilities\n",
        "\n",
        "import os, math, random, pickle, time\n",
        "from dataclasses import asdict\n",
        "from typing import Tuple, Dict, List, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, IterableDataset, DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Data: cached token streams + val windows + random-window train\n",
        "# =========================================================\n",
        "class StableValidationDataset(Dataset):\n",
        "    def __init__(self, samples: List[Tuple[torch.Tensor, torch.Tensor]]):\n",
        "        self.samples = samples\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx): return self.samples[idx]\n",
        "\n",
        "def _ensure_dir(path: str):\n",
        "    d = os.path.dirname(path)\n",
        "    if d:\n",
        "        os.makedirs(d, exist_ok=True)\n",
        "\n",
        "def build_or_load_token_stream(\n",
        "    *,\n",
        "    cache_path: str,\n",
        "    dataset_name: str,\n",
        "    dataset_config: str,\n",
        "    split: str,\n",
        "    tokenizer_name: str,\n",
        "    min_chars: int = 1,\n",
        "    add_eos_between_rows: bool = True,\n",
        "    max_rows: Optional[int] = None,\n",
        ") -> List[int]:\n",
        "    if os.path.exists(cache_path):\n",
        "        print(f\"Loading cached token stream: {cache_path}\")\n",
        "        with open(cache_path, \"rb\") as f:\n",
        "            stream = pickle.load(f)\n",
        "        print(f\"Loaded token stream tokens={len(stream):,}\")\n",
        "        return stream\n",
        "\n",
        "    print(f\"Building token stream for {dataset_name}/{dataset_config} split={split} ...\")\n",
        "    _ensure_dir(cache_path)\n",
        "    tok = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=True)\n",
        "    eos = tok.eos_token_id\n",
        "    assert eos is not None, \"Tokenizer must have eos_token_id\"\n",
        "\n",
        "    ds = load_dataset(dataset_name, dataset_config, split=split)\n",
        "\n",
        "    stream: List[int] = []\n",
        "    used = 0\n",
        "    for row in tqdm(ds, desc=f\"Tokenizing {split}\"):\n",
        "        if max_rows is not None and used >= max_rows:\n",
        "            break\n",
        "        text = (row.get(\"text\") or \"\").strip()\n",
        "        if len(text) < min_chars:\n",
        "            continue\n",
        "        ids = tok.encode(text, add_special_tokens=False)\n",
        "        if not ids:\n",
        "            continue\n",
        "        stream.extend(ids)\n",
        "        if add_eos_between_rows:\n",
        "            stream.append(eos)\n",
        "        used += 1\n",
        "\n",
        "    print(f\"Built stream: rows_used={used:,} tokens={len(stream):,}\")\n",
        "    with open(cache_path, \"wb\") as f:\n",
        "        pickle.dump(stream, f)\n",
        "    print(f\"Cached token stream to {cache_path}\")\n",
        "    return stream\n",
        "\n",
        "def build_or_load_validation_windows(\n",
        "    *,\n",
        "    cache_path: str,\n",
        "    token_stream: List[int],\n",
        "    max_seq_len: int,\n",
        "    stride_frac: float,\n",
        "    val_samples_target: int,\n",
        ") -> StableValidationDataset:\n",
        "    if os.path.exists(cache_path):\n",
        "        print(f\"Loading cached validation windows: {cache_path}\")\n",
        "        with open(cache_path, \"rb\") as f:\n",
        "            samples = pickle.load(f)\n",
        "        print(f\"Loaded val windows: {len(samples)}\")\n",
        "        return StableValidationDataset(samples)\n",
        "\n",
        "    print(\"Building validation windows (cached)...\")\n",
        "    _ensure_dir(cache_path)\n",
        "\n",
        "    T = int(max_seq_len)\n",
        "    stride = max(1, int(T * float(stride_frac)))\n",
        "    need = int(val_samples_target)\n",
        "\n",
        "    max_start = len(token_stream) - (T + 1)\n",
        "    if max_start <= 0:\n",
        "        raise ValueError(\"Validation token stream too small for max_seq_len+1\")\n",
        "\n",
        "    samples: List[Tuple[torch.Tensor, torch.Tensor]] = []\n",
        "    for start in tqdm(range(0, max_start + 1, stride), desc=\"Chunking val stream\"):\n",
        "        chunk = token_stream[start:start + T + 1]\n",
        "        if len(chunk) < T + 1:\n",
        "            break\n",
        "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
        "        samples.append((x, y))\n",
        "        if len(samples) >= need:\n",
        "            break\n",
        "\n",
        "    print(f\"Built val windows={len(samples)} (stride={stride}, stream_tokens={len(token_stream):,})\")\n",
        "    with open(cache_path, \"wb\") as f:\n",
        "        pickle.dump(samples, f)\n",
        "    print(f\"Cached validation windows to {cache_path}\")\n",
        "    return StableValidationDataset(samples)\n",
        "\n",
        "class WikiTextRandomWindowStream(IterableDataset):\n",
        "    def __init__(self, token_stream: List[int], max_seq_len: int, train_samples_target: int, seed: int):\n",
        "        super().__init__()\n",
        "        self.stream = token_stream\n",
        "        self.T = int(max_seq_len)\n",
        "        self.target = int(train_samples_target)\n",
        "        self.seed = int(seed)\n",
        "        self.max_start = len(self.stream) - (self.T + 1)\n",
        "        if self.max_start <= 0:\n",
        "            raise ValueError(\"Train token stream too small for max_seq_len+1\")\n",
        "\n",
        "    def __iter__(self):\n",
        "        info = torch.utils.data.get_worker_info()\n",
        "        wid = info.id if info is not None else 0\n",
        "        rng = random.Random(self.seed + 17 * wid)\n",
        "\n",
        "        yielded = 0\n",
        "        while yielded < self.target:\n",
        "            start = rng.randint(0, self.max_start)\n",
        "            chunk = self.stream[start:start + self.T + 1]\n",
        "            x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
        "            y = torch.tensor(chunk[1:], dtype=torch.long)\n",
        "            yield x, y\n",
        "            yielded += 1\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# LR schedule\n",
        "# =========================================================\n",
        "class WarmupCosine:\n",
        "    def __init__(self, opt, warmup_steps, total_steps, base_lr):\n",
        "        self.opt = opt\n",
        "        self.warmup = int(warmup_steps)\n",
        "        self.total = int(total_steps)\n",
        "        self.base = float(base_lr)\n",
        "        self.step_num = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.step_num += 1\n",
        "        if self.step_num <= self.warmup:\n",
        "            lr = self.base * self.step_num / max(1, self.warmup)\n",
        "        else:\n",
        "            prog = (self.step_num - self.warmup) / max(1, (self.total - self.warmup))\n",
        "            lr = self.base * 0.5 * (1 + math.cos(math.pi * min(1.0, prog)))\n",
        "        for g in self.opt.param_groups:\n",
        "            g[\"lr\"] = lr\n",
        "        return lr\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Light metrics helpers (fast + tolerant)\n",
        "# =========================================================\n",
        "@torch.no_grad()\n",
        "def _entropy_mean(read_w: torch.Tensor) -> float:\n",
        "    # read_w: [B,H,T,K]\n",
        "    eps = 1e-8\n",
        "    p = read_w.clamp_min(eps)\n",
        "    ent = -(p * p.log()).sum(dim=-1)  # [B,H,T]\n",
        "    return float(ent.mean().detach().cpu().item())\n",
        "\n",
        "@torch.no_grad()\n",
        "def _top1freq_mean(read_w: torch.Tensor) -> float:\n",
        "    # fraction of tokens assigned to most common top1 slot (averaged over batch+heads)\n",
        "    top1 = read_w.argmax(dim=-1)  # [B,H,T]\n",
        "    flat = top1.reshape(-1).detach().cpu()\n",
        "    K = read_w.shape[-1]\n",
        "    hist = torch.bincount(flat, minlength=K).float()\n",
        "    denom = hist.sum().clamp_min(1.0)\n",
        "    return float((hist.max() / denom).item())\n",
        "\n",
        "@torch.no_grad()\n",
        "def _write_stats(write_logits: torch.Tensor, last_k: int) -> Tuple[float, float]:\n",
        "    # write_logits: [B,H,K,T] (already biased/masked)\n",
        "    w = torch.softmax(write_logits, dim=-1)\n",
        "    T = w.shape[-1]\n",
        "    pos = torch.arange(T, device=w.device, dtype=w.dtype).view(1, 1, 1, T)\n",
        "    com = (w * pos).sum(dim=-1) / max(1.0, float(T - 1))   # [B,H,K] normalized 0..1\n",
        "    lastk = min(max(1, int(last_k)), T)\n",
        "    lastk_mass = w[..., -lastk:].sum(dim=-1)               # [B,H,K]\n",
        "    return float(com.mean().detach().cpu().item()), float(lastk_mass.mean().detach().cpu().item())\n",
        "\n",
        "@torch.no_grad()\n",
        "def _get_scalar(info: Dict, key: str) -> Optional[float]:\n",
        "    v = info.get(key, None)\n",
        "    if v is None:\n",
        "        return None\n",
        "    if isinstance(v, torch.Tensor):\n",
        "        if v.numel() == 1:\n",
        "            return float(v.detach().cpu().item())\n",
        "        return float(v.detach().float().mean().cpu().item())\n",
        "    if isinstance(v, (int, float)):\n",
        "        return float(v)\n",
        "    return None\n",
        "\n",
        "@torch.no_grad()\n",
        "def _layer_param_summaries(model) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Truthy parameter summaries:\n",
        "      - content_read_gamma uses softplus(raw) then clamped to content_read_max_gamma if present\n",
        "      - slotspace_gate uses softplus(raw)\n",
        "      - alibi_strength uses module's _alibi_strength when available\n",
        "    \"\"\"\n",
        "    gammas = []\n",
        "    gates = []\n",
        "    alibis = []\n",
        "\n",
        "    for blk in getattr(model, \"blocks\", []):\n",
        "        attn = getattr(blk, \"asa\", None)\n",
        "        if attn is None:\n",
        "            continue\n",
        "\n",
        "        # content read gamma (best-effort)\n",
        "        if hasattr(attn, \"_content_read_gamma_raw\"):\n",
        "            g = F.softplus(attn._content_read_gamma_raw.detach().float())\n",
        "            mx = getattr(attn, \"content_read_max_gamma\", None)\n",
        "            if mx is not None and float(mx) > 0:\n",
        "                g = g.clamp(max=float(mx))\n",
        "            gammas.append(float(g.cpu().item()))\n",
        "\n",
        "        # slot-space gate\n",
        "        if hasattr(attn, \"_slotspace_gate_raw\"):\n",
        "            kg = F.softplus(attn._slotspace_gate_raw.detach().float())\n",
        "            gates.append(float(kg.cpu().item()))\n",
        "\n",
        "        # alibi strength (actual used)\n",
        "        if hasattr(attn, \"_alibi_strength\"):\n",
        "            try:\n",
        "                a = attn._alibi_strength(dtype=torch.float32, device=next(attn.parameters()).device).detach().cpu().item()\n",
        "                alibis.append(float(a))\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    out: Dict[str, float] = {}\n",
        "    if gammas:\n",
        "        t = torch.tensor(gammas)\n",
        "        out[\"content_read_gamma_mean\"] = float(t.mean().item())\n",
        "        out[\"content_read_gamma_min\"]  = float(t.min().item())\n",
        "        out[\"content_read_gamma_max\"]  = float(t.max().item())\n",
        "    if gates:\n",
        "        t = torch.tensor(gates)\n",
        "        out[\"slotspace_gate_mean\"] = float(t.mean().item())\n",
        "        out[\"slotspace_gate_min\"]  = float(t.min().item())\n",
        "        out[\"slotspace_gate_max\"]  = float(t.max().item())\n",
        "    if alibis:\n",
        "        t = torch.tensor(alibis)\n",
        "        out[\"alibi_strength_mean\"] = float(t.mean().item())\n",
        "        out[\"alibi_strength_min\"]  = float(t.min().item())\n",
        "        out[\"alibi_strength_max\"]  = float(t.max().item())\n",
        "    return out\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Eval\n",
        "# =========================================================\n",
        "@torch.no_grad()\n",
        "def evaluate(model, val_loader, max_batches=50, last_k=8):\n",
        "    model.eval()\n",
        "\n",
        "    losses = []\n",
        "    infos = None\n",
        "    ent_acc = 0.0\n",
        "    top1_acc = 0.0\n",
        "    com_acc = 0.0\n",
        "    lastk_acc = 0.0\n",
        "\n",
        "    delta_acc = 0.0\n",
        "    delta_n = 0\n",
        "    n_batches = 0\n",
        "\n",
        "    for i, (xb, yb) in enumerate(val_loader):\n",
        "        if i >= max_batches:\n",
        "            break\n",
        "        xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
        "\n",
        "        with torch.autocast(device_type=device.type, dtype=amp_dtype, enabled=use_amp):\n",
        "            logits = model(xb, return_info=False)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), yb.view(-1))\n",
        "        losses.append(float(loss.item()))\n",
        "\n",
        "        # Light metrics: layer-avg per batch\n",
        "        if infos and infos[0] is not None:\n",
        "            eL, tL, cL, lL = [], [], [], []\n",
        "            dL = []\n",
        "            for info in infos:\n",
        "                read_w = info.get(\"read_weights\", None)\n",
        "                wl     = info.get(\"write_logits\", None)\n",
        "\n",
        "                if read_w is not None:\n",
        "                    eL.append(_entropy_mean(read_w))\n",
        "                    tL.append(_top1freq_mean(read_w))\n",
        "                if wl is not None:\n",
        "                    com, lastm = _write_stats(wl.to(torch.float32), last_k=last_k)\n",
        "                    cL.append(com)\n",
        "                    lL.append(lastm)\n",
        "\n",
        "                dd = _get_scalar(info, \"slotspace_delta_norm\")\n",
        "                if dd is not None:\n",
        "                    dL.append(dd)\n",
        "\n",
        "            if eL:\n",
        "                ent_acc += sum(eL) / len(eL)\n",
        "                top1_acc += sum(tL) / len(tL)\n",
        "            if cL:\n",
        "                com_acc += sum(cL) / len(cL)\n",
        "                lastk_acc += sum(lL) / len(lL)\n",
        "            if dL:\n",
        "                delta_acc += sum(dL) / len(dL)\n",
        "                delta_n += 1\n",
        "\n",
        "            n_batches += 1\n",
        "\n",
        "    mean = sum(losses) / max(1, len(losses))\n",
        "    ppl = float(math.exp(min(20.0, mean)))\n",
        "\n",
        "    stats: Dict[str, float] = {}\n",
        "    if n_batches > 0:\n",
        "        stats[\"entropy_mean\"] = ent_acc / n_batches\n",
        "        stats[\"top1freq_mean\"] = top1_acc / n_batches\n",
        "        stats[\"write_com_mean\"] = com_acc / n_batches\n",
        "        stats[\"write_lastk_mass_mean\"] = lastk_acc / n_batches\n",
        "    if delta_n > 0:\n",
        "        stats[\"slotspace_delta_norm\"] = delta_acc / delta_n\n",
        "\n",
        "    # Parameter summaries (truthy, cheap)\n",
        "    try:\n",
        "        stats.update(_layer_param_summaries(model))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    model.train()\n",
        "    return mean, ppl, stats\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Checkpointing\n",
        "# =========================================================\n",
        "def save_ckpt(path, cfg, model, opt, step, best_val):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    torch.save(\n",
        "        {\"cfg\": asdict(cfg), \"model\": model.state_dict(), \"opt\": opt.state_dict(),\n",
        "         \"step\": step, \"best_val\": best_val},\n",
        "        path,\n",
        "    )\n",
        "    print(f\"âœ“ Saved {path}\")\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Pretty stats formatting (not slow)\n",
        "# =========================================================\n",
        "def _fmt_stats(stats: Dict[str, float], last_k: int) -> str:\n",
        "    keys = [\n",
        "        \"alibi_strength_mean\",\n",
        "        \"entropy_mean\",\n",
        "        \"top1freq_mean\",\n",
        "        \"write_com_mean\",\n",
        "        \"write_lastk_mass_mean\",\n",
        "        \"content_read_gamma_mean\",\n",
        "        \"content_read_gamma_max\",\n",
        "        \"slotspace_gate_mean\",\n",
        "        \"slotspace_gate_max\",\n",
        "        \"slotspace_delta_norm\",\n",
        "    ]\n",
        "    parts = []\n",
        "    for k in keys:\n",
        "        if k in stats:\n",
        "            if k == \"write_lastk_mass_mean\":\n",
        "                parts.append(f\"{k}(last_k={last_k})={stats[k]:.4f}\")\n",
        "            else:\n",
        "                parts.append(f\"{k}={stats[k]:.4f}\")\n",
        "    return \" | \".join(parts)\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Train\n",
        "# =========================================================\n",
        "def train_asm(cfg: ASMTrainConfig):\n",
        "    random.seed(cfg.seed)\n",
        "    torch.manual_seed(cfg.seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(cfg.seed)\n",
        "\n",
        "    # ---------- Data prep (cached streams) ----------\n",
        "    os.makedirs(cfg.cache_dir, exist_ok=True)\n",
        "    train_stream_cache = os.path.join(cfg.cache_dir, f\"{cfg.dataset_config}_train_stream.pkl\")\n",
        "    val_stream_cache   = os.path.join(cfg.cache_dir, f\"{cfg.dataset_config}_val_stream.pkl\")\n",
        "\n",
        "    train_stream = build_or_load_token_stream(\n",
        "        cache_path=train_stream_cache,\n",
        "        dataset_name=cfg.dataset_name,\n",
        "        dataset_config=cfg.dataset_config,\n",
        "        split=\"train\",\n",
        "        tokenizer_name=cfg.tokenizer_name,\n",
        "        min_chars=1,\n",
        "        add_eos_between_rows=True,\n",
        "    )\n",
        "    val_stream = build_or_load_token_stream(\n",
        "        cache_path=val_stream_cache,\n",
        "        dataset_name=cfg.dataset_name,\n",
        "        dataset_config=cfg.dataset_config,\n",
        "        split=\"validation\",\n",
        "        tokenizer_name=cfg.tokenizer_name,\n",
        "        min_chars=1,\n",
        "        add_eos_between_rows=True,\n",
        "    )\n",
        "\n",
        "    val_dataset = build_or_load_validation_windows(\n",
        "        cache_path=cfg.val_windows_cache,\n",
        "        token_stream=val_stream,\n",
        "        max_seq_len=cfg.max_seq_len,\n",
        "        stride_frac=cfg.stride_frac_val,\n",
        "        val_samples_target=cfg.val_samples_target,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=cfg.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "        pin_memory=torch.cuda.is_available(),\n",
        "    )\n",
        "\n",
        "    train_ds = WikiTextRandomWindowStream(\n",
        "        token_stream=train_stream,\n",
        "        max_seq_len=cfg.max_seq_len,\n",
        "        train_samples_target=cfg.train_samples_target,\n",
        "        seed=cfg.seed,\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=cfg.batch_size,\n",
        "        num_workers=3,\n",
        "        pin_memory=torch.cuda.is_available(),\n",
        "    )\n",
        "\n",
        "    # ---------- Model ----------\n",
        "    model = ASMLanguageModel(\n",
        "        vocab_size=cfg.vocab_size,\n",
        "        embed_dim=cfg.embed_dim,\n",
        "        num_layers=cfg.num_layers,\n",
        "        num_heads=cfg.num_heads,\n",
        "        num_slots=cfg.num_slots,\n",
        "        max_seq_len=cfg.max_seq_len,\n",
        "        mlp_ratio=cfg.mlp_ratio,\n",
        "        dropout=cfg.dropout,\n",
        "\n",
        "        read_temperature=cfg.read_temperature,\n",
        "        write_temperature=cfg.write_temperature,\n",
        "        state_fp32=cfg.state_fp32,\n",
        "        slot_dropout=cfg.slot_dropout,\n",
        "        normalize_k=cfg.normalize_k,\n",
        "\n",
        "        tie_weights=cfg.tie_weights,\n",
        "\n",
        "        use_abs_pos=cfg.use_abs_pos,\n",
        "\n",
        "        use_rope_keys=cfg.use_rope_keys,\n",
        "        rope_base=cfg.rope_base,\n",
        "        use_alibi_write=cfg.use_alibi_write,\n",
        "        alibi_strength_init=cfg.alibi_strength_init,\n",
        "        learn_alibi_strength=cfg.learn_alibi_strength,\n",
        "        min_strength=cfg.min_strength,\n",
        "\n",
        "        use_content_read=cfg.use_content_read,\n",
        "        content_read_init=cfg.content_read_init,\n",
        "        content_read_max_gamma=cfg.content_read_max_gamma,\n",
        "\n",
        "        use_slotspace_refine=cfg.use_slotspace_refine,\n",
        "        slotspace_dim=cfg.slotspace_dim,\n",
        "        slotspace_gate_init=cfg.slotspace_gate_init,\n",
        "        slotspace_dropout=cfg.slotspace_dropout,\n",
        "        slotspace_signed_weights=cfg.slotspace_signed_weights,\n",
        "\n",
        "        use_rope_slotspace=cfg.use_rope_slotspace,\n",
        "        rope_base_slotspace=cfg.rope_base_slotspace,\n",
        "\n",
        "        write_chunk_size=cfg.write_chunk_size,\n",
        "        enable_compiled=cfg.enable_compiled,\n",
        "        #slotspace_chunk_size=cfg.slotspace_chunk_size,\n",
        "    ).to(device)\n",
        "\n",
        "    out_dir = os.path.join(cfg.output_dir, cfg.tag)\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    n_params = sum(p.numel() for p in model.parameters())\n",
        "    print(\"=\" * 108)\n",
        "    print(f\"Training [{cfg.tag}] on {cfg.dataset_name}/{cfg.dataset_config}\")\n",
        "    print(f\"Params: {n_params:,}\")\n",
        "    print(f\"Train tokens: {len(train_stream):,} | Val tokens: {len(val_stream):,} | Val windows: {len(val_dataset):,}\")\n",
        "    print(f\"T={cfg.max_seq_len} | val_stride_frac={cfg.stride_frac_val} | last_k={cfg.analytics_last_k}\")\n",
        "    #print(f\"Chunks: write={cfg.write_chunk_size} | slotspace={cfg.slotspace_chunk_size} | amp={use_amp}({amp_dtype}) | state_fp32={cfg.state_fp32}\")\n",
        "    print(f\"Chunks: write={cfg.write_chunk_size} | amp={use_amp}({amp_dtype}) | state_fp32={cfg.state_fp32}\")\n",
        "\n",
        "    print(f\"RoPE: keys={cfg.use_rope_keys}(base={cfg.rope_base:g}) | slotspace={cfg.use_rope_slotspace}(base={cfg.rope_base_slotspace:g})\")\n",
        "    print(\"=\" * 108)\n",
        "\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay, betas=cfg.betas)\n",
        "    sched = WarmupCosine(opt, cfg.warmup_steps, cfg.total_steps, cfg.learning_rate)\n",
        "\n",
        "    # ---------- Initial eval ----------\n",
        "    best_val = float(\"inf\")\n",
        "    vloss, vppl, vstats = evaluate(model, val_loader, max_batches=cfg.eval_max_batches, last_k=cfg.analytics_last_k)\n",
        "    best_val = vloss\n",
        "    save_ckpt(os.path.join(out_dir, \"best.pt\"), cfg, model, opt, 0, best_val)\n",
        "\n",
        "    print(f\"[VAL step 0] loss={vloss:.3f} ppl={vppl:.2f}\")\n",
        "    if vstats:\n",
        "        print(\"  \" + _fmt_stats(vstats, last_k=cfg.analytics_last_k))\n",
        "\n",
        "    # ---------- Training loop ----------\n",
        "    running = 0.0\n",
        "    step = 0\n",
        "    t_last = time.time()\n",
        "\n",
        "    pbar = tqdm(total=cfg.total_steps, desc=f\"[{cfg.tag}]\")\n",
        "    for xb, yb in train_loader:\n",
        "        xb = xb.to(device, non_blocking=True)\n",
        "        yb = yb.to(device, non_blocking=True)\n",
        "\n",
        "        with torch.autocast(device_type=device.type, dtype=amp_dtype, enabled=use_amp):\n",
        "            logits = model(xb)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), yb.view(-1))\n",
        "\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
        "        opt.step()\n",
        "        lr = sched.step()\n",
        "\n",
        "        step += 1\n",
        "        running += float(loss.item())\n",
        "        pbar.update(1)\n",
        "\n",
        "        if step % cfg.log_interval == 0:\n",
        "            avg = running / cfg.log_interval\n",
        "            running = 0.0\n",
        "\n",
        "            ps = _layer_param_summaries(model)\n",
        "\n",
        "            it_s = cfg.log_interval / max(1e-9, (time.time() - t_last))\n",
        "            t_last = time.time()\n",
        "\n",
        "            postfix = {\n",
        "                \"loss\": f\"{avg:.3f}\",\n",
        "                \"ppl\": f\"{math.exp(min(20.0, avg)):.2f}\",\n",
        "                \"lr\": f\"{lr:.2e}\",\n",
        "                \"it/s\": f\"{it_s:.2f}\",\n",
        "            }\n",
        "            if \"content_read_gamma_mean\" in ps: postfix[\"Î³Î¼\"] = f\"{ps['content_read_gamma_mean']:.3f}\"\n",
        "            if \"slotspace_gate_mean\" in ps: postfix[\"sgÎ¼\"] = f\"{ps['slotspace_gate_mean']:.3f}\"\n",
        "            pbar.set_postfix(postfix)\n",
        "\n",
        "            msg = f\"[step {step}] train_loss={avg:.3f} ppl={math.exp(min(20.0, avg)):.2f} lr={lr:.2e} it/s={it_s:.2f}\"\n",
        "            if \"content_read_gamma_mean\" in ps: msg += f\" | content_read_gamma_mean={ps['content_read_gamma_mean']:.4f}\"\n",
        "            if \"slotspace_gate_mean\" in ps: msg += f\" | slotspace_gate_mean={ps['slotspace_gate_mean']:.4f}\"\n",
        "            if \"alibi_strength_mean\" in ps: msg += f\" | alibi_strength_mean={ps['alibi_strength_mean']:.4f}\"\n",
        "            print(msg)\n",
        "\n",
        "        if step % cfg.eval_interval == 0:\n",
        "            vloss, vppl, vstats = evaluate(model, val_loader, max_batches=cfg.eval_max_batches, last_k=cfg.analytics_last_k)\n",
        "            print(f\"\\n[VAL step {step}] loss={vloss:.3f} ppl={vppl:.2f}\")\n",
        "            if vstats:\n",
        "                print(\"  \" + _fmt_stats(vstats, last_k=cfg.analytics_last_k))\n",
        "\n",
        "            if vloss < best_val:\n",
        "                best_val = vloss\n",
        "                save_ckpt(os.path.join(out_dir, \"best.pt\"), cfg, model, opt, step, best_val)\n",
        "\n",
        "        if step >= cfg.total_steps:\n",
        "            break\n",
        "\n",
        "    save_ckpt(os.path.join(out_dir, \"final.pt\"), cfg, model, opt, step, best_val)\n",
        "    print(f\"[{cfg.tag}] Done. Best val loss: {best_val:.4f}\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "k_lYCpPZByjq"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Train, Eval Functions and Utilities â€” Gradient Accumulation (no checkpointing)\n",
        "\n",
        "import os, math, random, pickle, time\n",
        "from dataclasses import asdict\n",
        "from typing import Tuple, Dict, List, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, IterableDataset, DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Data: cached token streams + val windows + random-window train\n",
        "# =========================================================\n",
        "class StableValidationDataset(Dataset):\n",
        "    def __init__(self, samples: List[Tuple[torch.Tensor, torch.Tensor]]):\n",
        "        self.samples = samples\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx): return self.samples[idx]\n",
        "\n",
        "def _ensure_dir(path: str):\n",
        "    d = os.path.dirname(path)\n",
        "    if d:\n",
        "        os.makedirs(d, exist_ok=True)\n",
        "\n",
        "def build_or_load_token_stream(\n",
        "    *,\n",
        "    cache_path: str,\n",
        "    dataset_name: str,\n",
        "    dataset_config: str,\n",
        "    split: str,\n",
        "    tokenizer_name: str,\n",
        "    min_chars: int = 1,\n",
        "    add_eos_between_rows: bool = True,\n",
        "    max_rows: Optional[int] = None,\n",
        ") -> List[int]:\n",
        "    if os.path.exists(cache_path):\n",
        "        print(f\"Loading cached token stream: {cache_path}\")\n",
        "        with open(cache_path, \"rb\") as f:\n",
        "            stream = pickle.load(f)\n",
        "        print(f\"Loaded token stream tokens={len(stream):,}\")\n",
        "        return stream\n",
        "\n",
        "    print(f\"Building token stream for {dataset_name}/{dataset_config} split={split} ...\")\n",
        "    _ensure_dir(cache_path)\n",
        "    tok = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=True)\n",
        "    eos = tok.eos_token_id\n",
        "    assert eos is not None, \"Tokenizer must have eos_token_id\"\n",
        "\n",
        "    ds = load_dataset(dataset_name, dataset_config, split=split)\n",
        "\n",
        "    stream: List[int] = []\n",
        "    used = 0\n",
        "    for row in tqdm(ds, desc=f\"Tokenizing {split}\"):\n",
        "        if max_rows is not None and used >= max_rows:\n",
        "            break\n",
        "        text = (row.get(\"text\") or \"\").strip()\n",
        "        if len(text) < min_chars:\n",
        "            continue\n",
        "        ids = tok.encode(text, add_special_tokens=False)\n",
        "        if not ids:\n",
        "            continue\n",
        "        stream.extend(ids)\n",
        "        if add_eos_between_rows:\n",
        "            stream.append(eos)\n",
        "        used += 1\n",
        "\n",
        "    print(f\"Built stream: rows_used={used:,} tokens={len(stream):,}\")\n",
        "    with open(cache_path, \"wb\") as f:\n",
        "        pickle.dump(stream, f)\n",
        "    print(f\"Cached token stream to {cache_path}\")\n",
        "    return stream\n",
        "\n",
        "def build_or_load_validation_windows(\n",
        "    *,\n",
        "    cache_path: str,\n",
        "    token_stream: List[int],\n",
        "    max_seq_len: int,\n",
        "    stride_frac: float,\n",
        "    val_samples_target: int,\n",
        ") -> StableValidationDataset:\n",
        "    if os.path.exists(cache_path):\n",
        "        print(f\"Loading cached validation windows: {cache_path}\")\n",
        "        with open(cache_path, \"rb\") as f:\n",
        "            samples = pickle.load(f)\n",
        "        print(f\"Loaded val windows: {len(samples)}\")\n",
        "        return StableValidationDataset(samples)\n",
        "\n",
        "    print(\"Building validation windows (cached)...\")\n",
        "    _ensure_dir(cache_path)\n",
        "\n",
        "    T = int(max_seq_len)\n",
        "    stride = max(1, int(T * float(stride_frac)))\n",
        "    need = int(val_samples_target)\n",
        "\n",
        "    max_start = len(token_stream) - (T + 1)\n",
        "    if max_start <= 0:\n",
        "        raise ValueError(\"Validation token stream too small for max_seq_len+1\")\n",
        "\n",
        "    samples: List[Tuple[torch.Tensor, torch.Tensor]] = []\n",
        "    for start in tqdm(range(0, max_start + 1, stride), desc=\"Chunking val stream\"):\n",
        "        chunk = token_stream[start:start + T + 1]\n",
        "        if len(chunk) < T + 1:\n",
        "            break\n",
        "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
        "        samples.append((x, y))\n",
        "        if len(samples) >= need:\n",
        "            break\n",
        "\n",
        "    print(f\"Built val windows={len(samples)} (stride={stride}, stream_tokens={len(token_stream):,})\")\n",
        "    with open(cache_path, \"wb\") as f:\n",
        "        pickle.dump(samples, f)\n",
        "    print(f\"Cached validation windows to {cache_path}\")\n",
        "    return StableValidationDataset(samples)\n",
        "\n",
        "class WikiTextRandomWindowStream(IterableDataset):\n",
        "    def __init__(self, token_stream: List[int], max_seq_len: int, train_samples_target: int, seed: int):\n",
        "        super().__init__()\n",
        "        self.stream = token_stream\n",
        "        self.T = int(max_seq_len)\n",
        "        self.target = int(train_samples_target)\n",
        "        self.seed = int(seed)\n",
        "        self.max_start = len(self.stream) - (self.T + 1)\n",
        "        if self.max_start <= 0:\n",
        "            raise ValueError(\"Train token stream too small for max_seq_len+1\")\n",
        "\n",
        "    def __iter__(self):\n",
        "        info = torch.utils.data.get_worker_info()\n",
        "        wid = info.id if info is not None else 0\n",
        "        rng = random.Random(self.seed + 17 * wid)\n",
        "\n",
        "        yielded = 0\n",
        "        while yielded < self.target:\n",
        "            start = rng.randint(0, self.max_start)\n",
        "            chunk = self.stream[start:start + self.T + 1]\n",
        "            x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
        "            y = torch.tensor(chunk[1:], dtype=torch.long)\n",
        "            yield x, y\n",
        "            yielded += 1\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# LR schedule\n",
        "# =========================================================\n",
        "class WarmupCosine:\n",
        "    def __init__(self, opt, warmup_steps, total_steps, base_lr):\n",
        "        self.opt = opt\n",
        "        self.warmup = int(warmup_steps)\n",
        "        self.total = int(total_steps)\n",
        "        self.base = float(base_lr)\n",
        "        self.step_num = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.step_num += 1\n",
        "        if self.step_num <= self.warmup:\n",
        "            lr = self.base * self.step_num / max(1, self.warmup)\n",
        "        else:\n",
        "            prog = (self.step_num - self.warmup) / max(1, (self.total - self.warmup))\n",
        "            lr = self.base * 0.5 * (1 + math.cos(math.pi * min(1.0, prog)))\n",
        "        for g in self.opt.param_groups:\n",
        "            g[\"lr\"] = lr\n",
        "        return lr\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Light metrics helpers (fast + tolerant)\n",
        "# =========================================================\n",
        "@torch.no_grad()\n",
        "def _entropy_mean(read_w: torch.Tensor) -> float:\n",
        "    eps = 1e-8\n",
        "    p = read_w.clamp_min(eps)\n",
        "    ent = -(p * p.log()).sum(dim=-1)\n",
        "    return float(ent.mean().detach().cpu().item())\n",
        "\n",
        "@torch.no_grad()\n",
        "def _top1freq_mean(read_w: torch.Tensor) -> float:\n",
        "    top1 = read_w.argmax(dim=-1)\n",
        "    flat = top1.reshape(-1).detach().cpu()\n",
        "    K = read_w.shape[-1]\n",
        "    hist = torch.bincount(flat, minlength=K).float()\n",
        "    denom = hist.sum().clamp_min(1.0)\n",
        "    return float((hist.max() / denom).item())\n",
        "\n",
        "@torch.no_grad()\n",
        "def _write_stats(write_logits: torch.Tensor, last_k: int) -> Tuple[float, float]:\n",
        "    w = torch.softmax(write_logits, dim=-1)\n",
        "    T = w.shape[-1]\n",
        "    pos = torch.arange(T, device=w.device, dtype=w.dtype).view(1, 1, 1, T)\n",
        "    com = (w * pos).sum(dim=-1) / max(1.0, float(T - 1))\n",
        "    lastk = min(max(1, int(last_k)), T)\n",
        "    lastk_mass = w[..., -lastk:].sum(dim=-1)\n",
        "    return float(com.mean().detach().cpu().item()), float(lastk_mass.mean().detach().cpu().item())\n",
        "\n",
        "@torch.no_grad()\n",
        "def _get_scalar(info: Dict, key: str) -> Optional[float]:\n",
        "    v = info.get(key, None)\n",
        "    if v is None:\n",
        "        return None\n",
        "    if isinstance(v, torch.Tensor):\n",
        "        if v.numel() == 1:\n",
        "            return float(v.detach().cpu().item())\n",
        "        return float(v.detach().float().mean().cpu().item())\n",
        "    if isinstance(v, (int, float)):\n",
        "        return float(v)\n",
        "    return None\n",
        "\n",
        "@torch.no_grad()\n",
        "def _layer_param_summaries(model) -> Dict[str, float]:\n",
        "    gammas = []\n",
        "    gates = []\n",
        "    alibis = []\n",
        "\n",
        "    for blk in getattr(model, \"blocks\", []):\n",
        "        attn = getattr(blk, \"asa\", None)\n",
        "        if attn is None:\n",
        "            continue\n",
        "\n",
        "        if hasattr(attn, \"_content_read_gamma_raw\"):\n",
        "            g = F.softplus(attn._content_read_gamma_raw.detach().float())\n",
        "            mx = getattr(attn, \"content_read_max_gamma\", None)\n",
        "            if mx is not None and float(mx) > 0:\n",
        "                g = g.clamp(max=float(mx))\n",
        "            gammas.append(float(g.cpu().item()))\n",
        "\n",
        "        if hasattr(attn, \"_slotspace_gate_raw\"):\n",
        "            kg = F.softplus(attn._slotspace_gate_raw.detach().float())\n",
        "            gates.append(float(kg.cpu().item()))\n",
        "\n",
        "        if hasattr(attn, \"_alibi_strength\"):\n",
        "            try:\n",
        "                a = attn._alibi_strength(dtype=torch.float32, device=next(attn.parameters()).device).detach().cpu().item()\n",
        "                alibis.append(float(a))\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    out: Dict[str, float] = {}\n",
        "    if gammas:\n",
        "        t = torch.tensor(gammas)\n",
        "        out[\"content_read_gamma_mean\"] = float(t.mean().item())\n",
        "        out[\"content_read_gamma_min\"]  = float(t.min().item())\n",
        "        out[\"content_read_gamma_max\"]  = float(t.max().item())\n",
        "    if gates:\n",
        "        t = torch.tensor(gates)\n",
        "        out[\"slotspace_gate_mean\"] = float(t.mean().item())\n",
        "        out[\"slotspace_gate_min\"]  = float(t.min().item())\n",
        "        out[\"slotspace_gate_max\"]  = float(t.max().item())\n",
        "    if alibis:\n",
        "        t = torch.tensor(alibis)\n",
        "        out[\"alibi_strength_mean\"] = float(t.mean().item())\n",
        "        out[\"alibi_strength_min\"]  = float(t.min().item())\n",
        "        out[\"alibi_strength_max\"]  = float(t.max().item())\n",
        "    return out\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Eval\n",
        "# =========================================================\n",
        "@torch.no_grad()\n",
        "def evaluate(model, val_loader, max_batches=50, last_k=8):\n",
        "    model.eval()\n",
        "\n",
        "    losses = []\n",
        "    infos = None\n",
        "    ent_acc = 0.0\n",
        "    top1_acc = 0.0\n",
        "    com_acc = 0.0\n",
        "    lastk_acc = 0.0\n",
        "\n",
        "    delta_acc = 0.0\n",
        "    delta_n = 0\n",
        "    n_batches = 0\n",
        "\n",
        "    for i, (xb, yb) in enumerate(val_loader):\n",
        "        if i >= max_batches:\n",
        "            break\n",
        "        xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
        "\n",
        "        with torch.autocast(device_type=device.type, dtype=amp_dtype, enabled=use_amp):\n",
        "            logits = model(xb, return_info=False)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), yb.view(-1))\n",
        "        losses.append(float(loss.item()))\n",
        "\n",
        "        # kept as-is: infos is None unless you enable return_info in eval\n",
        "        if infos and infos[0] is not None:\n",
        "            eL, tL, cL, lL = [], [], [], []\n",
        "            dL = []\n",
        "            for info in infos:\n",
        "                read_w = info.get(\"read_weights\", None)\n",
        "                wl     = info.get(\"write_logits\", None)\n",
        "\n",
        "                if read_w is not None:\n",
        "                    eL.append(_entropy_mean(read_w))\n",
        "                    tL.append(_top1freq_mean(read_w))\n",
        "                if wl is not None:\n",
        "                    com, lastm = _write_stats(wl.to(torch.float32), last_k=last_k)\n",
        "                    cL.append(com)\n",
        "                    lL.append(lastm)\n",
        "\n",
        "                dd = _get_scalar(info, \"slotspace_delta_norm\")\n",
        "                if dd is not None:\n",
        "                    dL.append(dd)\n",
        "\n",
        "            if eL:\n",
        "                ent_acc += sum(eL) / len(eL)\n",
        "                top1_acc += sum(tL) / len(tL)\n",
        "            if cL:\n",
        "                com_acc += sum(cL) / len(cL)\n",
        "                lastk_acc += sum(lL) / len(lL)\n",
        "            if dL:\n",
        "                delta_acc += sum(dL) / len(dL)\n",
        "                delta_n += 1\n",
        "\n",
        "            n_batches += 1\n",
        "\n",
        "    mean = sum(losses) / max(1, len(losses))\n",
        "    ppl = float(math.exp(min(20.0, mean)))\n",
        "\n",
        "    stats: Dict[str, float] = {}\n",
        "    if n_batches > 0:\n",
        "        stats[\"entropy_mean\"] = ent_acc / n_batches\n",
        "        stats[\"top1freq_mean\"] = top1_acc / n_batches\n",
        "        stats[\"write_com_mean\"] = com_acc / n_batches\n",
        "        stats[\"write_lastk_mass_mean\"] = lastk_acc / n_batches\n",
        "    if delta_n > 0:\n",
        "        stats[\"slotspace_delta_norm\"] = delta_acc / delta_n\n",
        "\n",
        "    try:\n",
        "        stats.update(_layer_param_summaries(model))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    model.train()\n",
        "    return mean, ppl, stats\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Checkpointing (model checkpoints, not activation checkpointing)\n",
        "# =========================================================\n",
        "def save_ckpt(path, cfg, model, opt, step, best_val):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    torch.save(\n",
        "        {\"cfg\": asdict(cfg), \"model\": model.state_dict(), \"opt\": opt.state_dict(),\n",
        "         \"step\": step, \"best_val\": best_val},\n",
        "        path,\n",
        "    )\n",
        "    print(f\"âœ“ Saved {path}\")\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Pretty stats formatting (not slow)\n",
        "# =========================================================\n",
        "def _fmt_stats(stats: Dict[str, float], last_k: int) -> str:\n",
        "    keys = [\n",
        "        \"alibi_strength_mean\",\n",
        "        \"entropy_mean\",\n",
        "        \"top1freq_mean\",\n",
        "        \"write_com_mean\",\n",
        "        \"write_lastk_mass_mean\",\n",
        "        \"content_read_gamma_mean\",\n",
        "        \"content_read_gamma_max\",\n",
        "        \"slotspace_gate_mean\",\n",
        "        \"slotspace_gate_max\",\n",
        "        \"slotspace_delta_norm\",\n",
        "    ]\n",
        "    parts = []\n",
        "    for k in keys:\n",
        "        if k in stats:\n",
        "            if k == \"write_lastk_mass_mean\":\n",
        "                parts.append(f\"{k}(last_k={last_k})={stats[k]:.4f}\")\n",
        "            else:\n",
        "                parts.append(f\"{k}={stats[k]:.4f}\")\n",
        "    return \" | \".join(parts)\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Train (with gradient accumulation)\n",
        "# =========================================================\n",
        "def train_asm(cfg: ASMTrainConfig):\n",
        "    \"\"\"\n",
        "    Drop-in replacement.\n",
        "    Adds gradient accumulation controlled by:\n",
        "      - cfg.micro_batch_size (DataLoader batch_size)\n",
        "      - cfg.grad_accum_steps (optimizer steps every N microbatches)\n",
        "\n",
        "    If these fields are missing on cfg, defaults are used:\n",
        "      micro_batch_size := cfg.batch_size\n",
        "      grad_accum_steps := 1\n",
        "    \"\"\"\n",
        "    random.seed(cfg.seed)\n",
        "    torch.manual_seed(cfg.seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(cfg.seed)\n",
        "\n",
        "    # ---- accumulation defaults (safe even if cfg lacks the fields) ----\n",
        "    micro_bs = int(getattr(cfg, \"micro_batch_size\", cfg.batch_size))\n",
        "    accum_steps = int(getattr(cfg, \"grad_accum_steps\", 1))\n",
        "    assert micro_bs >= 1\n",
        "    assert accum_steps >= 1\n",
        "\n",
        "    eff_bs = micro_bs * accum_steps\n",
        "\n",
        "    # ---------- Data prep (cached streams) ----------\n",
        "    os.makedirs(cfg.cache_dir, exist_ok=True)\n",
        "    train_stream_cache = os.path.join(cfg.cache_dir, f\"{cfg.dataset_config}_train_stream.pkl\")\n",
        "    val_stream_cache   = os.path.join(cfg.cache_dir, f\"{cfg.dataset_config}_val_stream.pkl\")\n",
        "\n",
        "    train_stream = build_or_load_token_stream(\n",
        "        cache_path=train_stream_cache,\n",
        "        dataset_name=cfg.dataset_name,\n",
        "        dataset_config=cfg.dataset_config,\n",
        "        split=\"train\",\n",
        "        tokenizer_name=cfg.tokenizer_name,\n",
        "        min_chars=1,\n",
        "        add_eos_between_rows=True,\n",
        "    )\n",
        "    val_stream = build_or_load_token_stream(\n",
        "        cache_path=val_stream_cache,\n",
        "        dataset_name=cfg.dataset_name,\n",
        "        dataset_config=cfg.dataset_config,\n",
        "        split=\"validation\",\n",
        "        tokenizer_name=cfg.tokenizer_name,\n",
        "        min_chars=1,\n",
        "        add_eos_between_rows=True,\n",
        "    )\n",
        "\n",
        "    val_dataset = build_or_load_validation_windows(\n",
        "        cache_path=cfg.val_windows_cache,\n",
        "        token_stream=val_stream,\n",
        "        max_seq_len=cfg.max_seq_len,\n",
        "        stride_frac=cfg.stride_frac_val,\n",
        "        val_samples_target=cfg.val_samples_target,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=micro_bs,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "        pin_memory=torch.cuda.is_available(),\n",
        "    )\n",
        "\n",
        "    train_ds = WikiTextRandomWindowStream(\n",
        "        token_stream=train_stream,\n",
        "        max_seq_len=cfg.max_seq_len,\n",
        "        train_samples_target=cfg.train_samples_target,\n",
        "        seed=cfg.seed,\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=micro_bs,\n",
        "        num_workers=3,\n",
        "        pin_memory=torch.cuda.is_available(),\n",
        "    )\n",
        "\n",
        "    # ---------- Model ----------\n",
        "    model = ASMLanguageModel(\n",
        "        vocab_size=cfg.vocab_size,\n",
        "        embed_dim=cfg.embed_dim,\n",
        "        num_layers=cfg.num_layers,\n",
        "        num_heads=cfg.num_heads,\n",
        "        num_slots=cfg.num_slots,\n",
        "        max_seq_len=cfg.max_seq_len,\n",
        "        mlp_ratio=cfg.mlp_ratio,\n",
        "        dropout=cfg.dropout,\n",
        "\n",
        "        read_temperature=cfg.read_temperature,\n",
        "        write_temperature=cfg.write_temperature,\n",
        "        state_fp32=cfg.state_fp32,\n",
        "        slot_dropout=cfg.slot_dropout,\n",
        "        normalize_k=cfg.normalize_k,\n",
        "\n",
        "        tie_weights=cfg.tie_weights,\n",
        "\n",
        "        use_abs_pos=cfg.use_abs_pos,\n",
        "\n",
        "        use_rope_keys=cfg.use_rope_keys,\n",
        "        rope_base=cfg.rope_base,\n",
        "        use_alibi_write=cfg.use_alibi_write,\n",
        "        alibi_strength_init=cfg.alibi_strength_init,\n",
        "        learn_alibi_strength=cfg.learn_alibi_strength,\n",
        "        min_strength=cfg.min_strength,\n",
        "\n",
        "        use_content_read=cfg.use_content_read,\n",
        "        content_read_init=cfg.content_read_init,\n",
        "        content_read_max_gamma=cfg.content_read_max_gamma,\n",
        "\n",
        "        use_slotspace_refine=cfg.use_slotspace_refine,\n",
        "        slotspace_dim=cfg.slotspace_dim,\n",
        "        slotspace_gate_init=cfg.slotspace_gate_init,\n",
        "        slotspace_dropout=cfg.slotspace_dropout,\n",
        "        slotspace_signed_weights=cfg.slotspace_signed_weights,\n",
        "\n",
        "        use_rope_slotspace=cfg.use_rope_slotspace,\n",
        "        rope_base_slotspace=cfg.rope_base_slotspace,\n",
        "\n",
        "        write_chunk_size=cfg.write_chunk_size,\n",
        "        enable_compiled=cfg.enable_compiled,\n",
        "    ).to(device)\n",
        "\n",
        "    out_dir = os.path.join(cfg.output_dir, cfg.tag)\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    n_params = sum(p.numel() for p in model.parameters())\n",
        "    print(\"=\" * 108)\n",
        "    print(f\"Training [{cfg.tag}] on {cfg.dataset_name}/{cfg.dataset_config}\")\n",
        "    print(f\"Params: {n_params:,}\")\n",
        "    print(f\"Train tokens: {len(train_stream):,} | Val tokens: {len(val_stream):,} | Val windows: {len(val_dataset):,}\")\n",
        "    print(f\"T={cfg.max_seq_len} | val_stride_frac={cfg.stride_frac_val} | last_k={cfg.analytics_last_k}\")\n",
        "    print(f\"Batching: micro_bs={micro_bs} | accum_steps={accum_steps} | effective_bs={eff_bs}\")\n",
        "    print(f\"Chunks: write={cfg.write_chunk_size} | amp={use_amp}({amp_dtype}) | state_fp32={cfg.state_fp32}\")\n",
        "    print(f\"RoPE: keys={cfg.use_rope_keys}(base={cfg.rope_base:g}) | slotspace={cfg.use_rope_slotspace}(base={cfg.rope_base_slotspace:g})\")\n",
        "    print(\"=\" * 108)\n",
        "\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay, betas=cfg.betas)\n",
        "    sched = WarmupCosine(opt, cfg.warmup_steps, cfg.total_steps, cfg.learning_rate)\n",
        "\n",
        "    # ---------- Initial eval ----------\n",
        "    best_val = float(\"inf\")\n",
        "    vloss, vppl, vstats = evaluate(model, val_loader, max_batches=cfg.eval_max_batches, last_k=cfg.analytics_last_k)\n",
        "    best_val = vloss\n",
        "    save_ckpt(os.path.join(out_dir, \"best.pt\"), cfg, model, opt, 0, best_val)\n",
        "\n",
        "    print(f\"[VAL step 0] loss={vloss:.3f} ppl={vppl:.2f}\")\n",
        "    if vstats:\n",
        "        print(\"  \" + _fmt_stats(vstats, last_k=cfg.analytics_last_k))\n",
        "\n",
        "    # ---------- Training loop (grad accumulation) ----------\n",
        "    model.train()\n",
        "    opt.zero_grad(set_to_none=True)\n",
        "\n",
        "    running = 0.0          # for logging (unscaled loss)\n",
        "    step = 0               # optimizer steps\n",
        "    micro_step = 0         # microbatches\n",
        "    t_last = time.time()\n",
        "\n",
        "    # tqdm counts optimizer steps (matches cfg.total_steps, sched, eval_interval)\n",
        "    pbar = tqdm(total=cfg.total_steps, desc=f\"[{cfg.tag}]\")\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "        xb = xb.to(device, non_blocking=True)\n",
        "        yb = yb.to(device, non_blocking=True)\n",
        "        micro_step += 1\n",
        "\n",
        "        with torch.autocast(device_type=device.type, dtype=amp_dtype, enabled=use_amp):\n",
        "            logits = model(xb)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), yb.view(-1))\n",
        "\n",
        "        # log unscaled loss for readability\n",
        "        running += float(loss.item())\n",
        "\n",
        "        # scale loss so accumulated grads match a single big batch\n",
        "        (loss / accum_steps).backward()\n",
        "\n",
        "        # take an optimizer step every accum_steps microbatches\n",
        "        if (micro_step % accum_steps) == 0:\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
        "            opt.step()\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            lr = sched.step()\n",
        "\n",
        "            step += 1\n",
        "            pbar.update(1)\n",
        "\n",
        "            if step % cfg.log_interval == 0:\n",
        "                avg = running / (cfg.log_interval * accum_steps)  # avg loss per microbatch (equiv per-sample batch loss)\n",
        "                running = 0.0\n",
        "\n",
        "                ps = _layer_param_summaries(model)\n",
        "                it_s = cfg.log_interval / max(1e-9, (time.time() - t_last))\n",
        "                t_last = time.time()\n",
        "\n",
        "                postfix = {\n",
        "                    \"loss\": f\"{avg:.3f}\",\n",
        "                    \"ppl\": f\"{math.exp(min(20.0, avg)):.2f}\",\n",
        "                    \"lr\": f\"{lr:.2e}\",\n",
        "                    \"it/s\": f\"{it_s:.2f}\",\n",
        "                }\n",
        "                if \"content_read_gamma_mean\" in ps: postfix[\"Î³Î¼\"] = f\"{ps['content_read_gamma_mean']:.3f}\"\n",
        "                if \"slotspace_gate_mean\" in ps: postfix[\"sgÎ¼\"] = f\"{ps['slotspace_gate_mean']:.3f}\"\n",
        "                pbar.set_postfix(postfix)\n",
        "\n",
        "                msg = f\"[step {step}] train_loss={avg:.3f} ppl={math.exp(min(20.0, avg)):.2f} lr={lr:.2e} it/s={it_s:.2f}\"\n",
        "                if \"content_read_gamma_mean\" in ps: msg += f\" | content_read_gamma_mean={ps['content_read_gamma_mean']:.4f}\"\n",
        "                if \"slotspace_gate_mean\" in ps: msg += f\" | slotspace_gate_mean={ps['slotspace_gate_mean']:.4f}\"\n",
        "                if \"alibi_strength_mean\" in ps: msg += f\" | alibi_strength_mean={ps['alibi_strength_mean']:.4f}\"\n",
        "                print(msg)\n",
        "\n",
        "            if step % cfg.eval_interval == 0:\n",
        "                vloss, vppl, vstats = evaluate(model, val_loader, max_batches=cfg.eval_max_batches, last_k=cfg.analytics_last_k)\n",
        "                print(f\"\\n[VAL step {step}] loss={vloss:.3f} ppl={vppl:.2f}\")\n",
        "                if vstats:\n",
        "                    print(\"  \" + _fmt_stats(vstats, last_k=cfg.analytics_last_k))\n",
        "\n",
        "                if vloss < best_val:\n",
        "                    best_val = vloss\n",
        "                    save_ckpt(os.path.join(out_dir, \"best.pt\"), cfg, model, opt, step, best_val)\n",
        "\n",
        "            if step >= cfg.total_steps:\n",
        "                break\n",
        "\n",
        "    # If we exit mid-accumulation (micro_step not divisible by accum_steps), you can optionally flush grads.\n",
        "    # For strict reproducibility, we *do not* flush by default.\n",
        "\n",
        "    save_ckpt(os.path.join(out_dir, \"final.pt\"), cfg, model, opt, step, best_val)\n",
        "    print(f\"[{cfg.tag}] Done. Best val loss: {best_val:.4f}\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4sAcHjTitrql"
      },
      "outputs": [],
      "source": [
        "#@title train asm func with resume\n",
        "def train_asm(cfg: ASMTrainConfig):\n",
        "    \"\"\"\n",
        "    Drop-in replacement for your current train_asm(cfg).\n",
        "\n",
        "    Adds:\n",
        "      - resume from cfg.resume_path if present and exists\n",
        "      - correct tqdm initial position\n",
        "      - scheduler step_num alignment\n",
        "      - skips \"initial eval + best save\" when resuming (optional: can still eval if you want)\n",
        "\n",
        "    Required globals (same as your current code expects):\n",
        "      device, use_amp, amp_dtype\n",
        "    Required symbols:\n",
        "      ASMLanguageModel, ASMTrainConfig, WarmupCosine\n",
        "      build_or_load_token_stream, build_or_load_validation_windows, WikiTextRandomWindowStream\n",
        "      evaluate, save_ckpt, _layer_param_summaries, _fmt_stats\n",
        "    \"\"\"\n",
        "    import os, math, random, time\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.nn.functional as F\n",
        "    from torch.utils.data import DataLoader\n",
        "    from tqdm.auto import tqdm\n",
        "\n",
        "    random.seed(cfg.seed)\n",
        "    torch.manual_seed(cfg.seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(cfg.seed)\n",
        "\n",
        "    # ---- accumulation defaults (safe even if cfg lacks the fields) ----\n",
        "    micro_bs = int(getattr(cfg, \"micro_batch_size\", cfg.batch_size))\n",
        "    accum_steps = int(getattr(cfg, \"grad_accum_steps\", 1))\n",
        "    assert micro_bs >= 1\n",
        "    assert accum_steps >= 1\n",
        "    eff_bs = micro_bs * accum_steps\n",
        "\n",
        "    # ---------- Data prep (cached streams) ----------\n",
        "    os.makedirs(cfg.cache_dir, exist_ok=True)\n",
        "    train_stream_cache = os.path.join(cfg.cache_dir, f\"{cfg.dataset_config}_train_stream.pkl\")\n",
        "    val_stream_cache   = os.path.join(cfg.cache_dir, f\"{cfg.dataset_config}_val_stream.pkl\")\n",
        "\n",
        "    train_stream = build_or_load_token_stream(\n",
        "        cache_path=train_stream_cache,\n",
        "        dataset_name=cfg.dataset_name,\n",
        "        dataset_config=cfg.dataset_config,\n",
        "        split=\"train\",\n",
        "        tokenizer_name=cfg.tokenizer_name,\n",
        "        min_chars=1,\n",
        "        add_eos_between_rows=True,\n",
        "    )\n",
        "    val_stream = build_or_load_token_stream(\n",
        "        cache_path=val_stream_cache,\n",
        "        dataset_name=cfg.dataset_name,\n",
        "        dataset_config=cfg.dataset_config,\n",
        "        split=\"validation\",\n",
        "        tokenizer_name=cfg.tokenizer_name,\n",
        "        min_chars=1,\n",
        "        add_eos_between_rows=True,\n",
        "    )\n",
        "\n",
        "    val_dataset = build_or_load_validation_windows(\n",
        "        cache_path=cfg.val_windows_cache,\n",
        "        token_stream=val_stream,\n",
        "        max_seq_len=cfg.max_seq_len,\n",
        "        stride_frac=cfg.stride_frac_val,\n",
        "        val_samples_target=cfg.val_samples_target,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=micro_bs,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "        pin_memory=torch.cuda.is_available(),\n",
        "    )\n",
        "\n",
        "    train_ds = WikiTextRandomWindowStream(\n",
        "        token_stream=train_stream,\n",
        "        max_seq_len=cfg.max_seq_len,\n",
        "        train_samples_target=cfg.train_samples_target,\n",
        "        seed=cfg.seed,\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=micro_bs,\n",
        "        num_workers=3,\n",
        "        pin_memory=torch.cuda.is_available(),\n",
        "    )\n",
        "\n",
        "    # ---------- Model ----------\n",
        "    model = ASMLanguageModel(\n",
        "        vocab_size=cfg.vocab_size,\n",
        "        embed_dim=cfg.embed_dim,\n",
        "        num_layers=cfg.num_layers,\n",
        "        num_heads=cfg.num_heads,\n",
        "        num_slots=cfg.num_slots,\n",
        "        max_seq_len=cfg.max_seq_len,\n",
        "        mlp_ratio=cfg.mlp_ratio,\n",
        "        dropout=cfg.dropout,\n",
        "\n",
        "        read_temperature=cfg.read_temperature,\n",
        "        write_temperature=cfg.write_temperature,\n",
        "        state_fp32=cfg.state_fp32,\n",
        "        slot_dropout=cfg.slot_dropout,\n",
        "        normalize_k=cfg.normalize_k,\n",
        "\n",
        "        tie_weights=cfg.tie_weights,\n",
        "\n",
        "        use_abs_pos=cfg.use_abs_pos,\n",
        "\n",
        "        use_rope_keys=cfg.use_rope_keys,\n",
        "        rope_base=cfg.rope_base,\n",
        "        use_alibi_write=cfg.use_alibi_write,\n",
        "        alibi_strength_init=cfg.alibi_strength_init,\n",
        "        learn_alibi_strength=cfg.learn_alibi_strength,\n",
        "        min_strength=cfg.min_strength,\n",
        "\n",
        "        use_content_read=cfg.use_content_read,\n",
        "        content_read_init=cfg.content_read_init,\n",
        "        content_read_max_gamma=cfg.content_read_max_gamma,\n",
        "\n",
        "        use_slotspace_refine=cfg.use_slotspace_refine,\n",
        "        slotspace_dim=cfg.slotspace_dim,\n",
        "        slotspace_gate_init=cfg.slotspace_gate_init,\n",
        "        slotspace_dropout=cfg.slotspace_dropout,\n",
        "        slotspace_signed_weights=cfg.slotspace_signed_weights,\n",
        "\n",
        "        use_rope_slotspace=cfg.use_rope_slotspace,\n",
        "        rope_base_slotspace=cfg.rope_base_slotspace,\n",
        "\n",
        "        write_chunk_size=cfg.write_chunk_size,\n",
        "        enable_compiled=cfg.enable_compiled,\n",
        "    ).to(device)\n",
        "\n",
        "    out_dir = os.path.join(cfg.output_dir, cfg.tag)\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    n_params = sum(p.numel() for p in model.parameters())\n",
        "    print(\"=\" * 108)\n",
        "    print(f\"Training [{cfg.tag}] on {cfg.dataset_name}/{cfg.dataset_config}\")\n",
        "    print(f\"Params: {n_params:,}\")\n",
        "    print(f\"Train tokens: {len(train_stream):,} | Val tokens: {len(val_stream):,} | Val windows: {len(val_dataset):,}\")\n",
        "    print(f\"T={cfg.max_seq_len} | val_stride_frac={cfg.stride_frac_val} | last_k={cfg.analytics_last_k}\")\n",
        "    print(f\"Batching: micro_bs={micro_bs} | accum_steps={accum_steps} | effective_bs={eff_bs}\")\n",
        "    print(f\"Chunks: write={cfg.write_chunk_size} | amp={use_amp}({amp_dtype}) | state_fp32={cfg.state_fp32}\")\n",
        "    print(f\"RoPE: keys={cfg.use_rope_keys}(base={cfg.rope_base:g}) | slotspace={cfg.use_rope_slotspace}(base={cfg.rope_base_slotspace:g})\")\n",
        "    print(\"=\" * 108)\n",
        "\n",
        "    # ---------- Optimizer + Scheduler ----------\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay, betas=cfg.betas)\n",
        "    sched = WarmupCosine(opt, cfg.warmup_steps, cfg.total_steps, cfg.learning_rate)\n",
        "\n",
        "    # ---------- Resume (if requested) ----------\n",
        "    resume_path = getattr(cfg, \"resume_path\", None)\n",
        "    start_step = 0\n",
        "    best_val = float(\"inf\")\n",
        "\n",
        "    if resume_path is not None and os.path.exists(resume_path):\n",
        "        print(f\"Resuming from checkpoint: {resume_path}\")\n",
        "        ckpt = torch.load(resume_path, map_location=device)\n",
        "        # Strict load by default to catch config/shape mismatches early\n",
        "        model.load_state_dict(ckpt[\"model\"], strict=True)\n",
        "        if \"opt\" in ckpt and ckpt[\"opt\"] is not None:\n",
        "            opt.load_state_dict(ckpt[\"opt\"])\n",
        "        start_step = int(ckpt.get(\"step\", 0))\n",
        "        best_val = float(ckpt.get(\"best_val\", float(\"inf\")))\n",
        "\n",
        "        # Align scheduler to resumed optimizer-step count\n",
        "        sched.step_num = start_step\n",
        "\n",
        "        # Optional sanity eval on resume (off by default for speed)\n",
        "        if bool(getattr(cfg, \"eval_on_resume\", False)):\n",
        "            vloss, vppl, vstats = evaluate(model, val_loader, max_batches=cfg.eval_max_batches, last_k=cfg.analytics_last_k)\n",
        "            print(f\"[VAL resume@step {start_step}] loss={vloss:.3f} ppl={vppl:.2f}\")\n",
        "            if vstats:\n",
        "                print(\"  \" + _fmt_stats(vstats, last_k=cfg.analytics_last_k))\n",
        "\n",
        "    else:\n",
        "        # ---------- Initial eval (fresh run) ----------\n",
        "        vloss, vppl, vstats = evaluate(model, val_loader, max_batches=cfg.eval_max_batches, last_k=cfg.analytics_last_k)\n",
        "        best_val = vloss\n",
        "        save_ckpt(os.path.join(out_dir, \"best.pt\"), cfg, model, opt, 0, best_val)\n",
        "\n",
        "        print(f\"[VAL step 0] loss={vloss:.3f} ppl={vppl:.2f}\")\n",
        "        if vstats:\n",
        "            print(\"  \" + _fmt_stats(vstats, last_k=cfg.analytics_last_k))\n",
        "\n",
        "    # ---------- Training loop (grad accumulation) ----------\n",
        "    model.train()\n",
        "    opt.zero_grad(set_to_none=True)\n",
        "\n",
        "    running = 0.0      # sum of *unscaled* microbatch loss for logging\n",
        "    step = start_step  # optimizer steps completed\n",
        "    micro_step = 0     # microbatches since (re)start\n",
        "    t_last = time.time()\n",
        "\n",
        "    # tqdm counts optimizer steps (matches cfg.total_steps, sched, eval_interval)\n",
        "    pbar = tqdm(total=cfg.total_steps, initial=step, desc=f\"[{cfg.tag}]\")\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "        if step >= cfg.total_steps:\n",
        "            break\n",
        "\n",
        "        xb = xb.to(device, non_blocking=True)\n",
        "        yb = yb.to(device, non_blocking=True)\n",
        "        micro_step += 1\n",
        "\n",
        "        with torch.autocast(device_type=device.type, dtype=amp_dtype, enabled=use_amp):\n",
        "            logits = model(xb)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), yb.view(-1))\n",
        "\n",
        "        running += float(loss.item())\n",
        "        (loss / accum_steps).backward()\n",
        "\n",
        "        if (micro_step % accum_steps) == 0:\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
        "            opt.step()\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            lr = sched.step()\n",
        "\n",
        "            step += 1\n",
        "            pbar.update(1)\n",
        "\n",
        "            if step % cfg.log_interval == 0:\n",
        "                # average microbatch loss across the last log window\n",
        "                avg = running / (cfg.log_interval * accum_steps)\n",
        "                running = 0.0\n",
        "\n",
        "                ps = _layer_param_summaries(model)\n",
        "                it_s = cfg.log_interval / max(1e-9, (time.time() - t_last))\n",
        "                t_last = time.time()\n",
        "\n",
        "                postfix = {\n",
        "                    \"loss\": f\"{avg:.3f}\",\n",
        "                    \"ppl\": f\"{math.exp(min(20.0, avg)):.2f}\",\n",
        "                    \"lr\": f\"{lr:.2e}\",\n",
        "                    \"it/s\": f\"{it_s:.2f}\",\n",
        "                }\n",
        "                if \"content_read_gamma_mean\" in ps: postfix[\"Î³Î¼\"] = f\"{ps['content_read_gamma_mean']:.3f}\"\n",
        "                if \"slotspace_gate_mean\" in ps: postfix[\"sgÎ¼\"] = f\"{ps['slotspace_gate_mean']:.3f}\"\n",
        "                pbar.set_postfix(postfix)\n",
        "\n",
        "                msg = f\"[step {step}] train_loss={avg:.3f} ppl={math.exp(min(20.0, avg)):.2f} lr={lr:.2e} it/s={it_s:.2f}\"\n",
        "                if \"content_read_gamma_mean\" in ps: msg += f\" | content_read_gamma_mean={ps['content_read_gamma_mean']:.4f}\"\n",
        "                if \"slotspace_gate_mean\" in ps: msg += f\" | slotspace_gate_mean={ps['slotspace_gate_mean']:.4f}\"\n",
        "                if \"alibi_strength_mean\" in ps: msg += f\" | alibi_strength_mean={ps['alibi_strength_mean']:.4f}\"\n",
        "                print(msg)\n",
        "\n",
        "            if step % cfg.eval_interval == 0:\n",
        "                vloss, vppl, vstats = evaluate(model, val_loader, max_batches=cfg.eval_max_batches, last_k=cfg.analytics_last_k)\n",
        "                print(f\"\\n[VAL step {step}] loss={vloss:.3f} ppl={vppl:.2f}\")\n",
        "                if vstats:\n",
        "                    print(\"  \" + _fmt_stats(vstats, last_k=cfg.analytics_last_k))\n",
        "\n",
        "                if vloss < best_val:\n",
        "                    best_val = vloss\n",
        "                    save_ckpt(os.path.join(out_dir, \"best.pt\"), cfg, model, opt, step, best_val)\n",
        "\n",
        "            # Optional: periodic \"last\" checkpoint so you can resume even if best isn't improving\n",
        "            save_last_every = int(getattr(cfg, \"save_last_every\", 0))\n",
        "            if save_last_every > 0 and (step % save_last_every) == 0:\n",
        "                save_ckpt(os.path.join(out_dir, \"last.pt\"), cfg, model, opt, step, best_val)\n",
        "\n",
        "    # NOTE: If we exit mid-accumulation (micro_step not divisible by accum_steps), we do NOT flush grads\n",
        "    # by default, to preserve a clean \"optimizer step = accum_steps microbatches\" invariant.\n",
        "\n",
        "    save_ckpt(os.path.join(out_dir, \"final.pt\"), cfg, model, opt, step, best_val)\n",
        "    print(f\"[{cfg.tag}] Done. Best val loss: {best_val:.4f} | last step: {step}\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmPm7xqgMCuM"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KhWEQY9FTqNm",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#@title Launch Run\n",
        "\n",
        "# =========================================================\n",
        "# Run\n",
        "# =========================================================\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # =========================================================\n",
        "    # Device + AMP\n",
        "    # =========================================================\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    use_amp = torch.cuda.is_available()\n",
        "    amp_dtype = torch.bfloat16  # A100-friendly\n",
        "\n",
        "    print(\"Using device:\", device)\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "        print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory/1e9:.1f} GB\")\n",
        "\n",
        "    cfg = ASMTrainConfig(\n",
        "        embed_dim=1024,\n",
        "        max_seq_len=1024,\n",
        "        num_heads=16,\n",
        "        num_layers=12,\n",
        "        num_slots=32,\n",
        "        slotspace_dim=128,\n",
        "        batch_size=4,\n",
        "        total_steps=200_000,\n",
        "        warmup_steps=3_000,\n",
        "\n",
        "        state_fp32=False,\n",
        "\n",
        "        use_content_read=True,\n",
        "        content_read_init=-2.0,\n",
        "        content_read_max_gamma=3.0,\n",
        "\n",
        "        write_chunk_size=128,\n",
        "        enable_compiled=False,\n",
        "        #slotspace_chunk_size=256,\n",
        "        use_slotspace_refine=True,\n",
        "        slotspace_gate_init=-3.0,\n",
        "        tag=\"asm_wikitext_1024t_1024d_32h_64sd_64s_128cs_12l\",\n",
        "        val_windows_cache=\"./drive/MyDrive/asm_nlp/val_cache_wikitext_windows_1024.pkl\",\n",
        "    )\n",
        "    print(\"CONFIG:\")\n",
        "    print(cfg)\n",
        "    train_asm(cfg)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "5P7GK1cnB2Zs"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "#@title Launch Run\n",
        "\n",
        "# =========================================================\n",
        "# Run\n",
        "# =========================================================\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # =========================================================\n",
        "    # Device + AMP\n",
        "    # =========================================================\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    use_amp = torch.cuda.is_available()\n",
        "    amp_dtype = torch.bfloat16  # A100-friendly\n",
        "\n",
        "    print(\"Using device:\", device)\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "        print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory/1e9:.1f} GB\")\n",
        "\n",
        "    cfg = ASMTrainConfig(\n",
        "        embed_dim=1024,\n",
        "        max_seq_len=1024,\n",
        "        num_heads=16,\n",
        "        num_layers=15,\n",
        "        num_slots=32,\n",
        "        slotspace_dim=128,\n",
        "        batch_size=2,\n",
        "        total_steps=50_000,\n",
        "        warmup_steps=300,\n",
        "\n",
        "        state_fp32=False,\n",
        "\n",
        "        use_content_read=True,\n",
        "        content_read_init=-2.0,\n",
        "        content_read_max_gamma=3.0,\n",
        "\n",
        "        write_chunk_size=128,\n",
        "        enable_compiled=True,\n",
        "        #slotspace_chunk_size=256,\n",
        "        use_slotspace_refine=True,\n",
        "        slotspace_gate_init=-3.0,\n",
        "        tag=\"asm_wikitext_1024t_1024d_32h_64sd_64s_128cs_12l\",\n",
        "        val_windows_cache=\"./drive/MyDrive/asm_nlp/val_cache_wikitext_windows_1024.pkl\",\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    # Preferred (explicit) if you added these attrs to cfg (train_asm uses getattr so it's safe):\n",
        "    cfg.micro_batch_size = 2\n",
        "    cfg.grad_accum_steps = 16\n",
        "\n",
        "    # If you want the *same token budget* as before, reduce train_samples_target by accum_steps\n",
        "    # because each optimizer step consumes accum_steps microbatches.\n",
        "    # Only do this if your train_samples_target is \"number of batches\" (it is, in your iterable).\n",
        "    cfg.train_samples_target = cfg.total_steps * cfg.grad_accum_steps\n",
        "\n",
        "    cfg.resume_path = \"./drive/MyDrive/asm_outputs/asm_wikitext_1024t_1024d_32h_64sd_64s_128cs_12l/best.pt\"\n",
        "    # Optional but recommended so you can resume even if best doesn't change:\n",
        "    cfg.save_last_every = 500  # saves last.pt every 500 optimizer steps\n",
        "    # Optional sanity check:\n",
        "    cfg.eval_on_resume = True\n",
        "\n",
        "    print(\"CONFIG:\")\n",
        "    print(cfg)\n",
        "\n",
        "    model = train_asm(cfg)\n",
        "\n",
        "#"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}