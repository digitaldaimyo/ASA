{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/digitaldaimyo/ASA/blob/main/building_blocks/ASAResearch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQmKZaH3JjJS"
      },
      "source": [
        "# Paper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VXCkwFnDIuE9"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Addressed State Attention\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Optional, Dict, Tuple\n",
        "\n",
        "# -------------------------\n",
        "# RoPE helper (rotate-half)\n",
        "# -------------------------\n",
        "def _rotate_half(x: torch.Tensor) -> torch.Tensor:\n",
        "    x1 = x[..., ::2]\n",
        "    x2 = x[..., 1::2]\n",
        "    return torch.stack((-x2, x1), dim=-1).flatten(-2)\n",
        "\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    def __init__(self, dim: int, base: float = 10000.0):\n",
        "        super().__init__()\n",
        "        assert dim % 2 == 0, \"RoPE requires even dim\"\n",
        "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
        "        self._cos_cached = None\n",
        "        self._sin_cached = None\n",
        "        self._t_cached = None\n",
        "        self._device_cached = None\n",
        "\n",
        "    def get_cos_sin(self, T: int, device, dtype):\n",
        "        if (\n",
        "            self._t_cached == T\n",
        "            and self._cos_cached is not None\n",
        "            and self._device_cached == device\n",
        "        ):\n",
        "            return self._cos_cached.to(dtype=dtype), self._sin_cached.to(dtype=dtype)\n",
        "\n",
        "        t = torch.arange(T, device=device, dtype=self.inv_freq.dtype)\n",
        "        freqs = torch.einsum(\"t,f->tf\", t, self.inv_freq)  # [T, d/2]\n",
        "        emb = torch.cat([freqs, freqs], dim=-1)            # [T, d]\n",
        "        cos = emb.cos()[None, None, :, :]                  # [1,1,T,d]\n",
        "        sin = emb.sin()[None, None, :, :]                  # [1,1,T,d]\n",
        "\n",
        "        self._t_cached = T\n",
        "        self._device_cached = device\n",
        "        self._cos_cached = cos\n",
        "        self._sin_cached = sin\n",
        "        return cos.to(dtype=dtype), sin.to(dtype=dtype)\n",
        "\n",
        "def apply_rope(x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> torch.Tensor:\n",
        "    return (x * cos) + (_rotate_half(x) * sin)\n",
        "\n",
        "# -------------------------\n",
        "# ALiBi slopes helper\n",
        "# -------------------------\n",
        "def alibi_slopes(num_heads: int, device=None, dtype=torch.float32) -> torch.Tensor:\n",
        "    def get_slopes(n):\n",
        "        def power_of_2_slopes(n):\n",
        "            start = 2.0 ** (-(2.0 ** -(math.log2(n) - 3)))\n",
        "            ratio = start\n",
        "            return [start * (ratio ** i) for i in range(n)]\n",
        "        if math.log2(n).is_integer():\n",
        "            return power_of_2_slopes(n)\n",
        "        closest = 2 ** math.floor(math.log2(n))\n",
        "        return power_of_2_slopes(closest) + get_slopes(2 * closest)[0::2][: n - closest]\n",
        "    return torch.tensor(get_slopes(num_heads), device=device, dtype=dtype)  # [H]\n",
        "\n",
        "# -------------------------\n",
        "# softplus init helpers\n",
        "# -------------------------\n",
        "def _inv_softplus(y: torch.Tensor) -> torch.Tensor:\n",
        "    return torch.log(torch.expm1(y))\n",
        "\n",
        "# -------------------------\n",
        "# Linear attention feature map (Performer-style)\n",
        "# -------------------------\n",
        "def phi(x: torch.Tensor) -> torch.Tensor:\n",
        "    return F.elu(x) + 1.0\n",
        "\n",
        "\n",
        "class AddressedStateAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Addressed State Attention (ASA):\n",
        "      - prefix-softmax WRITE into slots (O(T))\n",
        "      - READ routing from tokens -> slots (softmax over slots)\n",
        "      - content-conditioned READ term (gamma)\n",
        "      - RoPE on write keys (geometry)\n",
        "      - ALiBi bias on write logits (prefix-friendly)\n",
        "\n",
        "    slot-space refinement:\n",
        "      - causal linear attention in a low-dim slot-address coordinate space\n",
        "      - produces per-token signed weights over slots\n",
        "      - decoded through the same streaming slot-state basis\n",
        "      - gated by learnable slotspace_gate (softplus)\n",
        "\n",
        "    PERF (behavior-preserving):\n",
        "      - Streaming prefix write states in chunks (no [B,H,K,T,d] materialization)\n",
        "      - Slot-space prefix scan is chunked (exact)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int,\n",
        "        num_heads: int = 8,\n",
        "        num_slots: int = 8,\n",
        "        dropout: float = 0.1,\n",
        "\n",
        "        # temperatures / numerics\n",
        "        read_temperature: float = 1.0,\n",
        "        write_temperature: float = 1.0,\n",
        "        state_fp32: bool = True,\n",
        "        slot_dropout: float = 0.0,\n",
        "        normalize_k: bool = False,\n",
        "\n",
        "        # positions (write geometry)\n",
        "        use_rope_keys: bool = True,\n",
        "        rope_base: float = 10000.0,\n",
        "\n",
        "        # write bias (ALiBi)\n",
        "        use_alibi_write: bool = True,\n",
        "        alibi_strength_init: float = 0.1,\n",
        "        learn_alibi_strength: bool = True,\n",
        "        min_strength: float = 0.0,\n",
        "\n",
        "        # content-conditioned read term\n",
        "        use_content_read: bool = True,\n",
        "        content_read_init: float = -4.0,\n",
        "        content_read_max_gamma: float = 3.0,\n",
        "\n",
        "        # slot-space refinement\n",
        "        use_slotspace_refine: bool = True,\n",
        "        slotspace_dim: int = 32,\n",
        "        slotspace_gate_init: float = -4.0,\n",
        "        slotspace_dropout: float = 0.05,\n",
        "        slotspace_signed_weights: bool = True,\n",
        "\n",
        "        # RoPE in slot-space matcher (Q/K only)\n",
        "        use_rope_slotspace: bool = True,\n",
        "        rope_base_slotspace: float = 100000.0,\n",
        "\n",
        "        # perf knobs (no behavior change)\n",
        "        write_chunk_size: int = 128,\n",
        "        slotspace_chunk_size: int = 128,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.num_slots = num_slots\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.read_temperature = float(read_temperature)\n",
        "        self.write_temperature = float(write_temperature)\n",
        "        self.state_fp32 = bool(state_fp32)\n",
        "        self.slot_dropout = float(slot_dropout)\n",
        "        self.normalize_k = bool(normalize_k)\n",
        "        self.routing_override = None\n",
        "\n",
        "        self.use_rope_keys = bool(use_rope_keys)\n",
        "        self.use_alibi_write = bool(use_alibi_write)\n",
        "        self.learn_alibi_strength = bool(learn_alibi_strength)\n",
        "        self.min_strength = float(min_strength)\n",
        "\n",
        "        self.use_content_read = bool(use_content_read)\n",
        "        self.content_read_max_gamma = float(content_read_max_gamma)\n",
        "\n",
        "        self.use_slotspace_refine = bool(use_slotspace_refine)\n",
        "        self.slotspace_dim = int(slotspace_dim)\n",
        "        self.slotspace_dropout = nn.Dropout(float(slotspace_dropout))\n",
        "        self.slotspace_signed_weights = bool(slotspace_signed_weights)\n",
        "\n",
        "        self.write_chunk_size = int(write_chunk_size)\n",
        "        self.slotspace_chunk_size = int(slotspace_chunk_size)\n",
        "\n",
        "        # Learned slot keys per head: [H,K,d]\n",
        "        self.slot_keys = nn.Parameter(\n",
        "            torch.randn(num_heads, num_slots, self.head_dim) / math.sqrt(self.head_dim)\n",
        "        )\n",
        "\n",
        "        # Projections\n",
        "        self.Wk_write = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.Wv_write = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.Wq_read  = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "\n",
        "        # RoPE (write geometry)\n",
        "        self.rope = RotaryEmbedding(self.head_dim, base=rope_base) if self.use_rope_keys else None\n",
        "\n",
        "        # ALiBi slopes (buffer)\n",
        "        if self.use_alibi_write:\n",
        "            self.register_buffer(\"_alibi_slopes\", alibi_slopes(num_heads), persistent=False)  # [H]\n",
        "        else:\n",
        "            self.register_buffer(\"_alibi_slopes\", torch.zeros(num_heads), persistent=False)\n",
        "\n",
        "        # Learnable ALiBi strength (positive via softplus)\n",
        "        if self.use_alibi_write and self.learn_alibi_strength:\n",
        "            init = torch.tensor(float(alibi_strength_init) - self.min_strength).clamp_min(1e-8)\n",
        "            self._alibi_strength_param = nn.Parameter(_inv_softplus(init))\n",
        "        else:\n",
        "            self._alibi_strength_param = None\n",
        "            self.alibi_strength = float(alibi_strength_init)\n",
        "\n",
        "        # Content read gamma (>=0 via softplus)\n",
        "        if self.use_content_read:\n",
        "            self._content_read_gamma_raw = nn.Parameter(torch.tensor(float(content_read_init)))\n",
        "        else:\n",
        "            self._content_read_gamma_raw = None\n",
        "\n",
        "        # -------------------------\n",
        "        # slot-space refinement\n",
        "        # -------------------------\n",
        "        self.use_rope_slotspace = bool(use_rope_slotspace) and bool(self.use_slotspace_refine)\n",
        "        if self.use_slotspace_refine:\n",
        "            self.slot_in  = nn.Linear(num_slots, self.slotspace_dim, bias=False)\n",
        "            self.slot_q   = nn.Linear(self.slotspace_dim, self.slotspace_dim, bias=False)\n",
        "            self.slot_k   = nn.Linear(self.slotspace_dim, self.slotspace_dim, bias=False)\n",
        "            self.slot_v   = nn.Linear(self.slotspace_dim, self.slotspace_dim, bias=False)\n",
        "            self.slot_out = nn.Linear(self.slotspace_dim, num_slots, bias=False)\n",
        "            self._slotspace_gate_raw = nn.Parameter(torch.tensor(float(slotspace_gate_init)))\n",
        "\n",
        "            if self.use_rope_slotspace:\n",
        "                assert (self.slotspace_dim % 2) == 0, \"use_rope_slotspace requires even slotspace_dim\"\n",
        "                self.rope_slotspace = RotaryEmbedding(self.slotspace_dim, base=float(rope_base_slotspace))\n",
        "            else:\n",
        "                self.rope_slotspace = None\n",
        "        else:\n",
        "            self.slot_in = None\n",
        "            self.slot_q = self.slot_k = self.slot_v = None\n",
        "            self.slot_out = None\n",
        "            self._slotspace_gate_raw = None\n",
        "            self.rope_slotspace = None\n",
        "\n",
        "    def _alibi_strength(self, dtype, device) -> torch.Tensor:\n",
        "        if not (self.use_alibi_write and self.learn_alibi_strength):\n",
        "            return torch.tensor(self.alibi_strength, dtype=dtype, device=device)\n",
        "        return (F.softplus(self._alibi_strength_param) + self.min_strength).to(dtype=dtype, device=device)\n",
        "\n",
        "    def _content_read_gamma(self, dtype, device) -> torch.Tensor:\n",
        "        if not self.use_content_read:\n",
        "            return torch.tensor(0.0, dtype=dtype, device=device)\n",
        "        g = F.softplus(self._content_read_gamma_raw)  # >= 0\n",
        "        if self.content_read_max_gamma is not None and self.content_read_max_gamma > 0:\n",
        "            g = g.clamp(max=self.content_read_max_gamma)\n",
        "        return g.to(dtype=dtype, device=device)\n",
        "\n",
        "    def _slotspace_gate(self, dtype, device) -> torch.Tensor:\n",
        "        if not self.use_slotspace_refine:\n",
        "            return torch.tensor(0.0, dtype=dtype, device=device)\n",
        "        return F.softplus(self._slotspace_gate_raw).to(dtype=dtype, device=device)\n",
        "\n",
        "    @staticmethod\n",
        "    def _safe_exp_sub_max(s: torch.Tensor, m: torch.Tensor) -> torch.Tensor:\n",
        "        diff = s - m\n",
        "        diff = diff.masked_fill(~torch.isfinite(m), float(\"-inf\"))\n",
        "        return torch.exp(diff)\n",
        "\n",
        "    #@torch.compile\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        return_info: bool = False,\n",
        "        routing_mode: str = \"softmax\",           # \"softmax\" | \"top1\" | \"topk\" | \"external\"\n",
        "        routing_topk: int = 2,                   # used if routing_mode==\"topk\"\n",
        "        read_weights_override: Optional[torch.Tensor] = None,  # [B,H,T,K] or [B,H,L,K]\n",
        "        routing_noise: Optional[str] = None,     # None | \"gumbel\" | \"gaussian\"\n",
        "        routing_noise_scale: float = 1.0,\n",
        "\n",
        "    ) -> Tuple[torch.Tensor, Optional[Dict[str, torch.Tensor]]]:\n",
        "        B, T, C = x.shape\n",
        "        H, K, d = self.num_heads, self.num_slots, self.head_dim\n",
        "\n",
        "        # Project (write K/V, read Q)\n",
        "        k_write = self.Wk_write(x).view(B, T, H, d).transpose(1, 2)  # [B,H,T,d]\n",
        "        v_write = self.Wv_write(x).view(B, T, H, d).transpose(1, 2)  # [B,H,T,d]\n",
        "        q_read  = self.Wq_read(x).view(B, T, H, d).transpose(1, 2)   # [B,H,T,d]\n",
        "\n",
        "        if self.normalize_k:\n",
        "            k_write = F.normalize(k_write, dim=-1, eps=1e-8)\n",
        "\n",
        "        # RoPE on write keys\n",
        "        if self.use_rope_keys:\n",
        "            cos, sin = self.rope.get_cos_sin(T, device=x.device, dtype=k_write.dtype)\n",
        "            k_write = apply_rope(k_write, cos, sin)\n",
        "\n",
        "        # Slot dropout\n",
        "        slot_keys = self.slot_keys\n",
        "        if self.training and self.slot_dropout > 0.0:\n",
        "            drop = (torch.rand((H, K), device=x.device) < self.slot_dropout)\n",
        "            slot_keys = slot_keys * (~drop).to(slot_keys.dtype).unsqueeze(-1)\n",
        "\n",
        "        # WRITE logits: [B,H,K,T]\n",
        "        write_logits_raw = torch.einsum(\"hkd,bhtd->bhkt\", slot_keys, k_write) / math.sqrt(d)\n",
        "\n",
        "        # Stable dtype for prefix-softmax math\n",
        "        state_dtype = torch.float32 if (self.state_fp32 and x.dtype != torch.float32) else x.dtype\n",
        "        write_logits = write_logits_raw.to(state_dtype)\n",
        "\n",
        "        # Write temperature\n",
        "        wtemp = max(1e-6, self.write_temperature)\n",
        "        write_logits = write_logits / wtemp\n",
        "\n",
        "        # ALiBi distance bias (prefix-friendly)\n",
        "        alibi_bias_applied = None\n",
        "        if self.use_alibi_write:\n",
        "            strength = self._alibi_strength(dtype=state_dtype, device=x.device)  # scalar\n",
        "            slopes = self._alibi_slopes.to(device=x.device, dtype=state_dtype) * strength  # [H]\n",
        "            pos_i = torch.arange(T, device=x.device, dtype=state_dtype)  # [T]\n",
        "            alibi_bias = slopes.view(1, H, 1, 1) * pos_i.view(1, 1, 1, T) # [1,H,1,T]\n",
        "            write_logits = write_logits + alibi_bias\n",
        "            alibi_bias_applied = alibi_bias\n",
        "\n",
        "        # Key padding mask (mask positions that are padding)\n",
        "        if attention_mask is not None:\n",
        "            valid = attention_mask.to(dtype=torch.bool)\n",
        "            write_logits = write_logits.masked_fill(~valid.view(B, 1, 1, T), float(\"-inf\"))\n",
        "        else:\n",
        "            valid = None\n",
        "\n",
        "        # =====================================================\n",
        "        # STREAMING WRITE + READ (no [B,H,K,T,d] slot states)\n",
        "        # =====================================================\n",
        "        content_read_gamma = self._content_read_gamma(dtype=q_read.dtype, device=x.device)\n",
        "        rtemp = max(1e-6, self.read_temperature)\n",
        "\n",
        "        out_h = torch.empty((B, H, T, d), device=x.device, dtype=state_dtype)\n",
        "        read_weights = torch.empty((B, H, T, K), device=x.device, dtype=q_read.dtype)\n",
        "\n",
        "        # Optional analytics: [B,H,T,K] (later permuted to [B,H,K,T])\n",
        "        slot_state_norm_t = torch.empty((B, H, T, K), device=x.device, dtype=torch.float32) if return_info else None\n",
        "\n",
        "        denom_state = torch.zeros((B, H, K), device=x.device, dtype=state_dtype)\n",
        "        numer_state = torch.zeros((B, H, K, d), device=x.device, dtype=state_dtype)\n",
        "        m_state = torch.full((B, H, K), float(\"-inf\"), device=x.device, dtype=state_dtype)\n",
        "\n",
        "        # Optional analytics: full read logits across T (drop-in replacement)\n",
        "        if return_info:\n",
        "            read_logits_full = torch.empty((B, H, T, K), device=x.device, dtype=state_dtype)\n",
        "            read_logits_key_full = torch.empty((B, H, T, K), device=x.device, dtype=state_dtype)\n",
        "            read_logits_content_full = (\n",
        "                torch.empty((B, H, T, K), device=x.device, dtype=state_dtype) if self.use_content_read else None\n",
        "            )\n",
        "        else:\n",
        "            read_logits_full = None\n",
        "            read_logits_key_full = None\n",
        "            read_logits_content_full = None\n",
        "\n",
        "        WRITE_CHUNK = self.write_chunk_size\n",
        "\n",
        "        for t0 in range(0, T, WRITE_CHUNK):\n",
        "            t1 = min(T, t0 + WRITE_CHUNK)\n",
        "            L = t1 - t0\n",
        "\n",
        "            wlog_c = write_logits[:, :, :, t0:t1]  # [B,H,K,L]\n",
        "\n",
        "            # streaming cummax\n",
        "            m_c, _ = torch.cummax(wlog_c, dim=-1)  # [B,H,K,L]\n",
        "            m_new = torch.maximum(m_state.unsqueeze(-1), m_c)  # [B,H,K,L]\n",
        "\n",
        "            # rescale old prefix state to new max reference\n",
        "            scale = torch.exp(m_state.unsqueeze(-1) - m_new)  # [B,H,K,L] (exp(-inf)=0)\n",
        "\n",
        "            denom_c = denom_state.unsqueeze(-1) * scale                  # [B,H,K,L]\n",
        "            numer_c = numer_state.unsqueeze(-2) * scale.unsqueeze(-1)    # [B,H,K,L,d]\n",
        "\n",
        "            # new weights\n",
        "            w_new = self._safe_exp_sub_max(wlog_c, m_new)  # [B,H,K,L]\n",
        "\n",
        "            # accumulate within chunk\n",
        "            denom_c = denom_c + torch.cumsum(w_new, dim=-1)  # [B,H,K,L]\n",
        "            v_c = v_write[:, :, t0:t1, :].to(state_dtype)    # [B,H,L,d]\n",
        "            add = torch.cumsum(w_new.unsqueeze(-1) * v_c.unsqueeze(2), dim=-2)  # [B,H,K,L,d]\n",
        "            numer_c = numer_c + add\n",
        "\n",
        "            # per-token slot state for this chunk only\n",
        "            slot_state_c = numer_c / denom_c.clamp_min(1e-8).unsqueeze(-1)  # [B,H,K,L,d]\n",
        "            slot_state_t = slot_state_c.permute(0, 1, 3, 2, 4).contiguous() # [B,H,L,K,d]\n",
        "\n",
        "            # READ routing logits\n",
        "            q_read_c = q_read[:, :, t0:t1, :]  # [B,H,L,d]\n",
        "\n",
        "            # base (key) term\n",
        "            read_logits_key = torch.einsum(\"bhld,hkd->bhlk\", q_read_c, slot_keys) / math.sqrt(d)\n",
        "\n",
        "            # optional content term\n",
        "            read_logits_content = None\n",
        "            read_logits = read_logits_key\n",
        "            if self.use_content_read:\n",
        "                read_logits_content = torch.einsum(\n",
        "                    \"bhld,bhlkd->bhlk\",\n",
        "                    q_read_c,\n",
        "                    slot_state_t.to(q_read_c.dtype)\n",
        "                ) / math.sqrt(d)\n",
        "                read_logits = read_logits + content_read_gamma.to(read_logits.dtype) * read_logits_content\n",
        "\n",
        "            if return_info:\n",
        "                read_logits_full[:, :, t0:t1, :] = read_logits.to(state_dtype)\n",
        "                read_logits_key_full[:, :, t0:t1, :] = read_logits_key.to(state_dtype)\n",
        "                if self.use_content_read:\n",
        "                    read_logits_content_full[:, :, t0:t1, :] = read_logits_content.to(state_dtype)\n",
        "\n",
        "            # Optional: noise on logits to probe routing stability (off by default)\n",
        "            # You can plumb these as forward() kwargs; see signature snippet below.\n",
        "            if routing_noise is not None:\n",
        "                if routing_noise == \"gumbel\":\n",
        "                    # gumbel(0,1) noise; scale by routing_noise_scale\n",
        "                    u = torch.rand_like(read_logits)\n",
        "                    g = -torch.log(-torch.log(u.clamp_min(1e-8)).clamp_min(1e-8))\n",
        "                    read_logits = read_logits + routing_noise_scale * g\n",
        "                elif routing_noise == \"gaussian\":\n",
        "                    read_logits = read_logits + routing_noise_scale * torch.randn_like(read_logits)\n",
        "                else:\n",
        "                    raise ValueError(f\"Unknown routing_noise={routing_noise}\")\n",
        "\n",
        "\n",
        "            if self.routing_override is not None:\n",
        "                if callable(self.routing_override):\n",
        "                    ctx = {\n",
        "                        \"t0\": t0,\n",
        "                        \"t1\": t1,\n",
        "                        \"B\": B, \"H\": H, \"T\": T, \"K\": K, \"d\": d,\n",
        "                        \"rtemp\": rtemp,\n",
        "                        \"state_dtype\": state_dtype,\n",
        "                        \"q_read_c\": q_read_c,          # [B,H,L,d]\n",
        "                        \"slot_keys\": slot_keys,        # [H,K,d]\n",
        "                        \"slot_state_t\": slot_state_t,  # [B,H,L,K,d] (current prefix slot states)\n",
        "                        \"valid\": valid,                # [B,T] or None\n",
        "                    }\n",
        "\n",
        "\n",
        "\n",
        "                    # must return [B,H,L,K]\n",
        "                    read_w_c = self.routing_override(\n",
        "                        t0, t1, read_logits,    # [B,H,L,K] full (key + content + noise if applied),\n",
        "                        read_logits_key,        # [B,H,L,K] key-only\n",
        "                        read_logits_content,    # [B,H,L,K] or None\n",
        "                        ctx,\n",
        "                    )\n",
        "                else:\n",
        "                    # tensor override: [B,H,T,K]\n",
        "                    read_w_c = self.routing_override[:, :, t0:t1, :].to(read_logits.dtype)\n",
        "\n",
        "                # safety: ensure finite + normalize\n",
        "                read_w_c = torch.nan_to_num(read_w_c, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "                read_w_c = read_w_c.clamp_min(0.0)\n",
        "                read_w_c = read_w_c / read_w_c.sum(dim=-1, keepdim=True).clamp_min(1e-8)\n",
        "\n",
        "            else:\n",
        "\n",
        "                # Routing mode\n",
        "                if routing_mode == \"softmax\":\n",
        "                    read_w_c = torch.softmax(read_logits / rtemp, dim=-1)  # [B,H,L,K]\n",
        "\n",
        "                elif routing_mode == \"top1\":\n",
        "                    # hard one-hot on argmax\n",
        "                    top = read_logits.argmax(dim=-1)  # [B,H,L]\n",
        "                    read_w_c = F.one_hot(top, num_classes=K).to(read_logits.dtype)\n",
        "\n",
        "                elif routing_mode == \"topk\":\n",
        "                    kk = int(routing_topk)\n",
        "                    kk = max(1, min(K, kk))\n",
        "                    # mask out everything except top-k then renormalize with softmax-like\n",
        "                    vals, idx = torch.topk(read_logits, k=kk, dim=-1)\n",
        "                    masked = torch.full_like(read_logits, float(\"-inf\"))\n",
        "                    masked.scatter_(-1, idx, vals)\n",
        "                    read_w_c = torch.softmax(masked / rtemp, dim=-1)\n",
        "\n",
        "                elif routing_mode == \"external\":\n",
        "                    if read_weights_override is None:\n",
        "                        raise ValueError(\"routing_mode='external' requires read_weights_override\")\n",
        "                    # accept either full [B,H,T,K] or chunk [B,H,L,K]\n",
        "                    if read_weights_override.shape[-2] == T:\n",
        "                        read_w_c = read_weights_override[:, :, t0:t1, :]\n",
        "                    else:\n",
        "                        read_w_c = read_weights_override\n",
        "                    # safety: renormalize\n",
        "                    read_w_c = read_w_c / read_w_c.sum(dim=-1, keepdim=True).clamp_min(1e-8)\n",
        "\n",
        "                else:\n",
        "                    raise ValueError(f\"Unknown routing_mode={routing_mode}\")\n",
        "\n",
        "            read_weights[:, :, t0:t1, :] = read_w_c\n",
        "\n",
        "            # token output\n",
        "            out_h[:, :, t0:t1, :] = torch.einsum(\n",
        "                \"bhlk,bhlkd->bhld\",\n",
        "                read_w_c.to(state_dtype),\n",
        "                slot_state_t.to(state_dtype),\n",
        "            )\n",
        "\n",
        "            if return_info:\n",
        "                slot_state_norm_t[:, :, t0:t1, :] = slot_state_t.to(torch.float32).norm(dim=-1)\n",
        "\n",
        "            # update running states to end-of-chunk\n",
        "            m_state = m_new[:, :, :, -1]\n",
        "            denom_state = denom_c[:, :, :, -1]\n",
        "            numer_state = numer_c[:, :, :, -1, :]\n",
        "\n",
        "        # =====================================================\n",
        "        # causal linear attention in slot-space (CHUNKED prefix scan)\n",
        "        # =====================================================\n",
        "        slotspace_delta_norm_mean = None\n",
        "        if self.use_slotspace_refine:\n",
        "            slotspace_dtype = state_dtype\n",
        "            M = self.slotspace_dim\n",
        "\n",
        "            # Encode read weights into slot-space coordinates\n",
        "            u = self.slot_in(read_weights.to(slotspace_dtype))  # [B,H,T,M]\n",
        "            q_s  = self.slot_q(u)\n",
        "            k_s  = self.slot_k(u)\n",
        "            v_s  = self.slot_v(u)\n",
        "\n",
        "            # RoPE in slot-space matcher (Q/K only)\n",
        "            if self.use_rope_slotspace:\n",
        "                cos_s, sin_s = self.rope_slotspace.get_cos_sin(T, device=x.device, dtype=q_s.dtype)\n",
        "                q_s = apply_rope(q_s, cos_s, sin_s)\n",
        "                k_s = apply_rope(k_s, cos_s, sin_s)\n",
        "\n",
        "            qf = phi(q_s)\n",
        "            kf = phi(k_s)\n",
        "\n",
        "            if valid is not None:\n",
        "                mask = valid.view(B, 1, T, 1).to(slotspace_dtype)\n",
        "                qf = qf * mask\n",
        "                kf = kf * mask\n",
        "                v_s = v_s * mask\n",
        "\n",
        "            u2 = torch.empty((B, H, T, M), device=x.device, dtype=slotspace_dtype)\n",
        "\n",
        "            S_state = torch.zeros((B, H, M, M), device=x.device, dtype=slotspace_dtype)\n",
        "            Z_state = torch.zeros((B, H, M), device=x.device, dtype=slotspace_dtype)\n",
        "\n",
        "            SS_CHUNK = self.slotspace_chunk_size\n",
        "\n",
        "            for t0 in range(0, T, SS_CHUNK):\n",
        "                t1 = min(T, t0 + SS_CHUNK)\n",
        "                L = t1 - t0\n",
        "\n",
        "                qf_c = qf[:, :, t0:t1, :]   # [B,H,L,M]\n",
        "                kf_c = kf[:, :, t0:t1, :]   # [B,H,L,M]\n",
        "                v_c  = v_s[:, :, t0:t1, :]  # [B,H,L,M]\n",
        "\n",
        "                kv = torch.einsum(\"bhlm,bhln->bhlmn\", kf_c, v_c)  # [B,H,L,M,M]\n",
        "                S_c = torch.cumsum(kv, dim=2)\n",
        "                Z_c = torch.cumsum(kf_c, dim=2)\n",
        "\n",
        "                S_c = S_c + S_state.unsqueeze(2)\n",
        "                Z_c = (Z_c + Z_state.unsqueeze(2)).clamp_min(1e-8)\n",
        "\n",
        "                num = torch.einsum(\"bhlm,bhlmn->bhln\", qf_c, S_c)\n",
        "                den = torch.einsum(\"bhlm,bhlm->bhl\", qf_c, Z_c).unsqueeze(-1).clamp_min(1e-8)\n",
        "                u2[:, :, t0:t1, :] = num / den\n",
        "\n",
        "                S_state = S_c[:, :, -1, :, :]\n",
        "                Z_state = Z_c[:, :, -1, :]\n",
        "\n",
        "            u2 = self.slotspace_dropout(u2)\n",
        "\n",
        "            # Decode slot weights per token\n",
        "            slot_w = self.slot_out(u2)  # [B,H,T,K]\n",
        "            if self.slotspace_signed_weights:\n",
        "                slot_w = torch.tanh(slot_w)\n",
        "            else:\n",
        "                slot_w = torch.softmax(slot_w, dim=-1)\n",
        "\n",
        "            # Second streaming pass to decode slotspace contribution through slot states\n",
        "            gate = self._slotspace_gate(dtype=state_dtype, device=x.device).to(state_dtype)\n",
        "\n",
        "            denom_state = torch.zeros((B, H, K), device=x.device, dtype=state_dtype)\n",
        "            numer_state = torch.zeros((B, H, K, d), device=x.device, dtype=state_dtype)\n",
        "            m_state = torch.full((B, H, K), float(\"-inf\"), device=x.device, dtype=state_dtype)\n",
        "\n",
        "            delta_norm_sum = torch.zeros((), device=x.device, dtype=torch.float32)\n",
        "            delta_norm_count = 0\n",
        "\n",
        "            for t0 in range(0, T, WRITE_CHUNK):\n",
        "                t1 = min(T, t0 + WRITE_CHUNK)\n",
        "                L = t1 - t0\n",
        "\n",
        "                wlog_c = write_logits[:, :, :, t0:t1]  # [B,H,K,L]\n",
        "                m_c, _ = torch.cummax(wlog_c, dim=-1)\n",
        "                m_new = torch.maximum(m_state.unsqueeze(-1), m_c)\n",
        "\n",
        "                scale = torch.exp(m_state.unsqueeze(-1) - m_new)\n",
        "                denom_c = denom_state.unsqueeze(-1) * scale\n",
        "                numer_c = numer_state.unsqueeze(-2) * scale.unsqueeze(-1)\n",
        "\n",
        "                w_new = self._safe_exp_sub_max(wlog_c, m_new)\n",
        "                denom_c = denom_c + torch.cumsum(w_new, dim=-1)\n",
        "\n",
        "                v_c = v_write[:, :, t0:t1, :].to(state_dtype)  # [B,H,L,d]\n",
        "                add = torch.cumsum(w_new.unsqueeze(-1) * v_c.unsqueeze(2), dim=-2)\n",
        "                numer_c = numer_c + add\n",
        "\n",
        "                slot_state_c = numer_c / denom_c.clamp_min(1e-8).unsqueeze(-1)  # [B,H,K,L,d]\n",
        "                slot_state_t = slot_state_c.permute(0, 1, 3, 2, 4).contiguous() # [B,H,L,K,d]\n",
        "\n",
        "                slot_w_c = slot_w[:, :, t0:t1, :].to(state_dtype)  # [B,H,L,K]\n",
        "                delta_c = torch.einsum(\"bhlk,bhlkd->bhld\", slot_w_c, slot_state_t.to(state_dtype))  # [B,H,L,d]\n",
        "\n",
        "                out_h[:, :, t0:t1, :] = out_h[:, :, t0:t1, :] + gate * delta_c\n",
        "\n",
        "                delta_norm_sum = delta_norm_sum + delta_c.detach().to(torch.float32).norm(dim=-1).sum()\n",
        "                delta_norm_count += (B * H * L)\n",
        "\n",
        "                m_state = m_new[:, :, :, -1]\n",
        "                denom_state = denom_c[:, :, :, -1]\n",
        "                numer_state = numer_c[:, :, :, -1, :]\n",
        "\n",
        "            slotspace_delta_norm_mean = (delta_norm_sum / max(1, delta_norm_count)).detach().cpu()\n",
        "\n",
        "        # Finish\n",
        "        out = out_h.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        out = self.out_proj(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        info = None\n",
        "        if return_info:\n",
        "            info = {\n",
        "                \"write_logits_raw\": write_logits_raw.detach(),\n",
        "                \"write_logits\": write_logits.detach().to(torch.float32),\n",
        "                \"read_weights\": read_weights.detach(),\n",
        "                # [B,H,K,T]\n",
        "                \"slot_state_norm\": slot_state_norm_t.detach().permute(0, 1, 3, 2).contiguous() if slot_state_norm_t is not None else None,\n",
        "                \"content_read_gamma\": content_read_gamma.detach().to(torch.float32).cpu(),\n",
        "            }\n",
        "            if alibi_bias_applied is not None:\n",
        "                info[\"alibi_bias_applied\"] = alibi_bias_applied.detach().to(torch.float32)\n",
        "            if self.use_alibi_write and self.learn_alibi_strength:\n",
        "                info[\"alibi_strength\"] = self._alibi_strength(dtype=torch.float32, device=x.device).detach().cpu()\n",
        "            if self.use_slotspace_refine:\n",
        "                info[\"slotspace_gate\"] = self._slotspace_gate(dtype=torch.float32, device=x.device).detach().cpu()\n",
        "                info[\"use_rope_slotspace\"] = torch.tensor(bool(self.use_rope_slotspace))\n",
        "                if slotspace_delta_norm_mean is not None:\n",
        "                    info[\"slotspace_delta_norm\"] = slotspace_delta_norm_mean\n",
        "\n",
        "            info[\"read_logits\"] = read_logits_full.detach().to(torch.float32)\n",
        "            info[\"read_logits_key\"] = read_logits_key_full.detach().to(torch.float32)\n",
        "            info[\"read_logits_content\"] = (\n",
        "                read_logits_content_full.detach().to(torch.float32) if read_logits_content_full is not None else None\n",
        "            )\n",
        "\n",
        "            info[\"routing_mode\"] = routing_mode\n",
        "\n",
        "        return out, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HIZcvEKJbh9X"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Addressed State Attention (DROP-IN with slot_mask support)\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Optional, Dict, Tuple\n",
        "\n",
        "# -------------------------\n",
        "# RoPE helper (rotate-half)\n",
        "# -------------------------\n",
        "def _rotate_half(x: torch.Tensor) -> torch.Tensor:\n",
        "    x1 = x[..., ::2]\n",
        "    x2 = x[..., 1::2]\n",
        "    return torch.stack((-x2, x1), dim=-1).flatten(-2)\n",
        "\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    def __init__(self, dim: int, base: float = 10000.0):\n",
        "        super().__init__()\n",
        "        assert dim % 2 == 0, \"RoPE requires even dim\"\n",
        "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
        "        self._cos_cached = None\n",
        "        self._sin_cached = None\n",
        "        self._t_cached = None\n",
        "        self._device_cached = None\n",
        "\n",
        "    def get_cos_sin(self, T: int, device, dtype):\n",
        "        if (\n",
        "            self._t_cached == T\n",
        "            and self._cos_cached is not None\n",
        "            and self._device_cached == device\n",
        "        ):\n",
        "            return self._cos_cached.to(dtype=dtype), self._sin_cached.to(dtype=dtype)\n",
        "\n",
        "        t = torch.arange(T, device=device, dtype=self.inv_freq.dtype)\n",
        "        freqs = torch.einsum(\"t,f->tf\", t, self.inv_freq)  # [T, d/2]\n",
        "        emb = torch.cat([freqs, freqs], dim=-1)            # [T, d]\n",
        "        cos = emb.cos()[None, None, :, :]                  # [1,1,T,d]\n",
        "        sin = emb.sin()[None, None, :, :]                  # [1,1,T,d]\n",
        "\n",
        "        self._t_cached = T\n",
        "        self._device_cached = device\n",
        "        self._cos_cached = cos\n",
        "        self._sin_cached = sin\n",
        "        return cos.to(dtype=dtype), sin.to(dtype=dtype)\n",
        "\n",
        "def apply_rope(x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> torch.Tensor:\n",
        "    return (x * cos) + (_rotate_half(x) * sin)\n",
        "\n",
        "# -------------------------\n",
        "# ALiBi slopes helper\n",
        "# -------------------------\n",
        "def alibi_slopes(num_heads: int, device=None, dtype=torch.float32) -> torch.Tensor:\n",
        "    def get_slopes(n):\n",
        "        def power_of_2_slopes(n):\n",
        "            start = 2.0 ** (-(2.0 ** -(math.log2(n) - 3)))\n",
        "            ratio = start\n",
        "            return [start * (ratio ** i) for i in range(n)]\n",
        "        if math.log2(n).is_integer():\n",
        "            return power_of_2_slopes(n)\n",
        "        closest = 2 ** math.floor(math.log2(n))\n",
        "        return power_of_2_slopes(closest) + get_slopes(2 * closest)[0::2][: n - closest]\n",
        "    return torch.tensor(get_slopes(num_heads), device=device, dtype=dtype)  # [H]\n",
        "\n",
        "# -------------------------\n",
        "# softplus init helpers\n",
        "# -------------------------\n",
        "def _inv_softplus(y: torch.Tensor) -> torch.Tensor:\n",
        "    return torch.log(torch.expm1(y))\n",
        "\n",
        "# -------------------------\n",
        "# Linear attention feature map (Performer-style)\n",
        "# -------------------------\n",
        "def phi(x: torch.Tensor) -> torch.Tensor:\n",
        "    return F.elu(x) + 1.0\n",
        "\n",
        "\n",
        "class AddressedStateAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Addressed State Attention (ASA):\n",
        "      - prefix-softmax WRITE into slots (O(T))\n",
        "      - READ routing from tokens -> slots (softmax over slots)\n",
        "      - optional content-conditioned READ term (gamma)\n",
        "      - RoPE on write keys (geometry)\n",
        "      - ALiBi bias on write logits (prefix-friendly)\n",
        "\n",
        "    Optional slot-space refinement:\n",
        "      - causal linear attention in low-dim slot-address coordinate space\n",
        "      - gated by learnable slotspace_gate (softplus)\n",
        "\n",
        "    PERF:\n",
        "      - Streaming prefix write states in chunks\n",
        "      - Slot-space prefix scan chunked (exact)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int,\n",
        "        num_heads: int = 8,\n",
        "        num_slots: int = 8,\n",
        "        dropout: float = 0.1,\n",
        "\n",
        "        read_temperature: float = 1.0,\n",
        "        write_temperature: float = 1.0,\n",
        "        state_fp32: bool = True,\n",
        "        slot_dropout: float = 0.0,\n",
        "        normalize_k: bool = False,\n",
        "\n",
        "        use_rope_keys: bool = True,\n",
        "        rope_base: float = 10000.0,\n",
        "\n",
        "        use_alibi_write: bool = True,\n",
        "        alibi_strength_init: float = 0.1,\n",
        "        learn_alibi_strength: bool = True,\n",
        "        min_strength: float = 0.0,\n",
        "\n",
        "        use_content_read: bool = True,\n",
        "        content_read_init: float = -4.0,\n",
        "        content_read_max_gamma: float = 3.0,\n",
        "\n",
        "        use_slotspace_refine: bool = True,\n",
        "        slotspace_dim: int = 32,\n",
        "        slotspace_gate_init: float = -4.0,\n",
        "        slotspace_dropout: float = 0.05,\n",
        "        slotspace_signed_weights: bool = True,\n",
        "\n",
        "        use_rope_slotspace: bool = True,\n",
        "        rope_base_slotspace: float = 100000.0,\n",
        "\n",
        "        write_chunk_size: int = 128,\n",
        "        slotspace_chunk_size: int = 128,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.num_slots = num_slots\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.read_temperature = float(read_temperature)\n",
        "        self.write_temperature = float(write_temperature)\n",
        "        self.state_fp32 = bool(state_fp32)\n",
        "        self.slot_dropout = float(slot_dropout)\n",
        "        self.normalize_k = bool(normalize_k)\n",
        "        self.routing_override = None\n",
        "\n",
        "        self.use_rope_keys = bool(use_rope_keys)\n",
        "        self.use_alibi_write = bool(use_alibi_write)\n",
        "        self.learn_alibi_strength = bool(learn_alibi_strength)\n",
        "        self.min_strength = float(min_strength)\n",
        "\n",
        "        self.use_content_read = bool(use_content_read)\n",
        "        self.content_read_max_gamma = float(content_read_max_gamma)\n",
        "\n",
        "        self.use_slotspace_refine = bool(use_slotspace_refine)\n",
        "        self.slotspace_dim = int(slotspace_dim)\n",
        "        self.slotspace_dropout = nn.Dropout(float(slotspace_dropout))\n",
        "        self.slotspace_signed_weights = bool(slotspace_signed_weights)\n",
        "\n",
        "        self.write_chunk_size = int(write_chunk_size)\n",
        "        self.slotspace_chunk_size = int(slotspace_chunk_size)\n",
        "\n",
        "        self.slot_keys = nn.Parameter(\n",
        "            torch.randn(num_heads, num_slots, self.head_dim) / math.sqrt(self.head_dim)\n",
        "        )\n",
        "\n",
        "        self.Wk_write = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.Wv_write = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.Wq_read  = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "\n",
        "        self.rope = RotaryEmbedding(self.head_dim, base=rope_base) if self.use_rope_keys else None\n",
        "\n",
        "        if self.use_alibi_write:\n",
        "            self.register_buffer(\"_alibi_slopes\", alibi_slopes(num_heads), persistent=False)\n",
        "        else:\n",
        "            self.register_buffer(\"_alibi_slopes\", torch.zeros(num_heads), persistent=False)\n",
        "\n",
        "        if self.use_alibi_write and self.learn_alibi_strength:\n",
        "            init = torch.tensor(float(alibi_strength_init) - self.min_strength).clamp_min(1e-8)\n",
        "            self._alibi_strength_param = nn.Parameter(_inv_softplus(init))\n",
        "        else:\n",
        "            self._alibi_strength_param = None\n",
        "            self.alibi_strength = float(alibi_strength_init)\n",
        "\n",
        "        if self.use_content_read:\n",
        "            self._content_read_gamma_raw = nn.Parameter(torch.tensor(float(content_read_init)))\n",
        "        else:\n",
        "            self._content_read_gamma_raw = None\n",
        "\n",
        "        self.use_rope_slotspace = bool(use_rope_slotspace) and bool(self.use_slotspace_refine)\n",
        "        if self.use_slotspace_refine:\n",
        "            self.slot_in  = nn.Linear(num_slots, self.slotspace_dim, bias=False)\n",
        "            self.slot_q   = nn.Linear(self.slotspace_dim, self.slotspace_dim, bias=False)\n",
        "            self.slot_k   = nn.Linear(self.slotspace_dim, self.slotspace_dim, bias=False)\n",
        "            self.slot_v   = nn.Linear(self.slotspace_dim, self.slotspace_dim, bias=False)\n",
        "            self.slot_out = nn.Linear(self.slotspace_dim, num_slots, bias=False)\n",
        "            self._slotspace_gate_raw = nn.Parameter(torch.tensor(float(slotspace_gate_init)))\n",
        "\n",
        "            if self.use_rope_slotspace:\n",
        "                assert (self.slotspace_dim % 2) == 0\n",
        "                self.rope_slotspace = RotaryEmbedding(self.slotspace_dim, base=float(rope_base_slotspace))\n",
        "            else:\n",
        "                self.rope_slotspace = None\n",
        "        else:\n",
        "            self.slot_in = None\n",
        "            self.slot_q = self.slot_k = self.slot_v = None\n",
        "            self.slot_out = None\n",
        "            self._slotspace_gate_raw = None\n",
        "            self.rope_slotspace = None\n",
        "\n",
        "    def _alibi_strength(self, dtype, device) -> torch.Tensor:\n",
        "        if not (self.use_alibi_write and self.learn_alibi_strength):\n",
        "            return torch.tensor(self.alibi_strength, dtype=dtype, device=device)\n",
        "        return (F.softplus(self._alibi_strength_param) + self.min_strength).to(dtype=dtype, device=device)\n",
        "\n",
        "    def _content_read_gamma(self, dtype, device) -> torch.Tensor:\n",
        "        if not self.use_content_read:\n",
        "            return torch.tensor(0.0, dtype=dtype, device=device)\n",
        "        g = F.softplus(self._content_read_gamma_raw)\n",
        "        if self.content_read_max_gamma is not None and self.content_read_max_gamma > 0:\n",
        "            g = g.clamp(max=self.content_read_max_gamma)\n",
        "        return g.to(dtype=dtype, device=device)\n",
        "\n",
        "    def _slotspace_gate(self, dtype, device) -> torch.Tensor:\n",
        "        if not self.use_slotspace_refine:\n",
        "            return torch.tensor(0.0, dtype=dtype, device=device)\n",
        "        return F.softplus(self._slotspace_gate_raw).to(dtype=dtype, device=device)\n",
        "\n",
        "    @staticmethod\n",
        "    def _safe_exp_sub_max(s: torch.Tensor, m: torch.Tensor) -> torch.Tensor:\n",
        "        diff = s - m\n",
        "        diff = diff.masked_fill(~torch.isfinite(m), float(\"-inf\"))\n",
        "        return torch.exp(diff)\n",
        "\n",
        "    @staticmethod\n",
        "    def _canonicalize_slot_mask(slot_mask: torch.Tensor, B: int, H: int, L: int, K: int, device, dtype):\n",
        "        \"\"\"\n",
        "        Accepts [K] or any broadcastable shape to [B,H,L,K].\n",
        "        Returns:\n",
        "          m_bool: bool mask where True means \"KEEP\"\n",
        "          m_flt : float mask (0/1) broadcasted\n",
        "        \"\"\"\n",
        "        m = slot_mask\n",
        "        if m.dim() == 1:\n",
        "            m = m.view(1, 1, 1, K)\n",
        "        m = m.to(device=device, dtype=dtype)\n",
        "        # consider >0 as keep\n",
        "        m_bool = (m > 0)\n",
        "        # broadcast check happens implicitly in masked_fill\n",
        "        return m_bool, m\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        return_info: bool = False,\n",
        "        routing_mode: str = \"softmax\",           # \"softmax\" | \"top1\" | \"topk\" | \"external\"\n",
        "        routing_topk: int = 2,\n",
        "        read_weights_override: Optional[torch.Tensor] = None,  # [B,H,T,K] or [B,H,L,K]\n",
        "        routing_noise: Optional[str] = None,     # None | \"gumbel\" | \"gaussian\"\n",
        "        routing_noise_scale: float = 1.0,\n",
        "        slot_mask: Optional[torch.Tensor] = None,  # NEW: [K] or broadcastable to [B,H,L,K]\n",
        "    ) -> Tuple[torch.Tensor, Optional[Dict[str, torch.Tensor]]]:\n",
        "        B, T, C = x.shape\n",
        "        H, K, d = self.num_heads, self.num_slots, self.head_dim\n",
        "\n",
        "        k_write = self.Wk_write(x).view(B, T, H, d).transpose(1, 2)  # [B,H,T,d]\n",
        "        v_write = self.Wv_write(x).view(B, T, H, d).transpose(1, 2)\n",
        "        q_read  = self.Wq_read(x).view(B, T, H, d).transpose(1, 2)\n",
        "\n",
        "        if self.normalize_k:\n",
        "            k_write = F.normalize(k_write, dim=-1, eps=1e-8)\n",
        "\n",
        "        if self.use_rope_keys:\n",
        "            cos, sin = self.rope.get_cos_sin(T, device=x.device, dtype=k_write.dtype)\n",
        "            k_write = apply_rope(k_write, cos, sin)\n",
        "\n",
        "        slot_keys = self.slot_keys\n",
        "        if self.training and self.slot_dropout > 0.0:\n",
        "            drop = (torch.rand((H, K), device=x.device) < self.slot_dropout)\n",
        "            slot_keys = slot_keys * (~drop).to(slot_keys.dtype).unsqueeze(-1)\n",
        "\n",
        "        write_logits_raw = torch.einsum(\"hkd,bhtd->bhkt\", slot_keys, k_write) / math.sqrt(d)\n",
        "\n",
        "        state_dtype = torch.float32 if (self.state_fp32 and x.dtype != torch.float32) else x.dtype\n",
        "        write_logits = write_logits_raw.to(state_dtype)\n",
        "\n",
        "        wtemp = max(1e-6, self.write_temperature)\n",
        "        write_logits = write_logits / wtemp\n",
        "\n",
        "        alibi_bias_applied = None\n",
        "        if self.use_alibi_write:\n",
        "            strength = self._alibi_strength(dtype=state_dtype, device=x.device)\n",
        "            slopes = self._alibi_slopes.to(device=x.device, dtype=state_dtype) * strength\n",
        "            pos_i = torch.arange(T, device=x.device, dtype=state_dtype)\n",
        "            alibi_bias = slopes.view(1, H, 1, 1) * pos_i.view(1, 1, 1, T)\n",
        "            write_logits = write_logits + alibi_bias\n",
        "            alibi_bias_applied = alibi_bias\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            valid = attention_mask.to(dtype=torch.bool)\n",
        "            write_logits = write_logits.masked_fill(~valid.view(B, 1, 1, T), float(\"-inf\"))\n",
        "        else:\n",
        "            valid = None\n",
        "\n",
        "        content_read_gamma = self._content_read_gamma(dtype=q_read.dtype, device=x.device)\n",
        "        rtemp = max(1e-6, self.read_temperature)\n",
        "\n",
        "        out_h = torch.empty((B, H, T, d), device=x.device, dtype=state_dtype)\n",
        "        read_weights = torch.empty((B, H, T, K), device=x.device, dtype=q_read.dtype)\n",
        "        slot_state_norm_t = torch.empty((B, H, T, K), device=x.device, dtype=torch.float32) if return_info else None\n",
        "\n",
        "        denom_state = torch.zeros((B, H, K), device=x.device, dtype=state_dtype)\n",
        "        numer_state = torch.zeros((B, H, K, d), device=x.device, dtype=state_dtype)\n",
        "        m_state = torch.full((B, H, K), float(\"-inf\"), device=x.device, dtype=state_dtype)\n",
        "\n",
        "        if return_info:\n",
        "            read_logits_full = torch.empty((B, H, T, K), device=x.device, dtype=state_dtype)\n",
        "            read_logits_key_full = torch.empty((B, H, T, K), device=x.device, dtype=state_dtype)\n",
        "            read_logits_content_full = (\n",
        "                torch.empty((B, H, T, K), device=x.device, dtype=state_dtype) if self.use_content_read else None\n",
        "            )\n",
        "        else:\n",
        "            read_logits_full = None\n",
        "            read_logits_key_full = None\n",
        "            read_logits_content_full = None\n",
        "\n",
        "        WRITE_CHUNK = self.write_chunk_size\n",
        "\n",
        "        for t0 in range(0, T, WRITE_CHUNK):\n",
        "            t1 = min(T, t0 + WRITE_CHUNK)\n",
        "            L = t1 - t0\n",
        "\n",
        "            wlog_c = write_logits[:, :, :, t0:t1]  # [B,H,K,L]\n",
        "\n",
        "            m_c, _ = torch.cummax(wlog_c, dim=-1)\n",
        "            m_new = torch.maximum(m_state.unsqueeze(-1), m_c)\n",
        "\n",
        "            scale = torch.exp(m_state.unsqueeze(-1) - m_new)\n",
        "\n",
        "            denom_c = denom_state.unsqueeze(-1) * scale\n",
        "            numer_c = numer_state.unsqueeze(-2) * scale.unsqueeze(-1)\n",
        "\n",
        "            w_new = self._safe_exp_sub_max(wlog_c, m_new)\n",
        "\n",
        "            denom_c = denom_c + torch.cumsum(w_new, dim=-1)\n",
        "            v_c = v_write[:, :, t0:t1, :].to(state_dtype)\n",
        "            add = torch.cumsum(w_new.unsqueeze(-1) * v_c.unsqueeze(2), dim=-2)\n",
        "            numer_c = numer_c + add\n",
        "\n",
        "            slot_state_c = numer_c / denom_c.clamp_min(1e-8).unsqueeze(-1)\n",
        "            slot_state_t = slot_state_c.permute(0, 1, 3, 2, 4).contiguous()  # [B,H,L,K,d]\n",
        "\n",
        "            q_read_c = q_read[:, :, t0:t1, :]  # [B,H,L,d]\n",
        "\n",
        "            read_logits_key = torch.einsum(\"bhld,hkd->bhlk\", q_read_c, slot_keys) / math.sqrt(d)\n",
        "\n",
        "            read_logits_content = None\n",
        "            read_logits = read_logits_key\n",
        "            if self.use_content_read:\n",
        "                read_logits_content = torch.einsum(\n",
        "                    \"bhld,bhlkd->bhlk\",\n",
        "                    q_read_c,\n",
        "                    slot_state_t.to(q_read_c.dtype)\n",
        "                ) / math.sqrt(d)\n",
        "                read_logits = read_logits + content_read_gamma.to(read_logits.dtype) * read_logits_content\n",
        "\n",
        "            # ---------- APPLY SLOT MASK (causal intervention) ----------\n",
        "            if slot_mask is not None:\n",
        "                keep_bool, _m = self._canonicalize_slot_mask(\n",
        "                    slot_mask, B=B, H=H, L=L, K=K, device=read_logits.device, dtype=read_logits.dtype\n",
        "                )\n",
        "                read_logits = read_logits.masked_fill(~keep_bool, float(\"-inf\"))\n",
        "                read_logits_key = read_logits_key.masked_fill(~keep_bool, float(\"-inf\"))\n",
        "                if read_logits_content is not None:\n",
        "                    read_logits_content = read_logits_content.masked_fill(~keep_bool, float(\"-inf\"))\n",
        "\n",
        "            if return_info:\n",
        "                read_logits_full[:, :, t0:t1, :] = read_logits.to(state_dtype)\n",
        "                read_logits_key_full[:, :, t0:t1, :] = read_logits_key.to(state_dtype)\n",
        "                if self.use_content_read:\n",
        "                    read_logits_content_full[:, :, t0:t1, :] = (\n",
        "                        read_logits_content.to(state_dtype) if read_logits_content is not None else 0.0\n",
        "                    )\n",
        "\n",
        "            if routing_noise is not None:\n",
        "                if routing_noise == \"gumbel\":\n",
        "                    u = torch.rand_like(read_logits)\n",
        "                    g = -torch.log(-torch.log(u.clamp_min(1e-8)).clamp_min(1e-8))\n",
        "                    read_logits = read_logits + routing_noise_scale * g\n",
        "                elif routing_noise == \"gaussian\":\n",
        "                    read_logits = read_logits + routing_noise_scale * torch.randn_like(read_logits)\n",
        "                else:\n",
        "                    raise ValueError(f\"Unknown routing_noise={routing_noise}\")\n",
        "\n",
        "            if self.routing_override is not None:\n",
        "                if callable(self.routing_override):\n",
        "                    ctx = {\n",
        "                        \"t0\": t0, \"t1\": t1,\n",
        "                        \"B\": B, \"H\": H, \"T\": T, \"K\": K, \"d\": d,\n",
        "                        \"rtemp\": rtemp,\n",
        "                        \"state_dtype\": state_dtype,\n",
        "                        \"q_read_c\": q_read_c,\n",
        "                        \"slot_keys\": slot_keys,\n",
        "                        \"slot_state_t\": slot_state_t,\n",
        "                        \"valid\": valid,\n",
        "                        \"slot_mask\": slot_mask,  # handy\n",
        "                    }\n",
        "                    read_w_c = self.routing_override(\n",
        "                        t0, t1, read_logits, read_logits_key, read_logits_content, ctx\n",
        "                    )\n",
        "                else:\n",
        "                    read_w_c = self.routing_override[:, :, t0:t1, :].to(read_logits.dtype)\n",
        "\n",
        "                read_w_c = torch.nan_to_num(read_w_c, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "                read_w_c = read_w_c.clamp_min(0.0)\n",
        "\n",
        "                # if slot_mask provided, enforce it here too (in case override ignores it)\n",
        "                if slot_mask is not None:\n",
        "                    keep_bool, _ = self._canonicalize_slot_mask(\n",
        "                        slot_mask, B=B, H=H, L=L, K=K, device=read_w_c.device, dtype=read_w_c.dtype\n",
        "                    )\n",
        "                    read_w_c = read_w_c * keep_bool.to(read_w_c.dtype)\n",
        "\n",
        "                s = read_w_c.sum(dim=-1, keepdim=True)\n",
        "                # avoid all-zero rows\n",
        "                read_w_c = torch.where(\n",
        "                    s > 0,\n",
        "                    read_w_c / s.clamp_min(1e-8),\n",
        "                    torch.full_like(read_w_c, 1.0 / K),\n",
        "                )\n",
        "\n",
        "            else:\n",
        "                if routing_mode == \"softmax\":\n",
        "                    read_w_c = torch.softmax(read_logits / rtemp, dim=-1)\n",
        "\n",
        "                elif routing_mode == \"top1\":\n",
        "                    top = read_logits.argmax(dim=-1)\n",
        "                    read_w_c = F.one_hot(top, num_classes=K).to(read_logits.dtype)\n",
        "                    # enforce mask (top1 can pick masked if all -inf -> argmax=0)\n",
        "                    if slot_mask is not None:\n",
        "                        keep_bool, _ = self._canonicalize_slot_mask(\n",
        "                            slot_mask, B=B, H=H, L=L, K=K, device=read_w_c.device, dtype=read_w_c.dtype\n",
        "                        )\n",
        "                        read_w_c = read_w_c * keep_bool.to(read_w_c.dtype)\n",
        "                        s = read_w_c.sum(dim=-1, keepdim=True)\n",
        "                        read_w_c = torch.where(\n",
        "                            s > 0,\n",
        "                            read_w_c / s.clamp_min(1e-8),\n",
        "                            torch.full_like(read_w_c, 1.0 / K),\n",
        "                        )\n",
        "\n",
        "                elif routing_mode == \"topk\":\n",
        "                    kk = int(routing_topk)\n",
        "                    kk = max(1, min(K, kk))\n",
        "                    vals, idx = torch.topk(read_logits, k=kk, dim=-1)\n",
        "                    masked = torch.full_like(read_logits, float(\"-inf\"))\n",
        "                    masked.scatter_(-1, idx, vals)\n",
        "                    read_w_c = torch.softmax(masked / rtemp, dim=-1)\n",
        "\n",
        "                elif routing_mode == \"external\":\n",
        "                    if read_weights_override is None:\n",
        "                        raise ValueError(\"routing_mode='external' requires read_weights_override\")\n",
        "                    if read_weights_override.shape[-2] == T:\n",
        "                        read_w_c = read_weights_override[:, :, t0:t1, :]\n",
        "                    else:\n",
        "                        read_w_c = read_weights_override\n",
        "                    read_w_c = read_w_c / read_w_c.sum(dim=-1, keepdim=True).clamp_min(1e-8)\n",
        "\n",
        "                    if slot_mask is not None:\n",
        "                        keep_bool, _ = self._canonicalize_slot_mask(\n",
        "                            slot_mask, B=B, H=H, L=L, K=K, device=read_w_c.device, dtype=read_w_c.dtype\n",
        "                        )\n",
        "                        read_w_c = read_w_c * keep_bool.to(read_w_c.dtype)\n",
        "                        s = read_w_c.sum(dim=-1, keepdim=True)\n",
        "                        read_w_c = torch.where(\n",
        "                            s > 0,\n",
        "                            read_w_c / s.clamp_min(1e-8),\n",
        "                            torch.full_like(read_w_c, 1.0 / K),\n",
        "                        )\n",
        "\n",
        "                else:\n",
        "                    raise ValueError(f\"Unknown routing_mode={routing_mode}\")\n",
        "\n",
        "            read_weights[:, :, t0:t1, :] = read_w_c\n",
        "\n",
        "            out_h[:, :, t0:t1, :] = torch.einsum(\n",
        "                \"bhlk,bhlkd->bhld\",\n",
        "                read_w_c.to(state_dtype),\n",
        "                slot_state_t.to(state_dtype),\n",
        "            )\n",
        "\n",
        "            if return_info:\n",
        "                slot_state_norm_t[:, :, t0:t1, :] = slot_state_t.to(torch.float32).norm(dim=-1)\n",
        "\n",
        "            m_state = m_new[:, :, :, -1]\n",
        "            denom_state = denom_c[:, :, :, -1]\n",
        "            numer_state = numer_c[:, :, :, -1, :]\n",
        "\n",
        "        # =====================================================\n",
        "        # Optional slot-space refinement\n",
        "        # =====================================================\n",
        "        slotspace_delta_norm_mean = None\n",
        "        if self.use_slotspace_refine:\n",
        "            slotspace_dtype = state_dtype\n",
        "            M = self.slotspace_dim\n",
        "\n",
        "            u = self.slot_in(read_weights.to(slotspace_dtype))\n",
        "            q_s  = self.slot_q(u)\n",
        "            k_s  = self.slot_k(u)\n",
        "            v_s  = self.slot_v(u)\n",
        "\n",
        "            if self.use_rope_slotspace:\n",
        "                cos_s, sin_s = self.rope_slotspace.get_cos_sin(T, device=x.device, dtype=q_s.dtype)\n",
        "                q_s = apply_rope(q_s, cos_s, sin_s)\n",
        "                k_s = apply_rope(k_s, cos_s, sin_s)\n",
        "\n",
        "            qf = phi(q_s)\n",
        "            kf = phi(k_s)\n",
        "\n",
        "            if valid is not None:\n",
        "                mask = valid.view(B, 1, T, 1).to(slotspace_dtype)\n",
        "                qf = qf * mask\n",
        "                kf = kf * mask\n",
        "                v_s = v_s * mask\n",
        "\n",
        "            u2 = torch.empty((B, H, T, M), device=x.device, dtype=slotspace_dtype)\n",
        "\n",
        "            S_state = torch.zeros((B, H, M, M), device=x.device, dtype=slotspace_dtype)\n",
        "            Z_state = torch.zeros((B, H, M), device=x.device, dtype=slotspace_dtype)\n",
        "\n",
        "            SS_CHUNK = self.slotspace_chunk_size\n",
        "\n",
        "            for t0 in range(0, T, SS_CHUNK):\n",
        "                t1 = min(T, t0 + SS_CHUNK)\n",
        "                L = t1 - t0\n",
        "\n",
        "                qf_c = qf[:, :, t0:t1, :]\n",
        "                kf_c = kf[:, :, t0:t1, :]\n",
        "                v_c  = v_s[:, :, t0:t1, :]\n",
        "\n",
        "                kv = torch.einsum(\"bhlm,bhln->bhlmn\", kf_c, v_c)\n",
        "                S_c = torch.cumsum(kv, dim=2)\n",
        "                Z_c = torch.cumsum(kf_c, dim=2)\n",
        "\n",
        "                S_c = S_c + S_state.unsqueeze(2)\n",
        "                Z_c = (Z_c + Z_state.unsqueeze(2)).clamp_min(1e-8)\n",
        "\n",
        "                num = torch.einsum(\"bhlm,bhlmn->bhln\", qf_c, S_c)\n",
        "                den = torch.einsum(\"bhlm,bhlm->bhl\", qf_c, Z_c).unsqueeze(-1).clamp_min(1e-8)\n",
        "                u2[:, :, t0:t1, :] = num / den\n",
        "\n",
        "                S_state = S_c[:, :, -1, :, :]\n",
        "                Z_state = Z_c[:, :, -1, :]\n",
        "\n",
        "            u2 = self.slotspace_dropout(u2)\n",
        "\n",
        "            slot_w = self.slot_out(u2)\n",
        "            if self.slotspace_signed_weights:\n",
        "                slot_w = torch.tanh(slot_w)\n",
        "            else:\n",
        "                slot_w = torch.softmax(slot_w, dim=-1)\n",
        "\n",
        "            gate = self._slotspace_gate(dtype=state_dtype, device=x.device).to(state_dtype)\n",
        "\n",
        "            denom_state = torch.zeros((B, H, K), device=x.device, dtype=state_dtype)\n",
        "            numer_state = torch.zeros((B, H, K, d), device=x.device, dtype=state_dtype)\n",
        "            m_state = torch.full((B, H, K), float(\"-inf\"), device=x.device, dtype=state_dtype)\n",
        "\n",
        "            delta_norm_sum = torch.zeros((), device=x.device, dtype=torch.float32)\n",
        "            delta_norm_count = 0\n",
        "\n",
        "            for t0 in range(0, T, WRITE_CHUNK):\n",
        "                t1 = min(T, t0 + WRITE_CHUNK)\n",
        "                L = t1 - t0\n",
        "\n",
        "                wlog_c = write_logits[:, :, :, t0:t1]\n",
        "                m_c, _ = torch.cummax(wlog_c, dim=-1)\n",
        "                m_new = torch.maximum(m_state.unsqueeze(-1), m_c)\n",
        "\n",
        "                scale = torch.exp(m_state.unsqueeze(-1) - m_new)\n",
        "                denom_c = denom_state.unsqueeze(-1) * scale\n",
        "                numer_c = numer_state.unsqueeze(-2) * scale.unsqueeze(-1)\n",
        "\n",
        "                w_new = self._safe_exp_sub_max(wlog_c, m_new)\n",
        "                denom_c = denom_c + torch.cumsum(w_new, dim=-1)\n",
        "\n",
        "                v_c = v_write[:, :, t0:t1, :].to(state_dtype)\n",
        "                add = torch.cumsum(w_new.unsqueeze(-1) * v_c.unsqueeze(2), dim=-2)\n",
        "                numer_c = numer_c + add\n",
        "\n",
        "                slot_state_c = numer_c / denom_c.clamp_min(1e-8).unsqueeze(-1)\n",
        "                slot_state_t = slot_state_c.permute(0, 1, 3, 2, 4).contiguous()\n",
        "\n",
        "                slot_w_c = slot_w[:, :, t0:t1, :].to(state_dtype)\n",
        "\n",
        "                # If slot_mask is active, suppress slotspace weights too (consistent intervention)\n",
        "                if slot_mask is not None:\n",
        "                    keep_bool, _ = self._canonicalize_slot_mask(\n",
        "                        slot_mask, B=B, H=H, L=L, K=K, device=slot_w_c.device, dtype=slot_w_c.dtype\n",
        "                    )\n",
        "                    slot_w_c = slot_w_c * keep_bool.to(slot_w_c.dtype)\n",
        "\n",
        "                delta_c = torch.einsum(\"bhlk,bhlkd->bhld\", slot_w_c, slot_state_t.to(state_dtype))\n",
        "                out_h[:, :, t0:t1, :] = out_h[:, :, t0:t1, :] + gate * delta_c\n",
        "\n",
        "                delta_norm_sum = delta_norm_sum + delta_c.detach().to(torch.float32).norm(dim=-1).sum()\n",
        "                delta_norm_count += (B * H * L)\n",
        "\n",
        "                m_state = m_new[:, :, :, -1]\n",
        "                denom_state = denom_c[:, :, :, -1]\n",
        "                numer_state = numer_c[:, :, :, -1, :]\n",
        "\n",
        "            slotspace_delta_norm_mean = (delta_norm_sum / max(1, delta_norm_count)).detach().cpu()\n",
        "\n",
        "        out = out_h.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        out = self.out_proj(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        info = None\n",
        "        if return_info:\n",
        "            info = {\n",
        "                \"write_logits_raw\": write_logits_raw.detach(),\n",
        "                \"write_logits\": write_logits.detach().to(torch.float32),\n",
        "                \"read_weights\": read_weights.detach(),\n",
        "                \"slot_state_norm\": slot_state_norm_t.detach().permute(0, 1, 3, 2).contiguous() if slot_state_norm_t is not None else None,\n",
        "                \"content_read_gamma\": content_read_gamma.detach().to(torch.float32).cpu(),\n",
        "            }\n",
        "            if alibi_bias_applied is not None:\n",
        "                info[\"alibi_bias_applied\"] = alibi_bias_applied.detach().to(torch.float32)\n",
        "            if self.use_alibi_write and self.learn_alibi_strength:\n",
        "                info[\"alibi_strength\"] = self._alibi_strength(dtype=torch.float32, device=x.device).detach().cpu()\n",
        "            if self.use_slotspace_refine:\n",
        "                info[\"slotspace_gate\"] = self._slotspace_gate(dtype=torch.float32, device=x.device).detach().cpu()\n",
        "                info[\"use_rope_slotspace\"] = torch.tensor(bool(self.use_rope_slotspace))\n",
        "                if slotspace_delta_norm_mean is not None:\n",
        "                    info[\"slotspace_delta_norm\"] = slotspace_delta_norm_mean\n",
        "\n",
        "            info[\"read_logits\"] = read_logits_full.detach().to(torch.float32) if read_logits_full is not None else None\n",
        "            info[\"read_logits_key\"] = read_logits_key_full.detach().to(torch.float32) if read_logits_key_full is not None else None\n",
        "            info[\"read_logits_content\"] = (\n",
        "                read_logits_content_full.detach().to(torch.float32) if read_logits_content_full is not None else None\n",
        "            )\n",
        "            info[\"routing_mode\"] = routing_mode\n",
        "            info[\"slot_mask_present\"] = torch.tensor(slot_mask is not None)\n",
        "\n",
        "        return out, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_2LSjoIpkj_t"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Addressed State Attention (DROP-IN REPLACEMENT + slot_mask controls)\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Optional, Dict, Tuple\n",
        "\n",
        "# -------------------------\n",
        "# RoPE helper (rotate-half)\n",
        "# -------------------------\n",
        "def _rotate_half(x: torch.Tensor) -> torch.Tensor:\n",
        "    x1 = x[..., ::2]\n",
        "    x2 = x[..., 1::2]\n",
        "    return torch.stack((-x2, x1), dim=-1).flatten(-2)\n",
        "\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    def __init__(self, dim: int, base: float = 10000.0):\n",
        "        super().__init__()\n",
        "        assert dim % 2 == 0, \"RoPE requires even dim\"\n",
        "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
        "        self._cos_cached = None\n",
        "        self._sin_cached = None\n",
        "        self._t_cached = None\n",
        "        self._device_cached = None\n",
        "\n",
        "    def get_cos_sin(self, T: int, device, dtype):\n",
        "        if (\n",
        "            self._t_cached == T\n",
        "            and self._cos_cached is not None\n",
        "            and self._device_cached == device\n",
        "        ):\n",
        "            return self._cos_cached.to(dtype=dtype), self._sin_cached.to(dtype=dtype)\n",
        "\n",
        "        t = torch.arange(T, device=device, dtype=self.inv_freq.dtype)\n",
        "        freqs = torch.einsum(\"t,f->tf\", t, self.inv_freq)  # [T, d/2]\n",
        "        emb = torch.cat([freqs, freqs], dim=-1)            # [T, d]\n",
        "        cos = emb.cos()[None, None, :, :]                  # [1,1,T,d]\n",
        "        sin = emb.sin()[None, None, :, :]                  # [1,1,T,d]\n",
        "\n",
        "        self._t_cached = T\n",
        "        self._device_cached = device\n",
        "        self._cos_cached = cos\n",
        "        self._sin_cached = sin\n",
        "        return cos.to(dtype=dtype), sin.to(dtype=dtype)\n",
        "\n",
        "def apply_rope(x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> torch.Tensor:\n",
        "    return (x * cos) + (_rotate_half(x) * sin)\n",
        "\n",
        "# -------------------------\n",
        "# ALiBi slopes helper\n",
        "# -------------------------\n",
        "def alibi_slopes(num_heads: int, device=None, dtype=torch.float32) -> torch.Tensor:\n",
        "    def get_slopes(n):\n",
        "        def power_of_2_slopes(n):\n",
        "            start = 2.0 ** (-(2.0 ** -(math.log2(n) - 3)))\n",
        "            ratio = start\n",
        "            return [start * (ratio ** i) for i in range(n)]\n",
        "        if math.log2(n).is_integer():\n",
        "            return power_of_2_slopes(n)\n",
        "        closest = 2 ** math.floor(math.log2(n))\n",
        "        return power_of_2_slopes(closest) + get_slopes(2 * closest)[0::2][: n - closest]\n",
        "    return torch.tensor(get_slopes(num_heads), device=device, dtype=dtype)  # [H]\n",
        "\n",
        "# -------------------------\n",
        "# softplus init helpers\n",
        "# -------------------------\n",
        "def _inv_softplus(y: torch.Tensor) -> torch.Tensor:\n",
        "    return torch.log(torch.expm1(y))\n",
        "\n",
        "# -------------------------\n",
        "# Linear attention feature map (Performer-style)\n",
        "# -------------------------\n",
        "def phi(x: torch.Tensor) -> torch.Tensor:\n",
        "    return F.elu(x) + 1.0\n",
        "\n",
        "\n",
        "class AddressedStateAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Addressed State Attention (ASA):\n",
        "      - prefix-softmax WRITE into slots (O(T))\n",
        "      - READ routing from tokens -> slots (softmax over slots)\n",
        "      - content-conditioned READ term (gamma)\n",
        "      - RoPE on write keys (geometry)\n",
        "      - ALiBi bias on write logits (prefix-friendly)\n",
        "\n",
        "    slot-space refinement:\n",
        "      - causal linear attention in a low-dim slot-address coordinate space\n",
        "      - produces per-token signed weights over slots\n",
        "      - decoded through the same streaming slot-state basis\n",
        "      - gated by learnable slotspace_gate (softplus)\n",
        "\n",
        "    PERF (behavior-preserving):\n",
        "      - Streaming prefix write states in chunks (no [B,H,K,T,d] materialization)\n",
        "      - Slot-space prefix scan is chunked (exact)\n",
        "\n",
        "    ADDITION (drop-in):\n",
        "      - slot_mask controls for causal interventions:\n",
        "          slot_mask: [K] float/bool where 1=keep, 0=mask\n",
        "          slot_mask_where: \"read\" | \"content_read_only\" | \"slotspace_only\"\n",
        "          slot_mask_scope: \"all\" | \"last_pos_only\"\n",
        "      - also supports attribute fallback: self.slot_mask if slot_mask kwarg is None\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int,\n",
        "        num_heads: int = 8,\n",
        "        num_slots: int = 8,\n",
        "        dropout: float = 0.1,\n",
        "\n",
        "        # temperatures / numerics\n",
        "        read_temperature: float = 1.0,\n",
        "        write_temperature: float = 1.0,\n",
        "        state_fp32: bool = True,\n",
        "        slot_dropout: float = 0.0,\n",
        "        normalize_k: bool = False,\n",
        "\n",
        "        # positions (write geometry)\n",
        "        use_rope_keys: bool = True,\n",
        "        rope_base: float = 10000.0,\n",
        "\n",
        "        # write bias (ALiBi)\n",
        "        use_alibi_write: bool = True,\n",
        "        alibi_strength_init: float = 0.1,\n",
        "        learn_alibi_strength: bool = True,\n",
        "        min_strength: float = 0.0,\n",
        "\n",
        "        # content-conditioned read term\n",
        "        use_content_read: bool = True,\n",
        "        content_read_init: float = -4.0,\n",
        "        content_read_max_gamma: float = 3.0,\n",
        "\n",
        "        # slot-space refinement\n",
        "        use_slotspace_refine: bool = True,\n",
        "        slotspace_dim: int = 32,\n",
        "        slotspace_gate_init: float = -4.0,\n",
        "        slotspace_dropout: float = 0.05,\n",
        "        slotspace_signed_weights: bool = True,\n",
        "\n",
        "        # RoPE in slot-space matcher (Q/K only)\n",
        "        use_rope_slotspace: bool = True,\n",
        "        rope_base_slotspace: float = 100000.0,\n",
        "\n",
        "        # perf knobs (no behavior change)\n",
        "        write_chunk_size: int = 128,\n",
        "        slotspace_chunk_size: int = 128,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.num_slots = num_slots\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.read_temperature = float(read_temperature)\n",
        "        self.write_temperature = float(write_temperature)\n",
        "        self.state_fp32 = bool(state_fp32)\n",
        "        self.slot_dropout = float(slot_dropout)\n",
        "        self.normalize_k = bool(normalize_k)\n",
        "        self.routing_override = None\n",
        "\n",
        "        # mask attribute fallback (may be set externally)\n",
        "        # expected shape: [K], where 1=keep, 0=mask\n",
        "        # self.slot_mask = None\n",
        "\n",
        "        self.use_rope_keys = bool(use_rope_keys)\n",
        "        self.use_alibi_write = bool(use_alibi_write)\n",
        "        self.learn_alibi_strength = bool(learn_alibi_strength)\n",
        "        self.min_strength = float(min_strength)\n",
        "\n",
        "        self.use_content_read = bool(use_content_read)\n",
        "        self.content_read_max_gamma = float(content_read_max_gamma)\n",
        "\n",
        "        self.use_slotspace_refine = bool(use_slotspace_refine)\n",
        "        self.slotspace_dim = int(slotspace_dim)\n",
        "        self.slotspace_dropout = nn.Dropout(float(slotspace_dropout))\n",
        "        self.slotspace_signed_weights = bool(slotspace_signed_weights)\n",
        "\n",
        "        self.write_chunk_size = int(write_chunk_size)\n",
        "        self.slotspace_chunk_size = int(slotspace_chunk_size)\n",
        "\n",
        "        # Learned slot keys per head: [H,K,d]\n",
        "        self.slot_keys = nn.Parameter(\n",
        "            torch.randn(num_heads, num_slots, self.head_dim) / math.sqrt(self.head_dim)\n",
        "        )\n",
        "\n",
        "        # Projections\n",
        "        self.Wk_write = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.Wv_write = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.Wq_read  = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "\n",
        "        # RoPE (write geometry)\n",
        "        self.rope = RotaryEmbedding(self.head_dim, base=rope_base) if self.use_rope_keys else None\n",
        "\n",
        "        # ALiBi slopes (buffer)\n",
        "        if self.use_alibi_write:\n",
        "            self.register_buffer(\"_alibi_slopes\", alibi_slopes(num_heads), persistent=False)  # [H]\n",
        "        else:\n",
        "            self.register_buffer(\"_alibi_slopes\", torch.zeros(num_heads), persistent=False)\n",
        "\n",
        "        # Learnable ALiBi strength (positive via softplus)\n",
        "        if self.use_alibi_write and self.learn_alibi_strength:\n",
        "            init = torch.tensor(float(alibi_strength_init) - self.min_strength).clamp_min(1e-8)\n",
        "            self._alibi_strength_param = nn.Parameter(_inv_softplus(init))\n",
        "        else:\n",
        "            self._alibi_strength_param = None\n",
        "            self.alibi_strength = float(alibi_strength_init)\n",
        "\n",
        "        # Content read gamma (>=0 via softplus)\n",
        "        if self.use_content_read:\n",
        "            self._content_read_gamma_raw = nn.Parameter(torch.tensor(float(content_read_init)))\n",
        "        else:\n",
        "            self._content_read_gamma_raw = None\n",
        "\n",
        "        # -------------------------\n",
        "        # slot-space refinement\n",
        "        # -------------------------\n",
        "        self.use_rope_slotspace = bool(use_rope_slotspace) and bool(self.use_slotspace_refine)\n",
        "        if self.use_slotspace_refine:\n",
        "            self.slot_in  = nn.Linear(num_slots, self.slotspace_dim, bias=False)\n",
        "            self.slot_q   = nn.Linear(self.slotspace_dim, self.slotspace_dim, bias=False)\n",
        "            self.slot_k   = nn.Linear(self.slotspace_dim, self.slotspace_dim, bias=False)\n",
        "            self.slot_v   = nn.Linear(self.slotspace_dim, self.slotspace_dim, bias=False)\n",
        "            self.slot_out = nn.Linear(self.slotspace_dim, num_slots, bias=False)\n",
        "            self._slotspace_gate_raw = nn.Parameter(torch.tensor(float(slotspace_gate_init)))\n",
        "\n",
        "            if self.use_rope_slotspace:\n",
        "                assert (self.slotspace_dim % 2) == 0, \"use_rope_slotspace requires even slotspace_dim\"\n",
        "                self.rope_slotspace = RotaryEmbedding(self.slotspace_dim, base=float(rope_base_slotspace))\n",
        "            else:\n",
        "                self.rope_slotspace = None\n",
        "        else:\n",
        "            self.slot_in = None\n",
        "            self.slot_q = self.slot_k = self.slot_v = None\n",
        "            self.slot_out = None\n",
        "            self._slotspace_gate_raw = None\n",
        "            self.rope_slotspace = None\n",
        "\n",
        "    def _alibi_strength(self, dtype, device) -> torch.Tensor:\n",
        "        if not (self.use_alibi_write and self.learn_alibi_strength):\n",
        "            return torch.tensor(self.alibi_strength, dtype=dtype, device=device)\n",
        "        return (F.softplus(self._alibi_strength_param) + self.min_strength).to(dtype=dtype, device=device)\n",
        "\n",
        "    def _content_read_gamma(self, dtype, device) -> torch.Tensor:\n",
        "        if not self.use_content_read:\n",
        "            return torch.tensor(0.0, dtype=dtype, device=device)\n",
        "        g = F.softplus(self._content_read_gamma_raw)  # >= 0\n",
        "        if self.content_read_max_gamma is not None and self.content_read_max_gamma > 0:\n",
        "            g = g.clamp(max=self.content_read_max_gamma)\n",
        "        return g.to(dtype=dtype, device=device)\n",
        "\n",
        "    def _slotspace_gate(self, dtype, device) -> torch.Tensor:\n",
        "        if not self.use_slotspace_refine:\n",
        "            return torch.tensor(0.0, dtype=dtype, device=device)\n",
        "        return F.softplus(self._slotspace_gate_raw).to(dtype=dtype, device=device)\n",
        "\n",
        "    @staticmethod\n",
        "    def _safe_exp_sub_max(s: torch.Tensor, m: torch.Tensor) -> torch.Tensor:\n",
        "        diff = s - m\n",
        "        diff = diff.masked_fill(~torch.isfinite(m), float(\"-inf\"))\n",
        "        return torch.exp(diff)\n",
        "\n",
        "    def _resolve_slot_mask(\n",
        "        self,\n",
        "        slot_mask: Optional[torch.Tensor],\n",
        "        *,\n",
        "        B: int,\n",
        "        H: int,\n",
        "        L: int,\n",
        "        K: int,\n",
        "        device,\n",
        "        dtype,\n",
        "        scope: str,\n",
        "    ) -> Optional[torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Returns expanded mask of shape [B,H,L,K] or None.\n",
        "        slot_mask is expected as [K] where 1=keep, 0=mask.\n",
        "        \"\"\"\n",
        "        if slot_mask is None:\n",
        "            slot_mask = getattr(self, \"slot_mask\", None)\n",
        "        if slot_mask is None:\n",
        "            return None\n",
        "\n",
        "        sm = slot_mask.to(device=device, dtype=dtype)\n",
        "        if sm.ndim != 1 or sm.numel() != K:\n",
        "            raise ValueError(f\"slot_mask must be shape [K]={K}, got {tuple(sm.shape)}\")\n",
        "\n",
        "        sm = sm.view(1, 1, 1, K)  # [1,1,1,K]\n",
        "\n",
        "        if scope == \"all\":\n",
        "            return sm.expand(B, H, L, K)\n",
        "        if scope == \"last_pos_only\":\n",
        "            out = torch.ones((B, H, L, K), device=device, dtype=dtype)\n",
        "            out[:, :, -1:, :] = sm.expand(B, H, 1, K)\n",
        "            return out\n",
        "\n",
        "        raise ValueError(f\"Unknown slot_mask_scope={scope!r}\")\n",
        "\n",
        "    #@torch.compile\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        return_info: bool = False,\n",
        "        routing_mode: str = \"softmax\",           # \"softmax\" | \"top1\" | \"topk\" | \"external\"\n",
        "        routing_topk: int = 2,                   # used if routing_mode==\"topk\"\n",
        "        read_weights_override: Optional[torch.Tensor] = None,  # [B,H,T,K] or [B,H,L,K]\n",
        "        routing_noise: Optional[str] = None,     # None | \"gumbel\" | \"gaussian\"\n",
        "        routing_noise_scale: float = 1.0,\n",
        "\n",
        "        # --- NEW (drop-in; defaults preserve old behavior) ---\n",
        "        slot_mask: Optional[torch.Tensor] = None,            # [K], 1=keep, 0=mask\n",
        "        slot_mask_where: str = \"read\",                       # \"read\" | \"content_read_only\" | \"slotspace_only\"\n",
        "        slot_mask_scope: str = \"all\",                        # \"all\" | \"last_pos_only\"\n",
        "    ) -> Tuple[torch.Tensor, Optional[Dict[str, torch.Tensor]]]:\n",
        "        B, T, C = x.shape\n",
        "        H, K, d = self.num_heads, self.num_slots, self.head_dim\n",
        "\n",
        "        # Project (write K/V, read Q)\n",
        "        k_write = self.Wk_write(x).view(B, T, H, d).transpose(1, 2)  # [B,H,T,d]\n",
        "        v_write = self.Wv_write(x).view(B, T, H, d).transpose(1, 2)  # [B,H,T,d]\n",
        "        q_read  = self.Wq_read(x).view(B, T, H, d).transpose(1, 2)   # [B,H,T,d]\n",
        "\n",
        "        if self.normalize_k:\n",
        "            k_write = F.normalize(k_write, dim=-1, eps=1e-8)\n",
        "\n",
        "        # RoPE on write keys\n",
        "        if self.use_rope_keys:\n",
        "            cos, sin = self.rope.get_cos_sin(T, device=x.device, dtype=k_write.dtype)\n",
        "            k_write = apply_rope(k_write, cos, sin)\n",
        "\n",
        "        # Slot dropout\n",
        "        slot_keys = self.slot_keys\n",
        "        if self.training and self.slot_dropout > 0.0:\n",
        "            drop = (torch.rand((H, K), device=x.device) < self.slot_dropout)\n",
        "            slot_keys = slot_keys * (~drop).to(slot_keys.dtype).unsqueeze(-1)\n",
        "\n",
        "        # WRITE logits: [B,H,K,T]\n",
        "        write_logits_raw = torch.einsum(\"hkd,bhtd->bhkt\", slot_keys, k_write) / math.sqrt(d)\n",
        "\n",
        "        # Stable dtype for prefix-softmax math\n",
        "        state_dtype = torch.float32 if (self.state_fp32 and x.dtype != torch.float32) else x.dtype\n",
        "        write_logits = write_logits_raw.to(state_dtype)\n",
        "\n",
        "        # Write temperature\n",
        "        wtemp = max(1e-6, self.write_temperature)\n",
        "        write_logits = write_logits / wtemp\n",
        "\n",
        "        # ALiBi distance bias (prefix-friendly)\n",
        "        alibi_bias_applied = None\n",
        "        if self.use_alibi_write:\n",
        "            strength = self._alibi_strength(dtype=state_dtype, device=x.device)  # scalar\n",
        "            slopes = self._alibi_slopes.to(device=x.device, dtype=state_dtype) * strength  # [H]\n",
        "            pos_i = torch.arange(T, device=x.device, dtype=state_dtype)  # [T]\n",
        "            alibi_bias = slopes.view(1, H, 1, 1) * pos_i.view(1, 1, 1, T) # [1,H,1,T]\n",
        "            write_logits = write_logits + alibi_bias\n",
        "            alibi_bias_applied = alibi_bias\n",
        "\n",
        "        # Key padding mask (mask positions that are padding)\n",
        "        if attention_mask is not None:\n",
        "            valid = attention_mask.to(dtype=torch.bool)\n",
        "            write_logits = write_logits.masked_fill(~valid.view(B, 1, 1, T), float(\"-inf\"))\n",
        "        else:\n",
        "            valid = None\n",
        "\n",
        "        # =====================================================\n",
        "        # STREAMING WRITE + READ (no [B,H,K,T,d] slot states)\n",
        "        # =====================================================\n",
        "        content_read_gamma = self._content_read_gamma(dtype=q_read.dtype, device=x.device)\n",
        "        rtemp = max(1e-6, self.read_temperature)\n",
        "\n",
        "        out_h = torch.empty((B, H, T, d), device=x.device, dtype=state_dtype)\n",
        "        read_weights = torch.empty((B, H, T, K), device=x.device, dtype=q_read.dtype)\n",
        "\n",
        "        # Optional analytics: [B,H,T,K] (later permuted to [B,H,K,T])\n",
        "        slot_state_norm_t = torch.empty((B, H, T, K), device=x.device, dtype=torch.float32) if return_info else None\n",
        "\n",
        "        denom_state = torch.zeros((B, H, K), device=x.device, dtype=state_dtype)\n",
        "        numer_state = torch.zeros((B, H, K, d), device=x.device, dtype=state_dtype)\n",
        "        m_state = torch.full((B, H, K), float(\"-inf\"), device=x.device, dtype=state_dtype)\n",
        "\n",
        "        # Optional analytics: full read logits across T\n",
        "        if return_info:\n",
        "            read_logits_full = torch.empty((B, H, T, K), device=x.device, dtype=state_dtype)\n",
        "            read_logits_key_full = torch.empty((B, H, T, K), device=x.device, dtype=state_dtype)\n",
        "            read_logits_content_full = (\n",
        "                torch.empty((B, H, T, K), device=x.device, dtype=state_dtype) if self.use_content_read else None\n",
        "            )\n",
        "        else:\n",
        "            read_logits_full = None\n",
        "            read_logits_key_full = None\n",
        "            read_logits_content_full = None\n",
        "\n",
        "        WRITE_CHUNK = self.write_chunk_size\n",
        "\n",
        "        for t0 in range(0, T, WRITE_CHUNK):\n",
        "            t1 = min(T, t0 + WRITE_CHUNK)\n",
        "            L = t1 - t0\n",
        "\n",
        "            wlog_c = write_logits[:, :, :, t0:t1]  # [B,H,K,L]\n",
        "\n",
        "            # streaming cummax\n",
        "            m_c, _ = torch.cummax(wlog_c, dim=-1)  # [B,H,K,L]\n",
        "            m_new = torch.maximum(m_state.unsqueeze(-1), m_c)  # [B,H,K,L]\n",
        "\n",
        "            # rescale old prefix state to new max reference\n",
        "            scale = torch.exp(m_state.unsqueeze(-1) - m_new)  # [B,H,K,L] (exp(-inf)=0)\n",
        "\n",
        "            denom_c = denom_state.unsqueeze(-1) * scale                  # [B,H,K,L]\n",
        "            numer_c = numer_state.unsqueeze(-2) * scale.unsqueeze(-1)    # [B,H,K,L,d]\n",
        "\n",
        "            # new weights\n",
        "            w_new = self._safe_exp_sub_max(wlog_c, m_new)  # [B,H,K,L]\n",
        "\n",
        "            # accumulate within chunk\n",
        "            denom_c = denom_c + torch.cumsum(w_new, dim=-1)  # [B,H,K,L]\n",
        "            v_c = v_write[:, :, t0:t1, :].to(state_dtype)    # [B,H,L,d]\n",
        "            add = torch.cumsum(w_new.unsqueeze(-1) * v_c.unsqueeze(2), dim=-2)  # [B,H,K,L,d]\n",
        "            numer_c = numer_c + add\n",
        "\n",
        "            # per-token slot state for this chunk only\n",
        "            slot_state_c = numer_c / denom_c.clamp_min(1e-8).unsqueeze(-1)  # [B,H,K,L,d]\n",
        "            slot_state_t = slot_state_c.permute(0, 1, 3, 2, 4).contiguous() # [B,H,L,K,d]\n",
        "\n",
        "            # READ routing logits\n",
        "            q_read_c = q_read[:, :, t0:t1, :]  # [B,H,L,d]\n",
        "\n",
        "            read_logits_key = torch.einsum(\"bhld,hkd->bhlk\", q_read_c, slot_keys) / math.sqrt(d)\n",
        "\n",
        "            read_logits_content = None\n",
        "            if self.use_content_read:\n",
        "                read_logits_content = torch.einsum(\n",
        "                    \"bhld,bhlkd->bhlk\",\n",
        "                    q_read_c,\n",
        "                    slot_state_t.to(q_read_c.dtype)\n",
        "                ) / math.sqrt(d)\n",
        "\n",
        "            # ---- NEW: slot masking (logit-level or deferred) ----\n",
        "            sm = self._resolve_slot_mask(\n",
        "                slot_mask,\n",
        "                B=B, H=H, L=L, K=K,\n",
        "                device=x.device,\n",
        "                dtype=read_logits_key.dtype,\n",
        "                scope=slot_mask_scope,\n",
        "            )\n",
        "\n",
        "            if slot_mask_where == \"read\":\n",
        "                if sm is not None:\n",
        "                    read_logits_key = read_logits_key.masked_fill(sm <= 0.0, float(\"-inf\"))\n",
        "                    if self.use_content_read and read_logits_content is not None:\n",
        "                        read_logits_content = read_logits_content.masked_fill(sm <= 0.0, float(\"-inf\"))\n",
        "\n",
        "            elif slot_mask_where == \"content_read_only\":\n",
        "                if sm is not None and self.use_content_read and read_logits_content is not None:\n",
        "                    # Remove ONLY the content contribution for masked slots\n",
        "                    read_logits_content = read_logits_content.masked_fill(sm <= 0.0, 0.0)\n",
        "\n",
        "            elif slot_mask_where == \"slotspace_only\":\n",
        "                pass  # applied later on slot_w\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown slot_mask_where={slot_mask_where!r}\")\n",
        "\n",
        "            # combine logits\n",
        "            read_logits = read_logits_key\n",
        "            if self.use_content_read and read_logits_content is not None:\n",
        "                read_logits = read_logits + content_read_gamma.to(read_logits.dtype) * read_logits_content\n",
        "\n",
        "            if return_info:\n",
        "                read_logits_full[:, :, t0:t1, :] = read_logits.to(state_dtype)\n",
        "                read_logits_key_full[:, :, t0:t1, :] = read_logits_key.to(state_dtype)\n",
        "                if self.use_content_read and read_logits_content_full is not None:\n",
        "                    read_logits_content_full[:, :, t0:t1, :] = read_logits_content.to(state_dtype)\n",
        "\n",
        "            # Optional routing noise\n",
        "            if routing_noise is not None:\n",
        "                if routing_noise == \"gumbel\":\n",
        "                    u = torch.rand_like(read_logits)\n",
        "                    g = -torch.log(-torch.log(u.clamp_min(1e-8)).clamp_min(1e-8))\n",
        "                    read_logits = read_logits + routing_noise_scale * g\n",
        "                elif routing_noise == \"gaussian\":\n",
        "                    read_logits = read_logits + routing_noise_scale * torch.randn_like(read_logits)\n",
        "                else:\n",
        "                    raise ValueError(f\"Unknown routing_noise={routing_noise}\")\n",
        "\n",
        "            # Routing override / mode\n",
        "            if self.routing_override is not None:\n",
        "                if callable(self.routing_override):\n",
        "                    ctx = {\n",
        "                        \"t0\": t0, \"t1\": t1,\n",
        "                        \"B\": B, \"H\": H, \"T\": T, \"K\": K, \"d\": d,\n",
        "                        \"rtemp\": rtemp,\n",
        "                        \"state_dtype\": state_dtype,\n",
        "                        \"q_read_c\": q_read_c,          # [B,H,L,d]\n",
        "                        \"slot_keys\": slot_keys,        # [H,K,d]\n",
        "                        \"slot_state_t\": slot_state_t,  # [B,H,L,K,d]\n",
        "                        \"valid\": valid,\n",
        "                        \"slot_mask\": slot_mask,\n",
        "                        \"slot_mask_where\": slot_mask_where,\n",
        "                        \"slot_mask_scope\": slot_mask_scope,\n",
        "                    }\n",
        "                    read_w_c = self.routing_override(\n",
        "                        t0, t1,\n",
        "                        read_logits,\n",
        "                        read_logits_key,\n",
        "                        read_logits_content,\n",
        "                        ctx,\n",
        "                    )\n",
        "                else:\n",
        "                    read_w_c = self.routing_override[:, :, t0:t1, :].to(read_logits.dtype)\n",
        "\n",
        "                read_w_c = torch.nan_to_num(read_w_c, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "                read_w_c = read_w_c.clamp_min(0.0)\n",
        "                read_w_c = read_w_c / read_w_c.sum(dim=-1, keepdim=True).clamp_min(1e-8)\n",
        "\n",
        "            else:\n",
        "                if routing_mode == \"softmax\":\n",
        "                    read_w_c = torch.softmax(read_logits / rtemp, dim=-1)  # [B,H,L,K]\n",
        "                elif routing_mode == \"top1\":\n",
        "                    top = read_logits.argmax(dim=-1)  # [B,H,L]\n",
        "                    read_w_c = F.one_hot(top, num_classes=K).to(read_logits.dtype)\n",
        "                elif routing_mode == \"topk\":\n",
        "                    kk = max(1, min(K, int(routing_topk)))\n",
        "                    vals, idx = torch.topk(read_logits, k=kk, dim=-1)\n",
        "                    masked = torch.full_like(read_logits, float(\"-inf\"))\n",
        "                    masked.scatter_(-1, idx, vals)\n",
        "                    read_w_c = torch.softmax(masked / rtemp, dim=-1)\n",
        "                elif routing_mode == \"external\":\n",
        "                    if read_weights_override is None:\n",
        "                        raise ValueError(\"routing_mode='external' requires read_weights_override\")\n",
        "                    if read_weights_override.shape[-2] == T:\n",
        "                        read_w_c = read_weights_override[:, :, t0:t1, :]\n",
        "                    else:\n",
        "                        read_w_c = read_weights_override\n",
        "                    read_w_c = read_w_c / read_w_c.sum(dim=-1, keepdim=True).clamp_min(1e-8)\n",
        "                else:\n",
        "                    raise ValueError(f\"Unknown routing_mode={routing_mode}\")\n",
        "\n",
        "            # If slot_mask_where == \"read\" but override/mode produced nonzero on masked slots,\n",
        "            # enforce hard zero + renorm (keeps your masked_mass sanity check honest).\n",
        "            if slot_mask_where == \"read\" and sm is not None:\n",
        "                read_w_c = read_w_c * (sm > 0.0).to(read_w_c.dtype)\n",
        "                read_w_c = read_w_c / read_w_c.sum(dim=-1, keepdim=True).clamp_min(1e-8)\n",
        "\n",
        "            read_weights[:, :, t0:t1, :] = read_w_c\n",
        "\n",
        "            # token output\n",
        "            out_h[:, :, t0:t1, :] = torch.einsum(\n",
        "                \"bhlk,bhlkd->bhld\",\n",
        "                read_w_c.to(state_dtype),\n",
        "                slot_state_t.to(state_dtype),\n",
        "            )\n",
        "\n",
        "            if return_info:\n",
        "                slot_state_norm_t[:, :, t0:t1, :] = slot_state_t.to(torch.float32).norm(dim=-1)\n",
        "\n",
        "            # update running states\n",
        "            m_state = m_new[:, :, :, -1]\n",
        "            denom_state = denom_c[:, :, :, -1]\n",
        "            numer_state = numer_c[:, :, :, -1, :]\n",
        "\n",
        "            pre_slotspace_norm = None\n",
        "            if return_info:\n",
        "                pre_slotspace_norm = out_h.detach().to(torch.float32).norm(dim=-1).mean().cpu()\n",
        "\n",
        "\n",
        "        # =====================================================\n",
        "        # causal linear attention in slot-space (CHUNKED prefix scan)\n",
        "        # =====================================================\n",
        "        slotspace_delta_norm_mean = None\n",
        "        if self.use_slotspace_refine:\n",
        "            slotspace_dtype = state_dtype\n",
        "            M = self.slotspace_dim\n",
        "\n",
        "            # Encode read weights into slot-space coordinates\n",
        "            u = self.slot_in(read_weights.to(slotspace_dtype))  # [B,H,T,M]\n",
        "            q_s  = self.slot_q(u)\n",
        "            k_s  = self.slot_k(u)\n",
        "            v_s  = self.slot_v(u)\n",
        "\n",
        "            # RoPE in slot-space matcher (Q/K only)\n",
        "            if self.use_rope_slotspace:\n",
        "                cos_s, sin_s = self.rope_slotspace.get_cos_sin(T, device=x.device, dtype=q_s.dtype)\n",
        "                q_s = apply_rope(q_s, cos_s, sin_s)\n",
        "                k_s = apply_rope(k_s, cos_s, sin_s)\n",
        "\n",
        "            qf = phi(q_s)\n",
        "            kf = phi(k_s)\n",
        "\n",
        "            if valid is not None:\n",
        "                mask = valid.view(B, 1, T, 1).to(slotspace_dtype)\n",
        "                qf = qf * mask\n",
        "                kf = kf * mask\n",
        "                v_s = v_s * mask\n",
        "\n",
        "            u2 = torch.empty((B, H, T, M), device=x.device, dtype=slotspace_dtype)\n",
        "\n",
        "            S_state = torch.zeros((B, H, M, M), device=x.device, dtype=slotspace_dtype)\n",
        "            Z_state = torch.zeros((B, H, M), device=x.device, dtype=slotspace_dtype)\n",
        "\n",
        "            SS_CHUNK = self.slotspace_chunk_size\n",
        "\n",
        "            for t0 in range(0, T, SS_CHUNK):\n",
        "                t1 = min(T, t0 + SS_CHUNK)\n",
        "                L = t1 - t0\n",
        "\n",
        "                qf_c = qf[:, :, t0:t1, :]   # [B,H,L,M]\n",
        "                kf_c = kf[:, :, t0:t1, :]   # [B,H,L,M]\n",
        "                v_c  = v_s[:, :, t0:t1, :]  # [B,H,L,M]\n",
        "\n",
        "                kv = torch.einsum(\"bhlm,bhln->bhlmn\", kf_c, v_c)  # [B,H,L,M,M]\n",
        "                S_c = torch.cumsum(kv, dim=2)\n",
        "                Z_c = torch.cumsum(kf_c, dim=2)\n",
        "\n",
        "                S_c = S_c + S_state.unsqueeze(2)\n",
        "                Z_c = (Z_c + Z_state.unsqueeze(2)).clamp_min(1e-8)\n",
        "\n",
        "                num = torch.einsum(\"bhlm,bhlmn->bhln\", qf_c, S_c)\n",
        "                den = torch.einsum(\"bhlm,bhlm->bhl\", qf_c, Z_c).unsqueeze(-1).clamp_min(1e-8)\n",
        "                u2[:, :, t0:t1, :] = num / den\n",
        "\n",
        "                S_state = S_c[:, :, -1, :, :]\n",
        "                Z_state = Z_c[:, :, -1, :]\n",
        "\n",
        "            u2 = self.slotspace_dropout(u2)\n",
        "\n",
        "            # Decode slot weights per token\n",
        "            slot_w = self.slot_out(u2)  # [B,H,T,K]\n",
        "            if self.slotspace_signed_weights:\n",
        "                slot_w = torch.tanh(slot_w)\n",
        "            else:\n",
        "                slot_w = torch.softmax(slot_w, dim=-1)\n",
        "\n",
        "            # ---- NEW: slotspace-only mask (hard zero) ----\n",
        "            if slot_mask_where == \"slotspace_only\":\n",
        "                sm_full = self._resolve_slot_mask(\n",
        "                    slot_mask,\n",
        "                    B=B, H=H, L=T, K=K,\n",
        "                    device=x.device,\n",
        "                    dtype=slot_w.dtype,\n",
        "                    scope=slot_mask_scope,  # \"all\" masks everywhere; \"last_pos_only\" masks only last token\n",
        "                )\n",
        "                if sm_full is not None:\n",
        "                    slot_w = slot_w * (sm_full > 0.0).to(slot_w.dtype)\n",
        "                    if not self.slotspace_signed_weights:\n",
        "                        slot_w = slot_w / slot_w.sum(dim=-1, keepdim=True).clamp_min(1e-8)\n",
        "\n",
        "            gate = self._slotspace_gate(dtype=state_dtype, device=x.device).to(state_dtype)\n",
        "\n",
        "            # second streaming pass (decode delta through slot states)\n",
        "            denom_state = torch.zeros((B, H, K), device=x.device, dtype=state_dtype)\n",
        "            numer_state = torch.zeros((B, H, K, d), device=x.device, dtype=state_dtype)\n",
        "            m_state = torch.full((B, H, K), float(\"-inf\"), device=x.device, dtype=state_dtype)\n",
        "\n",
        "            delta_norm_sum = torch.zeros((), device=x.device, dtype=torch.float32)\n",
        "            delta_norm_count = 0\n",
        "\n",
        "            for t0 in range(0, T, WRITE_CHUNK):\n",
        "                t1 = min(T, t0 + WRITE_CHUNK)\n",
        "                L = t1 - t0\n",
        "\n",
        "                wlog_c = write_logits[:, :, :, t0:t1]  # [B,H,K,L]\n",
        "                m_c, _ = torch.cummax(wlog_c, dim=-1)\n",
        "                m_new = torch.maximum(m_state.unsqueeze(-1), m_c)\n",
        "\n",
        "                scale = torch.exp(m_state.unsqueeze(-1) - m_new)\n",
        "                denom_c = denom_state.unsqueeze(-1) * scale\n",
        "                numer_c = numer_state.unsqueeze(-2) * scale.unsqueeze(-1)\n",
        "\n",
        "                w_new = self._safe_exp_sub_max(wlog_c, m_new)\n",
        "                denom_c = denom_c + torch.cumsum(w_new, dim=-1)\n",
        "\n",
        "                v_c = v_write[:, :, t0:t1, :].to(state_dtype)  # [B,H,L,d]\n",
        "                add = torch.cumsum(w_new.unsqueeze(-1) * v_c.unsqueeze(2), dim=-2)\n",
        "                numer_c = numer_c + add\n",
        "\n",
        "                slot_state_c = numer_c / denom_c.clamp_min(1e-8).unsqueeze(-1)  # [B,H,K,L,d]\n",
        "                slot_state_t = slot_state_c.permute(0, 1, 3, 2, 4).contiguous() # [B,H,L,K,d]\n",
        "\n",
        "                slot_w_c = slot_w[:, :, t0:t1, :].to(state_dtype)  # [B,H,L,K]\n",
        "\n",
        "                delta_c = torch.einsum(\"bhlk,bhlkd->bhld\", slot_w_c, slot_state_t.to(state_dtype))  # [B,H,L,d]\n",
        "\n",
        "                out_h[:, :, t0:t1, :] = out_h[:, :, t0:t1, :] + gate * delta_c\n",
        "\n",
        "                delta_norm_sum = delta_norm_sum + delta_c.detach().to(torch.float32).norm(dim=-1).sum()\n",
        "                delta_norm_count += (B * H * L)\n",
        "\n",
        "                m_state = m_new[:, :, :, -1]\n",
        "                denom_state = denom_c[:, :, :, -1]\n",
        "                numer_state = numer_c[:, :, :, -1, :]\n",
        "\n",
        "            slotspace_delta_norm_mean = (delta_norm_sum / max(1, delta_norm_count)).detach().cpu()\n",
        "\n",
        "\n",
        "            post_slotspace_norm = None\n",
        "            if return_info:\n",
        "                post_slotspace_norm = out_h.detach().to(torch.float32).norm(dim=-1).mean().cpu()\n",
        "\n",
        "\n",
        "\n",
        "        # Finish\n",
        "        out = out_h.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        out = self.out_proj(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        info = None\n",
        "        if return_info:\n",
        "\n",
        "            info = {\n",
        "                \"write_logits_raw\": write_logits_raw.detach(),\n",
        "                \"write_logits\": write_logits.detach().to(torch.float32),\n",
        "                \"read_weights\": read_weights.detach(),\n",
        "                \"slot_state_norm\": slot_state_norm_t.detach().permute(0, 1, 3, 2).contiguous() if slot_state_norm_t is not None else None,\n",
        "                \"content_read_gamma\": content_read_gamma.detach().to(torch.float32).cpu(),\n",
        "                \"slot_mask_where\": slot_mask_where,\n",
        "                \"slot_mask_scope\": slot_mask_scope,\n",
        "            }\n",
        "            info[\"out_h_norm_pre_slotspace\"] = pre_slotspace_norm\n",
        "            info[\"out_h_norm_post_slotspace\"] = post_slotspace_norm\n",
        "\n",
        "            if alibi_bias_applied is not None:\n",
        "                info[\"alibi_bias_applied\"] = alibi_bias_applied.detach().to(torch.float32)\n",
        "            if self.use_alibi_write and self.learn_alibi_strength:\n",
        "                info[\"alibi_strength\"] = self._alibi_strength(dtype=torch.float32, device=x.device).detach().cpu()\n",
        "            if self.use_slotspace_refine:\n",
        "                info[\"slotspace_gate\"] = self._slotspace_gate(dtype=torch.float32, device=x.device).detach().cpu()\n",
        "                info[\"use_rope_slotspace\"] = torch.tensor(bool(self.use_rope_slotspace))\n",
        "                if slotspace_delta_norm_mean is not None:\n",
        "                    info[\"slotspace_delta_norm\"] = slotspace_delta_norm_mean\n",
        "\n",
        "            info[\"read_logits\"] = read_logits_full.detach().to(torch.float32) if read_logits_full is not None else None\n",
        "            info[\"read_logits_key\"] = read_logits_key_full.detach().to(torch.float32) if read_logits_key_full is not None else None\n",
        "            info[\"read_logits_content\"] = (\n",
        "                read_logits_content_full.detach().to(torch.float32) if read_logits_content_full is not None else None\n",
        "            )\n",
        "            info[\"routing_mode\"] = routing_mode\n",
        "\n",
        "        return out, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "saNmjRifrHos"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Addressed State Attention ( cleaned + safer; checkpoint-stable)\n",
        "#\n",
        "# Goals:\n",
        "# - Preserve checkpoint compatibility: parameter/buffer names and shapes are unchanged.\n",
        "# - Preserve public API: forward() signature unchanged (incl slot_mask controls).\n",
        "# - Make control-flow easier to reason about: isolate masking, routing, and streaming scans.\n",
        "# - Reduce footguns: no chunk-local pre/post metrics, no dangling hooks, fewer hidden branches.\n",
        "#\n",
        "# Notes on checkpoint stability:\n",
        "# - Do NOT rename any of the following: slot_keys, Wk_write, Wv_write, Wq_read, out_proj,\n",
        "#   _alibi_slopes, _alibi_strength_param, _content_read_gamma_raw, slot_in/slot_q/slot_k/slot_v/slot_out,\n",
        "#   _slotspace_gate_raw, rope/rope_slotspace buffers, etc.\n",
        "# - This class keeps those names intact.\n",
        "\n",
        "import math\n",
        "from typing import Optional, Dict, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# -------------------------\n",
        "# RoPE helper (rotate-half)\n",
        "# -------------------------\n",
        "def _rotate_half(x: torch.Tensor) -> torch.Tensor:\n",
        "    x1 = x[..., ::2]\n",
        "    x2 = x[..., 1::2]\n",
        "    return torch.stack((-x2, x1), dim=-1).flatten(-2)\n",
        "\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    def __init__(self, dim: int, base: float = 10000.0):\n",
        "        super().__init__()\n",
        "        assert dim % 2 == 0, \"RoPE requires even dim\"\n",
        "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
        "        self._cos_cached = None\n",
        "        self._sin_cached = None\n",
        "        self._t_cached = None\n",
        "        self._device_cached = None\n",
        "\n",
        "    def get_cos_sin(self, T: int, device, dtype):\n",
        "        if (\n",
        "            self._t_cached == T\n",
        "            and self._cos_cached is not None\n",
        "            and self._device_cached == device\n",
        "        ):\n",
        "            return self._cos_cached.to(dtype=dtype), self._sin_cached.to(dtype=dtype)\n",
        "\n",
        "        t = torch.arange(T, device=device, dtype=self.inv_freq.dtype)\n",
        "        freqs = torch.einsum(\"t,f->tf\", t, self.inv_freq)  # [T, d/2]\n",
        "        emb = torch.cat([freqs, freqs], dim=-1)            # [T, d]\n",
        "        cos = emb.cos()[None, None, :, :]                  # [1,1,T,d]\n",
        "        sin = emb.sin()[None, None, :, :]                  # [1,1,T,d]\n",
        "\n",
        "        self._t_cached = T\n",
        "        self._device_cached = device\n",
        "        self._cos_cached = cos\n",
        "        self._sin_cached = sin\n",
        "        return cos.to(dtype=dtype), sin.to(dtype=dtype)\n",
        "\n",
        "def apply_rope(x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> torch.Tensor:\n",
        "    return (x * cos) + (_rotate_half(x) * sin)\n",
        "\n",
        "# -------------------------\n",
        "# ALiBi slopes helper\n",
        "# -------------------------\n",
        "def alibi_slopes(num_heads: int, device=None, dtype=torch.float32) -> torch.Tensor:\n",
        "    def get_slopes(n):\n",
        "        def power_of_2_slopes(n):\n",
        "            start = 2.0 ** (-(2.0 ** -(math.log2(n) - 3)))\n",
        "            ratio = start\n",
        "            return [start * (ratio ** i) for i in range(n)]\n",
        "        if math.log2(n).is_integer():\n",
        "            return power_of_2_slopes(n)\n",
        "        closest = 2 ** math.floor(math.log2(n))\n",
        "        return power_of_2_slopes(closest) + get_slopes(2 * closest)[0::2][: n - closest]\n",
        "    return torch.tensor(get_slopes(num_heads), device=device, dtype=dtype)  # [H]\n",
        "\n",
        "# -------------------------\n",
        "# softplus init helpers\n",
        "# -------------------------\n",
        "def _inv_softplus(y: torch.Tensor) -> torch.Tensor:\n",
        "    return torch.log(torch.expm1(y))\n",
        "\n",
        "# -------------------------\n",
        "# Linear attention feature map (Performer-style)\n",
        "# -------------------------\n",
        "def phi(x: torch.Tensor) -> torch.Tensor:\n",
        "    return F.elu(x) + 1.0\n",
        "\n",
        "\n",
        "class AddressedStateAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Addressed State Attention (ASA):\n",
        "      - prefix-softmax WRITE into slots (O(T))\n",
        "      - READ routing from tokens -> slots (softmax over slots)\n",
        "      - content-conditioned READ term (gamma)\n",
        "      - RoPE on write keys (geometry)\n",
        "      - ALiBi bias on write logits (prefix-friendly)\n",
        "\n",
        "    slot-space refinement:\n",
        "      - causal linear attention in a low-dim slot-address coordinate space\n",
        "      - produces per-token signed weights over slots\n",
        "      - decoded through the same streaming slot-state basis\n",
        "      - gated by learnable slotspace_gate (softplus)\n",
        "\n",
        "    ADDITION:\n",
        "      - slot_mask controls for causal interventions:\n",
        "          slot_mask: [K] float/bool where 1=keep, 0=mask\n",
        "          slot_mask_where: \"read\" | \"content_read_only\" | \"slotspace_only\"\n",
        "          slot_mask_scope: \"all\" | \"last_pos_only\"\n",
        "      - also supports attribute fallback: self.slot_mask if slot_mask kwarg is None\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int,\n",
        "        num_heads: int = 8,\n",
        "        num_slots: int = 8,\n",
        "        dropout: float = 0.1,\n",
        "\n",
        "        # temperatures / numerics\n",
        "        read_temperature: float = 1.0,\n",
        "        write_temperature: float = 1.0,\n",
        "        state_fp32: bool = True,\n",
        "        slot_dropout: float = 0.0,\n",
        "        normalize_k: bool = False,\n",
        "\n",
        "        # positions (write geometry)\n",
        "        use_rope_keys: bool = True,\n",
        "        rope_base: float = 10000.0,\n",
        "\n",
        "        # write bias (ALiBi)\n",
        "        use_alibi_write: bool = True,\n",
        "        alibi_strength_init: float = 0.1,\n",
        "        learn_alibi_strength: bool = True,\n",
        "        min_strength: float = 0.0,\n",
        "\n",
        "        # content-conditioned read term\n",
        "        use_content_read: bool = True,\n",
        "        content_read_init: float = -4.0,\n",
        "        content_read_max_gamma: float = 3.0,\n",
        "\n",
        "        # slot-space refinement\n",
        "        use_slotspace_refine: bool = True,\n",
        "        slotspace_dim: int = 32,\n",
        "        slotspace_gate_init: float = -4.0,\n",
        "        slotspace_dropout: float = 0.05,\n",
        "        slotspace_signed_weights: bool = True,\n",
        "\n",
        "        # RoPE in slot-space matcher (Q/K only)\n",
        "        use_rope_slotspace: bool = True,\n",
        "        rope_base_slotspace: float = 100000.0,\n",
        "\n",
        "        # perf knobs (behavior change only if you change them)\n",
        "        write_chunk_size: int = 128,\n",
        "        slotspace_chunk_size: int = 128,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.num_slots = num_slots\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.read_temperature = float(read_temperature)\n",
        "        self.write_temperature = float(write_temperature)\n",
        "        self.state_fp32 = bool(state_fp32)\n",
        "        self.slot_dropout = float(slot_dropout)\n",
        "        self.normalize_k = bool(normalize_k)\n",
        "        self.routing_override = None  # external override hook\n",
        "\n",
        "        # mask attribute fallback (may be set externally)\n",
        "        # expected shape: [K], where 1=keep, 0=mask\n",
        "        # self.slot_mask = None\n",
        "\n",
        "        self.use_rope_keys = bool(use_rope_keys)\n",
        "        self.use_alibi_write = bool(use_alibi_write)\n",
        "        self.learn_alibi_strength = bool(learn_alibi_strength)\n",
        "        self.min_strength = float(min_strength)\n",
        "\n",
        "        self.use_content_read = bool(use_content_read)\n",
        "        self.content_read_max_gamma = float(content_read_max_gamma)\n",
        "\n",
        "        self.use_slotspace_refine = bool(use_slotspace_refine)\n",
        "        self.slotspace_dim = int(slotspace_dim)\n",
        "        self.slotspace_dropout = nn.Dropout(float(slotspace_dropout))\n",
        "        self.slotspace_signed_weights = bool(slotspace_signed_weights)\n",
        "\n",
        "        self.write_chunk_size = int(write_chunk_size)\n",
        "        self.slotspace_chunk_size = int(slotspace_chunk_size)\n",
        "\n",
        "        # Learned slot keys per head: [H,K,d]\n",
        "        self.slot_keys = nn.Parameter(\n",
        "            torch.randn(num_heads, num_slots, self.head_dim) / math.sqrt(self.head_dim)\n",
        "        )\n",
        "\n",
        "        # Projections\n",
        "        self.Wk_write = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.Wv_write = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.Wq_read  = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "\n",
        "        # RoPE (write geometry)\n",
        "        self.rope = RotaryEmbedding(self.head_dim, base=rope_base) if self.use_rope_keys else None\n",
        "\n",
        "        # ALiBi slopes (buffer)\n",
        "        if self.use_alibi_write:\n",
        "            self.register_buffer(\"_alibi_slopes\", alibi_slopes(num_heads), persistent=False)  # [H]\n",
        "        else:\n",
        "            self.register_buffer(\"_alibi_slopes\", torch.zeros(num_heads), persistent=False)\n",
        "\n",
        "        # Learnable ALiBi strength (positive via softplus)\n",
        "        if self.use_alibi_write and self.learn_alibi_strength:\n",
        "            init = torch.tensor(float(alibi_strength_init) - self.min_strength).clamp_min(1e-8)\n",
        "            self._alibi_strength_param = nn.Parameter(_inv_softplus(init))\n",
        "        else:\n",
        "            self._alibi_strength_param = None\n",
        "            self.alibi_strength = float(alibi_strength_init)\n",
        "\n",
        "        # Content read gamma (>=0 via softplus)\n",
        "        if self.use_content_read:\n",
        "            self._content_read_gamma_raw = nn.Parameter(torch.tensor(float(content_read_init)))\n",
        "        else:\n",
        "            self._content_read_gamma_raw = None\n",
        "\n",
        "        # slot-space refinement\n",
        "        self.use_rope_slotspace = bool(use_rope_slotspace) and bool(self.use_slotspace_refine)\n",
        "        if self.use_slotspace_refine:\n",
        "            self.slot_in  = nn.Linear(num_slots, self.slotspace_dim, bias=False)\n",
        "            self.slot_q   = nn.Linear(self.slotspace_dim, self.slotspace_dim, bias=False)\n",
        "            self.slot_k   = nn.Linear(self.slotspace_dim, self.slotspace_dim, bias=False)\n",
        "            self.slot_v   = nn.Linear(self.slotspace_dim, self.slotspace_dim, bias=False)\n",
        "            self.slot_out = nn.Linear(self.slotspace_dim, num_slots, bias=False)\n",
        "            self._slotspace_gate_raw = nn.Parameter(torch.tensor(float(slotspace_gate_init)))\n",
        "\n",
        "            if self.use_rope_slotspace:\n",
        "                assert (self.slotspace_dim % 2) == 0, \"use_rope_slotspace requires even slotspace_dim\"\n",
        "                self.rope_slotspace = RotaryEmbedding(self.slotspace_dim, base=float(rope_base_slotspace))\n",
        "            else:\n",
        "                self.rope_slotspace = None\n",
        "        else:\n",
        "            self.slot_in = None\n",
        "            self.slot_q = self.slot_k = self.slot_v = None\n",
        "            self.slot_out = None\n",
        "            self._slotspace_gate_raw = None\n",
        "            self.rope_slotspace = None\n",
        "\n",
        "    # -------------------------\n",
        "    # scalar params\n",
        "    # -------------------------\n",
        "    def _alibi_strength(self, dtype, device) -> torch.Tensor:\n",
        "        if not (self.use_alibi_write and self.learn_alibi_strength):\n",
        "            return torch.tensor(self.alibi_strength, dtype=dtype, device=device)\n",
        "        return (F.softplus(self._alibi_strength_param) + self.min_strength).to(dtype=dtype, device=device)\n",
        "\n",
        "    def _content_read_gamma(self, dtype, device) -> torch.Tensor:\n",
        "        if not self.use_content_read:\n",
        "            return torch.tensor(0.0, dtype=dtype, device=device)\n",
        "        g = F.softplus(self._content_read_gamma_raw)  # >=0\n",
        "        if self.content_read_max_gamma is not None and self.content_read_max_gamma > 0:\n",
        "            g = g.clamp(max=self.content_read_max_gamma)\n",
        "        return g.to(dtype=dtype, device=device)\n",
        "\n",
        "    def _slotspace_gate(self, dtype, device) -> torch.Tensor:\n",
        "        if not self.use_slotspace_refine:\n",
        "            return torch.tensor(0.0, dtype=dtype, device=device)\n",
        "        return F.softplus(self._slotspace_gate_raw).to(dtype=dtype, device=device)\n",
        "\n",
        "    # -------------------------\n",
        "    # numerics helpers\n",
        "    # -------------------------\n",
        "    @staticmethod\n",
        "    def _safe_exp_sub_max(s: torch.Tensor, m: torch.Tensor) -> torch.Tensor:\n",
        "        # s, m broadcastable\n",
        "        diff = s - m\n",
        "        diff = diff.masked_fill(~torch.isfinite(m), float(\"-inf\"))\n",
        "        return torch.exp(diff)\n",
        "\n",
        "    def _resolve_slot_mask(\n",
        "        self,\n",
        "        slot_mask: Optional[torch.Tensor],\n",
        "        *,\n",
        "        B: int,\n",
        "        H: int,\n",
        "        L: int,\n",
        "        K: int,\n",
        "        device,\n",
        "        dtype,\n",
        "        scope: str,\n",
        "    ) -> Optional[torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Returns expanded mask of shape [B,H,L,K] or None.\n",
        "        slot_mask is expected as [K] where 1=keep, 0=mask.\n",
        "        \"\"\"\n",
        "        if slot_mask is None:\n",
        "            slot_mask = getattr(self, \"slot_mask\", None)\n",
        "        if slot_mask is None:\n",
        "            return None\n",
        "\n",
        "        sm = slot_mask.to(device=device, dtype=dtype)\n",
        "        if sm.ndim != 1 or sm.numel() != K:\n",
        "            raise ValueError(f\"slot_mask must be shape [K]={K}, got {tuple(sm.shape)}\")\n",
        "\n",
        "        sm = sm.view(1, 1, 1, K)  # [1,1,1,K]\n",
        "        if scope == \"all\":\n",
        "            return sm.expand(B, H, L, K)\n",
        "        if scope == \"last_pos_only\":\n",
        "            out = torch.ones((B, H, L, K), device=device, dtype=dtype)\n",
        "            out[:, :, -1:, :] = sm.expand(B, H, 1, K)\n",
        "            return out\n",
        "        raise ValueError(f\"Unknown slot_mask_scope={scope!r}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def _apply_hard_mask_and_renorm(w: torch.Tensor, keep: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        w: [...,K]\n",
        "        keep: broadcastable boolean/float mask where True/1 means keep.\n",
        "        Ensures masked entries are exactly zero and rows renormalized.\n",
        "        \"\"\"\n",
        "        w = w * keep.to(w.dtype)\n",
        "        return w / w.sum(dim=-1, keepdim=True).clamp_min(1e-8)\n",
        "\n",
        "    # -------------------------\n",
        "    # routing helper\n",
        "    # -------------------------\n",
        "    def _compute_read_weights(\n",
        "        self,\n",
        "        *,\n",
        "        read_logits: torch.Tensor,               # [B,H,L,K]\n",
        "        read_logits_key: torch.Tensor,           # [B,H,L,K]\n",
        "        read_logits_content: Optional[torch.Tensor],  # [B,H,L,K] or None\n",
        "        routing_mode: str,\n",
        "        routing_topk: int,\n",
        "        read_weights_override: Optional[torch.Tensor],\n",
        "        routing_noise: Optional[str],\n",
        "        routing_noise_scale: float,\n",
        "        rtemp: float,\n",
        "        sm: Optional[torch.Tensor],              # [B,H,L,K] or None (expanded slot mask)\n",
        "        slot_mask_where: str,\n",
        "        B: int, H: int, L: int, K: int,\n",
        "        T_total: int,\n",
        "        t0: int, t1: int,\n",
        "    ) -> torch.Tensor:\n",
        "        # routing noise (applies to combined logits)\n",
        "        if routing_noise is not None:\n",
        "            if routing_noise == \"gumbel\":\n",
        "                u = torch.rand_like(read_logits)\n",
        "                g = -torch.log(-torch.log(u.clamp_min(1e-8)).clamp_min(1e-8))\n",
        "                read_logits = read_logits + routing_noise_scale * g\n",
        "            elif routing_noise == \"gaussian\":\n",
        "                read_logits = read_logits + routing_noise_scale * torch.randn_like(read_logits)\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown routing_noise={routing_noise}\")\n",
        "\n",
        "        # routing override (external) OR standard modes\n",
        "        if self.routing_override is not None:\n",
        "            if callable(self.routing_override):\n",
        "                ctx = {\n",
        "                    \"t0\": t0, \"t1\": t1,\n",
        "                    \"B\": B, \"H\": H, \"T\": T_total, \"K\": K, \"d\": self.head_dim,\n",
        "                    \"rtemp\": rtemp,\n",
        "                    \"state_dtype\": read_logits.dtype,\n",
        "                    \"q_read_c\": None,       # intentionally omitted here; keep ctx minimal/stable\n",
        "                    \"slot_keys\": self.slot_keys,\n",
        "                    \"slot_state_t\": None,   # intentionally omitted here\n",
        "                    \"valid\": None,          # omitted; caller can supply via closure if needed\n",
        "                    \"slot_mask\": None,      # omitted; caller already has it\n",
        "                    \"slot_mask_where\": slot_mask_where,\n",
        "                }\n",
        "                read_w = self.routing_override(\n",
        "                    t0, t1,\n",
        "                    read_logits,\n",
        "                    read_logits_key,\n",
        "                    read_logits_content,\n",
        "                    ctx,\n",
        "                )\n",
        "            else:\n",
        "                read_w = self.routing_override[:, :, t0:t1, :].to(read_logits.dtype)\n",
        "\n",
        "            read_w = torch.nan_to_num(read_w, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "            read_w = read_w.clamp_min(0.0)\n",
        "            read_w = read_w / read_w.sum(dim=-1, keepdim=True).clamp_min(1e-8)\n",
        "        else:\n",
        "            if routing_mode == \"softmax\":\n",
        "                read_w = torch.softmax(read_logits / rtemp, dim=-1)\n",
        "            elif routing_mode == \"top1\":\n",
        "                top = read_logits.argmax(dim=-1)  # [B,H,L]\n",
        "                read_w = F.one_hot(top, num_classes=K).to(read_logits.dtype)\n",
        "            elif routing_mode == \"topk\":\n",
        "                kk = max(1, min(K, int(routing_topk)))\n",
        "                vals, idx = torch.topk(read_logits, k=kk, dim=-1)\n",
        "                masked = torch.full_like(read_logits, float(\"-inf\"))\n",
        "                masked.scatter_(-1, idx, vals)\n",
        "                read_w = torch.softmax(masked / rtemp, dim=-1)\n",
        "            elif routing_mode == \"external\":\n",
        "                if read_weights_override is None:\n",
        "                    raise ValueError(\"routing_mode='external' requires read_weights_override\")\n",
        "                if read_weights_override.shape[-2] == T_total:\n",
        "                    read_w = read_weights_override[:, :, t0:t1, :]\n",
        "                else:\n",
        "                    read_w = read_weights_override\n",
        "                read_w = read_w / read_w.sum(dim=-1, keepdim=True).clamp_min(1e-8)\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown routing_mode={routing_mode}\")\n",
        "\n",
        "        # enforce mask if the user requested mask-at-read\n",
        "        if slot_mask_where == \"read\" and sm is not None:\n",
        "            read_w = self._apply_hard_mask_and_renorm(read_w, (sm > 0.0))\n",
        "        return read_w\n",
        "\n",
        "    # -------------------------\n",
        "    # forward\n",
        "    # -------------------------\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        return_info: bool = False,\n",
        "        routing_mode: str = \"softmax\",\n",
        "        routing_topk: int = 2,\n",
        "        read_weights_override: Optional[torch.Tensor] = None,\n",
        "        routing_noise: Optional[str] = None,\n",
        "        routing_noise_scale: float = 1.0,\n",
        "\n",
        "        slot_mask: Optional[torch.Tensor] = None,     # [K], 1=keep, 0=mask\n",
        "        slot_mask_where: str = \"read\",                # \"read\" | \"content_read_only\" | \"slotspace_only\"\n",
        "        slot_mask_scope: str = \"all\",                 # \"all\" | \"last_pos_only\"\n",
        "    ) -> Tuple[torch.Tensor, Optional[Dict[str, torch.Tensor]]]:\n",
        "\n",
        "        B, T, C = x.shape\n",
        "        H, K, d = self.num_heads, self.num_slots, self.head_dim\n",
        "\n",
        "        # Project (write K/V, read Q)\n",
        "        k_write = self.Wk_write(x).view(B, T, H, d).transpose(1, 2)  # [B,H,T,d]\n",
        "        v_write = self.Wv_write(x).view(B, T, H, d).transpose(1, 2)  # [B,H,T,d]\n",
        "        q_read  = self.Wq_read(x).view(B, T, H, d).transpose(1, 2)   # [B,H,T,d]\n",
        "\n",
        "        if self.normalize_k:\n",
        "            k_write = F.normalize(k_write, dim=-1, eps=1e-8)\n",
        "\n",
        "        # RoPE on write keys\n",
        "        if self.use_rope_keys:\n",
        "            cos, sin = self.rope.get_cos_sin(T, device=x.device, dtype=k_write.dtype)\n",
        "            k_write = apply_rope(k_write, cos, sin)\n",
        "\n",
        "        # Slot dropout (only affects slot_keys in training)\n",
        "        slot_keys = self.slot_keys\n",
        "        if self.training and self.slot_dropout > 0.0:\n",
        "            drop = (torch.rand((H, K), device=x.device) < self.slot_dropout)\n",
        "            slot_keys = slot_keys * (~drop).to(slot_keys.dtype).unsqueeze(-1)\n",
        "\n",
        "        # WRITE logits: [B,H,K,T]\n",
        "        write_logits_raw = torch.einsum(\"hkd,bhtd->bhkt\", slot_keys, k_write) / math.sqrt(d)\n",
        "\n",
        "        # Stable dtype for prefix-softmax math\n",
        "        state_dtype = torch.float32 if (self.state_fp32 and x.dtype != torch.float32) else x.dtype\n",
        "        write_logits = write_logits_raw.to(state_dtype)\n",
        "\n",
        "        # Write temperature\n",
        "        wtemp = max(1e-6, self.write_temperature)\n",
        "        write_logits = write_logits / wtemp\n",
        "\n",
        "        # ALiBi distance bias (prefix-friendly)\n",
        "        alibi_bias_applied = None\n",
        "        if self.use_alibi_write:\n",
        "            strength = self._alibi_strength(dtype=state_dtype, device=x.device)  # scalar\n",
        "            slopes = self._alibi_slopes.to(device=x.device, dtype=state_dtype) * strength  # [H]\n",
        "            pos_i = torch.arange(T, device=x.device, dtype=state_dtype)  # [T]\n",
        "            alibi_bias = slopes.view(1, H, 1, 1) * pos_i.view(1, 1, 1, T) # [1,H,1,T]\n",
        "            write_logits = write_logits + alibi_bias\n",
        "            alibi_bias_applied = alibi_bias\n",
        "\n",
        "        # Key padding mask\n",
        "        if attention_mask is not None:\n",
        "            valid = attention_mask.to(dtype=torch.bool)\n",
        "            write_logits = write_logits.masked_fill(~valid.view(B, 1, 1, T), float(\"-inf\"))\n",
        "        else:\n",
        "            valid = None\n",
        "\n",
        "        # -------------------------\n",
        "        # Streaming WRITE + READ\n",
        "        # -------------------------\n",
        "        content_read_gamma = self._content_read_gamma(dtype=q_read.dtype, device=x.device)\n",
        "        rtemp = max(1e-6, self.read_temperature)\n",
        "\n",
        "        out_h = torch.empty((B, H, T, d), device=x.device, dtype=state_dtype)\n",
        "        read_weights = torch.empty((B, H, T, K), device=x.device, dtype=q_read.dtype)\n",
        "\n",
        "        slot_state_norm_t = torch.empty((B, H, T, K), device=x.device, dtype=torch.float32) if return_info else None\n",
        "\n",
        "        denom_state = torch.zeros((B, H, K), device=x.device, dtype=state_dtype)\n",
        "        numer_state = torch.zeros((B, H, K, d), device=x.device, dtype=state_dtype)\n",
        "        m_state = torch.full((B, H, K), float(\"-inf\"), device=x.device, dtype=state_dtype)\n",
        "\n",
        "        if return_info:\n",
        "            read_logits_full = torch.empty((B, H, T, K), device=x.device, dtype=state_dtype)\n",
        "            read_logits_key_full = torch.empty((B, H, T, K), device=x.device, dtype=state_dtype)\n",
        "            read_logits_content_full = (\n",
        "                torch.empty((B, H, T, K), device=x.device, dtype=state_dtype) if self.use_content_read else None\n",
        "            )\n",
        "        else:\n",
        "            read_logits_full = None\n",
        "            read_logits_key_full = None\n",
        "            read_logits_content_full = None\n",
        "\n",
        "        WRITE_CHUNK = self.write_chunk_size\n",
        "\n",
        "        for t0 in range(0, T, WRITE_CHUNK):\n",
        "            t1 = min(T, t0 + WRITE_CHUNK)\n",
        "            L = t1 - t0\n",
        "\n",
        "            wlog_c = write_logits[:, :, :, t0:t1]  # [B,H,K,L]\n",
        "\n",
        "            # streaming cummax\n",
        "            m_c, _ = torch.cummax(wlog_c, dim=-1)                 # [B,H,K,L]\n",
        "            m_new = torch.maximum(m_state.unsqueeze(-1), m_c)     # [B,H,K,L]\n",
        "\n",
        "            # rescale old prefix state to new max reference\n",
        "            scale = torch.exp(m_state.unsqueeze(-1) - m_new)      # [B,H,K,L]\n",
        "\n",
        "            denom_c = denom_state.unsqueeze(-1) * scale           # [B,H,K,L]\n",
        "            numer_c = numer_state.unsqueeze(-2) * scale.unsqueeze(-1)  # [B,H,K,L,d]\n",
        "\n",
        "            # new weights under m_new reference\n",
        "            w_new = self._safe_exp_sub_max(wlog_c, m_new)         # [B,H,K,L]\n",
        "\n",
        "            # accumulate within chunk\n",
        "            denom_c = denom_c + torch.cumsum(w_new, dim=-1)       # [B,H,K,L]\n",
        "            v_c = v_write[:, :, t0:t1, :].to(state_dtype)         # [B,H,L,d]\n",
        "            add = torch.cumsum(w_new.unsqueeze(-1) * v_c.unsqueeze(2), dim=-2)  # [B,H,K,L,d]\n",
        "            numer_c = numer_c + add\n",
        "\n",
        "            # per-token slot state for this chunk: [B,H,L,K,d]\n",
        "            slot_state_c = numer_c / denom_c.clamp_min(1e-8).unsqueeze(-1)      # [B,H,K,L,d]\n",
        "            slot_state_t = slot_state_c.permute(0, 1, 3, 2, 4).contiguous()     # [B,H,L,K,d]\n",
        "\n",
        "            # READ routing logits\n",
        "            q_read_c = q_read[:, :, t0:t1, :]  # [B,H,L,d]\n",
        "            read_logits_key = torch.einsum(\"bhld,hkd->bhlk\", q_read_c, slot_keys) / math.sqrt(d)\n",
        "\n",
        "            read_logits_content = None\n",
        "            if self.use_content_read:\n",
        "                read_logits_content = torch.einsum(\n",
        "                    \"bhld,bhlkd->bhlk\",\n",
        "                    q_read_c,\n",
        "                    slot_state_t.to(q_read_c.dtype),\n",
        "                ) / math.sqrt(d)\n",
        "\n",
        "            # slot mask expanded for this chunk (if any)\n",
        "            sm = self._resolve_slot_mask(\n",
        "                slot_mask,\n",
        "                B=B, H=H, L=L, K=K,\n",
        "                device=x.device,\n",
        "                dtype=read_logits_key.dtype,\n",
        "                scope=slot_mask_scope,\n",
        "            )\n",
        "\n",
        "            # Apply mask according to \"where\"\n",
        "            if slot_mask_where == \"read\":\n",
        "                if sm is not None:\n",
        "                    read_logits_key = read_logits_key.masked_fill(sm <= 0.0, float(\"-inf\"))\n",
        "                    if self.use_content_read and read_logits_content is not None:\n",
        "                        read_logits_content = read_logits_content.masked_fill(sm <= 0.0, float(\"-inf\"))\n",
        "\n",
        "            elif slot_mask_where == \"content_read_only\":\n",
        "                # only remove the content contribution for masked slots;\n",
        "                # do NOT change key routing logits.\n",
        "                if sm is not None and self.use_content_read and read_logits_content is not None:\n",
        "                    read_logits_content = read_logits_content.masked_fill(sm <= 0.0, 0.0)\n",
        "\n",
        "            elif slot_mask_where == \"slotspace_only\":\n",
        "                pass  # applied later on slot_w\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown slot_mask_where={slot_mask_where!r}\")\n",
        "\n",
        "            # combine logits\n",
        "            read_logits = read_logits_key\n",
        "            if self.use_content_read and read_logits_content is not None:\n",
        "                read_logits = read_logits + content_read_gamma.to(read_logits.dtype) * read_logits_content\n",
        "\n",
        "            if return_info:\n",
        "                read_logits_full[:, :, t0:t1, :] = read_logits.to(state_dtype)\n",
        "                read_logits_key_full[:, :, t0:t1, :] = read_logits_key.to(state_dtype)\n",
        "                if self.use_content_read and read_logits_content_full is not None:\n",
        "                    read_logits_content_full[:, :, t0:t1, :] = read_logits_content.to(state_dtype)\n",
        "\n",
        "            # read weights\n",
        "            read_w_c = self._compute_read_weights(\n",
        "                read_logits=read_logits,\n",
        "                read_logits_key=read_logits_key,\n",
        "                read_logits_content=read_logits_content,\n",
        "                routing_mode=routing_mode,\n",
        "                routing_topk=routing_topk,\n",
        "                read_weights_override=read_weights_override,\n",
        "                routing_noise=routing_noise,\n",
        "                routing_noise_scale=routing_noise_scale,\n",
        "                rtemp=rtemp,\n",
        "                sm=sm,\n",
        "                slot_mask_where=slot_mask_where,\n",
        "                B=B, H=H, L=L, K=K,\n",
        "                T_total=T,\n",
        "                t0=t0, t1=t1,\n",
        "            )\n",
        "\n",
        "            read_weights[:, :, t0:t1, :] = read_w_c\n",
        "\n",
        "            # token output (base path)\n",
        "            out_h[:, :, t0:t1, :] = torch.einsum(\n",
        "                \"bhlk,bhlkd->bhld\",\n",
        "                read_w_c.to(state_dtype),\n",
        "                slot_state_t.to(state_dtype),\n",
        "            )\n",
        "\n",
        "            if return_info:\n",
        "                slot_state_norm_t[:, :, t0:t1, :] = slot_state_t.to(torch.float32).norm(dim=-1)\n",
        "\n",
        "            # update running states (carry prefix to next chunk)\n",
        "            m_state = m_new[:, :, :, -1]\n",
        "            denom_state = denom_c[:, :, :, -1]\n",
        "            numer_state = numer_c[:, :, :, -1, :]\n",
        "\n",
        "        # -------------------------\n",
        "        # slot-space refinement (additive delta)\n",
        "        # -------------------------\n",
        "        slotspace_delta_norm_mean = None\n",
        "        if self.use_slotspace_refine:\n",
        "            slotspace_dtype = state_dtype\n",
        "            M = self.slotspace_dim\n",
        "\n",
        "            # Encode read weights into slot-space coordinates\n",
        "            u = self.slot_in(read_weights.to(slotspace_dtype))  # [B,H,T,M]\n",
        "            q_s = self.slot_q(u)\n",
        "            k_s = self.slot_k(u)\n",
        "            v_s = self.slot_v(u)\n",
        "\n",
        "            # RoPE in slot-space matcher (Q/K only)\n",
        "            if self.use_rope_slotspace:\n",
        "                cos_s, sin_s = self.rope_slotspace.get_cos_sin(T, device=x.device, dtype=q_s.dtype)\n",
        "                q_s = apply_rope(q_s, cos_s, sin_s)\n",
        "                k_s = apply_rope(k_s, cos_s, sin_s)\n",
        "\n",
        "            qf = phi(q_s)\n",
        "            kf = phi(k_s)\n",
        "\n",
        "            if valid is not None:\n",
        "                mask = valid.view(B, 1, T, 1).to(slotspace_dtype)\n",
        "                qf = qf * mask\n",
        "                kf = kf * mask\n",
        "                v_s = v_s * mask\n",
        "\n",
        "            # causal linear attention prefix scan in chunks\n",
        "            u2 = torch.empty((B, H, T, M), device=x.device, dtype=slotspace_dtype)\n",
        "\n",
        "            S_state = torch.zeros((B, H, M, M), device=x.device, dtype=slotspace_dtype)\n",
        "            Z_state = torch.zeros((B, H, M), device=x.device, dtype=slotspace_dtype)\n",
        "\n",
        "            SS_CHUNK = self.slotspace_chunk_size\n",
        "\n",
        "            for t0 in range(0, T, SS_CHUNK):\n",
        "                t1 = min(T, t0 + SS_CHUNK)\n",
        "                L = t1 - t0\n",
        "\n",
        "                qf_c = qf[:, :, t0:t1, :]   # [B,H,L,M]\n",
        "                kf_c = kf[:, :, t0:t1, :]   # [B,H,L,M]\n",
        "                v_c  = v_s[:, :, t0:t1, :]  # [B,H,L,M]\n",
        "\n",
        "                kv = torch.einsum(\"bhlm,bhln->bhlmn\", kf_c, v_c)  # [B,H,L,M,M]\n",
        "                S_c = torch.cumsum(kv, dim=2) + S_state.unsqueeze(2)\n",
        "                Z_c = (torch.cumsum(kf_c, dim=2) + Z_state.unsqueeze(2)).clamp_min(1e-8)\n",
        "\n",
        "                num = torch.einsum(\"bhlm,bhlmn->bhln\", qf_c, S_c)\n",
        "                den = torch.einsum(\"bhlm,bhlm->bhl\", qf_c, Z_c).unsqueeze(-1).clamp_min(1e-8)\n",
        "                u2[:, :, t0:t1, :] = num / den\n",
        "\n",
        "                S_state = S_c[:, :, -1, :, :]\n",
        "                Z_state = Z_c[:, :, -1, :]\n",
        "\n",
        "            u2 = self.slotspace_dropout(u2)\n",
        "\n",
        "            # Decode slot weights per token: [B,H,T,K]\n",
        "            slot_w = self.slot_out(u2)\n",
        "            if self.slotspace_signed_weights:\n",
        "                slot_w = torch.tanh(slot_w)\n",
        "            else:\n",
        "                slot_w = torch.softmax(slot_w, dim=-1)\n",
        "\n",
        "            # optional: slotspace-only mask\n",
        "            if slot_mask_where == \"slotspace_only\":\n",
        "                sm_full = self._resolve_slot_mask(\n",
        "                    slot_mask,\n",
        "                    B=B, H=H, L=T, K=K,\n",
        "                    device=x.device,\n",
        "                    dtype=slot_w.dtype,\n",
        "                    scope=slot_mask_scope,\n",
        "                )\n",
        "                if sm_full is not None:\n",
        "                    slot_w = slot_w * (sm_full > 0.0).to(slot_w.dtype)\n",
        "                    if not self.slotspace_signed_weights:\n",
        "                        slot_w = slot_w / slot_w.sum(dim=-1, keepdim=True).clamp_min(1e-8)\n",
        "\n",
        "            gate = self._slotspace_gate(dtype=state_dtype, device=x.device).to(state_dtype)\n",
        "\n",
        "            # second streaming pass to decode delta through slot states\n",
        "            denom_state = torch.zeros((B, H, K), device=x.device, dtype=state_dtype)\n",
        "            numer_state = torch.zeros((B, H, K, d), device=x.device, dtype=state_dtype)\n",
        "            m_state = torch.full((B, H, K), float(\"-inf\"), device=x.device, dtype=state_dtype)\n",
        "\n",
        "            delta_norm_sum = torch.zeros((), device=x.device, dtype=torch.float32)\n",
        "            delta_norm_count = 0\n",
        "\n",
        "            for t0 in range(0, T, WRITE_CHUNK):\n",
        "                t1 = min(T, t0 + WRITE_CHUNK)\n",
        "                L = t1 - t0\n",
        "\n",
        "                wlog_c = write_logits[:, :, :, t0:t1]  # [B,H,K,L]\n",
        "\n",
        "                m_c, _ = torch.cummax(wlog_c, dim=-1)\n",
        "                m_new = torch.maximum(m_state.unsqueeze(-1), m_c)\n",
        "\n",
        "                scale = torch.exp(m_state.unsqueeze(-1) - m_new)\n",
        "                denom_c = denom_state.unsqueeze(-1) * scale\n",
        "                numer_c = numer_state.unsqueeze(-2) * scale.unsqueeze(-1)\n",
        "\n",
        "                w_new = self._safe_exp_sub_max(wlog_c, m_new)\n",
        "                denom_c = denom_c + torch.cumsum(w_new, dim=-1)\n",
        "\n",
        "                v_c = v_write[:, :, t0:t1, :].to(state_dtype)\n",
        "                add = torch.cumsum(w_new.unsqueeze(-1) * v_c.unsqueeze(2), dim=-2)\n",
        "                numer_c = numer_c + add\n",
        "\n",
        "                slot_state_c = numer_c / denom_c.clamp_min(1e-8).unsqueeze(-1)  # [B,H,K,L,d]\n",
        "                slot_state_t = slot_state_c.permute(0, 1, 3, 2, 4).contiguous() # [B,H,L,K,d]\n",
        "\n",
        "                slot_w_c = slot_w[:, :, t0:t1, :].to(state_dtype)                # [B,H,L,K]\n",
        "                delta_c = torch.einsum(\"bhlk,bhlkd->bhld\", slot_w_c, slot_state_t.to(state_dtype))\n",
        "\n",
        "                out_h[:, :, t0:t1, :] = out_h[:, :, t0:t1, :] + gate * delta_c\n",
        "\n",
        "                delta_norm_sum = delta_norm_sum + delta_c.detach().to(torch.float32).norm(dim=-1).sum()\n",
        "                delta_norm_count += (B * H * L)\n",
        "\n",
        "                m_state = m_new[:, :, :, -1]\n",
        "                denom_state = denom_c[:, :, :, -1]\n",
        "                numer_state = numer_c[:, :, :, -1, :]\n",
        "\n",
        "            slotspace_delta_norm_mean = (delta_norm_sum / max(1, delta_norm_count)).detach().cpu()\n",
        "\n",
        "        # -------------------------\n",
        "        # finish\n",
        "        # -------------------------\n",
        "        out = out_h.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        out = self.out_proj(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        info = None\n",
        "        if return_info:\n",
        "            info = {\n",
        "                \"write_logits_raw\": write_logits_raw.detach(),\n",
        "                \"write_logits\": write_logits.detach().to(torch.float32),\n",
        "                \"read_weights\": read_weights.detach(),\n",
        "                \"slot_state_norm\": slot_state_norm_t.detach().permute(0, 1, 3, 2).contiguous() if slot_state_norm_t is not None else None,\n",
        "                \"content_read_gamma\": content_read_gamma.detach().to(torch.float32).cpu(),\n",
        "                \"slot_mask_where\": slot_mask_where,\n",
        "                \"slot_mask_scope\": slot_mask_scope,\n",
        "            }\n",
        "            if alibi_bias_applied is not None:\n",
        "                info[\"alibi_bias_applied\"] = alibi_bias_applied.detach().to(torch.float32)\n",
        "            if self.use_alibi_write and self.learn_alibi_strength:\n",
        "                info[\"alibi_strength\"] = self._alibi_strength(dtype=torch.float32, device=x.device).detach().cpu()\n",
        "            if self.use_slotspace_refine:\n",
        "                info[\"slotspace_gate\"] = self._slotspace_gate(dtype=torch.float32, device=x.device).detach().cpu()\n",
        "                info[\"use_rope_slotspace\"] = torch.tensor(bool(self.use_rope_slotspace))\n",
        "                if slotspace_delta_norm_mean is not None:\n",
        "                    info[\"slotspace_delta_norm\"] = slotspace_delta_norm_mean\n",
        "\n",
        "            info[\"read_logits\"] = read_logits_full.detach().to(torch.float32) if read_logits_full is not None else None\n",
        "            info[\"read_logits_key\"] = read_logits_key_full.detach().to(torch.float32) if read_logits_key_full is not None else None\n",
        "            info[\"read_logits_content\"] = (\n",
        "                read_logits_content_full.detach().to(torch.float32) if read_logits_content_full is not None else None\n",
        "            )\n",
        "            info[\"routing_mode\"] = routing_mode\n",
        "\n",
        "        return out, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8Wy-Bqi_JsCF"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#@title LM and Config defs\n",
        "# ============================================================================\n",
        "# Addressed State Models (ASM): Config + Block + LM\n",
        "# - Naming aligned with paper: slots, read/write, slot-space refinement\n",
        "# - No compatibility layer (fresh public tooling)\n",
        "# - Assumes AddressedStateAttention is defined elsewhere (the primitive module)\n",
        "# ============================================================================\n",
        "\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Config\n",
        "# ============================================================================\n",
        "@dataclass\n",
        "class ASMTrainConfig:\n",
        "    # Data\n",
        "    dataset_name: str = \"wikitext\"\n",
        "    dataset_config: str = \"wikitext-103-raw-v1\"\n",
        "    tokenizer_name: str = \"gpt2\"\n",
        "\n",
        "    max_seq_len: int = 256\n",
        "    stride_frac_val: float = 0.50\n",
        "    seed: int = 1337\n",
        "\n",
        "    # Sample budgets\n",
        "    train_samples_target: int = 100_000_000\n",
        "    val_samples_target: int = 25_000\n",
        "\n",
        "    # Training\n",
        "    batch_size: int = 64\n",
        "    learning_rate: float = 3e-4\n",
        "    weight_decay: float = 0.01\n",
        "    betas: Tuple[float, float] = (0.9, 0.95)\n",
        "    grad_clip: float = 1.0\n",
        "    warmup_steps: int = 1_000\n",
        "    total_steps: int = 75_000\n",
        "    eval_interval: int = 1_000\n",
        "    log_interval: int = 100\n",
        "\n",
        "    # Model\n",
        "    vocab_size: int = 50257\n",
        "    embed_dim: int = 384\n",
        "    num_layers: int = 23\n",
        "    num_heads: int = 8\n",
        "    num_slots: int = 32\n",
        "    mlp_ratio: float = 4.0\n",
        "    dropout: float = 0.1\n",
        "    tie_weights: bool = True\n",
        "\n",
        "    # Addressed State Attention (ASA) / numerics\n",
        "    read_temperature: float = 1.0\n",
        "    write_temperature: float = 1.0\n",
        "    slot_dropout: float = 0.05\n",
        "    state_fp32: bool = True\n",
        "    normalize_k: bool = False\n",
        "\n",
        "    # Positions\n",
        "    use_abs_pos: bool = False\n",
        "    use_rope_keys: bool = True\n",
        "    rope_base: float = 10000.0\n",
        "    use_alibi_write: bool = True\n",
        "    alibi_strength_init: float = 0.1\n",
        "    learn_alibi_strength: bool = True\n",
        "    min_strength: float = 0.0\n",
        "\n",
        "    # Content-conditioned read term (gamma)\n",
        "    use_content_read: bool = True\n",
        "    content_read_init: float = -4.0\n",
        "    content_read_max_gamma: float = 3.0\n",
        "\n",
        "    # Optional slot-space refinement (formerly \"k-space\")\n",
        "    use_slotspace_refine: bool = True\n",
        "    slotspace_dim: int = 64\n",
        "    slotspace_gate_init: float = -4.0\n",
        "    slotspace_dropout: float = 0.05\n",
        "    slotspace_signed_weights: bool = True\n",
        "\n",
        "    # RoPE inside slot-space matcher (Q/K only)\n",
        "    use_rope_slotspace: bool = True\n",
        "    rope_base_slotspace: float = 100000.0\n",
        "\n",
        "    # Perf knobs (behavior-identical)\n",
        "    write_chunk_size: int = 128\n",
        "    slotspace_chunk_size: int = 128\n",
        "    enable_compiled: bool = False\n",
        "\n",
        "    # Analytics\n",
        "    eval_max_batches: int = 150\n",
        "    analytics_last_k: int = 32\n",
        "\n",
        "    # IO / caches\n",
        "    output_dir: str = \"./drive/MyDrive/asm_outputs\"\n",
        "    tag: str = \"asm_wikitext\"\n",
        "    cache_dir: str = \"./drive/MyDrive/asm_caches\"\n",
        "    val_windows_cache: str = \"./drive/MyDrive/asm_val_cache_windows_1024.pkl\"\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Block\n",
        "# ============================================================================\n",
        "class ASMBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int,\n",
        "        num_heads: int,\n",
        "        num_slots: int,\n",
        "        mlp_ratio: float = 4.0,\n",
        "        dropout: float = 0.1,\n",
        "\n",
        "        # temperatures / numerics\n",
        "        read_temperature: float = 1.0,\n",
        "        write_temperature: float = 1.0,\n",
        "        state_fp32: bool = True,\n",
        "        slot_dropout: float = 0.0,\n",
        "        normalize_k: bool = False,\n",
        "\n",
        "        # positions\n",
        "        use_rope_keys: bool = True,\n",
        "        rope_base: float = 10000.0,\n",
        "        use_alibi_write: bool = True,\n",
        "\n",
        "        # ALiBi params\n",
        "        alibi_strength_init: float = 0.1,\n",
        "        learn_alibi_strength: bool = True,\n",
        "        min_strength: float = 0.0,\n",
        "\n",
        "        # content-conditioned read (gamma)\n",
        "        use_content_read: bool = True,\n",
        "        content_read_init: float = -4.0,\n",
        "        content_read_max_gamma: float = 3.0,\n",
        "\n",
        "        # optional slot-space refinement\n",
        "        use_slotspace_refine: bool = True,\n",
        "        slotspace_dim: int = 32,\n",
        "        slotspace_gate_init: float = -10.0,\n",
        "        slotspace_dropout: float = 0.0,\n",
        "        slotspace_signed_weights: bool = True,\n",
        "\n",
        "        # RoPE inside slot-space matcher\n",
        "        use_rope_slotspace: bool = True,\n",
        "        rope_base_slotspace: float = 100000.0,\n",
        "\n",
        "        # chunk sizes\n",
        "        write_chunk_size: int = 128,\n",
        "        slotspace_chunk_size: int = 128,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        self.asa = AddressedStateAttention(\n",
        "            embed_dim=embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            num_slots=num_slots,\n",
        "            dropout=dropout,\n",
        "\n",
        "            read_temperature=read_temperature,\n",
        "            write_temperature=write_temperature,\n",
        "            state_fp32=state_fp32,\n",
        "            slot_dropout=slot_dropout,\n",
        "            normalize_k=normalize_k,\n",
        "\n",
        "            use_rope_keys=use_rope_keys,\n",
        "            rope_base=rope_base,\n",
        "            use_alibi_write=use_alibi_write,\n",
        "            alibi_strength_init=alibi_strength_init,\n",
        "            learn_alibi_strength=learn_alibi_strength,\n",
        "            min_strength=min_strength,\n",
        "\n",
        "            use_content_read=use_content_read,\n",
        "            content_read_init=content_read_init,\n",
        "            content_read_max_gamma=content_read_max_gamma,\n",
        "\n",
        "            use_slotspace_refine=use_slotspace_refine,\n",
        "            slotspace_dim=slotspace_dim,\n",
        "            slotspace_gate_init=slotspace_gate_init,\n",
        "            slotspace_dropout=slotspace_dropout,\n",
        "            slotspace_signed_weights=slotspace_signed_weights,\n",
        "\n",
        "            use_rope_slotspace=use_rope_slotspace,\n",
        "            rope_base_slotspace=rope_base_slotspace,\n",
        "\n",
        "            write_chunk_size=write_chunk_size,\n",
        "            slotspace_chunk_size=slotspace_chunk_size,\n",
        "        )\n",
        "\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        hidden = int(embed_dim * mlp_ratio)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden, bias=False),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden, embed_dim, bias=False),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        return_info: bool = False,\n",
        "\n",
        "        # passthrough (optional; keeps existing callers working)\n",
        "        routing_mode: str = \"softmax\",\n",
        "        routing_topk: int = 2,\n",
        "        read_weights_override: Optional[torch.Tensor] = None,\n",
        "        routing_noise: Optional[str] = None,\n",
        "        routing_noise_scale: float = 1.0,\n",
        "\n",
        "        slot_mask: Optional[torch.Tensor] = None,       # [K], 1=keep, 0=mask\n",
        "        slot_mask_where: str = \"read\",                  # \"read\" | \"content_read_only\" | \"slotspace_only\"\n",
        "        slot_mask_scope: str = \"all\",                   # \"all\" | \"last_pos_only\"\n",
        "    ):\n",
        "        a, info = self.asa(\n",
        "            self.norm1(x),\n",
        "            attention_mask=attention_mask,\n",
        "            return_info=return_info,\n",
        "            routing_mode=routing_mode,\n",
        "            routing_topk=routing_topk,\n",
        "            read_weights_override=read_weights_override,\n",
        "            routing_noise=routing_noise,\n",
        "            routing_noise_scale=routing_noise_scale,\n",
        "            slot_mask=slot_mask,\n",
        "            slot_mask_where=slot_mask_where,\n",
        "            slot_mask_scope=slot_mask_scope,\n",
        "        )\n",
        "        x = x + a\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x, info\n",
        "\n",
        "# ============================================================================\n",
        "# LM\n",
        "# ============================================================================\n",
        "class ASMLanguageModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        embed_dim: int = 384,\n",
        "        num_layers: int = 6,\n",
        "        num_heads: int = 8,\n",
        "        num_slots: int = 8,\n",
        "        max_seq_len: int = 1024,\n",
        "        mlp_ratio: float = 4.0,\n",
        "        dropout: float = 0.1,\n",
        "\n",
        "        # temperatures / numerics\n",
        "        read_temperature: float = 1.0,\n",
        "        write_temperature: float = 1.0,\n",
        "        state_fp32: bool = True,\n",
        "        slot_dropout: float = 0.05,\n",
        "        normalize_k: bool = False,\n",
        "\n",
        "        tie_weights: bool = True,\n",
        "\n",
        "        # LM-level abs pos\n",
        "        use_abs_pos: bool = False,\n",
        "\n",
        "        # positions\n",
        "        use_rope_keys: bool = True,\n",
        "        rope_base: float = 10000.0,\n",
        "        use_alibi_write: bool = True,\n",
        "\n",
        "        # ALiBi\n",
        "        alibi_strength_init: float = 0.1,\n",
        "        learn_alibi_strength: bool = True,\n",
        "        min_strength: float = 0.0,\n",
        "\n",
        "        # content-conditioned read (gamma)\n",
        "        use_content_read: bool = True,\n",
        "        content_read_init: float = -4.0,\n",
        "        content_read_max_gamma: float = 3.0,\n",
        "\n",
        "        # optional slot-space refinement\n",
        "        use_slotspace_refine: bool = True,\n",
        "        slotspace_dim: int = 32,\n",
        "        slotspace_gate_init: float = -10.0,\n",
        "        slotspace_dropout: float = 0.0,\n",
        "        slotspace_signed_weights: bool = True,\n",
        "\n",
        "        # RoPE inside slot-space matcher\n",
        "        use_rope_slotspace: bool = True,\n",
        "        rope_base_slotspace: float = 100000.0,\n",
        "\n",
        "        # chunk sizes\n",
        "        write_chunk_size: int = 128,\n",
        "        slotspace_chunk_size: int = 128,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.use_abs_pos = bool(use_abs_pos)\n",
        "\n",
        "        self.tok = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos = nn.Embedding(max_seq_len, embed_dim) if self.use_abs_pos else None\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            ASMBlock(\n",
        "                embed_dim=embed_dim,\n",
        "                num_heads=num_heads,\n",
        "                num_slots=num_slots,\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                dropout=dropout,\n",
        "\n",
        "                read_temperature=read_temperature,\n",
        "                write_temperature=write_temperature,\n",
        "                state_fp32=state_fp32,\n",
        "                slot_dropout=slot_dropout,\n",
        "                normalize_k=normalize_k,\n",
        "\n",
        "                use_rope_keys=use_rope_keys,\n",
        "                rope_base=rope_base,\n",
        "                use_alibi_write=use_alibi_write,\n",
        "\n",
        "                alibi_strength_init=alibi_strength_init,\n",
        "                learn_alibi_strength=learn_alibi_strength,\n",
        "                min_strength=min_strength,\n",
        "\n",
        "                use_content_read=use_content_read,\n",
        "                content_read_init=content_read_init,\n",
        "                content_read_max_gamma=content_read_max_gamma,\n",
        "\n",
        "                use_slotspace_refine=use_slotspace_refine,\n",
        "                slotspace_dim=slotspace_dim,\n",
        "                slotspace_gate_init=slotspace_gate_init,\n",
        "                slotspace_dropout=slotspace_dropout,\n",
        "                slotspace_signed_weights=slotspace_signed_weights,\n",
        "                use_rope_slotspace=use_rope_slotspace,\n",
        "                rope_base_slotspace=rope_base_slotspace,\n",
        "\n",
        "                write_chunk_size=write_chunk_size,\n",
        "                slotspace_chunk_size=slotspace_chunk_size,\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
        "        if tie_weights:\n",
        "            self.lm_head.weight = self.tok.weight\n",
        "\n",
        "        self.apply(self._init)\n",
        "\n",
        "    def _init(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.normal_(m.weight, std=0.02)\n",
        "        elif isinstance(m, nn.Embedding):\n",
        "            nn.init.normal_(m.weight, std=0.02)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.ones_(m.weight)\n",
        "            nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        return_info: bool = False,\n",
        "\n",
        "        # passthrough (optional)\n",
        "        routing_mode: str = \"softmax\",\n",
        "        routing_topk: int = 2,\n",
        "        read_weights_override: Optional[torch.Tensor] = None,\n",
        "        routing_noise: Optional[str] = None,\n",
        "        routing_noise_scale: float = 1.0,\n",
        "\n",
        "        slot_mask: Optional[torch.Tensor] = None,       # [K], 1=keep, 0=mask\n",
        "        slot_mask_where: str = \"read\",                  # \"read\" | \"content_read_only\" | \"slotspace_only\"\n",
        "        slot_mask_scope: str = \"all\",                   # \"all\" | \"last_pos_only\"\n",
        "        return_light_stats:bool = False,\n",
        "    ):\n",
        "        B, T = input_ids.shape\n",
        "        assert T <= self.max_seq_len, f\"T={T} exceeds max_seq_len={self.max_seq_len}\"\n",
        "\n",
        "        x = self.tok(input_ids)\n",
        "        if self.use_abs_pos:\n",
        "            pos = torch.arange(T, device=input_ids.device).unsqueeze(0).expand(B, -1)\n",
        "            x = x + self.pos(pos)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        infos = []\n",
        "        for blk in self.blocks:\n",
        "            x, info = blk(\n",
        "                x,\n",
        "                attention_mask=attention_mask,\n",
        "                return_info=return_info,\n",
        "                routing_mode=routing_mode,\n",
        "                routing_topk=routing_topk,\n",
        "                read_weights_override=read_weights_override,\n",
        "                routing_noise=routing_noise,\n",
        "                routing_noise_scale=routing_noise_scale,\n",
        "                slot_mask=slot_mask,\n",
        "                slot_mask_where=slot_mask_where,\n",
        "                slot_mask_scope=slot_mask_scope,\n",
        "            )\n",
        "            if return_info:\n",
        "                infos.append(info)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        logits = self.lm_head(x)\n",
        "        return (logits, infos) if return_info else logits\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Convenience: build model from config\n",
        "# ============================================================================\n",
        "def build_model_from_cfg(cfg: ASMTrainConfig) -> ASMLanguageModel:\n",
        "    return ASMLanguageModel(\n",
        "        vocab_size=cfg.vocab_size,\n",
        "        embed_dim=cfg.embed_dim,\n",
        "        num_layers=cfg.num_layers,\n",
        "        num_heads=cfg.num_heads,\n",
        "        num_slots=cfg.num_slots,\n",
        "        max_seq_len=cfg.max_seq_len,\n",
        "        mlp_ratio=cfg.mlp_ratio,\n",
        "        dropout=cfg.dropout,\n",
        "\n",
        "        read_temperature=cfg.read_temperature,\n",
        "        write_temperature=cfg.write_temperature,\n",
        "        state_fp32=cfg.state_fp32,\n",
        "        slot_dropout=cfg.slot_dropout,\n",
        "        normalize_k=cfg.normalize_k,\n",
        "\n",
        "        tie_weights=cfg.tie_weights,\n",
        "\n",
        "        use_abs_pos=cfg.use_abs_pos,\n",
        "        use_rope_keys=cfg.use_rope_keys,\n",
        "        rope_base=cfg.rope_base,\n",
        "        use_alibi_write=cfg.use_alibi_write,\n",
        "\n",
        "        alibi_strength_init=cfg.alibi_strength_init,\n",
        "        learn_alibi_strength=cfg.learn_alibi_strength,\n",
        "        min_strength=cfg.min_strength,\n",
        "\n",
        "        use_content_read=cfg.use_content_read,\n",
        "        content_read_init=cfg.content_read_init,\n",
        "        content_read_max_gamma=cfg.content_read_max_gamma,\n",
        "\n",
        "        use_slotspace_refine=cfg.use_slotspace_refine,\n",
        "        slotspace_dim=cfg.slotspace_dim,\n",
        "        slotspace_gate_init=cfg.slotspace_gate_init,\n",
        "        slotspace_dropout=cfg.slotspace_dropout,\n",
        "        slotspace_signed_weights=cfg.slotspace_signed_weights,\n",
        "        use_rope_slotspace=cfg.use_rope_slotspace,\n",
        "        rope_base_slotspace=cfg.rope_base_slotspace,\n",
        "\n",
        "        write_chunk_size=cfg.write_chunk_size,\n",
        "        slotspace_chunk_size=cfg.slotspace_chunk_size,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "veH68GTAJsTJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Train, Eval Functions and Utilities\n",
        "\n",
        "import os, math, random, pickle, time\n",
        "from dataclasses import asdict\n",
        "from typing import Tuple, Dict, List, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, IterableDataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Data: cached token streams + val windows + random-window train\n",
        "# =========================================================\n",
        "class StableValidationDataset(Dataset):\n",
        "    def __init__(self, samples: List[Tuple[torch.Tensor, torch.Tensor]]):\n",
        "        self.samples = samples\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx): return self.samples[idx]\n",
        "\n",
        "def _ensure_dir(path: str):\n",
        "    d = os.path.dirname(path)\n",
        "    if d:\n",
        "        os.makedirs(d, exist_ok=True)\n",
        "\n",
        "def build_or_load_token_stream(\n",
        "    *,\n",
        "    cache_path: str,\n",
        "    dataset_name: str,\n",
        "    dataset_config: str,\n",
        "    split: str,\n",
        "    tokenizer_name: str,\n",
        "    min_chars: int = 1,\n",
        "    add_eos_between_rows: bool = True,\n",
        "    max_rows: Optional[int] = None,\n",
        ") -> List[int]:\n",
        "    if os.path.exists(cache_path):\n",
        "        print(f\"Loading cached token stream: {cache_path}\")\n",
        "        with open(cache_path, \"rb\") as f:\n",
        "            stream = pickle.load(f)\n",
        "        print(f\"Loaded token stream tokens={len(stream):,}\")\n",
        "        return stream\n",
        "\n",
        "    print(f\"Building token stream for {dataset_name}/{dataset_config} split={split} ...\")\n",
        "    _ensure_dir(cache_path)\n",
        "    tok = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=True)\n",
        "    eos = tok.eos_token_id\n",
        "    assert eos is not None, \"Tokenizer must have eos_token_id\"\n",
        "\n",
        "    ds = load_dataset(dataset_name, dataset_config, split=split)\n",
        "\n",
        "    stream: List[int] = []\n",
        "    used = 0\n",
        "    for row in tqdm(ds, desc=f\"Tokenizing {split}\"):\n",
        "        if max_rows is not None and used >= max_rows:\n",
        "            break\n",
        "        text = (row.get(\"text\") or \"\").strip()\n",
        "        if len(text) < min_chars:\n",
        "            continue\n",
        "        ids = tok.encode(text, add_special_tokens=False)\n",
        "        if not ids:\n",
        "            continue\n",
        "        stream.extend(ids)\n",
        "        if add_eos_between_rows:\n",
        "            stream.append(eos)\n",
        "        used += 1\n",
        "\n",
        "    print(f\"Built stream: rows_used={used:,} tokens={len(stream):,}\")\n",
        "    with open(cache_path, \"wb\") as f:\n",
        "        pickle.dump(stream, f)\n",
        "    print(f\"Cached token stream to {cache_path}\")\n",
        "    return stream\n",
        "\n",
        "def build_or_load_validation_windows(\n",
        "    *,\n",
        "    cache_path: str,\n",
        "    token_stream: List[int],\n",
        "    max_seq_len: int,\n",
        "    stride_frac: float,\n",
        "    val_samples_target: int,\n",
        ") -> StableValidationDataset:\n",
        "    if os.path.exists(cache_path):\n",
        "        print(f\"Loading cached validation windows: {cache_path}\")\n",
        "        with open(cache_path, \"rb\") as f:\n",
        "            samples = pickle.load(f)\n",
        "        print(f\"Loaded val windows: {len(samples)}\")\n",
        "        return StableValidationDataset(samples)\n",
        "\n",
        "    print(\"Building validation windows (cached)...\")\n",
        "    _ensure_dir(cache_path)\n",
        "\n",
        "    T = int(max_seq_len)\n",
        "    stride = max(1, int(T * float(stride_frac)))\n",
        "    need = int(val_samples_target)\n",
        "\n",
        "    max_start = len(token_stream) - (T + 1)\n",
        "    if max_start <= 0:\n",
        "        raise ValueError(\"Validation token stream too small for max_seq_len+1\")\n",
        "\n",
        "    samples: List[Tuple[torch.Tensor, torch.Tensor]] = []\n",
        "    for start in tqdm(range(0, max_start + 1, stride), desc=\"Chunking val stream\"):\n",
        "        chunk = token_stream[start:start + T + 1]\n",
        "        if len(chunk) < T + 1:\n",
        "            break\n",
        "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
        "        samples.append((x, y))\n",
        "        if len(samples) >= need:\n",
        "            break\n",
        "\n",
        "    print(f\"Built val windows={len(samples)} (stride={stride}, stream_tokens={len(token_stream):,})\")\n",
        "    with open(cache_path, \"wb\") as f:\n",
        "        pickle.dump(samples, f)\n",
        "    print(f\"Cached validation windows to {cache_path}\")\n",
        "    return StableValidationDataset(samples)\n",
        "\n",
        "class WikiTextRandomWindowStream(IterableDataset):\n",
        "    def __init__(self, token_stream: List[int], max_seq_len: int, train_samples_target: int, seed: int):\n",
        "        super().__init__()\n",
        "        self.stream = token_stream\n",
        "        self.T = int(max_seq_len)\n",
        "        self.target = int(train_samples_target)\n",
        "        self.seed = int(seed)\n",
        "        self.max_start = len(self.stream) - (self.T + 1)\n",
        "        if self.max_start <= 0:\n",
        "            raise ValueError(\"Train token stream too small for max_seq_len+1\")\n",
        "\n",
        "    def __iter__(self):\n",
        "        info = torch.utils.data.get_worker_info()\n",
        "        wid = info.id if info is not None else 0\n",
        "        rng = random.Random(self.seed + 17 * wid)\n",
        "\n",
        "        yielded = 0\n",
        "        while yielded < self.target:\n",
        "            start = rng.randint(0, self.max_start)\n",
        "            chunk = self.stream[start:start + self.T + 1]\n",
        "            x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
        "            y = torch.tensor(chunk[1:], dtype=torch.long)\n",
        "            yield x, y\n",
        "            yielded += 1\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# LR schedule\n",
        "# =========================================================\n",
        "class WarmupCosine:\n",
        "    def __init__(self, opt, warmup_steps, total_steps, base_lr):\n",
        "        self.opt = opt\n",
        "        self.warmup = int(warmup_steps)\n",
        "        self.total = int(total_steps)\n",
        "        self.base = float(base_lr)\n",
        "        self.step_num = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.step_num += 1\n",
        "        if self.step_num <= self.warmup:\n",
        "            lr = self.base * self.step_num / max(1, self.warmup)\n",
        "        else:\n",
        "            prog = (self.step_num - self.warmup) / max(1, (self.total - self.warmup))\n",
        "            lr = self.base * 0.5 * (1 + math.cos(math.pi * min(1.0, prog)))\n",
        "        for g in self.opt.param_groups:\n",
        "            g[\"lr\"] = lr\n",
        "        return lr\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Light metrics helpers (fast + tolerant)\n",
        "# =========================================================\n",
        "@torch.no_grad()\n",
        "def _entropy_mean(read_w: torch.Tensor) -> float:\n",
        "    # read_w: [B,H,T,K]\n",
        "    eps = 1e-8\n",
        "    p = read_w.clamp_min(eps)\n",
        "    ent = -(p * p.log()).sum(dim=-1)  # [B,H,T]\n",
        "    return float(ent.mean().detach().cpu().item())\n",
        "\n",
        "@torch.no_grad()\n",
        "def _top1freq_mean(read_w: torch.Tensor) -> float:\n",
        "    # fraction of tokens assigned to most common top1 slot (averaged over batch+heads)\n",
        "    top1 = read_w.argmax(dim=-1)  # [B,H,T]\n",
        "    flat = top1.reshape(-1).detach().cpu()\n",
        "    K = read_w.shape[-1]\n",
        "    hist = torch.bincount(flat, minlength=K).float()\n",
        "    denom = hist.sum().clamp_min(1.0)\n",
        "    return float((hist.max() / denom).item())\n",
        "\n",
        "@torch.no_grad()\n",
        "def _write_stats(write_logits: torch.Tensor, last_k: int) -> Tuple[float, float]:\n",
        "    # write_logits: [B,H,K,T] (already biased/masked)\n",
        "    w = torch.softmax(write_logits, dim=-1)\n",
        "    T = w.shape[-1]\n",
        "    pos = torch.arange(T, device=w.device, dtype=w.dtype).view(1, 1, 1, T)\n",
        "    com = (w * pos).sum(dim=-1) / max(1.0, float(T - 1))   # [B,H,K] normalized 0..1\n",
        "    lastk = min(max(1, int(last_k)), T)\n",
        "    lastk_mass = w[..., -lastk:].sum(dim=-1)               # [B,H,K]\n",
        "    return float(com.mean().detach().cpu().item()), float(lastk_mass.mean().detach().cpu().item())\n",
        "\n",
        "@torch.no_grad()\n",
        "def _get_scalar(info: Dict, key: str) -> Optional[float]:\n",
        "    v = info.get(key, None)\n",
        "    if v is None:\n",
        "        return None\n",
        "    if isinstance(v, torch.Tensor):\n",
        "        if v.numel() == 1:\n",
        "            return float(v.detach().cpu().item())\n",
        "        return float(v.detach().float().mean().cpu().item())\n",
        "    if isinstance(v, (int, float)):\n",
        "        return float(v)\n",
        "    return None\n",
        "\n",
        "@torch.no_grad()\n",
        "def _layer_param_summaries(model) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Truthy parameter summaries:\n",
        "      - content_read_gamma uses softplus(raw) then clamped to content_read_max_gamma if present\n",
        "      - slotspace_gate uses softplus(raw)\n",
        "      - alibi_strength uses module's _alibi_strength when available\n",
        "    \"\"\"\n",
        "    gammas = []\n",
        "    gates = []\n",
        "    alibis = []\n",
        "\n",
        "    for blk in getattr(model, \"blocks\", []):\n",
        "        attn = getattr(blk, \"asa\", None)\n",
        "        if attn is None:\n",
        "            continue\n",
        "\n",
        "        # content read gamma (best-effort)\n",
        "        if hasattr(attn, \"_content_read_gamma_raw\"):\n",
        "            g = F.softplus(attn._content_read_gamma_raw.detach().float())\n",
        "            mx = getattr(attn, \"content_read_max_gamma\", None)\n",
        "            if mx is not None and float(mx) > 0:\n",
        "                g = g.clamp(max=float(mx))\n",
        "            gammas.append(float(g.cpu().item()))\n",
        "\n",
        "        # slot-space gate\n",
        "        if hasattr(attn, \"_slotspace_gate_raw\"):\n",
        "            kg = F.softplus(attn._slotspace_gate_raw.detach().float())\n",
        "            gates.append(float(kg.cpu().item()))\n",
        "\n",
        "        # alibi strength (actual used)\n",
        "        if hasattr(attn, \"_alibi_strength\"):\n",
        "            try:\n",
        "                a = attn._alibi_strength(dtype=torch.float32, device=next(attn.parameters()).device).detach().cpu().item()\n",
        "                alibis.append(float(a))\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    out: Dict[str, float] = {}\n",
        "    if gammas:\n",
        "        t = torch.tensor(gammas)\n",
        "        out[\"content_read_gamma_mean\"] = float(t.mean().item())\n",
        "        out[\"content_read_gamma_min\"]  = float(t.min().item())\n",
        "        out[\"content_read_gamma_max\"]  = float(t.max().item())\n",
        "    if gates:\n",
        "        t = torch.tensor(gates)\n",
        "        out[\"slotspace_gate_mean\"] = float(t.mean().item())\n",
        "        out[\"slotspace_gate_min\"]  = float(t.min().item())\n",
        "        out[\"slotspace_gate_max\"]  = float(t.max().item())\n",
        "    if alibis:\n",
        "        t = torch.tensor(alibis)\n",
        "        out[\"alibi_strength_mean\"] = float(t.mean().item())\n",
        "        out[\"alibi_strength_min\"]  = float(t.min().item())\n",
        "        out[\"alibi_strength_max\"]  = float(t.max().item())\n",
        "    return out\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Eval\n",
        "# =========================================================\n",
        "@torch.no_grad()\n",
        "def evaluate(model, val_loader, max_batches=50, last_k=8):\n",
        "    model.eval()\n",
        "\n",
        "    losses = []\n",
        "    ent_acc = 0.0\n",
        "    top1_acc = 0.0\n",
        "    com_acc = 0.0\n",
        "    lastk_acc = 0.0\n",
        "\n",
        "    delta_acc = 0.0\n",
        "    delta_n = 0\n",
        "    n_batches = 0\n",
        "\n",
        "    for i, (xb, yb) in enumerate(val_loader):\n",
        "        if i >= max_batches:\n",
        "            break\n",
        "        xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
        "\n",
        "        with torch.autocast(device_type=device.type, dtype=amp_dtype, enabled=use_amp):\n",
        "            logits, infos = model(xb, return_info=True)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), yb.view(-1))\n",
        "        losses.append(float(loss.item()))\n",
        "\n",
        "        # Light metrics: layer-avg per batch\n",
        "        if infos and infos[0] is not None:\n",
        "            eL, tL, cL, lL = [], [], [], []\n",
        "            dL = []\n",
        "            for info in infos:\n",
        "                read_w = info.get(\"read_weights\", None)\n",
        "                wl     = info.get(\"write_logits\", None)\n",
        "\n",
        "                if read_w is not None:\n",
        "                    eL.append(_entropy_mean(read_w))\n",
        "                    tL.append(_top1freq_mean(read_w))\n",
        "                if wl is not None:\n",
        "                    com, lastm = _write_stats(wl.to(torch.float32), last_k=last_k)\n",
        "                    cL.append(com)\n",
        "                    lL.append(lastm)\n",
        "\n",
        "                dd = _get_scalar(info, \"slotspace_delta_norm\")\n",
        "                if dd is not None:\n",
        "                    dL.append(dd)\n",
        "\n",
        "            if eL:\n",
        "                ent_acc += sum(eL) / len(eL)\n",
        "                top1_acc += sum(tL) / len(tL)\n",
        "            if cL:\n",
        "                com_acc += sum(cL) / len(cL)\n",
        "                lastk_acc += sum(lL) / len(lL)\n",
        "            if dL:\n",
        "                delta_acc += sum(dL) / len(dL)\n",
        "                delta_n += 1\n",
        "\n",
        "            n_batches += 1\n",
        "\n",
        "    mean = sum(losses) / max(1, len(losses))\n",
        "    ppl = float(math.exp(min(20.0, mean)))\n",
        "\n",
        "    stats: Dict[str, float] = {}\n",
        "    if n_batches > 0:\n",
        "        stats[\"entropy_mean\"] = ent_acc / n_batches\n",
        "        stats[\"top1freq_mean\"] = top1_acc / n_batches\n",
        "        stats[\"write_com_mean\"] = com_acc / n_batches\n",
        "        stats[\"write_lastk_mass_mean\"] = lastk_acc / n_batches\n",
        "    if delta_n > 0:\n",
        "        stats[\"slotspace_delta_norm\"] = delta_acc / delta_n\n",
        "\n",
        "    # Parameter summaries (truthy, cheap)\n",
        "    try:\n",
        "        stats.update(_layer_param_summaries(model))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    model.train()\n",
        "    return mean, ppl, stats\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Checkpointing\n",
        "# =========================================================\n",
        "def save_ckpt(path, cfg, model, opt, step, best_val):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    torch.save(\n",
        "        {\"cfg\": asdict(cfg), \"model\": model.state_dict(), \"opt\": opt.state_dict(),\n",
        "         \"step\": step, \"best_val\": best_val},\n",
        "        path,\n",
        "    )\n",
        "    print(f\" Saved {path}\")\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Pretty stats formatting (not slow)\n",
        "# =========================================================\n",
        "def _fmt_stats(stats: Dict[str, float], last_k: int) -> str:\n",
        "    keys = [\n",
        "        \"alibi_strength_mean\",\n",
        "        \"entropy_mean\",\n",
        "        \"top1freq_mean\",\n",
        "        \"write_com_mean\",\n",
        "        \"write_lastk_mass_mean\",\n",
        "        \"content_read_gamma_mean\",\n",
        "        \"content_read_gamma_max\",\n",
        "        \"slotspace_gate_mean\",\n",
        "        \"slotspace_gate_max\",\n",
        "        \"slotspace_delta_norm\",\n",
        "    ]\n",
        "    parts = []\n",
        "    for k in keys:\n",
        "        if k in stats:\n",
        "            if k == \"write_lastk_mass_mean\":\n",
        "                parts.append(f\"{k}(last_k={last_k})={stats[k]:.4f}\")\n",
        "            else:\n",
        "                parts.append(f\"{k}={stats[k]:.4f}\")\n",
        "    return \" | \".join(parts)\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Train\n",
        "# =========================================================\n",
        "def train_asm(cfg: ASMTrainConfig):\n",
        "    random.seed(cfg.seed)\n",
        "    torch.manual_seed(cfg.seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(cfg.seed)\n",
        "\n",
        "    # ---------- Data prep (cached streams) ----------\n",
        "    os.makedirs(cfg.cache_dir, exist_ok=True)\n",
        "    train_stream_cache = os.path.join(cfg.cache_dir, f\"{cfg.dataset_config}_train_stream.pkl\")\n",
        "    val_stream_cache   = os.path.join(cfg.cache_dir, f\"{cfg.dataset_config}_val_stream.pkl\")\n",
        "\n",
        "    train_stream = build_or_load_token_stream(\n",
        "        cache_path=train_stream_cache,\n",
        "        dataset_name=cfg.dataset_name,\n",
        "        dataset_config=cfg.dataset_config,\n",
        "        split=\"train\",\n",
        "        tokenizer_name=cfg.tokenizer_name,\n",
        "        min_chars=1,\n",
        "        add_eos_between_rows=True,\n",
        "    )\n",
        "    val_stream = build_or_load_token_stream(\n",
        "        cache_path=val_stream_cache,\n",
        "        dataset_name=cfg.dataset_name,\n",
        "        dataset_config=cfg.dataset_config,\n",
        "        split=\"validation\",\n",
        "        tokenizer_name=cfg.tokenizer_name,\n",
        "        min_chars=1,\n",
        "        add_eos_between_rows=True,\n",
        "    )\n",
        "\n",
        "    val_dataset = build_or_load_validation_windows(\n",
        "        cache_path=cfg.val_windows_cache,\n",
        "        token_stream=val_stream,\n",
        "        max_seq_len=cfg.max_seq_len,\n",
        "        stride_frac=cfg.stride_frac_val,\n",
        "        val_samples_target=cfg.val_samples_target,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=cfg.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "        pin_memory=torch.cuda.is_available(),\n",
        "    )\n",
        "\n",
        "    train_ds = WikiTextRandomWindowStream(\n",
        "        token_stream=train_stream,\n",
        "        max_seq_len=cfg.max_seq_len,\n",
        "        train_samples_target=cfg.train_samples_target,\n",
        "        seed=cfg.seed,\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=cfg.batch_size,\n",
        "        num_workers=3,\n",
        "        pin_memory=torch.cuda.is_available(),\n",
        "    )\n",
        "\n",
        "    # ---------- Model ----------\n",
        "    model = ASMLanguageModel(\n",
        "        vocab_size=cfg.vocab_size,\n",
        "        embed_dim=cfg.embed_dim,\n",
        "        num_layers=cfg.num_layers,\n",
        "        num_heads=cfg.num_heads,\n",
        "        num_slots=cfg.num_slots,\n",
        "        max_seq_len=cfg.max_seq_len,\n",
        "        mlp_ratio=cfg.mlp_ratio,\n",
        "        dropout=cfg.dropout,\n",
        "\n",
        "        read_temperature=cfg.read_temperature,\n",
        "        write_temperature=cfg.write_temperature,\n",
        "        state_fp32=cfg.state_fp32,\n",
        "        slot_dropout=cfg.slot_dropout,\n",
        "        normalize_k=cfg.normalize_k,\n",
        "\n",
        "        tie_weights=cfg.tie_weights,\n",
        "\n",
        "        use_abs_pos=cfg.use_abs_pos,\n",
        "\n",
        "        use_rope_keys=cfg.use_rope_keys,\n",
        "        rope_base=cfg.rope_base,\n",
        "        use_alibi_write=cfg.use_alibi_write,\n",
        "        alibi_strength_init=cfg.alibi_strength_init,\n",
        "        learn_alibi_strength=cfg.learn_alibi_strength,\n",
        "        min_strength=cfg.min_strength,\n",
        "\n",
        "        use_content_read=cfg.use_content_read,\n",
        "        content_read_init=cfg.content_read_init,\n",
        "        content_read_max_gamma=cfg.content_read_max_gamma,\n",
        "\n",
        "        use_slotspace_refine=cfg.use_slotspace_refine,\n",
        "        slotspace_dim=cfg.slotspace_dim,\n",
        "        slotspace_gate_init=cfg.slotspace_gate_init,\n",
        "        slotspace_dropout=cfg.slotspace_dropout,\n",
        "        slotspace_signed_weights=cfg.slotspace_signed_weights,\n",
        "\n",
        "        use_rope_slotspace=cfg.use_rope_slotspace,\n",
        "        rope_base_slotspace=cfg.rope_base_slotspace,\n",
        "\n",
        "        write_chunk_size=cfg.write_chunk_size,\n",
        "        slotspace_chunk_size=cfg.slotspace_chunk_size,\n",
        "    ).to(device)\n",
        "\n",
        "    out_dir = os.path.join(cfg.output_dir, cfg.tag)\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    n_params = sum(p.numel() for p in model.parameters())\n",
        "    print(\"=\" * 108)\n",
        "    print(f\"Training [{cfg.tag}] on {cfg.dataset_name}/{cfg.dataset_config}\")\n",
        "    print(f\"Params: {n_params:,}\")\n",
        "    print(f\"Train tokens: {len(train_stream):,} | Val tokens: {len(val_stream):,} | Val windows: {len(val_dataset):,}\")\n",
        "    print(f\"T={cfg.max_seq_len} | val_stride_frac={cfg.stride_frac_val} | last_k={cfg.analytics_last_k}\")\n",
        "    print(f\"Chunks: write={cfg.write_chunk_size} | slotspace={cfg.slotspace_chunk_size} | amp={use_amp}({amp_dtype}) | state_fp32={cfg.state_fp32}\")\n",
        "    print(f\"RoPE: keys={cfg.use_rope_keys}(base={cfg.rope_base:g}) | slotspace={cfg.use_rope_slotspace}(base={cfg.rope_base_slotspace:g})\")\n",
        "    print(\"=\" * 108)\n",
        "\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay, betas=cfg.betas)\n",
        "    sched = WarmupCosine(opt, cfg.warmup_steps, cfg.total_steps, cfg.learning_rate)\n",
        "\n",
        "    # ---------- Initial eval ----------\n",
        "    best_val = float(\"inf\")\n",
        "    vloss, vppl, vstats = evaluate(model, val_loader, max_batches=cfg.eval_max_batches, last_k=cfg.analytics_last_k)\n",
        "    best_val = vloss\n",
        "    save_ckpt(os.path.join(out_dir, \"best.pt\"), cfg, model, opt, 0, best_val)\n",
        "\n",
        "    print(f\"[VAL step 0] loss={vloss:.3f} ppl={vppl:.2f}\")\n",
        "    if vstats:\n",
        "        print(\"  \" + _fmt_stats(vstats, last_k=cfg.analytics_last_k))\n",
        "\n",
        "    # ---------- Training loop ----------\n",
        "    running = 0.0\n",
        "    step = 0\n",
        "    t_last = time.time()\n",
        "\n",
        "    pbar = tqdm(total=cfg.total_steps, desc=f\"[{cfg.tag}]\")\n",
        "    for xb, yb in train_loader:\n",
        "        xb = xb.to(device, non_blocking=True)\n",
        "        yb = yb.to(device, non_blocking=True)\n",
        "\n",
        "        with torch.autocast(device_type=device.type, dtype=amp_dtype, enabled=use_amp):\n",
        "            logits = model(xb)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), yb.view(-1))\n",
        "\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
        "        opt.step()\n",
        "        lr = sched.step()\n",
        "\n",
        "        step += 1\n",
        "        running += float(loss.item())\n",
        "        pbar.update(1)\n",
        "\n",
        "        if step % cfg.log_interval == 0:\n",
        "            avg = running / cfg.log_interval\n",
        "            running = 0.0\n",
        "\n",
        "            ps = _layer_param_summaries(model)\n",
        "\n",
        "            it_s = cfg.log_interval / max(1e-9, (time.time() - t_last))\n",
        "            t_last = time.time()\n",
        "\n",
        "            postfix = {\n",
        "                \"loss\": f\"{avg:.3f}\",\n",
        "                \"ppl\": f\"{math.exp(min(20.0, avg)):.2f}\",\n",
        "                \"lr\": f\"{lr:.2e}\",\n",
        "                \"it/s\": f\"{it_s:.2f}\",\n",
        "            }\n",
        "            if \"content_read_gamma_mean\" in ps: postfix[\"\"] = f\"{ps['content_read_gamma_mean']:.3f}\"\n",
        "            if \"slotspace_gate_mean\" in ps: postfix[\"sg\"] = f\"{ps['slotspace_gate_mean']:.3f}\"\n",
        "            pbar.set_postfix(postfix)\n",
        "\n",
        "            msg = f\"[step {step}] train_loss={avg:.3f} ppl={math.exp(min(20.0, avg)):.2f} lr={lr:.2e} it/s={it_s:.2f}\"\n",
        "            if \"content_read_gamma_mean\" in ps: msg += f\" | content_read_gamma_mean={ps['content_read_gamma_mean']:.4f}\"\n",
        "            if \"slotspace_gate_mean\" in ps: msg += f\" | slotspace_gate_mean={ps['slotspace_gate_mean']:.4f}\"\n",
        "            if \"alibi_strength_mean\" in ps: msg += f\" | alibi_strength_mean={ps['alibi_strength_mean']:.4f}\"\n",
        "            print(msg)\n",
        "\n",
        "        if step % cfg.eval_interval == 0:\n",
        "            vloss, vppl, vstats = evaluate(model, val_loader, max_batches=cfg.eval_max_batches, last_k=cfg.analytics_last_k)\n",
        "            print(f\"\\n[VAL step {step}] loss={vloss:.3f} ppl={vppl:.2f}\")\n",
        "            if vstats:\n",
        "                print(\"  \" + _fmt_stats(vstats, last_k=cfg.analytics_last_k))\n",
        "\n",
        "            if vloss < best_val:\n",
        "                best_val = vloss\n",
        "                save_ckpt(os.path.join(out_dir, \"best.pt\"), cfg, model, opt, step, best_val)\n",
        "\n",
        "        if step >= cfg.total_steps:\n",
        "            break\n",
        "\n",
        "    save_ckpt(os.path.join(out_dir, \"final.pt\"), cfg, model, opt, step, best_val)\n",
        "    print(f\"[{cfg.tag}] Done. Best val loss: {best_val:.4f}\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Kljuj3nHIuQT"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Load Checkpointed Config, Model\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ============================\n",
        "# USER EDIT SECTION\n",
        "# ============================\n",
        "\n",
        "CKPT_PATH = \"./drive/MyDrive/asm_outputs/asm_wikitext_1024t_1024d_32h_64sd_64s_128cs_12l/best.pt\"\n",
        "\n",
        "\n",
        "# ppl ? on test, 56.71M\n",
        "#CKPT_PATH = \"./drive/MyDrive/asm_outputs/asm_wikitext_1024t_384d_32sd_16s_35l/best.pt\"\n",
        "\n",
        "# ppl ? on test, ?M\n",
        "#CKPT_PATH = \"./drive/MyDrive/asm_outputs/asm_wikitext_1024t_256d_16a_11l_75ksteps/best.pt\"\n",
        "\n",
        "# ppl ? on test, 79.63M\n",
        "#CKPT_PATH = \"./drive/MyDrive/asm_outputs/asm_wikitext_1024t_384d_16a_15l/best.pt\"\n",
        "\n",
        "# ppl ? on test, ?M\n",
        "#CKPT_PATH = \"./drive/MyDrive/asm_outputs/asm_wikitext_1024t_384d_32a_5l/best.pt\"\n",
        "\n",
        "# ppl ? on test, ?M\n",
        "#CKPT_PATH = \"./drive/MyDrive/asm_outputs/asm_wikitext_3072t_384d_32sd_16s_7l/best.pt\"\n",
        "\n",
        "#ppl ? on test, 91.55M\n",
        "#CKPT_PATH = \"./drive/MyDrive/asm_outputs/asm_wikitext_1024t_640d_64sd_24s_12l/best.pt\"\n",
        "\n",
        "# ppl ? on test, 91.94M\n",
        "#CKPT_PATH = \"./drive/MyDrive/asm_outputs/asm_wikitext_1024t_384d_16sd_8s_21l/best.pt\"\n",
        "\n",
        "# ppl ? on test, 159.48M\n",
        "#CKPT_PATH = \"./drive/MyDrive/asm_outputs/asm_wikitext_1024t_768d_32sd_32cs_17l/best.pt\"\n",
        "\n",
        "# ppl ? on test, 230.59M\n",
        "#CKPT_PATH = \"./drive/MyDrive/asm_outputs/asm_wikitext_1024t_640d_64sd_24s_17l/best.pt\"\n",
        "\n",
        "\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def build_model_from_cfg_dict(cfg: dict):\n",
        "    cfg_obj = ASMTrainConfig(**cfg)\n",
        "    model = build_model_from_cfg(cfg_obj)\n",
        "    return model, cfg_obj\n",
        "\n",
        "# 2) If your checkpoint stores model state under a different key, edit here\n",
        "MODEL_STATE_KEY = \"model\"\n",
        "CFG_KEY = \"cfg\"\n",
        "\n",
        "# ============================\n",
        "# END USER EDIT SECTION\n",
        "# ============================\n",
        "\n",
        "def count_params(model):\n",
        "    n = sum(p.numel() for p in model.parameters())\n",
        "    n_train = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return n, n_train\n",
        "\n",
        "def cfg_summary(cfg):\n",
        "    return {\n",
        "        \"layers\": cfg.num_layers,\n",
        "        \"d_model\": cfg.embed_dim,\n",
        "        \"heads\": cfg.num_heads,\n",
        "        \"slots\": cfg.num_slots,\n",
        "        \"T\": cfg.max_seq_len,\n",
        "        \"slotspace\": cfg.use_slotspace_refine,\n",
        "        \"slotspace_dim\": cfg.slotspace_dim,\n",
        "        \"content_read\": cfg.use_content_read,\n",
        "        \"alibi_write\": cfg.use_alibi_write,\n",
        "        \"rope_keys\": cfg.use_rope_keys,\n",
        "    }\n",
        "\n",
        "def print_cfg_summary(cfg, model):\n",
        "    n, n_train = count_params(model)\n",
        "    summ = cfg_summary(cfg)\n",
        "    print(\"Config:\", \", \".join([f\"{k}={v}\" for k,v in summ.items()]))\n",
        "    print(f\"Params: {n/1e6:.2f}M\")\n",
        "\n",
        "\n",
        "def load_model_and_cfg(ckpt_path: str):\n",
        "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "    cfg = ckpt.get(CFG_KEY, None)\n",
        "    if cfg is None:\n",
        "        raise KeyError(f\"Checkpoint missing '{CFG_KEY}'. Keys: {list(ckpt.keys())}\")\n",
        "\n",
        "    model, cfg_obj = build_model_from_cfg_dict(cfg)\n",
        "    sd = ckpt.get(MODEL_STATE_KEY, None)\n",
        "    if sd is None:\n",
        "        raise KeyError(f\"Checkpoint missing '{MODEL_STATE_KEY}'. Keys: {list(ckpt.keys())}\")\n",
        "\n",
        "    missing, unexpected = model.load_state_dict(sd, strict=False)\n",
        "    print(\"Loaded state_dict.\")\n",
        "    if missing: print(\"  Missing keys:\", len(missing))\n",
        "    if unexpected: print(\"  Unexpected keys:\", len(unexpected))\n",
        "\n",
        "    model = model.to(DEVICE).eval()\n",
        "    return model, cfg_obj, cfg, ckpt\n",
        "\n",
        "model, cfg, cfg_dict, ckpt = load_model_and_cfg(CKPT_PATH)\n",
        "print(\" Model ready on\", DEVICE)\n",
        "\n",
        "print(\"cfg keys:\", list(cfg_dict.keys()), \"...\")\n",
        "print(cfg)\n",
        "\n",
        "#model, cfg, cfg_dict, ckpt = load_model_and_cfg(CKPT_PATH)\n",
        "#print_cfg_summary(cfg, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "dy8HaDkNKTnD"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================\n",
        "#@title ASA / ASM: Trajectory Bundle + Energy Basin (B + D combined)\n",
        "# Single comprehensive cell\n",
        "# - Collect routing-logit trajectories (per-layer)\n",
        "# - Fit PCA (2D) on read_logits (head-mean)\n",
        "# - Plot trajectory bundle\n",
        "# - Build an energy basin via causal intervention:\n",
        "#     force routing distribution at each samples own last valid position\n",
        "#     and measure E = -log P(target) averaged over prompts\n",
        "# - Optional improvements:\n",
        "#     * per-example t_target (last non-pad)\n",
        "#     * choose layer for PCA and a (possibly different) layer for intervention\n",
        "#     * optional multi-layer intervention\n",
        "#     * optional intervention \"where\" (read / content_read_only / slotspace_only)\n",
        "#     * optional smoothing / robust bounds\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from typing import List, Optional, Tuple, Dict, Union\n",
        "\n",
        "# If transformers isn't available in your runtime, replace tokenizer bits with your own tokenizer.\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# ----------------------------\n",
        "# User edit section\n",
        "# ----------------------------\n",
        "PROMPTS: List[str] = [\n",
        "    \"The capital of France is\",\n",
        "    \"In Europe, the city that serves as France's capital is\",\n",
        "    \"France's capital city, known for the Eiffel Tower, is\",\n",
        "    \"A major European capital located on the Seine is\",\n",
        "    \"The French capital is\",\n",
        "]\n",
        "\n",
        "TARGET_TEXT: str = \" Paris\"   # IMPORTANT: must be a single token for energy calculation\n",
        "PCA_LAYER: int = 9 # None         # None => auto pick mid-late layer (min(10, n_layers-1))\n",
        "INTERVENE_LAYERS: Union[int, List[int]] = [3,7,13] # None  # None => [PCA_LAYER]; int => single; list => multiple\n",
        "\n",
        "# Grid + plotting\n",
        "GRID_N: int = 60              # try 40-60 while iterating; 70+ is expensive\n",
        "GRID_PAD: float = 0.35        # pad around trajectory extents\n",
        "CONTOUR_LEVELS: int = 35\n",
        "\n",
        "# Routing intervention settings\n",
        "# Where to apply slot masks if provided (matches your API)\n",
        "SLOT_MASK: Optional[torch.Tensor] = None         # shape [K], 1 keep / 0 mask\n",
        "SLOT_MASK_WHERE: str = \"read\"                    # \"read\" | \"content_read_only\" | \"slotspace_only\"\n",
        "SLOT_MASK_SCOPE: str = \"all\"                     # \"all\" | \"last_pos_only\"\n",
        "\n",
        "# Control for the forced routing distribution mapping\n",
        "FORCED_ROUTING_TEMP: float = 1.0   # temp used to convert PCA-inverse logits -> distribution; keep 1.0 unless you want sharper basins\n",
        "FORCED_ROUTING_FLOOR: float = 0.0  # e.g. 1e-6 to avoid exact zeros; usually not needed\n",
        "\n",
        "# Performance knobs\n",
        "USE_HALF_FOR_INFOS: bool = False   # set True only if you know your info tensors are safe in fp16\n",
        "DEVICE = DEVICE  # assumes you already set DEVICE earlier\n",
        "\n",
        "# ----------------------------\n",
        "# Sanity: model exists\n",
        "# ----------------------------\n",
        "assert \"model\" in globals(), \"Expected `model` to exist (from your checkpoint load cell).\"\n",
        "assert \"cfg\" in globals(),   \"Expected `cfg` to exist (your ASMTrainConfig).\"\n",
        "\n",
        "# ----------------------------\n",
        "# Tokenizer\n",
        "# ----------------------------\n",
        "def get_tokenizer(cfg):\n",
        "    tok = AutoTokenizer.from_pretrained(cfg.tokenizer_name, use_fast=True)\n",
        "    if tok.pad_token is None:\n",
        "        tok.pad_token = tok.eos_token\n",
        "    return tok\n",
        "\n",
        "def encode_prompts(tokenizer, prompts: List[str], max_len: int, device):\n",
        "    enc = tokenizer(\n",
        "        prompts,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_len,\n",
        "    )\n",
        "    input_ids = enc[\"input_ids\"].to(device)\n",
        "    attn_mask = enc[\"attention_mask\"].to(device)\n",
        "    return input_ids, attn_mask\n",
        "\n",
        "def token_id_single(tokenizer, text: str) -> int:\n",
        "    ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "    if len(ids) != 1:\n",
        "        raise ValueError(\n",
        "            f\"TARGET_TEXT {text!r} maps to {len(ids)} tokens {ids}. \"\n",
        "            \"Pick a single-token target (often include leading space for GPT2 tokenizers).\"\n",
        "        )\n",
        "    return ids[0]\n",
        "\n",
        "tokenizer = get_tokenizer(cfg)\n",
        "target_id = token_id_single(tokenizer, TARGET_TEXT)\n",
        "\n",
        "# ----------------------------\n",
        "# Helpers: PCA\n",
        "# ----------------------------\n",
        "def fit_pca_2d(Z_btk: np.ndarray):\n",
        "    # Z_btk: [B,T,K]\n",
        "    X = Z_btk.reshape(-1, Z_btk.shape[-1])  # [B*T, K]\n",
        "    mean = X.mean(axis=0, keepdims=True)    # [1,K]\n",
        "    Xc = X - mean\n",
        "    # SVD PCA\n",
        "    _, _, Vt = np.linalg.svd(Xc, full_matrices=False)\n",
        "    W = Vt[:2].T  # [K,2]\n",
        "    return mean.squeeze(0), W\n",
        "\n",
        "def pca_project(Z_btk: np.ndarray, mean_k: np.ndarray, W_k2: np.ndarray):\n",
        "    X = Z_btk.reshape(-1, Z_btk.shape[-1])\n",
        "    X2 = (X - mean_k) @ W_k2\n",
        "    return X2.reshape(Z_btk.shape[0], Z_btk.shape[1], 2)  # [B,T,2]\n",
        "\n",
        "def pca_inverse(xy_2: np.ndarray, mean_k: np.ndarray, W_k2: np.ndarray):\n",
        "    # [2] -> [K]\n",
        "    return mean_k + (W_k2 @ xy_2)\n",
        "\n",
        "def robust_bounds(x: np.ndarray, qlo=0.01, qhi=0.99):\n",
        "    lo = np.quantile(x, qlo)\n",
        "    hi = np.quantile(x, qhi)\n",
        "    return float(lo), float(hi)\n",
        "\n",
        "# ----------------------------\n",
        "# Routing override callable\n",
        "# ----------------------------\n",
        "class ForceRoutingAtLastValid:\n",
        "    def __init__(self, t_targets_b: torch.Tensor, r_forced_bhk: torch.Tensor):\n",
        "        \"\"\"\n",
        "        t_targets_b: [B] last valid position index per example (global time index)\n",
        "        r_forced_bhk: [B,H,K] forced routing distribution\n",
        "        \"\"\"\n",
        "        self.t_targets_b = t_targets_b\n",
        "        self.r_forced_bhk = r_forced_bhk\n",
        "\n",
        "    def __call__(self, t0, t1, read_logits, read_logits_key, read_logits_content, ctx):\n",
        "        # Default softmax routing on combined logits\n",
        "        rtemp = float(ctx.get(\"rtemp\", 1.0))\n",
        "        w = torch.softmax(read_logits / max(1e-6, rtemp), dim=-1)  # [B,H,L,K]\n",
        "\n",
        "        # Override each sample's target position if it falls within this chunk\n",
        "        B = w.shape[0]\n",
        "        for b in range(B):\n",
        "            tt = int(self.t_targets_b[b].item())\n",
        "            if t0 <= tt < t1:\n",
        "                local = tt - t0\n",
        "                w[b, :, local, :] = self.r_forced_bhk[b].to(dtype=w.dtype, device=w.device)\n",
        "\n",
        "        w = torch.nan_to_num(w, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "        w = w.clamp_min(0.0)\n",
        "        w = w / w.sum(dim=-1, keepdim=True).clamp_min(1e-8)\n",
        "        return w\n",
        "\n",
        "# ----------------------------\n",
        "# Run model with infos\n",
        "# ----------------------------\n",
        "@torch.no_grad()\n",
        "def run_with_info(model, input_ids, attention_mask):\n",
        "    logits, infos = model(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        return_info=True,\n",
        "        routing_mode=\"softmax\",\n",
        "        routing_topk=2,\n",
        "        routing_noise=None,\n",
        "        slot_mask=SLOT_MASK,\n",
        "        slot_mask_where=SLOT_MASK_WHERE,\n",
        "        slot_mask_scope=SLOT_MASK_SCOPE,\n",
        "    )\n",
        "    return logits, infos\n",
        "\n",
        "# ----------------------------\n",
        "# Energy computation (supports multi-layer intervention)\n",
        "# ----------------------------\n",
        "@torch.no_grad()\n",
        "def energy_at_xy(\n",
        "    model,\n",
        "    *,\n",
        "    input_ids: torch.Tensor,\n",
        "    attention_mask: torch.Tensor,\n",
        "    layers: List[int],\n",
        "    t_targets_b: torch.Tensor,       # [B]\n",
        "    xy_2: np.ndarray,                # [2]\n",
        "    mean_k: np.ndarray,              # [K]\n",
        "    W_k2: np.ndarray,                # [K,2]\n",
        "    target_id: int,\n",
        "):\n",
        "    # Map xy -> K logits -> forced distribution\n",
        "    z_k = pca_inverse(xy_2, mean_k, W_k2)  # [K]\n",
        "    z = torch.tensor(z_k, device=DEVICE, dtype=torch.float32)\n",
        "\n",
        "    # Build [B,H,K] per layer (H,K must match that layer)\n",
        "    overrides = []\n",
        "    old = []\n",
        "\n",
        "    try:\n",
        "        for li in layers:\n",
        "            asa = model.blocks[li].asa\n",
        "            H = asa.num_heads\n",
        "            K = asa.num_slots\n",
        "            assert z.numel() == K, f\"PCA space K={z.numel()} but layer {li} has K={K}. Use same K layers/models.\"\n",
        "\n",
        "            logits = z / max(1e-6, float(FORCED_ROUTING_TEMP))\n",
        "            r = torch.softmax(logits, dim=-1)\n",
        "\n",
        "            if FORCED_ROUTING_FLOOR and FORCED_ROUTING_FLOOR > 0:\n",
        "                r = (r + float(FORCED_ROUTING_FLOOR))\n",
        "                r = r / r.sum()\n",
        "\n",
        "            r_bhk = r.view(1, 1, K).expand(input_ids.shape[0], H, K).contiguous()\n",
        "\n",
        "            old.append(asa.routing_override)\n",
        "            override = ForceRoutingAtLastValid(t_targets_b=t_targets_b, r_forced_bhk=r_bhk)\n",
        "            asa.routing_override = override\n",
        "            overrides.append(override)\n",
        "\n",
        "        # Forward pass with overrides active\n",
        "        logits_out = model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            return_info=False,\n",
        "            routing_mode=\"softmax\",\n",
        "            routing_topk=2,\n",
        "            routing_noise=None,\n",
        "            slot_mask=SLOT_MASK,\n",
        "            slot_mask_where=SLOT_MASK_WHERE,\n",
        "            slot_mask_scope=SLOT_MASK_SCOPE,\n",
        "        )\n",
        "\n",
        "        # Compute per-example -log P(target) at each examples own t_target, then average\n",
        "        logp = F.log_softmax(logits_out, dim=-1)  # [B,T,V]\n",
        "        B = logp.shape[0]\n",
        "        idx = t_targets_b.view(B, 1, 1).expand(B, 1, 1)\n",
        "        # gather logp[b, t_targets_b[b], target_id]\n",
        "        gather_t = logp.gather(dim=1, index=idx.expand(B, 1, logp.shape[-1]))[:, 0, :]  # [B,V]\n",
        "        tgt = gather_t[:, target_id]  # [B]\n",
        "        E = (-tgt).mean().item()\n",
        "\n",
        "    finally:\n",
        "        # restore\n",
        "        for li, prev in zip(layers, old):\n",
        "            model.blocks[li].asa.routing_override = prev\n",
        "\n",
        "    return E\n",
        "\n",
        "# ----------------------------\n",
        "# Plotting\n",
        "# ----------------------------\n",
        "def plot_trajectories(traj_xy: np.ndarray, prompts: List[str], title: str):\n",
        "    plt.figure(figsize=(8.5, 6.5))\n",
        "    for b in range(traj_xy.shape[0]):\n",
        "        xy = traj_xy[b]\n",
        "        plt.plot(xy[:, 0], xy[:, 1], linewidth=1.5)\n",
        "        plt.scatter([xy[-1, 0]], [xy[-1, 1]], s=30)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"PC1\")\n",
        "    plt.ylabel(\"PC2\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "def plot_basin_with_trajectories(XX, YY, Egrid, traj_xy, title: str):\n",
        "    plt.figure(figsize=(9.5, 7.5))\n",
        "    cs = plt.contourf(XX, YY, Egrid, levels=int(CONTOUR_LEVELS))\n",
        "    plt.colorbar(cs, label=\"-log P(target)\")\n",
        "    for b in range(traj_xy.shape[0]):\n",
        "        xy = traj_xy[b]\n",
        "        plt.plot(xy[:, 0], xy[:, 1], linewidth=1.5)\n",
        "        plt.scatter([xy[-1, 0]], [xy[-1, 1]], s=30)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"PC1\")\n",
        "    plt.ylabel(\"PC2\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "# ----------------------------\n",
        "# Execute pipeline\n",
        "# ----------------------------\n",
        "# 1) Encode\n",
        "input_ids, attention_mask = encode_prompts(tokenizer, PROMPTS, max_len=cfg.max_seq_len, device=DEVICE)\n",
        "\n",
        "# 2) Determine layer(s)\n",
        "n_layers = len(model.blocks)\n",
        "if PCA_LAYER is None:\n",
        "    PCA_LAYER = min(10, n_layers - 1)\n",
        "if INTERVENE_LAYERS is None:\n",
        "    INTERVENE_LAYERS = [PCA_LAYER]\n",
        "elif isinstance(INTERVENE_LAYERS, int):\n",
        "    INTERVENE_LAYERS = [INTERVENE_LAYERS]\n",
        "else:\n",
        "    INTERVENE_LAYERS = list(INTERVENE_LAYERS)\n",
        "\n",
        "print(f\"[INFO] Using PCA_LAYER={PCA_LAYER}, INTERVENE_LAYERS={INTERVENE_LAYERS}\")\n",
        "\n",
        "# 3) Last valid position per example\n",
        "t_targets_b = attention_mask.long().sum(dim=1) - 1  # [B]\n",
        "t_targets_b = t_targets_b.clamp_min(0)\n",
        "\n",
        "# 4) Baseline forward with infos\n",
        "logits_base, infos_base = run_with_info(model, input_ids, attention_mask)\n",
        "\n",
        "# 5) Extract read_logits at PCA_LAYER and build trajectory vectors\n",
        "read_logits = infos_base[PCA_LAYER][\"read_logits\"]  # [B,H,T,K] float32 (as returned)\n",
        "if USE_HALF_FOR_INFOS:\n",
        "    read_logits = read_logits.half()\n",
        "\n",
        "B, H, T, K = read_logits.shape\n",
        "Z = read_logits.mean(dim=1)  # [B,T,K]\n",
        "Z_np = Z.detach().cpu().numpy()\n",
        "\n",
        "# 6) PCA fit + project\n",
        "mean_k, W_k2 = fit_pca_2d(Z_np)\n",
        "traj_xy = pca_project(Z_np, mean_k, W_k2)  # [B,T,2]\n",
        "\n",
        "# 7) Trajectory plot\n",
        "plot_trajectories(traj_xy, PROMPTS, title=f\"Trajectory bundle in PCA(read_logits) (layer {PCA_LAYER})\")\n",
        "plt.show()\n",
        "\n",
        "# 8) Grid bounds (robust, to avoid outliers dominating)\n",
        "x_all = traj_xy[..., 0].reshape(-1)\n",
        "y_all = traj_xy[..., 1].reshape(-1)\n",
        "xmin, xmax = robust_bounds(x_all, 0.01, 0.99)\n",
        "ymin, ymax = robust_bounds(y_all, 0.01, 0.99)\n",
        "dx = (xmax - xmin) * float(GRID_PAD)\n",
        "dy = (ymax - ymin) * float(GRID_PAD)\n",
        "\n",
        "xs = np.linspace(xmin - dx, xmax + dx, int(GRID_N))\n",
        "ys = np.linspace(ymin - dy, ymax + dy, int(GRID_N))\n",
        "XX, YY = np.meshgrid(xs, ys)\n",
        "\n",
        "print(f\"[INFO] Grid: {GRID_N}x{GRID_N} over x=[{xs[0]:.3f},{xs[-1]:.3f}], y=[{ys[0]:.3f},{ys[-1]:.3f}]\")\n",
        "\n",
        "# 9) Energy basin computation\n",
        "Egrid = np.zeros_like(XX, dtype=np.float32)\n",
        "\n",
        "# Small speed hack: disable grad + ensure eval\n",
        "model.eval()\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "# Compute energy at each grid point\n",
        "for i in range(YY.shape[0]):\n",
        "    for j in range(XX.shape[1]):\n",
        "        xy = np.array([XX[i, j], YY[i, j]], dtype=np.float32)\n",
        "        Egrid[i, j] = energy_at_xy(\n",
        "            model,\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            layers=INTERVENE_LAYERS,\n",
        "            t_targets_b=t_targets_b,\n",
        "            xy_2=xy,\n",
        "            mean_k=mean_k,\n",
        "            W_k2=W_k2,\n",
        "            target_id=target_id,\n",
        "        )\n",
        "    if (i + 1) % max(1, (YY.shape[0] // 10)) == 0:\n",
        "        print(f\"[INFO] Basin progress: {i+1}/{YY.shape[0]} rows\")\n",
        "\n",
        "# 10) Basin + trajectories overlay\n",
        "plot_basin_with_trajectories(\n",
        "    XX, YY, Egrid, traj_xy,\n",
        "    title=(\n",
        "        f\"Energy basin (forced routing) + trajectories\\n\"\n",
        "        f\"target={TARGET_TEXT!r} | PCA layer={PCA_LAYER} | intervene layers={INTERVENE_LAYERS} | last-valid t per sample\"\n",
        "    )\n",
        ")\n",
        "plt.show()\n",
        "\n",
        "# 11) Report baseline target logprob (for reference)\n",
        "with torch.no_grad():\n",
        "    logp_base = F.log_softmax(logits_base, dim=-1)  # [B,T,V]\n",
        "    B0 = logp_base.shape[0]\n",
        "    # gather per-example at last valid position\n",
        "    idx = t_targets_b.view(B0, 1, 1).expand(B0, 1, 1)\n",
        "    gather_t = logp_base.gather(dim=1, index=idx.expand(B0, 1, logp_base.shape[-1]))[:, 0, :]\n",
        "    tgt = gather_t[:, target_id]\n",
        "    print(f\"[INFO] Baseline mean -logP(target) at last valid pos: {(-tgt).mean().item():.4f}\")\n",
        "    print(f\"[INFO] Baseline mean P(target): {tgt.exp().mean().item():.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ot9_G-A7L0ws"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================\n",
        "#@title Visualization A: Concept Constellations on a Simplex Sphere\n",
        "#   - Extract ASA routing logits (pre-softmax) per token\n",
        "#   - Reduce to 3D (PCA) and project to S (unit sphere)\n",
        "#   - Plot each prompt as an arc (trajectory) on a globe\n",
        "#   - Optionally color by final predicted token or by semantic class\n",
        "# ============================\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
        "\n",
        "# ----------------------------\n",
        "# 0) Tokenizer (robust)\n",
        "# ----------------------------\n",
        "try:\n",
        "    from transformers import GPT2TokenizerFast\n",
        "    tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "except Exception as e:\n",
        "    tokenizer = None\n",
        "    print(\"WARNING: Could not load GPT2TokenizerFast. If you already have `tokenizer`, ignore this.\")\n",
        "    print(\"Error:\", repr(e))\n",
        "\n",
        "# ----------------------------\n",
        "# 1) Prompt set (edit here)\n",
        "# ----------------------------\n",
        "# Provide either:\n",
        "#  - prompts: list[str], and optional classes: list[str] same length\n",
        "# Or keep defaults to validate plumbing.\n",
        "\n",
        "prompts = [\n",
        "    \"The capital of France is\",\n",
        "    \"France's capital city is\",\n",
        "    \"Paris is the capital of\",\n",
        "    \"In Europe, the capital of France is\",\n",
        "    \"The Eiffel Tower is located in\",\n",
        "]\n",
        "\n",
        "# Optional: semantic labels (same length as prompts). If None, we color by final token.\n",
        "classes = [\n",
        "    \"capital\",\n",
        "    \"capital\",\n",
        "    \"capital_reverse\",\n",
        "    \"capital_context\",\n",
        "    \"location_landmark\",\n",
        "]\n",
        "\n",
        "assert len(prompts) == len(classes), \"If you set `classes`, it must match `prompts` length.\"\n",
        "\n",
        "# ----------------------------\n",
        "# 2) Which layer / aggregation settings\n",
        "# ----------------------------\n",
        "LAYER_IDX = 9           # choose which ASA layer to inspect\n",
        "AGG_HEADS = \"mean\"      # \"mean\" or \"none\"\n",
        "TOKEN_RANGE = \"all\"     # \"all\" | \"last_k\"\n",
        "LAST_K = 64             # used if TOKEN_RANGE == \"last_k\"\n",
        "\n",
        "# ----------------------------\n",
        "# 3) Helpers\n",
        "# ----------------------------\n",
        "def _safe_decode(tok_ids):\n",
        "    if tokenizer is None:\n",
        "        return str(tok_ids)\n",
        "    return tokenizer.decode(tok_ids)\n",
        "\n",
        "def get_last_valid_index(attn_mask_1d: torch.Tensor) -> int:\n",
        "    # attn_mask_1d: [T] {0,1}\n",
        "    idx = int(attn_mask_1d.long().sum().item()) - 1\n",
        "    return max(idx, 0)\n",
        "\n",
        "@torch.no_grad()\n",
        "def run_and_extract_read_logits(model, text: str, layer_idx: int):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      read_logits_seq: [T, K] float32 (pre-softmax combined logits, head-aggregated if AGG_HEADS='mean')\n",
        "      attn_mask: [T] bool\n",
        "      final_pred_token: str (decoded argmax at last valid position)\n",
        "      input_ids: [T] long\n",
        "    \"\"\"\n",
        "    assert tokenizer is not None, \"Tokenizer is required here. Please define `tokenizer`.\"\n",
        "\n",
        "    enc = tokenizer(text, return_tensors=\"pt\")\n",
        "    input_ids = enc[\"input_ids\"].to(DEVICE)\n",
        "    attn_mask = enc.get(\"attention_mask\", torch.ones_like(input_ids)).to(DEVICE)\n",
        "\n",
        "    # model forward\n",
        "    logits, infos = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attn_mask,\n",
        "        return_info=True,\n",
        "        routing_mode=\"softmax\",   # keep default routing\n",
        "    )\n",
        "\n",
        "    # select layer\n",
        "    info = infos[layer_idx]\n",
        "    rl = info[\"read_logits\"]  # [B,H,T,K] float32 (per your module)\n",
        "    # sanity\n",
        "    if rl is None:\n",
        "        raise RuntimeError(\"info['read_logits'] is None. Ensure return_info=True and ASA returns read_logits.\")\n",
        "\n",
        "    # squeeze B=1\n",
        "    rl = rl[0]  # [H,T,K]\n",
        "    if AGG_HEADS == \"mean\":\n",
        "        rl = rl.mean(dim=0)   # [T,K]\n",
        "    elif AGG_HEADS == \"none\":\n",
        "        # keep heads: [H,T,K] -> we will flatten H into feature dim later\n",
        "        pass\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown AGG_HEADS={AGG_HEADS}\")\n",
        "\n",
        "    # final predicted token at last valid position\n",
        "    last_t = get_last_valid_index(attn_mask[0])\n",
        "    pred_id = int(torch.argmax(logits[0, last_t]).item())\n",
        "    pred_tok = _safe_decode([pred_id])\n",
        "\n",
        "    # token trimming choice\n",
        "    T = input_ids.shape[1]\n",
        "    if TOKEN_RANGE == \"last_k\":\n",
        "        t0 = max(0, T - LAST_K)\n",
        "        if AGG_HEADS == \"mean\":\n",
        "            rl = rl[t0:T, :]                  # [L,K]\n",
        "        else:\n",
        "            rl = rl[:, t0:T, :]               # [H,L,K]\n",
        "        input_ids = input_ids[:, t0:T]\n",
        "        attn_mask = attn_mask[:, t0:T]\n",
        "\n",
        "    return rl.detach().cpu().float(), attn_mask[0].detach().cpu().bool(), pred_tok, input_ids[0].detach().cpu()\n",
        "\n",
        "def stack_features(read_logits, agg_heads: str):\n",
        "    \"\"\"\n",
        "    Convert per-prompt read_logits into a 2D array of points for PCA.\n",
        "      if agg_heads == \"mean\":  read_logits is [T,K] -> points [T,K]\n",
        "      if agg_heads == \"none\":  read_logits is [H,T,K] -> points [T,H*K]\n",
        "    \"\"\"\n",
        "    if agg_heads == \"mean\":\n",
        "        return read_logits.numpy()  # [T,K]\n",
        "    else:\n",
        "        H, T, K = read_logits.shape\n",
        "        return read_logits.permute(1,0,2).reshape(T, H*K).numpy()  # [T,H*K]\n",
        "\n",
        "def sphere_project(x3: np.ndarray, eps: float = 1e-9) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    x3: [N,3]\n",
        "    return: [N,3] normalized to unit sphere\n",
        "    \"\"\"\n",
        "    n = np.linalg.norm(x3, axis=1, keepdims=True)\n",
        "    return x3 / np.maximum(n, eps)\n",
        "\n",
        "# ----------------------------\n",
        "# 4) Extract trajectories\n",
        "# ----------------------------\n",
        "trajectories = []\n",
        "all_points = []\n",
        "meta = []\n",
        "\n",
        "for i, (p, cls) in enumerate(zip(prompts, classes)):\n",
        "    rl, am, pred_tok, inp_ids = run_and_extract_read_logits(model, p, LAYER_IDX)\n",
        "\n",
        "    # filter valid tokens only (mask)\n",
        "    # (for GPT2, attention mask is typically all-ones, but we keep it correct)\n",
        "    valid_idx = np.where(am.numpy())[0]\n",
        "    rl_valid = rl if AGG_HEADS == \"mean\" else rl  # same object; we slice below\n",
        "\n",
        "    if AGG_HEADS == \"mean\":\n",
        "        rl_valid = rl_valid[valid_idx, :]                  # [Tv,K]\n",
        "    else:\n",
        "        rl_valid = rl_valid[:, valid_idx, :]               # [H,Tv,K]\n",
        "\n",
        "    pts = stack_features(rl_valid, AGG_HEADS)              # [Tv,D]\n",
        "    trajectories.append(pts)\n",
        "    all_points.append(pts)\n",
        "\n",
        "    # token strings for hover-like inspection (printed later)\n",
        "    toks = [_safe_decode([int(t)]) for t in inp_ids.numpy().tolist()]\n",
        "    toks_valid = [toks[j] for j in valid_idx.tolist()]\n",
        "\n",
        "    meta.append({\n",
        "        \"prompt\": p,\n",
        "        \"class\": cls,\n",
        "        \"pred_tok\": pred_tok,\n",
        "        \"tokens\": toks_valid,\n",
        "        \"n\": pts.shape[0],\n",
        "    })\n",
        "\n",
        "X = np.concatenate(all_points, axis=0)  # [N_total, D]\n",
        "\n",
        "print(f\"[OK] Collected {len(trajectories)} trajectories | total points: {X.shape[0]} | feature dim: {X.shape[1]}\")\n",
        "print(\"Layer:\", LAYER_IDX, \"| Heads aggregation:\", AGG_HEADS, \"| Token range:\", TOKEN_RANGE)\n",
        "\n",
        "# ----------------------------\n",
        "# 5) PCA -> 3D -> Sphere\n",
        "# ----------------------------\n",
        "pca = PCA(n_components=3, random_state=0)\n",
        "X3 = pca.fit_transform(X)               # [N,3]\n",
        "X3s = sphere_project(X3)                # [N,3]\n",
        "\n",
        "# Split back per-trajectory\n",
        "splits = np.cumsum([tr.shape[0] for tr in trajectories])[:-1]\n",
        "traj3s = np.split(X3s, splits, axis=0)\n",
        "\n",
        "print(\"Explained variance (PC1..3):\", np.round(pca.explained_variance_ratio_, 4))\n",
        "\n",
        "# ----------------------------\n",
        "# 6) Plot: Simplex Sphere globe + arcs\n",
        "# ----------------------------\n",
        "def plot_globe_with_trajectories(traj3_list, meta, color_by=\"pred_tok\"):\n",
        "    \"\"\"\n",
        "    color_by:\n",
        "      - \"pred_tok\" (final predicted token string)\n",
        "      - \"class\"    (semantic class label you provided)\n",
        "      - \"index\"    (each trajectory unique color)\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(10, 9))\n",
        "    ax = fig.add_subplot(111, projection=\"3d\")\n",
        "    ax.set_title(f\"Concept Constellations on a Simplex Sphere\\n(read_logits PCAS) | layer={LAYER_IDX} | color_by={color_by}\")\n",
        "\n",
        "    # draw a faint sphere wireframe\n",
        "    u = np.linspace(0, 2*np.pi, 40)\n",
        "    v = np.linspace(0, np.pi, 20)\n",
        "    xs = np.outer(np.cos(u), np.sin(v))\n",
        "    ys = np.outer(np.sin(u), np.sin(v))\n",
        "    zs = np.outer(np.ones_like(u), np.cos(v))\n",
        "    ax.plot_wireframe(xs, ys, zs, rstride=2, cstride=2, linewidth=0.3, alpha=0.15)\n",
        "\n",
        "    # grouping key\n",
        "    if color_by == \"pred_tok\":\n",
        "        keys = [m[\"pred_tok\"] for m in meta]\n",
        "    elif color_by == \"class\":\n",
        "        keys = [m[\"class\"] for m in meta]\n",
        "    elif color_by == \"index\":\n",
        "        keys = [f\"traj_{i}\" for i in range(len(meta))]\n",
        "    else:\n",
        "        raise ValueError(\"color_by must be one of: pred_tok, class, index\")\n",
        "\n",
        "    uniq = list(dict.fromkeys(keys))  # preserve order\n",
        "    key_to_id = {k:i for i,k in enumerate(uniq)}\n",
        "\n",
        "    for i, (pts, m) in enumerate(zip(traj3_list, meta)):\n",
        "        k = keys[i]\n",
        "        cid = key_to_id[k]\n",
        "\n",
        "        # trajectory curve\n",
        "        ax.plot(pts[:,0], pts[:,1], pts[:,2], linewidth=2.0, alpha=0.9)\n",
        "\n",
        "        # start/end markers\n",
        "        ax.scatter([pts[0,0]], [pts[0,1]], [pts[0,2]], s=40, marker=\"o\", alpha=0.9)\n",
        "        ax.scatter([pts[-1,0]], [pts[-1,1]], [pts[-1,2]], s=60, marker=\"X\", alpha=0.95)\n",
        "\n",
        "        # label at end\n",
        "        label = f\"{i}: {k}\"\n",
        "        ax.text(pts[-1,0], pts[-1,1], pts[-1,2], label, fontsize=9)\n",
        "\n",
        "    ax.set_xlabel(\"PC1 (sphere)\")\n",
        "    ax.set_ylabel(\"PC2 (sphere)\")\n",
        "    ax.set_zlabel(\"PC3 (sphere)\")\n",
        "    ax.set_box_aspect([1,1,1])\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Choose coloring mode:\n",
        "# - If you want the many routes lead to the same city punchline: use pred_tok.\n",
        "# - If you want semantic grouping: use class.\n",
        "plot_globe_with_trajectories(traj3s, meta, color_by=\"pred_tok\")\n",
        "\n",
        "# ----------------------------\n",
        "# 7) Optional: quick textual inspection\n",
        "# ----------------------------\n",
        "for i, m in enumerate(meta):\n",
        "    print(f\"\\n--- Trajectory {i} ---\")\n",
        "    print(\"class:\", m[\"class\"], \"| final_pred_tok:\", repr(m[\"pred_tok\"]))\n",
        "    print(\"prompt:\", m[\"prompt\"])\n",
        "    print(\"tokens (last 12):\", m[\"tokens\"][-12:])\n",
        "\n",
        "# ----------------------------\n",
        "# 8) Optional: 2D fallback (PC1/PC2) WITHOUT sphere normalization\n",
        "#     (Useful if you want a flat map of the constellation)\n",
        "# ----------------------------\n",
        "def plot_flat_constellation(traj3_list, meta, use_sphere=True, color_by=\"pred_tok\"):\n",
        "    fig = plt.figure(figsize=(10, 7))\n",
        "    ax = fig.add_subplot(111)\n",
        "    title = \"Flat Constellation (PCA)\" + (\" on S (PC normalized)\" if use_sphere else \" (raw PC coords)\")\n",
        "    ax.set_title(f\"{title} | layer={LAYER_IDX} | color_by={color_by}\")\n",
        "\n",
        "    if color_by == \"pred_tok\":\n",
        "        keys = [m[\"pred_tok\"] for m in meta]\n",
        "    elif color_by == \"class\":\n",
        "        keys = [m[\"class\"] for m in meta]\n",
        "    else:\n",
        "        keys = [f\"traj_{i}\" for i in range(len(meta))]\n",
        "\n",
        "    for i, (pts, m) in enumerate(zip(traj3_list, meta)):\n",
        "        ax.plot(pts[:,0], pts[:,1], linewidth=2.0, alpha=0.9)\n",
        "        ax.scatter([pts[0,0]], [pts[0,1]], s=35, marker=\"o\", alpha=0.9)\n",
        "        ax.scatter([pts[-1,0]], [pts[-1,1]], s=55, marker=\"X\", alpha=0.95)\n",
        "        ax.text(pts[-1,0], pts[-1,1], f\"{i}:{keys[i]}\", fontsize=9)\n",
        "\n",
        "    ax.set_xlabel(\"PC1\")\n",
        "    ax.set_ylabel(\"PC2\")\n",
        "    ax.grid(True, alpha=0.2)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Uncomment if you want the flat view too:\n",
        "# plot_flat_constellation(traj3s, meta, use_sphere=True, color_by=\"pred_tok\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fuUjjInGPsnT"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================\n",
        "#@title Visualization C: Slot Simplex Barycentric Animation (faithful)\n",
        "#   - Cluster slots into M meta-slots (M=3 or 4)\n",
        "#   - Aggregate routing mass per meta-slot per token\n",
        "#   - Plot barycentric point on simplex (triangle / tetrahedron)\n",
        "#   - Animate token-by-token with trails\n",
        "#   - Vertex colors reflect affinity to the last token via\n",
        "#     content-read logits at the last valid position (proxy for\n",
        "#     slot_state similarity; mechanically available in info)\n",
        "# ============================================================\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "\n",
        "# If you want automatic meta-slot grouping:\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# ----------------------------\n",
        "# User knobs\n",
        "# ----------------------------\n",
        "LAYER_IDX = 7                  # ASA layer to visualize\n",
        "META_SLOTS = 3                  # 3 -> triangle, 4 -> tetrahedron\n",
        "PROMPTS = [\n",
        "    \"The capital of France is\",\n",
        "    \"France's capital city is\",\n",
        "    \"Paris is the capital of\",\n",
        "    \"In Europe, the capital of France is\",\n",
        "    \"The Eiffel Tower is located in\",\n",
        "]\n",
        "# Optional names for the META_SLOTS vertices (purely cosmetic)\n",
        "META_NAMES = None               # e.g. [\"geo\",\"hist\",\"narr\"] or None\n",
        "\n",
        "# How to aggregate heads:\n",
        "HEAD_AGG = \"mean\"               # \"mean\" (recommended) or \"none\" (not supported here)\n",
        "\n",
        "# Slot grouping method:\n",
        "GROUPING = \"kmeans_slotkeys\"    # \"kmeans_slotkeys\" or \"manual\"\n",
        "# If manual, define groups as list[list[int]] of length META_SLOTS\n",
        "MANUAL_GROUPS = None            # e.g. [[0,1,2,3],[4,5,6,7],[8,9,10,11]] etc.\n",
        "\n",
        "# Animation options\n",
        "TAIL = 40                       # number of past points shown in trail\n",
        "INTERVAL_MS = 1000                # frame interval\n",
        "SHOW_START_END = True\n",
        "USE_LAST_VALID_T = True         # if False, uses final token index T-1\n",
        "LIMIT_TOKENS = None             # None or int (e.g., 128) to cap length for speed\n",
        "\n",
        "# Vertex coloring proxy:\n",
        "# We use mean content-read logits at last valid position per meta-slot.\n",
        "# If content-read logits absent, fallback to combined read_logits.\n",
        "VERTEX_SCORE_SOURCE = \"content\" # \"content\" or \"combined\"\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Helpers\n",
        "# ------------------------------------------------------------\n",
        "def _get_last_valid_index(attn_mask_1d: torch.Tensor) -> int:\n",
        "    idx = int(attn_mask_1d.long().sum().item()) - 1\n",
        "    return max(idx, 0)\n",
        "\n",
        "def _barycentric_to_xy(w3: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"w3: [T,3] -> xy: [T,2] on an equilateral triangle\"\"\"\n",
        "    # vertices\n",
        "    v0 = np.array([0.0, 0.0])\n",
        "    v1 = np.array([1.0, 0.0])\n",
        "    v2 = np.array([0.5, math.sqrt(3)/2.0])\n",
        "    V = np.stack([v0, v1, v2], axis=0)  # [3,2]\n",
        "    return w3 @ V                       # [T,2]\n",
        "\n",
        "def _barycentric_to_xyz(w4: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"w4: [T,4] -> xyz: [T,3] on a regular tetrahedron\"\"\"\n",
        "    v0 = np.array([0.0, 0.0, 0.0])\n",
        "    v1 = np.array([1.0, 0.0, 0.0])\n",
        "    v2 = np.array([0.5, math.sqrt(3)/2.0, 0.0])\n",
        "    v3 = np.array([0.5, math.sqrt(3)/6.0, math.sqrt(2.0/3.0)])\n",
        "    V = np.stack([v0, v1, v2, v3], axis=0)  # [4,3]\n",
        "    return w4 @ V                            # [T,3]\n",
        "\n",
        "def _normalize01(x: np.ndarray, eps: float = 1e-9) -> np.ndarray:\n",
        "    lo, hi = np.min(x), np.max(x)\n",
        "    if hi - lo < eps:\n",
        "        return np.zeros_like(x)\n",
        "    return (x - lo) / (hi - lo)\n",
        "\n",
        "@torch.no_grad()\n",
        "def extract_routing_and_scores(model, tokenizer, prompt: str, layer_idx: int):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      mass: [T, K] float32  (mean over heads)\n",
        "      score: [T, K] float32 (mean over heads)  content-read logits or combined read_logits\n",
        "      tokens: list[str] length T (decoded per token)\n",
        "      last_t: int last valid position\n",
        "    \"\"\"\n",
        "    enc = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = enc[\"input_ids\"].to(DEVICE)\n",
        "    attn_mask = enc.get(\"attention_mask\", torch.ones_like(input_ids)).to(DEVICE)\n",
        "\n",
        "    logits, infos = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attn_mask,\n",
        "        return_info=True,\n",
        "        routing_mode=\"softmax\",\n",
        "    )\n",
        "    info = infos[layer_idx]\n",
        "\n",
        "    rw = info[\"read_weights\"]          # [B,H,T,K]\n",
        "    rl = info[\"read_logits\"]           # [B,H,T,K] combined\n",
        "    rlc = info.get(\"read_logits_content\", None)  # [B,H,T,K] or None\n",
        "\n",
        "    # mean over heads\n",
        "    rw = rw[0].mean(dim=0).float().cpu()     # [T,K]\n",
        "    rl = rl[0].mean(dim=0).float().cpu()     # [T,K]\n",
        "    if rlc is not None:\n",
        "        rlc = rlc[0].mean(dim=0).float().cpu()  # [T,K]\n",
        "\n",
        "    # token strings\n",
        "    toks = [tokenizer.decode([int(t)]) for t in input_ids[0].detach().cpu().tolist()]\n",
        "\n",
        "    last_t = _get_last_valid_index(attn_mask[0].detach().cpu()) if USE_LAST_VALID_T else (rw.shape[0] - 1)\n",
        "\n",
        "    # optional cap for speed\n",
        "    if LIMIT_TOKENS is not None:\n",
        "        Tcap = int(LIMIT_TOKENS)\n",
        "        rw = rw[:Tcap]\n",
        "        rl = rl[:Tcap]\n",
        "        if rlc is not None:\n",
        "            rlc = rlc[:Tcap]\n",
        "        toks = toks[:Tcap]\n",
        "        last_t = min(last_t, Tcap - 1)\n",
        "\n",
        "    if VERTEX_SCORE_SOURCE == \"content\" and rlc is not None:\n",
        "        score = rlc\n",
        "    else:\n",
        "        score = rl\n",
        "\n",
        "    return rw.numpy(), score.numpy(), toks, last_t\n",
        "\n",
        "def build_groups_from_slotkeys_kmeans(model, layer_idx: int, meta_slots: int):\n",
        "    \"\"\"\n",
        "    Cluster slots into meta slots using mean slot_keys over heads\n",
        "    from the ASA module at the chosen layer.\n",
        "    \"\"\"\n",
        "    asa = model.blocks[layer_idx].asa\n",
        "    with torch.no_grad():\n",
        "        # slot_keys: [H,K,d] -> mean over heads -> [K,d]\n",
        "        sk = asa.slot_keys.detach().float().mean(dim=0).cpu().numpy()\n",
        "\n",
        "    km = KMeans(n_clusters=meta_slots, random_state=0, n_init=\"auto\")\n",
        "    labels = km.fit_predict(sk)  # [K]\n",
        "\n",
        "    groups = []\n",
        "    for m in range(meta_slots):\n",
        "        idxs = np.where(labels == m)[0].tolist()\n",
        "        groups.append(idxs)\n",
        "\n",
        "    # stable ordering: largest group first (optional; makes plots more stable)\n",
        "    groups = sorted(groups, key=lambda g: (-len(g), g[0] if len(g) else 10**9))\n",
        "    return groups\n",
        "\n",
        "def aggregate_to_meta(rw_TK: np.ndarray, score_TK: np.ndarray, groups):\n",
        "    \"\"\"\n",
        "    rw_TK: [T,K] routing masses\n",
        "    score_TK: [T,K] logits proxy for affinity\n",
        "    groups: list[list[int]] length M\n",
        "    Returns:\n",
        "      meta_mass: [T,M]\n",
        "      meta_vertex_score: [M] score at last_t aggregated over group\n",
        "      meta_per_token_score: [T,M] (sometimes useful)\n",
        "    \"\"\"\n",
        "    T, K = rw_TK.shape\n",
        "    M = len(groups)\n",
        "    meta_mass = np.zeros((T, M), dtype=np.float32)\n",
        "    meta_score = np.zeros((T, M), dtype=np.float32)\n",
        "\n",
        "    for m, idxs in enumerate(groups):\n",
        "        if len(idxs) == 0:\n",
        "            continue\n",
        "        meta_mass[:, m] = rw_TK[:, idxs].sum(axis=1)\n",
        "        meta_score[:, m] = score_TK[:, idxs].mean(axis=1)\n",
        "\n",
        "    # renormalize mass (should already sum to 1, but grouping can introduce small drift)\n",
        "    meta_mass = meta_mass / np.clip(meta_mass.sum(axis=1, keepdims=True), 1e-8, None)\n",
        "    return meta_mass, meta_score\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1) Build / validate groups\n",
        "# ------------------------------------------------------------\n",
        "assert META_SLOTS in (3, 4), \"META_SLOTS must be 3 or 4 for triangle/tetrahedron.\"\n",
        "\n",
        "if GROUPING == \"manual\":\n",
        "    assert MANUAL_GROUPS is not None and len(MANUAL_GROUPS) == META_SLOTS, \\\n",
        "        \"Provide MANUAL_GROUPS as list of length META_SLOTS.\"\n",
        "    groups = MANUAL_GROUPS\n",
        "elif GROUPING == \"kmeans_slotkeys\":\n",
        "    groups = build_groups_from_slotkeys_kmeans(model, LAYER_IDX, META_SLOTS)\n",
        "else:\n",
        "    raise ValueError(f\"Unknown GROUPING={GROUPING}\")\n",
        "\n",
        "print(f\"[OK] META_SLOTS={META_SLOTS} grouping={GROUPING}\")\n",
        "for i, g in enumerate(groups):\n",
        "    print(f\"  meta {i}: n={len(g)} slots -> {g[:10]}{'...' if len(g)>10 else ''}\")\n",
        "\n",
        "if META_NAMES is None:\n",
        "    META_NAMES = [f\"meta_{i}\" for i in range(META_SLOTS)]\n",
        "else:\n",
        "    assert len(META_NAMES) == META_SLOTS\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2) Extract per-prompt barycentric trajectories\n",
        "# ------------------------------------------------------------\n",
        "all_trajs = []\n",
        "all_tokens = []\n",
        "all_last_t = []\n",
        "all_vertex_scores = []  # per prompt: [M]\n",
        "\n",
        "for p in PROMPTS:\n",
        "    rw, score, toks, last_t = extract_routing_and_scores(model, tokenizer, p, LAYER_IDX)\n",
        "    meta_mass, meta_score = aggregate_to_meta(rw, score, groups)\n",
        "\n",
        "    # Vertex score: mean meta_score at last_t (then normalized later)\n",
        "    vscore = meta_score[last_t].copy()\n",
        "\n",
        "    all_trajs.append(meta_mass)     # [T,M]\n",
        "    all_tokens.append(toks)\n",
        "    all_last_t.append(last_t)\n",
        "    all_vertex_scores.append(vscore)\n",
        "\n",
        "# Normalize vertex scores across vertices per prompt for colormap scaling\n",
        "all_vertex_scores = np.stack(all_vertex_scores, axis=0)  # [P,M]\n",
        "# global scaling across all prompts for comparability:\n",
        "vscore_norm = _normalize01(all_vertex_scores.reshape(-1)).reshape(all_vertex_scores.shape)\n",
        "\n",
        "print(\"[OK] Extracted trajectories.\")\n",
        "for i, p in enumerate(PROMPTS):\n",
        "    print(f\"  {i}: T={all_trajs[i].shape[0]} last_t={all_last_t[i]} | prompt={p}\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3) Plot + animate\n",
        "# ------------------------------------------------------------\n",
        "P = len(PROMPTS)\n",
        "Tmax = max(tr.shape[0] for tr in all_trajs)\n",
        "\n",
        "if META_SLOTS == 3:\n",
        "    # precompute XY\n",
        "    XY = [_barycentric_to_xy(tr) for tr in all_trajs]  # list of [T,2]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(9, 8))\n",
        "    ax.set_title(f\"Slot Simplex Barycentric Animation (Triangle)\\nlayer={LAYER_IDX} | groups={GROUPING} | score={VERTEX_SCORE_SOURCE}\")\n",
        "    ax.set_aspect(\"equal\")\n",
        "    ax.set_xlabel(\"simplex-x\")\n",
        "    ax.set_ylabel(\"simplex-y\")\n",
        "\n",
        "    # draw triangle\n",
        "    tri = np.array([[0,0],[1,0],[0.5,math.sqrt(3)/2],[0,0]])\n",
        "    ax.plot(tri[:,0], tri[:,1], linewidth=1.5)\n",
        "    verts = tri[:3]\n",
        "\n",
        "    # vertex colors from vscore_norm averaged over prompts (or you can pick a prompt)\n",
        "    vcol = vscore_norm.mean(axis=0)  # [3]\n",
        "    scat_v = ax.scatter(verts[:,0], verts[:,1], s=140, c=vcol, cmap=\"viridis\")\n",
        "    for i, name in enumerate(META_NAMES):\n",
        "        ax.text(verts[i,0], verts[i,1], f\"  {name}\", fontsize=10, va=\"center\")\n",
        "\n",
        "    # one trail + point per prompt\n",
        "    trails = []\n",
        "    points = []\n",
        "    labels = []\n",
        "    for i in range(P):\n",
        "        (ln,) = ax.plot([], [], linewidth=2.0, alpha=0.9)\n",
        "        pt = ax.scatter([], [], s=80, marker=\"o\")\n",
        "        trails.append(ln)\n",
        "        points.append(pt)\n",
        "        labels.append(ax.text(0.02, 0.98 - 0.04*i, f\"{i}: {PROMPTS[i][:40]}\",\n",
        "                              transform=ax.transAxes, fontsize=9, va=\"top\"))\n",
        "\n",
        "    token_text = ax.text(0.02, 0.02, \"\", transform=ax.transAxes, fontsize=11, va=\"bottom\")\n",
        "\n",
        "    def init():\n",
        "        for ln in trails:\n",
        "            ln.set_data([], [])\n",
        "        for pt in points:\n",
        "            pt.set_offsets(np.array([[np.nan, np.nan]]))\n",
        "        token_text.set_text(\"\")\n",
        "        return trails + points + [token_text]\n",
        "\n",
        "    def update(frame):\n",
        "        # frame is global token index; each prompt uses min(frame, T-1)\n",
        "        frame = int(frame)\n",
        "        txt_parts = []\n",
        "        for i in range(P):\n",
        "            xy = XY[i]\n",
        "            t = min(frame, xy.shape[0]-1)\n",
        "            t0 = max(0, t - TAIL)\n",
        "            seg = xy[t0:t+1]\n",
        "\n",
        "            trails[i].set_data(seg[:,0], seg[:,1])\n",
        "            points[i].set_offsets(seg[-1:, :])\n",
        "\n",
        "            tok = all_tokens[i][t] if t < len(all_tokens[i]) else \"\"\n",
        "            txt_parts.append(f\"{i}:{repr(tok)}\")\n",
        "\n",
        "        token_text.set_text(\" | \".join(txt_parts[:3]) + (\" | ...\" if len(txt_parts) > 3 else \"\"))\n",
        "        return trails + points + [token_text]\n",
        "\n",
        "    ani = FuncAnimation(fig, update, frames=Tmax, init_func=init, interval=INTERVAL_MS, blit=False)\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    # META_SLOTS == 4 -> tetrahedron in 3D\n",
        "    from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
        "\n",
        "    XYZ = [_barycentric_to_xyz(tr) for tr in all_trajs]  # list of [T,3]\n",
        "\n",
        "    fig = plt.figure(figsize=(10, 9))\n",
        "    ax = fig.add_subplot(111, projection=\"3d\")\n",
        "    ax.set_title(f\"Slot Simplex Barycentric Animation (Tetrahedron)\\nlayer={LAYER_IDX} | groups={GROUPING} | score={VERTEX_SCORE_SOURCE}\")\n",
        "    ax.set_xlabel(\"x\")\n",
        "    ax.set_ylabel(\"y\")\n",
        "    ax.set_zlabel(\"z\")\n",
        "\n",
        "    # tetrahedron vertices\n",
        "    V = np.array([\n",
        "        [0.0, 0.0, 0.0],\n",
        "        [1.0, 0.0, 0.0],\n",
        "        [0.5, math.sqrt(3)/2.0, 0.0],\n",
        "        [0.5, math.sqrt(3)/6.0, math.sqrt(2.0/3.0)],\n",
        "    ])\n",
        "\n",
        "    # edges\n",
        "    edges = [(0,1),(0,2),(0,3),(1,2),(1,3),(2,3)]\n",
        "    for a,b in edges:\n",
        "        ax.plot([V[a,0],V[b,0]],[V[a,1],V[b,1]],[V[a,2],V[b,2]], linewidth=1.3, alpha=0.7)\n",
        "\n",
        "    vcol = vscore_norm.mean(axis=0)  # [4]\n",
        "    ax.scatter(V[:,0], V[:,1], V[:,2], s=140, c=vcol, cmap=\"viridis\")\n",
        "    for i, name in enumerate(META_NAMES):\n",
        "        ax.text(V[i,0], V[i,1], V[i,2], f\"  {name}\", fontsize=10)\n",
        "\n",
        "    trails = []\n",
        "    points = []\n",
        "    for i in range(P):\n",
        "        (ln,) = ax.plot([], [], [], linewidth=2.0, alpha=0.9)\n",
        "        pt = ax.scatter([], [], [], s=80, marker=\"o\")\n",
        "        trails.append(ln)\n",
        "        points.append(pt)\n",
        "\n",
        "    def init():\n",
        "        for ln in trails:\n",
        "            ln.set_data([], [])\n",
        "            ln.set_3d_properties([])\n",
        "        for pt in points:\n",
        "            pt._offsets3d = (np.array([np.nan]), np.array([np.nan]), np.array([np.nan]))\n",
        "        return trails + points\n",
        "\n",
        "    def update(frame):\n",
        "        frame = int(frame)\n",
        "        for i in range(P):\n",
        "            xyz = XYZ[i]\n",
        "            t = min(frame, xyz.shape[0]-1)\n",
        "            t0 = max(0, t - TAIL)\n",
        "            seg = xyz[t0:t+1]\n",
        "\n",
        "            trails[i].set_data(seg[:,0], seg[:,1])\n",
        "            trails[i].set_3d_properties(seg[:,2])\n",
        "\n",
        "            points[i]._offsets3d = (seg[-1:,0], seg[-1:,1], seg[-1:,2])\n",
        "\n",
        "        return trails + points\n",
        "\n",
        "    ani = FuncAnimation(fig, update, frames=Tmax, init_func=init, interval=INTERVAL_MS, blit=False)\n",
        "    plt.show()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Optional: Save the animation (uncomment one)\n",
        "# ------------------------------------------------------------\n",
        "# ani.save(\"slot_simplex_barycentric.mp4\", dpi=150)  # requires ffmpeg in environment\n",
        "ani.save(\"slot_simplex_barycentric.gif\", dpi=120)  # may require imagemagick/pillow writer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BgYIdzUd0S_4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ================================================================\n",
        "#@title Next Cell (Robust): Paris Basin on a Simplex Sphere (Viz A++) + Health Gate\n",
        "# Fixes:\n",
        "#  - pad_token\n",
        "#  - NaN/Inf detection with layer/token localization\n",
        "#  - Sanitization for visualization (without hiding diagnostics)\n",
        "#  - Stable PCA on CPU\n",
        "#  - Safe sphere normalization (no division by zero)\n",
        "#  - KMeans clustering with NaN filtering\n",
        "# ================================================================\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
        "\n",
        "# ---------------------------\n",
        "# SETTINGS\n",
        "# ---------------------------\n",
        "LAYER = 10\n",
        "HEAD_AGG = \"mean\"          # \"mean\" or \"none\"\n",
        "MAX_PROMPT_LEN = cfg.max_seq_len\n",
        "\n",
        "PLOT_STRIDE = 1\n",
        "SHOW_START_MARKERS = True\n",
        "SHOW_END_MARKERS = True\n",
        "LABEL_ENDS = True\n",
        "LABEL_MAX = 12\n",
        "\n",
        "SPHERE_ALPHA = 0.06\n",
        "SPHERE_WIREFRAME = True\n",
        "\n",
        "ENDPOINT_COLOR_MODE = \"paris_margin\"  # \"paris_logp\" | \"paris_margin\"\n",
        "RIVAL_TEXT = \" the\"\n",
        "\n",
        "COLOR_TRAJ_BY_ENTROPY = False\n",
        "ENTROPY_TEMP = 1.0\n",
        "\n",
        "DO_ENDPOINT_CLUSTERING = True\n",
        "N_CLUSTERS = 3\n",
        "\n",
        "PROMPTS = [\n",
        "    \"The capital of France is\",\n",
        "    \"France's capital city is\",\n",
        "    \"In France, the capital is\",\n",
        "    \"The seat of government of France is\",\n",
        "    \"The city that is France's capital is\",\n",
        "    \"The Eiffel Tower is located in\",\n",
        "    \"The Louvre is in\",\n",
        "    \"The Seine river flows through\",\n",
        "    \"During the French Revolution, the city of\",\n",
        "    \"Napoleon crowned himself in\",\n",
        "    \"In World War II, the liberation of\",\n",
        "    \"The largest city in France is\",\n",
        "    \"A major European city in France is\",\n",
        "    \"A famous city on the Seine is\",\n",
        "    \"The capital of Germany is\",\n",
        "    \"The capital of England is\",\n",
        "    \"The capital of Italy is\",\n",
        "]\n",
        "\n",
        "# Health gate knobs\n",
        "DEBUG_PRINT_MAX = 6\n",
        "SANITIZE_FOR_VIZ = True   # keep True; diagnostics still report non-finites\n",
        "FORCE_CPU_ONE_PASS = False  # set True if you suspect GPU numeric issues\n",
        "\n",
        "# ---------------------------\n",
        "# Tokenizer\n",
        "# ---------------------------\n",
        "def _get_tokenizer_from_context():\n",
        "    if \"tokenizer\" in globals():\n",
        "        tok = globals()[\"tokenizer\"]\n",
        "    else:\n",
        "        from transformers import AutoTokenizer\n",
        "        tok = AutoTokenizer.from_pretrained(getattr(cfg, \"tokenizer_name\", \"gpt2\"))\n",
        "\n",
        "    if tok.pad_token is None:\n",
        "        tok.pad_token = tok.eos_token\n",
        "    tok.padding_side = \"left\"\n",
        "    return tok\n",
        "\n",
        "tokenizer = _get_tokenizer_from_context()\n",
        "\n",
        "def ensure_single_token(text: str) -> int:\n",
        "    ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "    if len(ids) != 1:\n",
        "        raise ValueError(f\"{text!r} must be exactly 1 token. Got ids={ids}\")\n",
        "    return ids[0]\n",
        "\n",
        "TARGET_ID = ensure_single_token(\" Paris\")\n",
        "RIVAL_ID = ensure_single_token(RIVAL_TEXT)\n",
        "\n",
        "# ---------------------------\n",
        "# Helpers: finiteness diagnostics\n",
        "# ---------------------------\n",
        "def _finite_stats(name, x: torch.Tensor):\n",
        "    if x is None:\n",
        "        return f\"{name}: None\"\n",
        "    x = x.detach()\n",
        "    fin = torch.isfinite(x)\n",
        "    n = x.numel()\n",
        "    nf = fin.sum().item()\n",
        "    msg = f\"{name}: finite {nf}/{n} ({100.0*nf/max(1,n):.2f}%)\"\n",
        "    if nf < n:\n",
        "        # report some indices\n",
        "        bad = (~fin).nonzero(as_tuple=False)\n",
        "        msg += f\" | first_bad_idx={bad[0].tolist() if bad.numel() else None}\"\n",
        "    # value stats on finite subset only\n",
        "    if nf > 0:\n",
        "        xf = x[fin]\n",
        "        msg += f\" | min={xf.min().item():.4g} max={xf.max().item():.4g} mean={xf.mean().item():.4g}\"\n",
        "    return msg\n",
        "\n",
        "def _sanitize(x: torch.Tensor, name=\"x\"):\n",
        "    # convert NaN/Inf -> 0; leave finite untouched\n",
        "    if x is None:\n",
        "        return None\n",
        "    x2 = torch.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    return x2\n",
        "\n",
        "def _row_has_all_neg_inf(logits_row: torch.Tensor):\n",
        "    # logits_row: [V]\n",
        "    return torch.isneginf(logits_row).all().item()\n",
        "\n",
        "# ---------------------------\n",
        "# Forward\n",
        "# ---------------------------\n",
        "@torch.no_grad()\n",
        "def forward_with_info(ids, mask):\n",
        "    if FORCE_CPU_ONE_PASS:\n",
        "        m_cpu = model.to(\"cpu\").eval()\n",
        "        ids2 = ids.to(\"cpu\")\n",
        "        mask2 = mask.to(\"cpu\")\n",
        "        logits, infos = m_cpu(ids2, attention_mask=mask2, return_info=True, routing_mode=\"softmax\")\n",
        "        # bring back to DEVICE for plotting convenience\n",
        "        model.to(DEVICE).eval()\n",
        "        return logits.to(DEVICE), infos\n",
        "    else:\n",
        "        logits, infos = model(ids, attention_mask=mask, return_info=True, routing_mode=\"softmax\")\n",
        "        return logits, infos\n",
        "\n",
        "enc = tokenizer(\n",
        "    PROMPTS,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=MAX_PROMPT_LEN,\n",
        ")\n",
        "input_ids = enc[\"input_ids\"].to(DEVICE)\n",
        "attention_mask = enc.get(\"attention_mask\", torch.ones_like(input_ids)).to(DEVICE)\n",
        "\n",
        "B_prompts, T = input_ids.shape\n",
        "lengths = attention_mask.sum(dim=1).long()\n",
        "t_last = (lengths - 1).clamp(min=0)\n",
        "\n",
        "logits, infos = forward_with_info(input_ids, attention_mask)\n",
        "\n",
        "# ---------------------------\n",
        "# Health gate: logits\n",
        "# ---------------------------\n",
        "print(_finite_stats(\"logits\", logits))\n",
        "if not torch.isfinite(logits).all():\n",
        "    # localize offending batch/time\n",
        "    bad = (~torch.isfinite(logits)).nonzero(as_tuple=False)\n",
        "    # bad indices are [n_bad, 3] = (b,t,v)\n",
        "    print(f\"[health] logits non-finite count={bad.shape[0]}\")\n",
        "    for i in range(min(DEBUG_PRINT_MAX, bad.shape[0])):\n",
        "        b, t, v = bad[i].tolist()\n",
        "        print(f\"  bad@ b={b} t={t} vocab={v} | prompt={PROMPTS[b][:60]!r}\")\n",
        "        # check if whole row is -inf\n",
        "        row = logits[b, t]\n",
        "        if _row_has_all_neg_inf(row):\n",
        "            print(\"    row is all -inf (likely masking issue).\")\n",
        "\n",
        "info = infos[LAYER]\n",
        "read_logits = info.get(\"read_logits\", None)\n",
        "if read_logits is None:\n",
        "    raise KeyError(f\"infos[{LAYER}] missing 'read_logits'. Keys: {list(info.keys())}\")\n",
        "\n",
        "print(_finite_stats(\"read_logits(layer)\", read_logits))\n",
        "\n",
        "# If read_logits are busted, identify where\n",
        "if not torch.isfinite(read_logits).all():\n",
        "    bad = (~torch.isfinite(read_logits)).nonzero(as_tuple=False)\n",
        "    # indices are (b,h,t,k)\n",
        "    print(f\"[health] read_logits non-finite count={bad.shape[0]}\")\n",
        "    for i in range(min(DEBUG_PRINT_MAX, bad.shape[0])):\n",
        "        b, h, t, k = bad[i].tolist()\n",
        "        print(f\"  bad@ b={b} h={h} t={t} k={k} | prompt={PROMPTS[b][:60]!r}\")\n",
        "\n",
        "# Optional sanitize for visualization (still keep diagnostics above)\n",
        "if SANITIZE_FOR_VIZ:\n",
        "    logits_viz = _sanitize(logits, \"logits\")\n",
        "    read_logits_viz = _sanitize(read_logits, \"read_logits\")\n",
        "else:\n",
        "    logits_viz = logits\n",
        "    read_logits_viz = read_logits\n",
        "\n",
        "# ---------------------------\n",
        "# Head aggregation\n",
        "# ---------------------------\n",
        "B0, H0, T0, K = read_logits_viz.shape\n",
        "assert B0 == B_prompts and T0 == T, (read_logits_viz.shape, (B_prompts, T))\n",
        "\n",
        "if HEAD_AGG == \"mean\":\n",
        "    Z = read_logits_viz.mean(dim=1).float()  # [B,T,K]\n",
        "    attn_mask_Z = attention_mask\n",
        "    t_last_Z = t_last\n",
        "    B = B_prompts\n",
        "elif HEAD_AGG == \"none\":\n",
        "    Z = read_logits_viz.reshape(B_prompts * H0, T, K).float()\n",
        "    attn_mask_Z = attention_mask.repeat_interleave(H0, dim=0)\n",
        "    t_last_Z = t_last.repeat_interleave(H0, dim=0)\n",
        "    B = B_prompts * H0\n",
        "else:\n",
        "    raise ValueError(\"HEAD_AGG must be 'mean' or 'none'\")\n",
        "\n",
        "# ---------------------------\n",
        "# Stable PCA on CPU (covariance + eigh)\n",
        "# ---------------------------\n",
        "def robust_pca_3d(Z, mask, ridge=1e-6):\n",
        "    # Z: [B,T,K]\n",
        "    BtK = Z.reshape(-1, Z.shape[-1])\n",
        "    m = mask.reshape(-1).bool()\n",
        "    X = BtK[m].to(torch.float32)\n",
        "    # If X is degenerate (all zeros), bail early\n",
        "    X = torch.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    mu = X.mean(dim=0, keepdim=True)\n",
        "    Xc = (X - mu).detach().cpu()\n",
        "    N, Kc = Xc.shape\n",
        "    C = (Xc.T @ Xc) / float(max(1, N - 1))\n",
        "    C = C + ridge * torch.eye(Kc)\n",
        "    evals, evecs = torch.linalg.eigh(C)\n",
        "    idx = torch.argsort(evals, descending=True)\n",
        "    evals3 = evals[idx][:3].contiguous()\n",
        "    W3 = evecs[:, idx][:, :3].contiguous()  # [K,3]\n",
        "    return mu.to(Z.device), W3.to(Z.device), evals3.to(Z.device)\n",
        "\n",
        "mu, W3, evals3 = robust_pca_3d(Z, attn_mask_Z, ridge=1e-6)\n",
        "evr = (evals3 / (evals3.sum() + 1e-12)).detach().cpu().numpy()\n",
        "print(f\"[PCA] evals={evals3.detach().cpu().numpy()} | EVR{np.round(evr,4)}\")\n",
        "\n",
        "XYZ = (Z - mu.view(1,1,K)) @ W3  # [B,T,3]\n",
        "XYZ_np = XYZ.detach().cpu().numpy()\n",
        "\n",
        "# Safe sphere normalization\n",
        "norm = np.linalg.norm(XYZ_np, axis=-1, keepdims=True)\n",
        "bad_norm = ~np.isfinite(norm) | (norm < 1e-9)\n",
        "if bad_norm.any():\n",
        "    # set those points to origin; they will be ignored by plotting constraints\n",
        "    norm = np.where(bad_norm, 1.0, norm)\n",
        "XYZ_s = XYZ_np / norm\n",
        "XYZ_s = np.nan_to_num(XYZ_s, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "# ---------------------------\n",
        "# Endpoint scoring (Paris logp/margin) with NaN-safe softmax\n",
        "# ---------------------------\n",
        "logp = F.log_softmax(logits_viz, dim=-1)  # [B_prompts,T,V]\n",
        "lp_paris = logp[torch.arange(B_prompts, device=DEVICE), t_last, TARGET_ID].detach().cpu().numpy()\n",
        "lp_rival = logp[torch.arange(B_prompts, device=DEVICE), t_last, RIVAL_ID].detach().cpu().numpy()\n",
        "lp_paris = np.nan_to_num(lp_paris, nan=-1e9, posinf=1e9, neginf=-1e9)\n",
        "lp_rival = np.nan_to_num(lp_rival, nan=-1e9, posinf=1e9, neginf=-1e9)\n",
        "\n",
        "if ENDPOINT_COLOR_MODE == \"paris_logp\":\n",
        "    end_score = lp_paris\n",
        "    end_label = \"logP(' Paris')\"\n",
        "else:\n",
        "    end_score = lp_paris - lp_rival\n",
        "    end_label = f\"logP(' Paris') - logP({RIVAL_TEXT!r})\"\n",
        "\n",
        "if HEAD_AGG == \"none\":\n",
        "    end_score_plot = np.repeat(end_score, H0)\n",
        "else:\n",
        "    end_score_plot = end_score\n",
        "\n",
        "print(f\"[Paris-score] {end_label}: mean={np.mean(end_score):.4g} med={np.median(end_score):.4g} \"\n",
        "      f\"min={np.min(end_score):.4g} max={np.max(end_score):.4g}\")\n",
        "\n",
        "# ---------------------------\n",
        "# Optional entropy coloring\n",
        "# ---------------------------\n",
        "entropy = None\n",
        "if COLOR_TRAJ_BY_ENTROPY:\n",
        "    w = torch.softmax(Z / max(1e-6, float(ENTROPY_TEMP)), dim=-1)\n",
        "    ent = -(w * (w.clamp_min(1e-9).log())).sum(dim=-1)\n",
        "    entropy = ent.detach().cpu().numpy()\n",
        "    entropy = np.nan_to_num(entropy, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "# ---------------------------\n",
        "# Endpoint clustering (NaN-safe)\n",
        "# ---------------------------\n",
        "cluster_id = None\n",
        "cluster_centers = None\n",
        "if DO_ENDPOINT_CLUSTERING and HEAD_AGG == \"mean\":\n",
        "    try:\n",
        "        from sklearn.cluster import KMeans\n",
        "        ends = []\n",
        "        keep = []\n",
        "        for b in range(B_prompts):\n",
        "            p = XYZ_s[b, int(t_last[b].item()), :]\n",
        "            if np.isfinite(p).all() and (np.linalg.norm(p) > 1e-9):\n",
        "                ends.append(p)\n",
        "                keep.append(b)\n",
        "        ends = np.stack(ends, axis=0) if len(ends) else None\n",
        "\n",
        "        if ends is not None and ends.shape[0] >= N_CLUSTERS:\n",
        "            km = KMeans(n_clusters=N_CLUSTERS, n_init=10, random_state=0).fit(ends)\n",
        "            cluster_centers = km.cluster_centers_\n",
        "            cluster_id = -np.ones((B_prompts,), dtype=np.int64)\n",
        "            cluster_id[np.array(keep, dtype=np.int64)] = km.labels_\n",
        "        else:\n",
        "            print(\"[warn] clustering skipped (not enough finite endpoints).\")\n",
        "            DO_ENDPOINT_CLUSTERING = False\n",
        "    except Exception as e:\n",
        "        print(\"[warn] clustering unavailable or failed:\", repr(e))\n",
        "        DO_ENDPOINT_CLUSTERING = False\n",
        "\n",
        "# ---------------------------\n",
        "# Plot: sphere + trajectories\n",
        "# ---------------------------\n",
        "fig = plt.figure(figsize=(10.5, 8.5))\n",
        "ax = fig.add_subplot(111, projection=\"3d\")\n",
        "\n",
        "u = np.linspace(0, 2 * np.pi, 48)\n",
        "v = np.linspace(0, np.pi, 24)\n",
        "xs = np.outer(np.cos(u), np.sin(v))\n",
        "ys = np.outer(np.sin(u), np.sin(v))\n",
        "zs = np.outer(np.ones_like(u), np.cos(v))\n",
        "if SPHERE_WIREFRAME:\n",
        "    ax.plot_wireframe(xs, ys, zs, rstride=3, cstride=3, linewidth=0.5, alpha=SPHERE_ALPHA)\n",
        "else:\n",
        "    ax.plot_surface(xs, ys, zs, alpha=SPHERE_ALPHA, linewidth=0)\n",
        "\n",
        "for b in range(B):\n",
        "    t_end = int(t_last_Z[b].item())\n",
        "    pts = XYZ_s[b, :t_end + 1:PLOT_STRIDE, :]\n",
        "    if pts.shape[0] < 2:\n",
        "        continue\n",
        "\n",
        "    # If points are all-zero (degenerate), skip\n",
        "    if np.linalg.norm(pts, axis=1).max() < 1e-9:\n",
        "        continue\n",
        "\n",
        "    if COLOR_TRAJ_BY_ENTROPY and entropy is not None:\n",
        "        ent_b = entropy[b, :t_end + 1:PLOT_STRIDE]\n",
        "        ax.scatter(pts[:, 0], pts[:, 1], pts[:, 2], c=ent_b, s=12, alpha=0.9)\n",
        "        ax.plot(pts[:, 0], pts[:, 1], pts[:, 2], alpha=0.35)\n",
        "    else:\n",
        "        ax.plot(pts[:, 0], pts[:, 1], pts[:, 2], linewidth=2.0, alpha=0.85)\n",
        "\n",
        "    if SHOW_START_MARKERS:\n",
        "        ax.scatter([pts[0, 0]], [pts[0, 1]], [pts[0, 2]], s=40, marker=\"o\", alpha=0.9)\n",
        "    if SHOW_END_MARKERS:\n",
        "        ax.scatter([pts[-1, 0]], [pts[-1, 1]], [pts[-1, 2]],\n",
        "                   c=[end_score_plot[b]], s=70, marker=\"o\", alpha=0.95)\n",
        "\n",
        "import matplotlib as mpl\n",
        "normc = mpl.colors.Normalize(vmin=float(np.min(end_score_plot)), vmax=float(np.max(end_score_plot)))\n",
        "sm = mpl.cm.ScalarMappable(cmap=plt.cm.viridis, norm=normc)\n",
        "sm.set_array([])\n",
        "cbar = plt.colorbar(sm, ax=ax, fraction=0.03, pad=0.08)\n",
        "cbar.set_label(end_label)\n",
        "\n",
        "if LABEL_ENDS and HEAD_AGG == \"mean\":\n",
        "    order = np.argsort(-end_score)\n",
        "    for i in order[:min(LABEL_MAX, B_prompts)]:\n",
        "        p = XYZ_s[i, int(t_last[i].item()), :]\n",
        "        if np.isfinite(p).all() and np.linalg.norm(p) > 1e-9:\n",
        "            ax.text(p[0], p[1], p[2], f\" {i}:{PROMPTS[i][:40]}\", fontsize=8)\n",
        "\n",
        "if DO_ENDPOINT_CLUSTERING and cluster_centers is not None:\n",
        "    cc = np.nan_to_num(cluster_centers, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    ax.scatter(cc[:, 0], cc[:, 1], cc[:, 2], s=180, marker=\"X\", alpha=0.95)\n",
        "\n",
        "ax.set_title(\n",
        "    f\"Paris Basin on a Simplex Sphere (read_logits PCAS)\\n\"\n",
        "    f\"layer={LAYER} | head_agg={HEAD_AGG} | endpoints colored by {end_label}\\n\"\n",
        "    f\"PCA EVR (PC1..3)  {np.round(evr, 4)}\"\n",
        ")\n",
        "ax.set_xlabel(\"PC1 (sphere)\")\n",
        "ax.set_ylabel(\"PC2 (sphere)\")\n",
        "ax.set_zlabel(\"PC3 (sphere)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---------------------------\n",
        "# 2D endpoint view (raw PC1,PC2 before sphere)\n",
        "# ---------------------------\n",
        "ends2 = []\n",
        "for b in range(B):\n",
        "    ends2.append(XYZ_np[b, int(t_last_Z[b].item()), :2])\n",
        "ends2 = np.stack(ends2, axis=0)\n",
        "ends2 = np.nan_to_num(ends2, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "plt.figure(figsize=(7.5, 6.2))\n",
        "plt.scatter(ends2[:, 0], ends2[:, 1], c=end_score_plot, s=60)\n",
        "plt.colorbar(label=end_label)\n",
        "for i in range(min(B_prompts if HEAD_AGG==\"mean\" else B, LABEL_MAX)):\n",
        "    plt.text(ends2[i, 0], ends2[i, 1], f\" {i}\", fontsize=9)\n",
        "plt.title(f\"Endpoints in PCA(PC1,PC2) space (layer={LAYER})\")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---------------------------\n",
        "# Cluster composition summary\n",
        "# ---------------------------\n",
        "if DO_ENDPOINT_CLUSTERING and cluster_id is not None:\n",
        "    for c in range(N_CLUSTERS):\n",
        "        idxs = np.where(cluster_id == c)[0].tolist()\n",
        "        idxs_sorted = sorted(idxs, key=lambda i: float(end_score[i]), reverse=True)\n",
        "        show = idxs_sorted[:3]\n",
        "        print(f\"[cluster {c}] n={len(idxs)} | top prompts:\", [(i, PROMPTS[i][:30]) for i in show])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cVhgotXr2eq_"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ================================================================\n",
        "#@title COMPLETE REPLACEMENT: Paris Basin on a Simplex Sphere (A++)\n",
        "# Robust to: missing pad_token, NaNs/Infs in logits/read_logits, GPU SVD failures,\n",
        "# device mismatches, and partial non-finite activations.\n",
        "#\n",
        "# Outputs:\n",
        "#  1) 3D simplex sphere trajectories (read_logits PCAS) with endpoint color = Paris margin\n",
        "#  2) 2D PCA endpoint scatter (PC1, PC2)\n",
        "#  3) Health report + optional clustering that auto-drops NaN endpoints\n",
        "#\n",
        "# Notes:\n",
        "#  - This does NOT require attention_mask for causality; it uses it only for padding hygiene.\n",
        "#  - PCA is fit on CPU from finite points only (stable).\n",
        "# ================================================================\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
        "\n",
        "# ---------------------------\n",
        "# (Optional) progress bar\n",
        "# ---------------------------\n",
        "try:\n",
        "    from tqdm.auto import tqdm\n",
        "except Exception:\n",
        "    def tqdm(x, **kwargs): return x\n",
        "\n",
        "# ---------------------------\n",
        "# SETTINGS\n",
        "# ---------------------------\n",
        "LAYER = 10                 # router layer (infos[LAYER][\"read_logits\"])\n",
        "HEAD_AGG = \"mean\"          # \"mean\" or \"none\" (none flattens heads into batch; heavier)\n",
        "MAX_PROMPT_LEN = cfg.max_seq_len\n",
        "\n",
        "# Plotting\n",
        "PLOT_STRIDE = 1\n",
        "SHOW_START_MARKERS = True\n",
        "SHOW_END_MARKERS = True\n",
        "LABEL_ENDS = True\n",
        "LABEL_MAX = 12\n",
        "\n",
        "# Sphere aesthetics\n",
        "SPHERE_ALPHA = 0.06\n",
        "SPHERE_WIREFRAME = True\n",
        "\n",
        "# Endpoint coloring\n",
        "ENDPOINT_COLOR_MODE = \"paris_margin\"     # \"paris_logp\" or \"paris_margin\"\n",
        "RIVAL_TEXT = \" the\"                      # single-token rival for margin\n",
        "TARGET_TEXT = \" Paris\"                   # single-token target (GPT2-style leading space)\n",
        "\n",
        "# PCA / sanitation\n",
        "SKIP_T0_IN_PCA = True                    # t=0 is often pathological when padding or NaNs occur\n",
        "SANITIZE_MODE = \"drop\"                   # \"drop\" (recommended) or \"nan_to_num\"\n",
        "MIN_PCA_POINTS = 64\n",
        "EPS = 1e-9\n",
        "\n",
        "# Clustering\n",
        "DO_ENDPOINT_CLUSTERING = True\n",
        "N_CLUSTERS = 3\n",
        "\n",
        "# Prompts\n",
        "PROMPTS = [\n",
        "    # direct capital route\n",
        "    \"The capital of France is\",\n",
        "    \"France's capital city is\",\n",
        "    \"In France, the capital is\",\n",
        "    \"The seat of government of France is\",\n",
        "    \"The city that is France's capital is\",\n",
        "\n",
        "    # landmark route\n",
        "    \"The Eiffel Tower is located in\",\n",
        "    \"The Louvre is in\",\n",
        "    \"The Seine river flows through\",\n",
        "\n",
        "    # history route\n",
        "    \"During the French Revolution, the city of\",\n",
        "    \"Napoleon crowned himself in\",\n",
        "    \"In World War II, the liberation of\",\n",
        "\n",
        "    # geography route\n",
        "    \"The largest city in France is\",\n",
        "    \"A major European city in France is\",\n",
        "    \"A famous city on the Seine is\",\n",
        "\n",
        "    # distractors / contrast\n",
        "    \"The capital of Germany is\",\n",
        "    \"The capital of England is\",\n",
        "    \"The capital of Italy is\",\n",
        "]\n",
        "\n",
        "# ---------------------------\n",
        "# Tokenizer helpers (pad_token fix included)\n",
        "# ---------------------------\n",
        "def _get_tokenizer_from_context():\n",
        "    if \"tokeizer\" in globals():\n",
        "        tok = globals()[\"tokenizer\"]\n",
        "    else:\n",
        "        from transformers import AutoTokenizer\n",
        "        tok = AutoTokenizer.from_pretrained(getattr(cfg, \"tokenizer_name\", \"gpt2\"))\n",
        "\n",
        "    tok.padding_side = \"right\"\n",
        "    if tok.pad_token is None:\n",
        "        tok.pad_token = tok.eos_token\n",
        "    return tok\n",
        "\n",
        "tokenizer = _get_tokenizer_from_context()\n",
        "\n",
        "def ensure_single_token(text: str) -> int:\n",
        "    ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "    if len(ids) != 1:\n",
        "        raise ValueError(f\"{text!r} must be exactly 1 token under tokenizer. Got ids={ids}\")\n",
        "    return ids[0]\n",
        "\n",
        "TARGET_ID = ensure_single_token(TARGET_TEXT)\n",
        "RIVAL_ID  = ensure_single_token(RIVAL_TEXT)\n",
        "\n",
        "# ---------------------------\n",
        "# Forward helper\n",
        "# ---------------------------\n",
        "@torch.no_grad()\n",
        "def forward_with_info(ids, mask=None, routing_mode=\"softmax\"):\n",
        "    # If you suspect attention_mask causes issues, you can pass mask=None.\n",
        "    # We keep it for padding hygiene, but it should not be required for causality.\n",
        "    if mask is None:\n",
        "        logits, infos = model(ids, return_info=True, routing_mode=routing_mode)\n",
        "    else:\n",
        "        logits, infos = model(ids, attention_mask=mask, return_info=True, routing_mode=routing_mode)\n",
        "    return logits, infos\n",
        "\n",
        "# ---------------------------\n",
        "# Tokenize\n",
        "# ---------------------------\n",
        "enc = tokenizer(\n",
        "    PROMPTS,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=MAX_PROMPT_LEN,\n",
        ")\n",
        "input_ids = enc[\"input_ids\"].to(DEVICE)\n",
        "attention_mask = enc.get(\"attention_mask\", torch.ones_like(input_ids)).to(DEVICE)\n",
        "\n",
        "B_prompts, T = input_ids.shape\n",
        "lengths = attention_mask.sum(dim=1).long()\n",
        "t_last = (lengths - 1).clamp(min=0)  # last valid index per prompt\n",
        "\n",
        "# ---------------------------\n",
        "# Run model\n",
        "# ---------------------------\n",
        "logits, infos = forward_with_info(input_ids, attention_mask, routing_mode=\"softmax\")\n",
        "info = infos[LAYER]\n",
        "read_logits = info.get(\"read_logits\", None)\n",
        "if read_logits is None:\n",
        "    raise KeyError(f\"infos[{LAYER}] missing 'read_logits'. Keys: {list(info.keys())}\")\n",
        "\n",
        "# read_logits: [B,H,T,K]\n",
        "B0, H0, T0, K = read_logits.shape\n",
        "assert B0 == B_prompts and T0 == T, (read_logits.shape, (B_prompts, T))\n",
        "\n",
        "# ---------------------------\n",
        "# Health checks (logits + read_logits)\n",
        "# ---------------------------\n",
        "def _health(name, x, prompt_list=None):\n",
        "    x_det = x.detach()\n",
        "    fin = torch.isfinite(x_det)\n",
        "    n_fin = int(fin.sum().item())\n",
        "    n_tot = x_det.numel()\n",
        "    print(f\"[health] {name}: finite {n_fin}/{n_tot} ({100*n_fin/max(1,n_tot):.2f}%)\", end=\"\")\n",
        "    if n_fin > 0:\n",
        "        x_ok = x_det[fin]\n",
        "        print(f\" | min={x_ok.min().item():.4g} max={x_ok.max().item():.4g} mean={x_ok.mean().item():.4g}\")\n",
        "    else:\n",
        "        print(\" | ALL NON-FINITE\")\n",
        "    if n_fin < n_tot and prompt_list is not None:\n",
        "        # print first few bad indices in a friendly way\n",
        "        bad = (~fin).nonzero(as_tuple=False)\n",
        "        show = bad[:6]\n",
        "        for idx in show:\n",
        "            # best-effort mapping for common shapes\n",
        "            if x_det.ndim == 3:   # [B,T,V]\n",
        "                b, t, v = idx.tolist()\n",
        "                print(f\"   bad@ b={b} t={t} vocab={v} | prompt={prompt_list[b]!r}\")\n",
        "            elif x_det.ndim == 4: # [B,H,T,K]\n",
        "                b, h, t, k = idx.tolist()\n",
        "                print(f\"   bad@ b={b} h={h} t={t} k={k} | prompt={prompt_list[b]!r}\")\n",
        "\n",
        "_health(\"logits\", logits, PROMPTS)\n",
        "_health(\"read_logits(layer)\", read_logits, PROMPTS)\n",
        "\n",
        "# ---------------------------\n",
        "# Head aggregation\n",
        "# ---------------------------\n",
        "if HEAD_AGG == \"mean\":\n",
        "    Z = read_logits.mean(dim=1).float()   # [B,T,K]\n",
        "    B = B_prompts\n",
        "    t_last_plot = t_last\n",
        "    mask_plot = attention_mask\n",
        "elif HEAD_AGG == \"none\":\n",
        "    # Flatten heads into batch dimension\n",
        "    Z = read_logits.reshape(B_prompts * H0, T, K).float()\n",
        "    B = B_prompts * H0\n",
        "    t_last_plot = t_last.repeat_interleave(H0, dim=0)\n",
        "    mask_plot = attention_mask.repeat_interleave(H0, dim=0)\n",
        "else:\n",
        "    raise ValueError(\"HEAD_AGG must be 'mean' or 'none'\")\n",
        "\n",
        "# ---------------------------\n",
        "# PCA fit + projection (CPU end-to-end, finite-only)\n",
        "# ---------------------------\n",
        "Z_cpu = Z.detach().cpu()                          # [B,T,K] CPU\n",
        "Z_flat_cpu = Z_cpu.reshape(B*T, K)                # [BT,K] CPU\n",
        "\n",
        "finite_mask = torch.isfinite(Z_flat_cpu).all(dim=-1)\n",
        "if SKIP_T0_IN_PCA:\n",
        "    tt = torch.arange(T).repeat(B)                # [BT]\n",
        "    finite_mask = finite_mask & (tt != 0)\n",
        "\n",
        "n_total = Z_flat_cpu.shape[0]\n",
        "n_good = int(finite_mask.sum().item())\n",
        "print(f\"[health] PCA fit points: good={n_good}/{n_total} ({100*n_good/max(1,n_total):.2f}%) \"\n",
        "      f\"| SKIP_T0_IN_PCA={SKIP_T0_IN_PCA} | SANITIZE_MODE={SANITIZE_MODE}\")\n",
        "\n",
        "if n_good < MIN_PCA_POINTS:\n",
        "    raise RuntimeError(\n",
        "        f\"Too few finite points to fit PCA (good={n_good}). \"\n",
        "        \"Fix numerical stability first or lower MIN_PCA_POINTS.\"\n",
        "    )\n",
        "\n",
        "X_good = Z_flat_cpu[finite_mask]                  # [Ng,K] CPU\n",
        "mu = X_good.mean(dim=0, keepdim=True)             # [1,K] CPU\n",
        "Xc = X_good - mu                                  # [Ng,K] CPU\n",
        "\n",
        "# SVD on CPU (stable). If this still fails, fallback to PCA-lowrank.\n",
        "try:\n",
        "    U, S, Vh = torch.linalg.svd(Xc, full_matrices=False)\n",
        "    W2 = Vh[:2].T.contiguous()                    # [K,2]\n",
        "    W3 = Vh[:3].T.contiguous()                    # [K,3]\n",
        "    evals = (S[:3]**2).numpy()\n",
        "    evr = evals / (S**2).sum().numpy()\n",
        "except Exception as e:\n",
        "    print(\"[warn] CPU SVD failed; falling back to torch.pca_lowrank:\", repr(e))\n",
        "    # q must be >= 3\n",
        "    q = 8\n",
        "    U2, S2, V2 = torch.pca_lowrank(Xc, q=q, center=False)\n",
        "    W3 = V2[:, :3].contiguous()\n",
        "    W2 = V2[:, :2].contiguous()\n",
        "    # approximate EVR from singular values\n",
        "    evals = (S2[:3]**2).numpy()\n",
        "    evr = evals / (S2**2).sum().numpy()\n",
        "\n",
        "print(f\"[PCA] EVR(PC1..3)  {np.round(evr, 4)}\")\n",
        "\n",
        "# Project ALL points (CPU)\n",
        "Z2 = (Z_cpu - mu) @ W2                            # [B,T,2]\n",
        "Z3 = (Z_cpu - mu) @ W3                            # [B,T,3]\n",
        "\n",
        "Z2_np = Z2.numpy()\n",
        "Z3_np = Z3.numpy()\n",
        "\n",
        "# Sanitize projections if needed\n",
        "if SANITIZE_MODE == \"nan_to_num\":\n",
        "    Z2_np = np.nan_to_num(Z2_np, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    Z3_np = np.nan_to_num(Z3_np, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "# Sphere projection\n",
        "norm = np.linalg.norm(Z3_np, axis=-1, keepdims=True)\n",
        "Z3_sphere = Z3_np / np.clip(norm, EPS, None)\n",
        "\n",
        "# ---------------------------\n",
        "# Endpoint Paris score (compute on original prompts only)\n",
        "# ---------------------------\n",
        "logp = F.log_softmax(logits, dim=-1)              # [B_prompts,T,V] on DEVICE\n",
        "idx = torch.arange(B_prompts, device=DEVICE)\n",
        "\n",
        "lp_paris = logp[idx, t_last, TARGET_ID].detach().cpu().numpy()\n",
        "lp_rival = logp[idx, t_last, RIVAL_ID].detach().cpu().numpy()\n",
        "\n",
        "if ENDPOINT_COLOR_MODE == \"paris_logp\":\n",
        "    end_score = lp_paris\n",
        "    end_label = f\"logP({TARGET_TEXT!r})\"\n",
        "elif ENDPOINT_COLOR_MODE == \"paris_margin\":\n",
        "    end_score = (lp_paris - lp_rival)\n",
        "    end_label = f\"logP({TARGET_TEXT!r}) - logP({RIVAL_TEXT!r})\"\n",
        "else:\n",
        "    raise ValueError(\"ENDPOINT_COLOR_MODE must be 'paris_logp' or 'paris_margin'\")\n",
        "\n",
        "# If HEAD_AGG==\"none\", replicate scores per head\n",
        "if HEAD_AGG == \"none\":\n",
        "    end_score_plot = np.repeat(end_score, H0)\n",
        "else:\n",
        "    end_score_plot = end_score\n",
        "\n",
        "# Replace non-finite scores with NaN for plotting/clustering decisions\n",
        "end_score_plot = np.array(end_score_plot, dtype=np.float64)\n",
        "end_score_plot[~np.isfinite(end_score_plot)] = np.nan\n",
        "\n",
        "print(f\"[Paris-score] {end_label}: mean={np.nanmean(end_score_plot):.4f} \"\n",
        "      f\"med={np.nanmedian(end_score_plot):.4f} \"\n",
        "      f\"min={np.nanmin(end_score_plot):.4f} max={np.nanmax(end_score_plot):.4f}\")\n",
        "\n",
        "# ---------------------------\n",
        "# Compute endpoints on sphere + 2D (for clustering & endpoint plot)\n",
        "# ---------------------------\n",
        "ends_sphere = []\n",
        "ends2 = []\n",
        "for b in range(B):\n",
        "    te = int(t_last_plot[b].item())\n",
        "    ends_sphere.append(Z3_sphere[b, te, :])\n",
        "    ends2.append(Z2_np[b, te, :])\n",
        "ends_sphere = np.stack(ends_sphere, axis=0)       # [B,3]\n",
        "ends2 = np.stack(ends2, axis=0)                   # [B,2]\n",
        "\n",
        "# Drop non-finite endpoints for clustering\n",
        "valid_end = np.isfinite(ends_sphere).all(axis=1) & np.isfinite(end_score_plot)\n",
        "n_valid_end = int(valid_end.sum())\n",
        "if DO_ENDPOINT_CLUSTERING:\n",
        "    if n_valid_end < max(N_CLUSTERS, 3):\n",
        "        print(f\"[warn] clustering disabled (valid endpoints={n_valid_end} < clusters={N_CLUSTERS}).\")\n",
        "        DO_ENDPOINT_CLUSTERING = False\n",
        "\n",
        "cluster_id = None\n",
        "cluster_centers = None\n",
        "if DO_ENDPOINT_CLUSTERING:\n",
        "    try:\n",
        "        from sklearn.cluster import KMeans\n",
        "        Xc_end = ends_sphere[valid_end]\n",
        "        km = KMeans(n_clusters=N_CLUSTERS, n_init=10, random_state=0).fit(Xc_end)\n",
        "        cluster_centers = km.cluster_centers_\n",
        "        # assign labels back into full array with -1 for invalid\n",
        "        cluster_id = -np.ones((B,), dtype=np.int64)\n",
        "        cluster_id[valid_end] = km.labels_\n",
        "    except Exception as e:\n",
        "        print(\"[warn] clustering unavailable or failed:\", repr(e))\n",
        "        DO_ENDPOINT_CLUSTERING = False\n",
        "\n",
        "# ---------------------------\n",
        "# Plot 1: Sphere + trajectories\n",
        "# ---------------------------\n",
        "fig = plt.figure(figsize=(11.0, 8.7))\n",
        "ax = fig.add_subplot(111, projection=\"3d\")\n",
        "\n",
        "# Sphere wireframe\n",
        "u = np.linspace(0, 2 * np.pi, 48)\n",
        "v = np.linspace(0, np.pi, 24)\n",
        "xs = np.outer(np.cos(u), np.sin(v))\n",
        "ys = np.outer(np.sin(u), np.sin(v))\n",
        "zs = np.outer(np.ones_like(u), np.cos(v))\n",
        "if SPHERE_WIREFRAME:\n",
        "    ax.plot_wireframe(xs, ys, zs, rstride=3, cstride=3, linewidth=0.5, alpha=SPHERE_ALPHA)\n",
        "else:\n",
        "    ax.plot_surface(xs, ys, zs, alpha=SPHERE_ALPHA, linewidth=0)\n",
        "\n",
        "# Plot trajectories (skip any non-finite segments if SANITIZE_MODE==\"drop\")\n",
        "for b in range(B):\n",
        "    te = int(t_last_plot[b].item())\n",
        "    pts = Z3_sphere[b, :te+1:PLOT_STRIDE, :]     # [L,3]\n",
        "\n",
        "    if SANITIZE_MODE == \"drop\":\n",
        "        ok = np.isfinite(pts).all(axis=1)\n",
        "        pts = pts[ok]\n",
        "    if pts.shape[0] < 2:\n",
        "        continue\n",
        "\n",
        "    ax.plot(pts[:,0], pts[:,1], pts[:,2], linewidth=2.0, alpha=0.85)\n",
        "\n",
        "    if SHOW_START_MARKERS:\n",
        "        ax.scatter([pts[0,0]], [pts[0,1]], [pts[0,2]], s=40, marker=\"o\", alpha=0.9)\n",
        "\n",
        "    if SHOW_END_MARKERS:\n",
        "        cval = end_score_plot[b]\n",
        "        # if NaN score, plot in gray\n",
        "        if np.isfinite(cval):\n",
        "            ax.scatter([pts[-1,0]], [pts[-1,1]], [pts[-1,2]], c=[cval], s=70, marker=\"o\", alpha=0.95)\n",
        "        else:\n",
        "            ax.scatter([pts[-1,0]], [pts[-1,1]], [pts[-1,2]], c=\"gray\", s=70, marker=\"o\", alpha=0.8)\n",
        "\n",
        "# Colorbar for endpoint scores\n",
        "import matplotlib as mpl\n",
        "finite_scores = end_score_plot[np.isfinite(end_score_plot)]\n",
        "if finite_scores.size > 0:\n",
        "    normc = mpl.colors.Normalize(vmin=float(finite_scores.min()), vmax=float(finite_scores.max()))\n",
        "else:\n",
        "    normc = mpl.colors.Normalize(vmin=-1.0, vmax=1.0)\n",
        "sm = mpl.cm.ScalarMappable(cmap=plt.cm.viridis, norm=normc)\n",
        "sm.set_array([])\n",
        "cbar = plt.colorbar(sm, ax=ax, fraction=0.03, pad=0.08)\n",
        "cbar.set_label(end_label)\n",
        "\n",
        "# Label endpoints (best score first) only if head-agg is prompt-level\n",
        "if LABEL_ENDS and HEAD_AGG == \"mean\":\n",
        "    order = np.argsort(-end_score)  # descending; end_score is per prompt\n",
        "    for i in order[: min(LABEL_MAX, B_prompts)]:\n",
        "        te = int(t_last[i].item())\n",
        "        p = Z3_sphere[i, te, :]\n",
        "        snippet = PROMPTS[i][:45].replace(\"\\n\",\" \")\n",
        "        ax.text(p[0], p[1], p[2], f\" {i}:{snippet}\", fontsize=8)\n",
        "\n",
        "# Cluster centers\n",
        "if DO_ENDPOINT_CLUSTERING and cluster_centers is not None:\n",
        "    cc = cluster_centers\n",
        "    ax.scatter(cc[:,0], cc[:,1], cc[:,2], s=180, marker=\"X\", alpha=0.95)\n",
        "\n",
        "ax.set_title(\n",
        "    f\"Paris Basin on a Simplex Sphere (read_logits PCAS)\\n\"\n",
        "    f\"layer={LAYER} | head_agg={HEAD_AGG} | endpoints colored by {end_label}\\n\"\n",
        "    f\"PCA EVR (PC1..3)  {np.round(evr, 4)}\"\n",
        ")\n",
        "ax.set_xlabel(\"PC1 (sphere)\")\n",
        "ax.set_ylabel(\"PC2 (sphere)\")\n",
        "ax.set_zlabel(\"PC3 (sphere)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---------------------------\n",
        "# Plot 2: Endpoints in PCA(PC1,PC2) space\n",
        "# ---------------------------\n",
        "plt.figure(figsize=(7.8, 6.4))\n",
        "c = end_score_plot\n",
        "plt.scatter(ends2[:,0], ends2[:,1], c=c, s=70)\n",
        "plt.colorbar(label=end_label)\n",
        "if HEAD_AGG == \"mean\":\n",
        "    for i in range(min(B_prompts, LABEL_MAX)):\n",
        "        plt.text(ends2[i,0], ends2[i,1], f\" {i}\", fontsize=9)\n",
        "plt.title(f\"Endpoints in PCA(PC1,PC2) space (layer={LAYER})\")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---------------------------\n",
        "# Cluster report\n",
        "# ---------------------------\n",
        "print(f\"[OK] prompts={B_prompts} | seq_len(T)={T} | slots(K)={K} | layer={LAYER} | head_agg={HEAD_AGG}\")\n",
        "if DO_ENDPOINT_CLUSTERING and cluster_id is not None and HEAD_AGG == \"mean\":\n",
        "    for c_id in range(N_CLUSTERS):\n",
        "        idxs = np.where(cluster_id[:B_prompts] == c_id)[0].tolist()\n",
        "        idxs_sorted = sorted(idxs, key=lambda i: float(end_score[i]), reverse=True)\n",
        "        top3 = [(i, PROMPTS[i][:35]) for i in idxs_sorted[:3]]\n",
        "        print(f\"[cluster {c_id}] n={len(idxs)} | top prompts:\", top3)\n",
        "else:\n",
        "    if HEAD_AGG == \"mean\":\n",
        "        print(\"[cluster] disabled or unavailable (either NaNs, sklearn missing, or insufficient points).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2m5y662x58xa"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ================================================================\n",
        "#@title Next Cell: Late-Phase Trajectory Bundle + Commitment + Paris Attractor\n",
        "# Uses right-padding to avoid NaNs. Highlights the phenomenon:\n",
        "#   - show only last W tokens (late-stage convergence)\n",
        "#   - color endpoints by Paris margin\n",
        "#   - show routing entropy vs time (gradual commitment)\n",
        "#   - compute and plot a Paris-attractor centroid (weighted by margin)\n",
        "#\n",
        "# Assumptions:\n",
        "#   - model, cfg, DEVICE exist\n",
        "#   - model(..., return_info=True) -> (logits, infos)\n",
        "#   - infos[layer][\"read_logits\"] is [B,H,T,K] float32\n",
        "# ================================================================\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
        "\n",
        "# ---------------------------\n",
        "# SETTINGS\n",
        "# ---------------------------\n",
        "LAYER = 7\n",
        "HEAD_AGG = \"mean\"          # \"mean\" only here (keeps plots readable)\n",
        "MAX_LEN = cfg.max_seq_len\n",
        "\n",
        "# Focus window: last W tokens (where alignment should appear)\n",
        "W_LATE = 5                 # try 4-8\n",
        "PLOT_STRIDE = 1\n",
        "\n",
        "# Paris scoring\n",
        "TARGET_TEXT = \" Paris\"\n",
        "RIVAL_TEXT  = \" the\"        # margin vs function-word continuation\n",
        "\n",
        "# Commitment / entropy\n",
        "ENTROPY_TEMP = 1.0\n",
        "\n",
        "# Visual declutter\n",
        "LABEL_MAX = 10              # label only top Paris-margin prompts\n",
        "SPHERE_ALPHA = 0.06\n",
        "SPHERE_WIREFRAME = True\n",
        "\n",
        "# Prompts (use your current 17 set)\n",
        "PROMPTS = [\n",
        "    \"The capital of France is\",\n",
        "    \"France's capital city is\",\n",
        "    \"In France, the capital is\",\n",
        "    \"The seat of government of France is\",\n",
        "    \"The city that is France's capital is\",\n",
        "    \"The Eiffel Tower is located in\",\n",
        "    \"The Louvre is in\",\n",
        "    \"The Seine river flows through\",\n",
        "    \"During the French Revolution, the city of\",\n",
        "    \"Napoleon crowned himself in\",\n",
        "    \"In World War II, the liberation of\",\n",
        "    \"The largest city in France is\",\n",
        "    \"A major European city in France is\",\n",
        "    \"A famous city on the Seine is\",\n",
        "    \"The capital of Germany is\",\n",
        "    \"The capital of England is\",\n",
        "    \"The capital of Italy is\",\n",
        "]\n",
        "\n",
        "# ---------------------------\n",
        "# Tokenizer (robust) + RIGHT PADDING\n",
        "# ---------------------------\n",
        "def _get_tokenizer():\n",
        "    if \"tokenizer\" in globals():\n",
        "        tok = globals()[\"tokenizer\"]\n",
        "    else:\n",
        "        from transformers import AutoTokenizer\n",
        "        tok = AutoTokenizer.from_pretrained(getattr(cfg, \"tokenizer_name\", \"gpt2\"))\n",
        "\n",
        "    tok.padding_side = \"right\"                     # IMPORTANT: right padding\n",
        "    if tok.pad_token is None:\n",
        "        tok.pad_token = tok.eos_token\n",
        "    return tok\n",
        "\n",
        "tokenizer = _get_tokenizer()\n",
        "\n",
        "def ensure_single_token(text: str) -> int:\n",
        "    ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "    if len(ids) != 1:\n",
        "        raise ValueError(f\"{text!r} must be exactly 1 token. Got ids={ids}\")\n",
        "    return ids[0]\n",
        "\n",
        "TARGET_ID = ensure_single_token(TARGET_TEXT)\n",
        "RIVAL_ID  = ensure_single_token(RIVAL_TEXT)\n",
        "\n",
        "# ---------------------------\n",
        "# Forward\n",
        "# ---------------------------\n",
        "@torch.no_grad()\n",
        "def forward_with_info(ids, mask):\n",
        "    logits, infos = model(ids, attention_mask=mask, return_info=True, routing_mode=\"softmax\")\n",
        "    return logits, infos\n",
        "\n",
        "enc = tokenizer(\n",
        "    PROMPTS,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=MAX_LEN,\n",
        ")\n",
        "input_ids = enc[\"input_ids\"].to(DEVICE)\n",
        "attention_mask = enc.get(\"attention_mask\", torch.ones_like(input_ids)).to(DEVICE)\n",
        "\n",
        "B, T = input_ids.shape\n",
        "lengths = attention_mask.sum(dim=1).long()\n",
        "t_last = (lengths - 1).clamp(min=0)\n",
        "\n",
        "logits, infos = forward_with_info(input_ids, attention_mask)\n",
        "info = infos[LAYER]\n",
        "read_logits = info.get(\"read_logits\", None)\n",
        "if read_logits is None:\n",
        "    raise KeyError(f\"infos[{LAYER}] missing 'read_logits'. Keys={list(info.keys())}\")\n",
        "\n",
        "# read_logits: [B,H,T,K]\n",
        "B0, H0, T0, K = read_logits.shape\n",
        "assert (B0, T0) == (B, T), (read_logits.shape, (B, T))\n",
        "\n",
        "# Aggregate heads\n",
        "Z = read_logits.mean(dim=1).float()  # [B,T,K]\n",
        "\n",
        "# ---------------------------\n",
        "# Health checks (finite)\n",
        "# ---------------------------\n",
        "def _health(name, x):\n",
        "    x = x.detach()\n",
        "    finite = torch.isfinite(x)\n",
        "    frac = finite.float().mean().item()\n",
        "    if frac < 1.0:\n",
        "        idx = torch.nonzero(~finite, as_tuple=False)[0].tolist()\n",
        "        mn = torch.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0).min().item()\n",
        "        mx = torch.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0).max().item()\n",
        "        print(f\"[health] {name}: finite={frac*100:.2f}% | first_bad_idx={idx} | min~={mn:.3f} max~={mx:.3f}\")\n",
        "    else:\n",
        "        print(f\"[health] {name}: finite=100% | min={x.min().item():.3f} max={x.max().item():.3f} mean={x.mean().item():.3f}\")\n",
        "\n",
        "_health(\"logits\", logits)\n",
        "_health(\"read_logits(layer)\", Z)\n",
        "\n",
        "# ---------------------------\n",
        "# Paris margin score at last valid\n",
        "# ---------------------------\n",
        "logp = F.log_softmax(logits, dim=-1)  # [B,T,V]\n",
        "idx = torch.arange(B, device=DEVICE)\n",
        "lp_paris = logp[idx, t_last, TARGET_ID]\n",
        "lp_rival = logp[idx, t_last, RIVAL_ID]\n",
        "margin = (lp_paris - lp_rival).detach().cpu().numpy()\n",
        "\n",
        "print(f\"[Paris-score] mean={margin.mean():.4f} med={np.median(margin):.4f} min={margin.min():.4f} max={margin.max():.4f}\")\n",
        "\n",
        "# ---------------------------\n",
        "# Routing entropy vs time (commitment curve)\n",
        "# ---------------------------\n",
        "w = torch.softmax(Z / max(1e-6, float(ENTROPY_TEMP)), dim=-1)  # [B,T,K]\n",
        "entropy = -(w * (w.clamp_min(1e-9).log())).sum(dim=-1)         # [B,T]\n",
        "entropy_np = entropy.detach().cpu().numpy()\n",
        "\n",
        "# ---------------------------\n",
        "# PCA (robust): fit on LAST-W window only (makes convergence structure pop)\n",
        "# We fit PCA in torch on GPU then move results to CPU for plotting.\n",
        "# ---------------------------\n",
        "# Build late-window indices per sequence\n",
        "late_mask = torch.zeros((B, T), device=DEVICE, dtype=torch.bool)\n",
        "for b in range(B):\n",
        "    t_end = int(t_last[b].item())\n",
        "    t0 = max(0, t_end - (W_LATE - 1))\n",
        "    late_mask[b, t0:t_end+1] = True\n",
        "\n",
        "# collect points [N,K] from late window\n",
        "pts = Z[late_mask]  # [N,K]\n",
        "# center\n",
        "mu = pts.mean(dim=0, keepdim=True)\n",
        "X = pts - mu\n",
        "\n",
        "# PCA via covariance eig (more stable than SVD when nearly rank-deficient)\n",
        "# C = X^T X / (N-1)\n",
        "N = X.shape[0]\n",
        "C = (X.T @ X) / max(1, (N - 1))\n",
        "# eigendecomp (symmetric)\n",
        "evals, evecs = torch.linalg.eigh(C)  # ascending\n",
        "# take top-3\n",
        "W3 = evecs[:, -3:]                   # [K,3]\n",
        "# explained variance ratio\n",
        "evr = (evals[-3:] / evals.clamp_min(1e-12).sum()).detach().cpu().numpy()[::-1]\n",
        "print(f\"[PCA] late-window points={N} | EVR(PC1..3) {np.round(evr,4)}\")\n",
        "\n",
        "# Project ALL timepoints into 3D using the same basis, then normalize to sphere\n",
        "XYZ = (Z - mu.view(1,1,K)) @ W3       # [B,T,3]\n",
        "XYZ_np = XYZ.detach().cpu().numpy()\n",
        "norm = np.linalg.norm(XYZ_np, axis=-1, keepdims=True)\n",
        "XYZ_s = XYZ_np / np.clip(norm, 1e-9, None)\n",
        "\n",
        "# Endpoints (sphere + raw PC1/PC2)\n",
        "end_s = np.stack([XYZ_s[b, int(t_last[b].item())] for b in range(B)], axis=0)   # [B,3]\n",
        "end_2 = np.stack([XYZ_np[b, int(t_last[b].item()), :2] for b in range(B)], axis=0)\n",
        "\n",
        "# Paris-attractor centroid in endpoint space (weighted by margin)\n",
        "# weights: sigmoid(margin) so negative margins still contribute slightly\n",
        "w_cent = 1.0 / (1.0 + np.exp(-margin))\n",
        "w_cent = w_cent / (w_cent.sum() + 1e-9)\n",
        "cent_s = (end_s * w_cent[:,None]).sum(axis=0)\n",
        "cent_s = cent_s / (np.linalg.norm(cent_s) + 1e-9)\n",
        "cent_2 = (end_2 * w_cent[:,None]).sum(axis=0)\n",
        "\n",
        "# ---------------------------\n",
        "# Plot 1: Late-phase bundle on sphere\n",
        "# ---------------------------\n",
        "fig = plt.figure(figsize=(10.5, 8.5))\n",
        "ax = fig.add_subplot(111, projection=\"3d\")\n",
        "\n",
        "# faint sphere\n",
        "u = np.linspace(0, 2*np.pi, 48)\n",
        "v = np.linspace(0, np.pi, 24)\n",
        "xs = np.outer(np.cos(u), np.sin(v))\n",
        "ys = np.outer(np.sin(u), np.sin(v))\n",
        "zs = np.outer(np.ones_like(u), np.cos(v))\n",
        "if SPHERE_WIREFRAME:\n",
        "    ax.plot_wireframe(xs, ys, zs, rstride=3, cstride=3, linewidth=0.5, alpha=SPHERE_ALPHA)\n",
        "else:\n",
        "    ax.plot_surface(xs, ys, zs, alpha=SPHERE_ALPHA, linewidth=0)\n",
        "\n",
        "# trajectories: plot only late window for each prompt\n",
        "for b in range(B):\n",
        "    t_end = int(t_last[b].item())\n",
        "    t0 = max(0, t_end - (W_LATE - 1))\n",
        "    pts_b = XYZ_s[b, t0:t_end+1:PLOT_STRIDE, :]\n",
        "    if pts_b.shape[0] >= 2:\n",
        "        ax.plot(pts_b[:,0], pts_b[:,1], pts_b[:,2], linewidth=2.2, alpha=0.9)\n",
        "\n",
        "    # endpoint marker colored by margin\n",
        "    ax.scatter([end_s[b,0]],[end_s[b,1]],[end_s[b,2]], c=[margin[b]], s=70, alpha=0.95)\n",
        "\n",
        "# attractor centroid\n",
        "ax.scatter([cent_s[0]],[cent_s[1]],[cent_s[2]], s=220, marker=\"X\", alpha=0.95)\n",
        "\n",
        "# colorbar (margin)\n",
        "import matplotlib as mpl\n",
        "normc = mpl.colors.Normalize(vmin=float(np.min(margin)), vmax=float(np.max(margin)))\n",
        "sm = mpl.cm.ScalarMappable(cmap=plt.cm.viridis, norm=normc)\n",
        "sm.set_array([])\n",
        "cbar = plt.colorbar(sm, ax=ax, fraction=0.03, pad=0.08)\n",
        "cbar.set_label(f\"logP({TARGET_TEXT!r}) - logP({RIVAL_TEXT!r})\")\n",
        "\n",
        "# label top prompts by margin\n",
        "order = np.argsort(-margin)\n",
        "for i in order[:min(LABEL_MAX, B)]:\n",
        "    p = end_s[i]\n",
        "    ax.text(p[0], p[1], p[2], f\" {i}\", fontsize=9)\n",
        "\n",
        "ax.set_title(\n",
        "    f\"Late-Phase Bundle on Simplex Sphere (read_logits PCAS)\\n\"\n",
        "    f\"layer={LAYER} | window=last {W_LATE} tokens | EVR{np.round(evr,4)} | X = Paris-attractor centroid\"\n",
        ")\n",
        "ax.set_xlabel(\"PC1 (sphere)\")\n",
        "ax.set_ylabel(\"PC2 (sphere)\")\n",
        "ax.set_zlabel(\"PC3 (sphere)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---------------------------\n",
        "# Plot 2: 2D endpoints + centroid\n",
        "# ---------------------------\n",
        "plt.figure(figsize=(7.8, 6.3))\n",
        "plt.scatter(end_2[:,0], end_2[:,1], c=margin, s=70)\n",
        "plt.scatter([cent_2[0]],[cent_2[1]], s=220, marker=\"X\")\n",
        "plt.colorbar(label=f\"logP({TARGET_TEXT!r}) - logP({RIVAL_TEXT!r})\")\n",
        "for i in range(B):\n",
        "    plt.text(end_2[i,0], end_2[i,1], f\" {i}\", fontsize=9)\n",
        "plt.title(f\"Endpoints in PCA(PC1,PC2) space (layer={LAYER}) | X = Paris-attractor centroid\")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---------------------------\n",
        "# Plot 3: Commitment curves (entropy vs time)\n",
        "# ---------------------------\n",
        "plt.figure(figsize=(9.2, 5.6))\n",
        "for b in range(B):\n",
        "    t_end = int(t_last[b].item())\n",
        "    ts = np.arange(t_end+1)\n",
        "    plt.plot(ts, entropy_np[b, :t_end+1], alpha=0.55)\n",
        "plt.title(f\"Routing Entropy vs Time (commitment) | layer={LAYER} | lower = more committed\")\n",
        "plt.xlabel(\"token index\")\n",
        "plt.ylabel(\"entropy(softmax(read_logits))\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---------------------------\n",
        "# Print: rank prompts by margin\n",
        "# ---------------------------\n",
        "rank = np.argsort(-margin)\n",
        "print(\"\\n=== Prompts ranked by Paris-margin (highest first) ===\")\n",
        "for r, i in enumerate(rank):\n",
        "    print(f\"{r:2d}  idx={i:2d}  margin={margin[i]: .3f}   prompt={PROMPTS[i]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8pKgTF998T7K"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ================================================================\n",
        "#@title Layer Sweep: Where does the Paris-attractor geometry emerge?\n",
        "# Computes per-layer summary metrics using the same probe:\n",
        "#   - router read_logits (mean over heads)\n",
        "#   - late-window PCA fit (last W tokens per prompt)\n",
        "#   - Paris margin (logP(' Paris') - logP(' the')) at last valid\n",
        "#   - attractor sharpness: corr( -dist_to_centroid , margin )\n",
        "#   - EVR(PC1) of late-window PCA\n",
        "#   - mean routing entropy in late window\n",
        "#\n",
        "# Assumptions: model, cfg, DEVICE, tokenizer available (or cfg.tokenizer_name).\n",
        "# ================================================================\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------------------------\n",
        "# SETTINGS\n",
        "# ---------------------------\n",
        "W_LATE = 5\n",
        "ENTROPY_TEMP = 1.0\n",
        "TARGET_TEXT = \" Paris\"\n",
        "RIVAL_TEXT  = \" the\"\n",
        "\n",
        "# Auto sweep control\n",
        "USE_EVERY_N = 1          # set 2 or 3 for quicker coarse scan\n",
        "ALSO_INCLUDE_LAST = True # ensure last layer included even if stepping skips it\n",
        "\n",
        "# Prompts (same set you used)\n",
        "PROMPTS = [\n",
        "    \"The capital of France is\",\n",
        "    \"France's capital city is\",\n",
        "    \"In France, the capital is\",\n",
        "    \"The seat of government of France is\",\n",
        "    \"The city that is France's capital is\",\n",
        "    \"The Eiffel Tower is located in\",\n",
        "    \"The Louvre is in\",\n",
        "    \"The Seine river flows through\",\n",
        "    \"During the French Revolution, the city of\",\n",
        "    \"Napoleon crowned himself in\",\n",
        "    \"In World War II, the liberation of\",\n",
        "    \"The largest city in France is\",\n",
        "    \"A major European city in France is\",\n",
        "    \"A famous city on the Seine is\",\n",
        "    \"The capital of Germany is\",\n",
        "    \"The capital of England is\",\n",
        "    \"The capital of Italy is\",\n",
        "]\n",
        "\n",
        "# ---------------------------\n",
        "# Tokenizer (RIGHT padding)\n",
        "# ---------------------------\n",
        "def _get_tokenizer():\n",
        "    if \"tokenizer\" in globals():\n",
        "        tok = globals()[\"tokenizer\"]\n",
        "    else:\n",
        "        from transformers import AutoTokenizer\n",
        "        tok = AutoTokenizer.from_pretrained(getattr(cfg, \"tokenizer_name\", \"gpt2\"))\n",
        "    tok.padding_side = \"right\"\n",
        "    if tok.pad_token is None:\n",
        "        tok.pad_token = tok.eos_token\n",
        "    return tok\n",
        "\n",
        "tokenizer = _get_tokenizer()\n",
        "\n",
        "def ensure_single_token(text: str) -> int:\n",
        "    ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "    if len(ids) != 1:\n",
        "        raise ValueError(f\"{text!r} must be exactly 1 token. Got ids={ids}\")\n",
        "    return ids[0]\n",
        "\n",
        "TARGET_ID = ensure_single_token(TARGET_TEXT)\n",
        "RIVAL_ID  = ensure_single_token(RIVAL_TEXT)\n",
        "\n",
        "# ---------------------------\n",
        "# Prepare batch\n",
        "# ---------------------------\n",
        "enc = tokenizer(PROMPTS, return_tensors=\"pt\", padding=True, truncation=True, max_length=cfg.max_seq_len)\n",
        "input_ids = enc[\"input_ids\"].to(DEVICE)\n",
        "attention_mask = enc.get(\"attention_mask\", torch.ones_like(input_ids)).to(DEVICE)\n",
        "\n",
        "B, T = input_ids.shape\n",
        "lengths = attention_mask.sum(dim=1).long()\n",
        "t_last = (lengths - 1).clamp(min=0)\n",
        "\n",
        "# ---------------------------\n",
        "# Forward once to get logits + infos for all layers\n",
        "# ---------------------------\n",
        "@torch.no_grad()\n",
        "def forward_with_info(ids, mask):\n",
        "    logits, infos = model(ids, attention_mask=mask, return_info=True, routing_mode=\"softmax\")\n",
        "    return logits, infos\n",
        "\n",
        "logits, infos = forward_with_info(input_ids, attention_mask)\n",
        "\n",
        "n_layers = len(infos)\n",
        "print(f\"[info] detected layers = {n_layers}\")\n",
        "\n",
        "# Paris margin (fixed across layer; uses final logits only)\n",
        "logp = F.log_softmax(logits, dim=-1)\n",
        "idx = torch.arange(B, device=DEVICE)\n",
        "margin = (logp[idx, t_last, TARGET_ID] - logp[idx, t_last, RIVAL_ID]).detach().cpu().numpy()\n",
        "\n",
        "# Layer list\n",
        "layers = list(range(0, n_layers, max(1, int(USE_EVERY_N))))\n",
        "if ALSO_INCLUDE_LAST and (n_layers - 1) not in layers:\n",
        "    layers.append(n_layers - 1)\n",
        "layers = sorted(set(layers))\n",
        "\n",
        "# ---------------------------\n",
        "# Metrics per layer\n",
        "# ---------------------------\n",
        "rows = []\n",
        "\n",
        "def late_mask_from_tlast(t_last_tensor, T, W):\n",
        "    lm = torch.zeros((t_last_tensor.shape[0], T), device=t_last_tensor.device, dtype=torch.bool)\n",
        "    for b in range(t_last_tensor.shape[0]):\n",
        "        t_end = int(t_last_tensor[b].item())\n",
        "        t0 = max(0, t_end - (W - 1))\n",
        "        lm[b, t0:t_end+1] = True\n",
        "    return lm\n",
        "\n",
        "late_mask = late_mask_from_tlast(t_last, T, W_LATE)\n",
        "\n",
        "for L in layers:\n",
        "    rl = infos[L].get(\"read_logits\", None)\n",
        "    if rl is None:\n",
        "        print(f\"[warn] layer {L} missing read_logits, skipping\")\n",
        "        continue\n",
        "\n",
        "    # [B,H,T,K] -> [B,T,K]\n",
        "    Z = rl.mean(dim=1).float()\n",
        "\n",
        "    # entropy in late window\n",
        "    w = torch.softmax(Z / max(1e-6, float(ENTROPY_TEMP)), dim=-1)\n",
        "    ent = -(w * w.clamp_min(1e-9).log()).sum(dim=-1)  # [B,T]\n",
        "    ent_late = ent[late_mask].mean().item()\n",
        "\n",
        "    # late-window PCA in slot-logit space (cov eig; stable)\n",
        "    pts = Z[late_mask]            # [N,K]\n",
        "    mu = pts.mean(dim=0, keepdim=True)\n",
        "    X = pts - mu\n",
        "    N = X.shape[0]\n",
        "    C = (X.T @ X) / max(1, (N - 1))\n",
        "    evals, evecs = torch.linalg.eigh(C)\n",
        "    top = evals[-3:].clamp_min(1e-12)\n",
        "    evr3 = (top / evals.clamp_min(1e-12).sum()).detach().cpu().numpy()[::-1]  # PC1..3\n",
        "    W2 = evecs[:, -2:]  # [K,2]\n",
        "\n",
        "    # project endpoints into 2D PCA space\n",
        "    Z_end = Z[torch.arange(B, device=DEVICE), t_last, :]          # [B,K]\n",
        "    end2 = ((Z_end - mu) @ W2).detach().cpu().numpy()             # [B,2]\n",
        "\n",
        "    # centroid weighted by sigmoid(margin)\n",
        "    w_cent = 1.0 / (1.0 + np.exp(-margin))\n",
        "    w_cent = w_cent / (w_cent.sum() + 1e-9)\n",
        "    cent2 = (end2 * w_cent[:, None]).sum(axis=0)\n",
        "\n",
        "    # attractor sharpness: corr(-dist, margin)\n",
        "    dist = np.linalg.norm(end2 - cent2[None, :], axis=1)\n",
        "    x = -dist\n",
        "    y = margin\n",
        "    # safe corr\n",
        "    if np.std(x) < 1e-9 or np.std(y) < 1e-9:\n",
        "        sharp = np.nan\n",
        "    else:\n",
        "        sharp = float(np.corrcoef(x, y)[0, 1])\n",
        "\n",
        "    rows.append({\n",
        "        \"layer\": L,\n",
        "        \"evr_pc1\": float(evr3[0]),\n",
        "        \"evr_pc2\": float(evr3[1]),\n",
        "        \"evr_pc3\": float(evr3[2]),\n",
        "        \"entropy_late\": float(ent_late),\n",
        "        \"margin_mean\": float(np.mean(margin)),\n",
        "        \"margin_median\": float(np.median(margin)),\n",
        "        \"margin_max\": float(np.max(margin)),\n",
        "        \"margin_min\": float(np.min(margin)),\n",
        "        \"attractor_sharp\": sharp,\n",
        "    })\n",
        "\n",
        "# sort by layer\n",
        "rows = sorted(rows, key=lambda r: r[\"layer\"])\n",
        "\n",
        "# ---------------------------\n",
        "# Print summary + best layers\n",
        "# ---------------------------\n",
        "def best_by(key, maximize=True):\n",
        "    vals = [(r[key], r[\"layer\"]) for r in rows if np.isfinite(r[key])]\n",
        "    if not vals:\n",
        "        return None\n",
        "    return max(vals) if maximize else min(vals)\n",
        "\n",
        "best_sharp = best_by(\"attractor_sharp\", maximize=True)\n",
        "best_evr   = best_by(\"evr_pc1\", maximize=True)\n",
        "best_ent   = best_by(\"entropy_late\", maximize=False)\n",
        "\n",
        "print(\"\\n=== Best layers (by metric) ===\")\n",
        "print(\"best attractor_sharp (corr(-dist,margin)):\", best_sharp)\n",
        "print(\"best evr_pc1 (most 1D late space):\", best_evr)\n",
        "print(\"best entropy_late (most committed):\", best_ent)\n",
        "\n",
        "print(\"\\n=== Per-layer table (compact) ===\")\n",
        "for r in rows:\n",
        "    print(f\"L{r['layer']:2d} | sharp={r['attractor_sharp']:+.3f} | EVR1={r['evr_pc1']:.3f} | ent={r['entropy_late']:.3f} | margin_med={r['margin_median']:+.3f}\")\n",
        "\n",
        "# ---------------------------\n",
        "# Plots\n",
        "# ---------------------------\n",
        "Lx = [r[\"layer\"] for r in rows]\n",
        "sharp = [r[\"attractor_sharp\"] for r in rows]\n",
        "evr1  = [r[\"evr_pc1\"] for r in rows]\n",
        "entl  = [r[\"entropy_late\"] for r in rows]\n",
        "\n",
        "plt.figure(figsize=(9.5, 4.8))\n",
        "plt.plot(Lx, sharp, marker=\"o\")\n",
        "plt.axhline(0.0, linewidth=1)\n",
        "plt.title(\"Attractor sharpness vs layer  (corr(-dist_to_centroid, Paris-margin))\")\n",
        "plt.xlabel(\"layer\")\n",
        "plt.ylabel(\"corr\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(9.5, 4.8))\n",
        "plt.plot(Lx, evr1, marker=\"o\")\n",
        "plt.title(\"Late-window PCA EVR(PC1) vs layer  (how 1D is router commitment space?)\")\n",
        "plt.xlabel(\"layer\")\n",
        "plt.ylabel(\"EVR(PC1)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(9.5, 4.8))\n",
        "plt.plot(Lx, entl, marker=\"o\")\n",
        "plt.title(f\"Mean routing entropy in late window (W={W_LATE}) vs layer  (lower=more committed)\")\n",
        "plt.xlabel(\"layer\")\n",
        "plt.ylabel(\"entropy\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Ea_uqQ7kAo1F"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ================================================================\n",
        "#@title Experiment 1: Slot Simplex Barycentric Animation (Surgical)\n",
        "# Layers: [0,5,13] (auto-clipped)\n",
        "# Clustering: late-window covariance of routing mass (NOT magnitude)\n",
        "# Reduce: K -> meta_K in {3,4}\n",
        "# Animate: barycentric point over time on simplex (triangle/tetra)\n",
        "#\n",
        "# Assumptions:\n",
        "#   - model, cfg, DEVICE exist\n",
        "#   - model(input_ids, attention_mask=..., return_info=True) -> (logits, infos)\n",
        "#   - infos[layer][\"read_logits\"] exists: [B,H,T,K] float32\n",
        "#\n",
        "# Output:\n",
        "#   - For each chosen layer: clustering summary + animation(s)\n",
        "# ================================================================\n",
        "import os, math, numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "# --------------------------\n",
        "# USER SETTINGS\n",
        "# --------------------------\n",
        "LAYERS_WANTED = [ 0, 5, 13 ]\n",
        "META_K = 3               # 3=triangle (recommended first), 4=tetra (also supported)\n",
        "LATE_W = 2               # late window length (tokens from end, per-sample)\n",
        "ENTROPY_TEMP = 1.0       # softmax temp for entropy readout (not changing routing)\n",
        "\n",
        "PADDING_SIDE = \"right\"   # \"right\" fixed your NaNs earlier. keep it.\n",
        "USE_ATTENTION_MASK = True  # you can set False to ignore mask if you want\n",
        "\n",
        "PROMPTS = [\n",
        "    \"The capital of France is\",\n",
        "    \"France's capital city is\",\n",
        "    \"In France, the capital is\",\n",
        "    \"The seat of government of France is\",\n",
        "    \"The city that is France's capital is\",\n",
        "    \"The Eiffel Tower is located in\",\n",
        "    \"The Louvre is in\",\n",
        "    \"The Seine river flows through\",\n",
        "    \"During the French Revolution, the city of\",\n",
        "    \"Napoleon crowned himself in\",\n",
        "    \"In World War II, the liberation of\",\n",
        "    \"The largest city in France is\",\n",
        "    \"A major European city in France is\",\n",
        "    \"A famous city on the Seine is\",\n",
        "    \"The capital of Germany is\",\n",
        "    \"The capital of England is\",\n",
        "    \"The capital of Italy is\",\n",
        "]\n",
        "\n",
        "TARGET_TEXT = \" Paris\"     # must be single token\n",
        "RIVAL_TEXT  = \" the\"       # must be single token\n",
        "\n",
        "FPS = 1\n",
        "SAVE_GIF = True\n",
        "GIF_DIR = \"./asm_viz_gifs\"\n",
        "os.makedirs(GIF_DIR, exist_ok=True)\n",
        "\n",
        "# --------------------------\n",
        "# Tokenizer (robust)\n",
        "# --------------------------\n",
        "def _get_tokenizer():\n",
        "    if \"tokenizer\" in globals():\n",
        "        tok = globals()[\"tokenizer\"]\n",
        "    else:\n",
        "        from transformers import AutoTokenizer\n",
        "        tok = AutoTokenizer.from_pretrained(getattr(cfg, \"tokenizer_name\", \"gpt2\"))\n",
        "\n",
        "    # padding token & side\n",
        "    tok.padding_side = PADDING_SIDE\n",
        "    if tok.pad_token is None:\n",
        "        # GPT2-style: use eos as pad\n",
        "        tok.pad_token = tok.eos_token\n",
        "    return tok\n",
        "\n",
        "tokenizer = _get_tokenizer()\n",
        "\n",
        "def ensure_single_token(text: str) -> int:\n",
        "    ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "    if len(ids) != 1:\n",
        "        raise ValueError(f\"{text!r} must be exactly 1 token. Got ids={ids} (len={len(ids)})\")\n",
        "    return ids[0]\n",
        "\n",
        "TARGET_ID = ensure_single_token(TARGET_TEXT)\n",
        "RIVAL_ID  = ensure_single_token(RIVAL_TEXT)\n",
        "\n",
        "# --------------------------\n",
        "# Forward helper\n",
        "# --------------------------\n",
        "@torch.no_grad()\n",
        "def forward_with_info(input_ids, attention_mask):\n",
        "    out = model(input_ids, attention_mask=attention_mask, return_info=True, routing_mode=\"softmax\")\n",
        "    logits, infos = out\n",
        "    return logits, infos\n",
        "\n",
        "# --------------------------\n",
        "# Encode prompts\n",
        "# --------------------------\n",
        "enc = tokenizer(\n",
        "    PROMPTS,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=getattr(cfg, \"max_seq_len\", 1024),\n",
        ")\n",
        "input_ids = enc[\"input_ids\"].to(DEVICE)\n",
        "attention_mask = enc.get(\"attention_mask\", torch.ones_like(input_ids)).to(DEVICE)\n",
        "\n",
        "B, T = input_ids.shape\n",
        "lengths = attention_mask.sum(dim=1).long()\n",
        "t_last = (lengths - 1).clamp(min=0)\n",
        "\n",
        "# late window indices per sample (clip)\n",
        "def late_indices_for_sample(t_end, W, T_total):\n",
        "    t1 = int(t_end)\n",
        "    t0 = max(0, t1 - (W - 1))\n",
        "    return list(range(t0, t1 + 1))\n",
        "\n",
        "# --------------------------\n",
        "# Determine available layers\n",
        "# --------------------------\n",
        "n_layers = len(getattr(model, \"blocks\", []))\n",
        "if n_layers == 0:\n",
        "    raise RuntimeError(\"Could not detect model.blocks; expected ASMLanguageModel style.\")\n",
        "layers = [L for L in LAYERS_WANTED if 0 <= L < n_layers]\n",
        "if len(layers) == 0:\n",
        "    raise RuntimeError(f\"No requested layers are valid. model has n_layers={n_layers}\")\n",
        "\n",
        "print(f\"[info] model layers={n_layers} | using layers={layers} | META_K={META_K} | LATE_W={LATE_W} | padding_side={PADDING_SIDE}\")\n",
        "\n",
        "# --------------------------\n",
        "# Sanitize utility\n",
        "# --------------------------\n",
        "def sanitize(x: torch.Tensor, name: str):\n",
        "    # Replace non-finite with zeros; also report rate.\n",
        "    good = torch.isfinite(x)\n",
        "    frac = good.float().mean().item()\n",
        "    if frac < 0.999:\n",
        "        print(f\"[health] {name}: finite={100*frac:.2f}%  (sanitizing non-finite to 0)\")\n",
        "    return torch.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "# --------------------------\n",
        "# Extract routing mass for a layer\n",
        "# --------------------------\n",
        "@torch.no_grad()\n",
        "def get_routing_mass_for_layer(infos, layer_idx):\n",
        "    info = infos[layer_idx]\n",
        "    read_logits = info.get(\"read_logits\", None)  # [B,H,T,K]\n",
        "    if read_logits is None:\n",
        "        raise KeyError(f\"infos[{layer_idx}] missing 'read_logits'. keys={list(info.keys())}\")\n",
        "\n",
        "    rl = sanitize(read_logits.float(), f\"read_logits(L{layer_idx})\")\n",
        "    # head-mean (router mass is per-head in module; here we use mean-head lens)\n",
        "    Z = rl.mean(dim=1)  # [B,T,K]\n",
        "    # routing mass\n",
        "    W = torch.softmax(Z, dim=-1)  # [B,T,K]\n",
        "    return Z, W  # logits, mass\n",
        "\n",
        "# --------------------------\n",
        "# Late-window covariance clustering (NOT magnitude)\n",
        "# --------------------------\n",
        "def cluster_slots_by_late_cov(W_btk: torch.Tensor, t_last: torch.Tensor, meta_k: int, late_w: int):\n",
        "    \"\"\"\n",
        "    W_btk: [B,T,K] routing mass\n",
        "    Clustering features: covariance structure across (b,t in late window)\n",
        "    Approach:\n",
        "      1) collect late-window W rows -> X [N,K]\n",
        "      2) compute covariance C [K,K]\n",
        "      3) cluster slots based on rows of C (covariance fingerprint)\n",
        "    \"\"\"\n",
        "    B, T, K = W_btk.shape\n",
        "    rows = []\n",
        "    for b in range(B):\n",
        "        for t in late_indices_for_sample(t_last[b].item(), late_w, T):\n",
        "            rows.append(W_btk[b, t].detach().cpu())\n",
        "    X = torch.stack(rows, dim=0).float()  # [N,K]\n",
        "    # center\n",
        "    X = X - X.mean(dim=0, keepdim=True)\n",
        "    # covariance\n",
        "    C = (X.T @ X) / max(1, X.shape[0] - 1)  # [K,K]\n",
        "    C = C.numpy()\n",
        "\n",
        "    # cluster on covariance rows (each slot i represented by C[i,:])\n",
        "    feats = C.copy()\n",
        "\n",
        "    # Try sklearn KMeans, fallback to a simple spectral-ish heuristic if unavailable\n",
        "    labels = None\n",
        "    try:\n",
        "        from sklearn.cluster import KMeans\n",
        "        km = KMeans(n_clusters=meta_k, n_init=20, random_state=0)\n",
        "        labels = km.fit_predict(feats)\n",
        "    except Exception as e:\n",
        "        print(\"[warn] sklearn KMeans unavailable/failed, using fallback clustering:\", repr(e))\n",
        "        # fallback: farthest-point seeding + assignment by cosine sim\n",
        "        feats_t = feats / (np.linalg.norm(feats, axis=1, keepdims=True) + 1e-9)\n",
        "        # seed 0 = max variance row\n",
        "        seeds = [int(np.argmax(np.linalg.norm(feats_t, axis=1)))]\n",
        "        for _ in range(1, meta_k):\n",
        "            sims = feats_t @ feats_t[seeds].T  # [K, len(seeds)]\n",
        "            dist = 1.0 - np.max(sims, axis=1)\n",
        "            seeds.append(int(np.argmax(dist)))\n",
        "        seed_vecs = feats_t[seeds]  # [meta_k,K]\n",
        "        sims = feats_t @ seed_vecs.T  # [K,meta_k]\n",
        "        labels = np.argmax(sims, axis=1)\n",
        "\n",
        "    # build groups\n",
        "    groups = [[] for _ in range(meta_k)]\n",
        "    for i, g in enumerate(labels.tolist()):\n",
        "        groups[g].append(i)\n",
        "\n",
        "    # ensure no empty groups (rare but possible): fix by moving one slot from largest group\n",
        "    empties = [i for i,g in enumerate(groups) if len(g)==0]\n",
        "    if len(empties) > 0:\n",
        "        sizes = [len(g) for g in groups]\n",
        "        for eg in empties:\n",
        "            lg = int(np.argmax(sizes))\n",
        "            move = groups[lg].pop()  # move one\n",
        "            groups[eg].append(move)\n",
        "            sizes[lg] -= 1\n",
        "            sizes[eg] += 1\n",
        "\n",
        "    return groups, C\n",
        "\n",
        "# --------------------------\n",
        "# Aggregate routing mass to meta-slots\n",
        "# --------------------------\n",
        "def aggregate_to_meta(W_btk: torch.Tensor, groups):\n",
        "    \"\"\"\n",
        "    returns W_meta: [B,T,M] where M=len(groups)\n",
        "    \"\"\"\n",
        "    B,T,K = W_btk.shape\n",
        "    M = len(groups)\n",
        "    out = torch.zeros((B,T,M), device=W_btk.device, dtype=W_btk.dtype)\n",
        "    for m, idxs in enumerate(groups):\n",
        "        out[:,:,m] = W_btk[:,:,idxs].sum(dim=-1)\n",
        "    # renorm safety\n",
        "    out = out / out.sum(dim=-1, keepdim=True).clamp_min(1e-9)\n",
        "    return out\n",
        "\n",
        "# --------------------------\n",
        "# Paris margin per prompt\n",
        "# --------------------------\n",
        "@torch.no_grad()\n",
        "def paris_margin(logits, t_last):\n",
        "    logp = F.log_softmax(sanitize(logits.float(), \"logits\"), dim=-1)\n",
        "    idx = torch.arange(logp.shape[0], device=logp.device)\n",
        "    lp_t = logp[idx, t_last, TARGET_ID]\n",
        "    lp_r = logp[idx, t_last, RIVAL_ID]\n",
        "    return (lp_t - lp_r).detach().cpu().numpy()\n",
        "\n",
        "# --------------------------\n",
        "# Barycentric plotting helpers\n",
        "# --------------------------\n",
        "def simplex_vertices(M):\n",
        "    if M == 3:\n",
        "        # 2D equilateral triangle\n",
        "        V = np.array([\n",
        "            [0.0, 0.0],\n",
        "            [1.0, 0.0],\n",
        "            [0.5, math.sqrt(3)/2],\n",
        "        ], dtype=np.float32)\n",
        "        return V\n",
        "    if M == 4:\n",
        "        # 3D tetrahedron\n",
        "        V = np.array([\n",
        "            [0.0, 0.0, 0.0],\n",
        "            [1.0, 0.0, 0.0],\n",
        "            [0.5, math.sqrt(3)/2, 0.0],\n",
        "            [0.5, math.sqrt(3)/6, math.sqrt(2/3)],\n",
        "        ], dtype=np.float32)\n",
        "        return V\n",
        "    raise ValueError(\"M must be 3 or 4\")\n",
        "\n",
        "def bary_to_cart(w_m, V):\n",
        "    # w_m: [M], V: [M,D] -> [D]\n",
        "    return (w_m.reshape(-1,1) * V).sum(axis=0)\n",
        "\n",
        "# --------------------------\n",
        "# Make animation for a single layer\n",
        "# --------------------------\n",
        "def animate_layer(layer_idx, Z_btk, W_btk, groups, margins_np, meta_k, late_w):\n",
        "    \"\"\"\n",
        "    For each prompt b, animate barycentric point over t=0..t_last[b].\n",
        "    Color endpoints by Paris margin.\n",
        "    Also show trails (last N points).\n",
        "    \"\"\"\n",
        "    B, T, K = W_btk.shape\n",
        "    M = meta_k\n",
        "    V = simplex_vertices(M)\n",
        "\n",
        "    # aggregate\n",
        "    Wm = aggregate_to_meta(W_btk, groups)  # [B,T,M]\n",
        "    Wm_cpu = Wm.detach().cpu().numpy()\n",
        "\n",
        "    # compute per-token entropy (commitment proxy)\n",
        "    w_ent = -(W_btk * (W_btk.clamp_min(1e-9).log())).sum(dim=-1)  # [B,T]\n",
        "    ent_cpu = w_ent.detach().cpu().numpy()\n",
        "\n",
        "    # time axis alignment: choose max t_end across prompts, animate in global time\n",
        "    t_ends = t_last.detach().cpu().numpy().astype(int)\n",
        "    T_anim = int(t_ends.max()) + 1\n",
        "\n",
        "    # Precompute paths in cart coords\n",
        "    if M == 3:\n",
        "        paths = np.zeros((B, T_anim, 2), dtype=np.float32)\n",
        "    else:\n",
        "        paths = np.zeros((B, T_anim, 3), dtype=np.float32)\n",
        "\n",
        "    for b in range(B):\n",
        "        for t in range(T_anim):\n",
        "            tt = min(t, t_ends[b])\n",
        "            paths[b, t] = bary_to_cart(Wm_cpu[b, tt], V)\n",
        "\n",
        "    # figure\n",
        "    if M == 3:\n",
        "        fig, ax = plt.subplots(figsize=(8.2, 7.2))\n",
        "        ax.set_title(f\"Layer {layer_idx} | META_K=3 barycentric routing (late cov clustering)\\n\"\n",
        "                     f\"endpoint color = Paris-margin | trails show motion\")\n",
        "        # draw triangle\n",
        "        tri = np.vstack([V, V[0]])\n",
        "        ax.plot(tri[:,0], tri[:,1], linewidth=2.0, alpha=0.7)\n",
        "        for i in range(3):\n",
        "            ax.text(V[i,0], V[i,1], f\"  G{i} (|S|={len(groups[i])})\", fontsize=10)\n",
        "        ax.set_aspect(\"equal\", adjustable=\"box\")\n",
        "        ax.set_xlim(-0.1, 1.1)\n",
        "        ax.set_ylim(-0.1, math.sqrt(3)/2 + 0.1)\n",
        "        ax.grid(True, alpha=0.2)\n",
        "\n",
        "        # artists\n",
        "        scat = ax.scatter([], [], s=50)\n",
        "        trails = [ax.plot([], [], alpha=0.35, linewidth=1.5)[0] for _ in range(B)]\n",
        "\n",
        "        # color normalize by margins\n",
        "        vmin, vmax = float(np.min(margins_np)), float(np.max(margins_np))\n",
        "        def col(b):\n",
        "            # use viridis mapping\n",
        "            import matplotlib as mpl\n",
        "            cmap = plt.cm.viridis\n",
        "            z = (margins_np[b] - vmin) / (vmax - vmin + 1e-9)\n",
        "            return cmap(np.clip(z, 0, 1))\n",
        "\n",
        "        def init():\n",
        "            scat.set_offsets(np.zeros((0,2)))\n",
        "            for tr in trails:\n",
        "                tr.set_data([], [])\n",
        "            return [scat] + trails\n",
        "\n",
        "        TRAIL_LEN = max(3, min(10, T_anim))\n",
        "\n",
        "        def update(t):\n",
        "            pts = np.zeros((B,2), dtype=np.float32)\n",
        "            colors = []\n",
        "            for b in range(B):\n",
        "                pts[b] = paths[b, t]\n",
        "                colors.append(col(b))\n",
        "                t0 = max(0, t - TRAIL_LEN + 1)\n",
        "                seg = paths[b, t0:t+1]\n",
        "                trails[b].set_data(seg[:,0], seg[:,1])\n",
        "            scat.set_offsets(pts)\n",
        "            scat.set_color(colors)\n",
        "            ax.set_xlabel(f\"t={t} (global), entropy mean={ent_cpu[:, min(t, ent_cpu.shape[1]-1)].mean():.3f}\")\n",
        "            return [scat] + trails\n",
        "\n",
        "        anim = animation.FuncAnimation(fig, update, init_func=init, frames=T_anim, interval=1000//FPS, blit=True)\n",
        "        plt.close(fig)\n",
        "        return anim\n",
        "\n",
        "    else:\n",
        "        # META_K=4 tetra animation (3D)\n",
        "        from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
        "        fig = plt.figure(figsize=(9.0, 7.5))\n",
        "        ax = fig.add_subplot(111, projection=\"3d\")\n",
        "        ax.set_title(f\"Layer {layer_idx} | META_K=4 barycentric routing (late cov clustering)\\n\"\n",
        "                     f\"endpoint color = Paris-margin | trails show motion\")\n",
        "\n",
        "        # draw tetra edges\n",
        "        edges = [(0,1),(0,2),(0,3),(1,2),(1,3),(2,3)]\n",
        "        for (i,j) in edges:\n",
        "            ax.plot([V[i,0], V[j,0]], [V[i,1], V[j,1]], [V[i,2], V[j,2]], alpha=0.5)\n",
        "        for i in range(4):\n",
        "            ax.text(V[i,0], V[i,1], V[i,2], f\"G{i}(|S|={len(groups[i])})\", fontsize=9)\n",
        "\n",
        "        ax.set_xlim(-0.1, 1.1)\n",
        "        ax.set_ylim(-0.1, math.sqrt(3)/2 + 0.1)\n",
        "        ax.set_zlim(-0.1, math.sqrt(2/3) + 0.1)\n",
        "\n",
        "        # artists\n",
        "        scat = ax.scatter([], [], [], s=50)\n",
        "        trails = [ax.plot([], [], [], alpha=0.35, linewidth=1.5)[0] for _ in range(B)]\n",
        "\n",
        "        vmin, vmax = float(np.min(margins_np)), float(np.max(margins_np))\n",
        "        def col(b):\n",
        "            import matplotlib as mpl\n",
        "            cmap = plt.cm.viridis\n",
        "            z = (margins_np[b] - vmin) / (vmax - vmin + 1e-9)\n",
        "            return cmap(np.clip(z, 0, 1))\n",
        "\n",
        "        def init():\n",
        "            scat._offsets3d = ([], [], [])\n",
        "            for tr in trails:\n",
        "                tr.set_data([], [])\n",
        "                tr.set_3d_properties([])\n",
        "            return [scat] + trails\n",
        "\n",
        "        TRAIL_LEN = max(3, min(10, T_anim))\n",
        "\n",
        "        def update(t):\n",
        "            xs, ys, zs, colors = [], [], [], []\n",
        "            for b in range(B):\n",
        "                p = paths[b, t]\n",
        "                xs.append(p[0]); ys.append(p[1]); zs.append(p[2])\n",
        "                colors.append(col(b))\n",
        "                t0 = max(0, t - TRAIL_LEN + 1)\n",
        "                seg = paths[b, t0:t+1]\n",
        "                trails[b].set_data(seg[:,0], seg[:,1])\n",
        "                trails[b].set_3d_properties(seg[:,2])\n",
        "            scat._offsets3d = (xs, ys, zs)\n",
        "            scat.set_color(colors)\n",
        "            ax.set_xlabel(f\"t={t} | entropy mean={ent_cpu[:, min(t, ent_cpu.shape[1]-1)].mean():.3f}\")\n",
        "            return [scat] + trails\n",
        "\n",
        "        anim = animation.FuncAnimation(fig, update, init_func=init, frames=T_anim, interval=1000//FPS, blit=False)\n",
        "        plt.close(fig)\n",
        "        return anim\n",
        "\n",
        "# --------------------------\n",
        "# Run: forward once, then per-layer analysis+animation\n",
        "# --------------------------\n",
        "with torch.no_grad():\n",
        "    logits, infos = forward_with_info(\n",
        "        input_ids,\n",
        "        attention_mask if USE_ATTENTION_MASK else None\n",
        "    )\n",
        "\n",
        "margins_np = paris_margin(logits, t_last)\n",
        "\n",
        "# top-level progress\n",
        "pbar = tqdm(layers, desc=\"Experiment 1: barycentric animations (per layer)\")\n",
        "for L in pbar:\n",
        "    pbar.set_postfix(layer=L)\n",
        "\n",
        "    # extract routing mass\n",
        "    Z, W = get_routing_mass_for_layer(infos, L)  # [B,T,K] logits & mass\n",
        "\n",
        "    # cluster slots by late-window covariance\n",
        "    groups, C = cluster_slots_by_late_cov(W, t_last, META_K, LATE_W)\n",
        "\n",
        "    # print cluster summary\n",
        "    sizes = [len(g) for g in groups]\n",
        "    print(f\"\\n=== Layer {L} | META_K={META_K} late-cov clustering ===\")\n",
        "    print(\"group sizes:\", sizes)\n",
        "    # show a few slot ids per group\n",
        "    for gi, g in enumerate(groups):\n",
        "        print(f\"  G{gi}: {g[:12]}{' ...' if len(g) > 12 else ''}\")\n",
        "\n",
        "    # animate\n",
        "    anim = animate_layer(L, Z, W, groups, margins_np, META_K, LATE_W)\n",
        "\n",
        "    # render HTML\n",
        "    display(HTML(anim.to_jshtml()))\n",
        "\n",
        "    # optional save\n",
        "    if SAVE_GIF:\n",
        "        out_path = os.path.join(GIF_DIR, f\"bary_L{L}_M{META_K}.gif\")\n",
        "        try:\n",
        "            anim.save(out_path, writer=animation.PillowWriter(fps=FPS))\n",
        "            print(\"[saved]\", out_path)\n",
        "        except Exception as e:\n",
        "            print(\"[warn] gif save failed:\", repr(e))\n",
        "\n",
        "print(\"\\n[done] Experiment 1 complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nSI84OZZLFCT"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ================================================================\n",
        "#@title Experiment 2 (robust): Prompt-conditional Jacobian alignment via\n",
        "# FINITE DIFFERENCES in PCA space (works even if autograd is detached).\n",
        "#\n",
        "# For each layer:\n",
        "#  1) Collect late-window read_logits [B,T,K] (head-mean) and fit PCA (PC1..3)\n",
        "#  2) For each PC direction i=1..3:\n",
        "#       - Inject read_logits += +eps * PCi at late-window tokens (per-sample)\n",
        "#       - Inject read_logits += -eps * PCi\n",
        "#       - Slope g_i  (margin_plus - margin_minus) / (2*eps)\n",
        "#     This gives gradient coordinates directly in PCA basis.\n",
        "#\n",
        "# Outputs:\n",
        "#  - frac_pc1 = g1^2 / (g1^2+g2^2+g3^2)  (PC1 dominance)\n",
        "#  - frac_pc2, frac_pc3 similarly (rotation/spread)\n",
        "#  - angle_pc12 = atan2(g2, g1)\n",
        "#\n",
        "# Assumptions:\n",
        "#  - model, cfg exist\n",
        "#  - model(..., return_info=True, routing_mode=\"softmax\") -> (logits, infos)\n",
        "#  - infos[layer][\"read_logits\"] exists as [B,H,T,K]\n",
        "#  - model.blocks[li].asa.routing_override exists and is used like earlier cells\n",
        "# ================================================================\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "DEVICE = next(model.parameters()).device\n",
        "\n",
        "# ---------------------------\n",
        "# SETTINGS\n",
        "# ---------------------------\n",
        "PROMPTS = [\n",
        "    \"The capital of France is\",\n",
        "    \"France's capital city is\",\n",
        "    \"In France, the capital is\",\n",
        "    \"The seat of government of France is\",\n",
        "    \"The city that is France's capital is\",\n",
        "    \"The Eiffel Tower is located in\",\n",
        "    \"The Louvre is in\",\n",
        "    \"The Seine river flows through\",\n",
        "    \"During the French Revolution, the city of\",\n",
        "    \"Napoleon crowned himself in\",\n",
        "    \"In World War II, the liberation of\",\n",
        "    \"The largest city in France is\",\n",
        "    \"A major European city in France is\",\n",
        "    \"A famous city on the Seine is\",\n",
        "    \"The capital of Germany is\",\n",
        "    \"The capital of England is\",\n",
        "    \"The capital of Italy is\",\n",
        "]\n",
        "\n",
        "TARGET_TEXT = \" Paris\"   # must be single token\n",
        "RIVAL_TEXT  = \" the\"     # must be single token\n",
        "\n",
        "MAX_LEN = cfg.max_seq_len\n",
        "LATE_W = 5               # late window length\n",
        "PCA_D = 3\n",
        "SKIP_T0 = True\n",
        "\n",
        "# finite difference magnitude (start here; if noisy, try 2e-2; if saturating, try 5e-3)\n",
        "FD_EPS = 1e-2\n",
        "\n",
        "# how many layers to scan (auto-detect)\n",
        "# (well scan all that expose read_logits)\n",
        "PLOT = True\n",
        "\n",
        "EPS = 1e-9\n",
        "MIN_PCA_POINTS = 32\n",
        "\n",
        "# ---------------------------\n",
        "# Tokenizer (robust + RIGHT padding)\n",
        "# ---------------------------\n",
        "def _get_tokenizer():\n",
        "    if \"tokenizer\" in globals():\n",
        "        tok = globals()[\"tokenizer\"]\n",
        "    else:\n",
        "        from transformers import AutoTokenizer\n",
        "        tok = AutoTokenizer.from_pretrained(getattr(cfg, \"tokenizer_name\", \"gpt2\"))\n",
        "    if tok.pad_token is None:\n",
        "        tok.pad_token = tok.eos_token\n",
        "    tok.padding_side = \"right\"\n",
        "    return tok\n",
        "\n",
        "tokenizer = _get_tokenizer()\n",
        "\n",
        "def ensure_single_token(text):\n",
        "    ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "    if len(ids) != 1:\n",
        "        raise ValueError(f\"{text!r} must be exactly 1 token. Got {ids}\")\n",
        "    return ids[0]\n",
        "\n",
        "TARGET_ID = ensure_single_token(TARGET_TEXT)\n",
        "RIVAL_ID  = ensure_single_token(RIVAL_TEXT)\n",
        "\n",
        "# ---------------------------\n",
        "# Batch tokenize prompts\n",
        "# ---------------------------\n",
        "enc = tokenizer(PROMPTS, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LEN)\n",
        "input_ids = enc[\"input_ids\"].to(DEVICE)\n",
        "attention_mask = enc.get(\"attention_mask\", torch.ones_like(input_ids)).to(DEVICE)\n",
        "\n",
        "B, T = input_ids.shape\n",
        "lengths = attention_mask.sum(dim=1).long()\n",
        "t_last = (lengths - 1).clamp(min=0)  # [B]\n",
        "\n",
        "def late_bounds(t_last_1d, late_w):\n",
        "    bounds = []\n",
        "    for b in range(t_last_1d.shape[0]):\n",
        "        te = int(t_last_1d[b].item())\n",
        "        t0 = max(0, te - late_w + 1)\n",
        "        t1 = te\n",
        "        if SKIP_T0 and t0 == 0:\n",
        "            t0 = 1\n",
        "        bounds.append((t0, t1))\n",
        "    return bounds\n",
        "\n",
        "bounds = late_bounds(t_last, LATE_W)\n",
        "\n",
        "# ---------------------------\n",
        "# Forward helper\n",
        "# ---------------------------\n",
        "@torch.no_grad()\n",
        "def forward_with_info(ids, mask):\n",
        "    model.eval()\n",
        "    logits, infos = model(ids, attention_mask=mask, return_info=True, routing_mode=\"softmax\")\n",
        "    return logits, infos\n",
        "\n",
        "@torch.no_grad()\n",
        "def paris_margin_from_logits(logits):\n",
        "    logp = F.log_softmax(logits, dim=-1)\n",
        "    idx = torch.arange(B, device=logits.device)\n",
        "    m = (logp[idx, t_last, TARGET_ID] - logp[idx, t_last, RIVAL_ID]).mean().item()\n",
        "    return m\n",
        "\n",
        "# ---------------------------\n",
        "# PCA fit (stable SVD on CPU)\n",
        "# ---------------------------\n",
        "def fit_pca_svd(X_cpu, d=3):\n",
        "    # X_cpu: [N,K] on CPU, finite\n",
        "    mu = X_cpu.mean(dim=0, keepdim=True)\n",
        "    Xc = X_cpu - mu\n",
        "    U, S, Vh = torch.linalg.svd(Xc, full_matrices=False)\n",
        "    W = Vh[:d].T.contiguous()  # [K,d]\n",
        "    s2 = (S**2)\n",
        "    evr = (s2[:d] / s2.sum().clamp_min(EPS)).cpu().numpy()\n",
        "    return mu, W, evr\n",
        "\n",
        "# ---------------------------\n",
        "# Routing override that ADDS delta to read_logits before softmax\n",
        "# at specified tokens (late window), per example.\n",
        "# ---------------------------\n",
        "def clear_routing_override():\n",
        "    for li in range(len(model.blocks)):\n",
        "        model.blocks[li].asa.routing_override = None\n",
        "\n",
        "def set_additive_readlogits_override(layer_idx, delta_btK, bounds, rtemp=1.0):\n",
        "    \"\"\"\n",
        "    delta_btK: [B,T,K] additive perturbation (mostly zeros except late window)\n",
        "    bounds: list[(t0,t1)] per example; used only for safety checks\n",
        "    \"\"\"\n",
        "    clear_routing_override()\n",
        "\n",
        "    def _override(t0, t1, read_logits_chunk, read_logits_key, read_logits_content, ctx):\n",
        "        # read_logits_chunk: [B,H,L,K], chunk time is [t0,t1)\n",
        "        Bc, Hc, Lc, Kc = read_logits_chunk.shape\n",
        "        # softmax weights from (read_logits + delta)\n",
        "        # Build delta chunk: [B,1,L,K]\n",
        "        # Note: delta_btK is full length T; slice to this chunk.\n",
        "        delta_chunk = delta_btK[:, t0:t1, :].to(read_logits_chunk.device)  # [B,L,K]\n",
        "        delta_chunk = delta_chunk.unsqueeze(1)  # [B,1,L,K]\n",
        "        rl = read_logits_chunk + delta_chunk  # broadcast heads\n",
        "        w = torch.softmax(rl / max(1e-6, float(rtemp)), dim=-1)\n",
        "        w = w / w.sum(dim=-1, keepdim=True).clamp_min(1e-8)\n",
        "        return w\n",
        "\n",
        "    model.blocks[layer_idx].asa.routing_override = _override\n",
        "\n",
        "# ---------------------------\n",
        "# Main scan\n",
        "# ---------------------------\n",
        "# First baseline forward to get all infos + determine layers\n",
        "logits0, infos0 = forward_with_info(input_ids, attention_mask)\n",
        "margin0 = paris_margin_from_logits(logits0)\n",
        "\n",
        "n_layers = len(infos0)\n",
        "K = infos0[0][\"read_logits\"].shape[-1]\n",
        "H = infos0[0][\"read_logits\"].shape[1]\n",
        "\n",
        "print(f\"[info] detected layers={n_layers} | B={B} T={T} K={K} H={H} | baseline margin={margin0:.4f}\")\n",
        "\n",
        "rows = []\n",
        "pbar = tqdm(range(n_layers), desc=\"FD Jacobian alignment per layer\", leave=True)\n",
        "\n",
        "for layer in pbar:\n",
        "    if \"read_logits\" not in infos0[layer]:\n",
        "        rows.append({\"layer\": layer, \"ok\": False, \"reason\": \"no_read_logits\"})\n",
        "        continue\n",
        "\n",
        "    # collect late-window read_logits head-mean from this layer\n",
        "    rl_bhTK = infos0[layer][\"read_logits\"]  # [B,H,T,K]\n",
        "    rl_btK = rl_bhTK.mean(dim=1).float()    # [B,T,K]\n",
        "\n",
        "    # build PCA dataset from late window\n",
        "    pts = []\n",
        "    for b in range(B):\n",
        "        t0, t1 = bounds[b]\n",
        "        if t0 > t1:\n",
        "            continue\n",
        "        pts.append(rl_btK[b, t0:t1+1, :])\n",
        "    X = torch.cat(pts, dim=0)  # [N,K]\n",
        "\n",
        "    # sanitize finites\n",
        "    finite = torch.isfinite(X).all(dim=-1)\n",
        "    X = X[finite]\n",
        "    n_good = int(X.shape[0])\n",
        "    if n_good < MIN_PCA_POINTS:\n",
        "        rows.append({\"layer\": layer, \"ok\": False, \"reason\": f\"too_few_points({n_good})\"})\n",
        "        continue\n",
        "\n",
        "    mu, W, evr = fit_pca_svd(X.detach().cpu(), d=PCA_D)  # mu [1,K], W [K,3] on CPU\n",
        "\n",
        "    # FD gradients in PC coords: g_i = d margin / d alpha_i\n",
        "    g_pc = np.zeros((PCA_D,), dtype=np.float64)\n",
        "\n",
        "    for i in range(PCA_D):\n",
        "        v = W[:, i].detach().cpu().numpy().astype(np.float32)  # [K], unit direction\n",
        "\n",
        "        # build delta tensor [B,T,K] (mostly zeros; apply to late window tokens)\n",
        "        delta = torch.zeros((B, T, K), device=DEVICE, dtype=torch.float32)\n",
        "        for b in range(B):\n",
        "            t0, t1 = bounds[b]\n",
        "            if t0 > t1:\n",
        "                continue\n",
        "            delta[b, t0:t1+1, :] = torch.from_numpy(v).to(DEVICE)  # broadcast over time\n",
        "\n",
        "        # +eps\n",
        "        set_additive_readlogits_override(layer, delta * float(FD_EPS), bounds, rtemp=1.0)\n",
        "        lp, _ = forward_with_info(input_ids, attention_mask)\n",
        "        m_plus = paris_margin_from_logits(lp)\n",
        "\n",
        "        # -eps\n",
        "        set_additive_readlogits_override(layer, delta * float(-FD_EPS), bounds, rtemp=1.0)\n",
        "        lm, _ = forward_with_info(input_ids, attention_mask)\n",
        "        m_minus = paris_margin_from_logits(lm)\n",
        "\n",
        "        clear_routing_override()\n",
        "\n",
        "        g_pc[i] = (m_plus - m_minus) / (2.0 * float(FD_EPS))\n",
        "\n",
        "    # energy fractions and angle\n",
        "    g2 = float((g_pc**2).sum()) + 1e-18\n",
        "    frac = (g_pc**2) / g2\n",
        "    ang12 = float(math.atan2(g_pc[1], g_pc[0])) if PCA_D >= 2 else float(\"nan\")\n",
        "\n",
        "    rows.append({\n",
        "        \"layer\": layer,\n",
        "        \"ok\": True,\n",
        "        \"n_points\": n_good,\n",
        "        \"evr1\": float(evr[0]),\n",
        "        \"evr2\": float(evr[1]),\n",
        "        \"evr3\": float(evr[2]),\n",
        "        \"g_pc1\": float(g_pc[0]),\n",
        "        \"g_pc2\": float(g_pc[1]),\n",
        "        \"g_pc3\": float(g_pc[2]),\n",
        "        \"frac_pc1\": float(frac[0]),\n",
        "        \"frac_pc2\": float(frac[1]),\n",
        "        \"frac_pc3\": float(frac[2]),\n",
        "        \"angle_pc12\": ang12,\n",
        "    })\n",
        "\n",
        "# ---------------------------\n",
        "# Report + plots\n",
        "# ---------------------------\n",
        "ok = [r for r in rows if r.get(\"ok\", False)]\n",
        "bad = [r for r in rows if not r.get(\"ok\", False)]\n",
        "print(f\"[done] ok={len(ok)} skipped={len(bad)} FD_EPS={FD_EPS}\")\n",
        "\n",
        "print(\"\\n=== Per-layer FD Jacobian (compact) ===\")\n",
        "for r in ok:\n",
        "    print(\n",
        "        f\"L{r['layer']:2d} | frac=[{r['frac_pc1']:.3f},{r['frac_pc2']:.3f},{r['frac_pc3']:.3f}] | \"\n",
        "        f\"g=[{r['g_pc1']:+.3g},{r['g_pc2']:+.3g},{r['g_pc3']:+.3g}] | \"\n",
        "        f\"EVR1={r['evr1']:.3f} | ang12={r['angle_pc12']:+.2f} | n={r['n_points']}\"\n",
        "    )\n",
        "\n",
        "if PLOT and ok:\n",
        "    layers = [r[\"layer\"] for r in ok]\n",
        "    frac1 = [r[\"frac_pc1\"] for r in ok]\n",
        "    frac2 = [r[\"frac_pc2\"] for r in ok]\n",
        "    frac3 = [r[\"frac_pc3\"] for r in ok]\n",
        "    ang12 = [r[\"angle_pc12\"] for r in ok]\n",
        "\n",
        "    plt.figure(figsize=(8,4.5))\n",
        "    plt.plot(layers, frac1, marker=\"o\", label=\"PC1\")\n",
        "    plt.plot(layers, frac2, marker=\"o\", label=\"PC2\")\n",
        "    plt.plot(layers, frac3, marker=\"o\", label=\"PC3\")\n",
        "    plt.title(\"FD Jacobian energy fraction in PCA coords vs layer (Paris-margin)\")\n",
        "    plt.xlabel(\"layer\")\n",
        "    plt.ylabel(\"frac of ||grad||^2\")\n",
        "    plt.grid(alpha=0.25)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(8,4.5))\n",
        "    plt.plot(layers, ang12, marker=\"o\")\n",
        "    plt.title(\"FD mean Jacobian direction angle in (PC1,PC2) vs layer\")\n",
        "    plt.xlabel(\"layer\")\n",
        "    plt.ylabel(\"atan2(g2, g1) [rad]\")\n",
        "    plt.grid(alpha=0.25)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "__VRVzGNOO-W"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ================================================================\n",
        "#@title Next Cell: Tangent Control Field on the Simplex Sphere (Late-Window Centroid)\n",
        "# For layers [0,5,13]:\n",
        "#   - Extract router read_logits trajectories (head-mean) per prompt\n",
        "#   - Fit PCA->3D on late-window points, project to S^2\n",
        "#   - Compute FD gradient of Paris-margin w.r.t. a 3D PCA coordinate at each late-window point\n",
        "#   - Average per-prompt gradients across late-window points\n",
        "#   - Project gradient to tangent plane at each prompt's late-window centroid on the sphere\n",
        "#   - Plot: trajectories + centroid points (colored by Paris-margin) + tangent arrows (control direction)\n",
        "#\n",
        "# Robustness:\n",
        "#   - Enforces RIGHT padding + right padding side to avoid NaN / mask issues\n",
        "#   - Sanitizes non-finite values\n",
        "#   - Uses FD (finite difference) so no need for autograd plumbing through infos\n",
        "#   - Includes tqdm progress bars\n",
        "#\n",
        "# Assumptions:\n",
        "#   - model, cfg, DEVICE exist\n",
        "#   - model(..., return_info=True, routing_mode=\"softmax\") -> (logits, infos)\n",
        "#   - infos[layer][\"read_logits\"] exists, shape [B,H,T,K]\n",
        "# ================================================================\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ---------------------------\n",
        "# SETTINGS\n",
        "# ---------------------------\n",
        "LAYERS = [0, 5, 13]\n",
        "LATE_W = 5                   # late-window size (last W valid tokens)\n",
        "FD_EPS = 1e-2                # finite difference step in PCA coords\n",
        "FD_MODE = \"central\"          # \"central\" or \"forward\"\n",
        "PLOT_STRIDE = 1              # downsample trajectory points\n",
        "ARROW_SCALE = 0.25           # arrow length scaling in sphere coords (visual)\n",
        "ARROW_ALPHA = 0.9\n",
        "TRAJ_ALPHA = 0.45\n",
        "TRAJ_LW = 1.8\n",
        "SPHERE_ALPHA = 0.06\n",
        "SPHERE_WIREFRAME = True\n",
        "LABEL_MAX = 10               # label top prompts by Paris-margin\n",
        "SKIP_T0_IN_PCA = True        # skip token index 0 (often special / unstable)\n",
        "SANITIZE_MODE = \"drop\"       # \"drop\" or \"clamp\" for non-finite read_logits in PCA fit\n",
        "CLAMP_VAL = 5.0              # used if SANITIZE_MODE=\"clamp\"\n",
        "\n",
        "# prompts (use your current set; replace if desired)\n",
        "PROMPTS = [\n",
        "    \"The capital of France is\",\n",
        "    \"France's capital city is\",\n",
        "    \"In France, the capital is\",\n",
        "    \"The seat of government of France is\",\n",
        "    \"The city that is France's capital is\",\n",
        "    \"The Eiffel Tower is located in\",\n",
        "    \"The Louvre is in\",\n",
        "    \"The Seine river flows through\",\n",
        "    \"During the French Revolution, the city of\",\n",
        "    \"Napoleon crowned himself in\",\n",
        "    \"In World War II, the liberation of\",\n",
        "    \"The largest city in France is\",\n",
        "    \"A major European city in France is\",\n",
        "    \"A famous city on the Seine is\",\n",
        "    \"The capital of Germany is\",\n",
        "    \"The capital of England is\",\n",
        "    \"The capital of Italy is\",\n",
        "]\n",
        "\n",
        "ENDPOINT_COLOR_MODE = \"paris_margin\"  # only mode supported here (for colorbar)\n",
        "RIVAL_TEXT = \" the\"                   # single-token rival for margin\n",
        "\n",
        "# ---------------------------\n",
        "# Tokenizer helpers (RIGHT padding enforced)\n",
        "# ---------------------------\n",
        "def _get_tokenizer_from_context():\n",
        "    if \"tokenizer\" in globals():\n",
        "        tok = globals()[\"tokenizer\"]\n",
        "        # enforce right padding (critical)\n",
        "        tok.padding_side = \"right\"\n",
        "        if tok.pad_token is None:\n",
        "            tok.pad_token = tok.eos_token\n",
        "        return tok\n",
        "    from transformers import AutoTokenizer\n",
        "    tok = AutoTokenizer.from_pretrained(getattr(cfg, \"tokenizer_name\", \"gpt2\"))\n",
        "    tok.padding_side = \"right\"\n",
        "    if tok.pad_token is None:\n",
        "        tok.pad_token = tok.eos_token\n",
        "    return tok\n",
        "\n",
        "tokenizer = _get_tokenizer_from_context()\n",
        "\n",
        "def ensure_single_token(text: str) -> int:\n",
        "    ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "    if len(ids) != 1:\n",
        "        raise ValueError(f\"{text!r} must be exactly 1 token. Got ids={ids}\")\n",
        "    return ids[0]\n",
        "\n",
        "PARIS_ID = ensure_single_token(\" Paris\")\n",
        "RIVAL_ID = ensure_single_token(RIVAL_TEXT)\n",
        "\n",
        "# ---------------------------\n",
        "# Forward with info\n",
        "# ---------------------------\n",
        "def forward_with_info(ids, mask, routing_mode=\"softmax\"):\n",
        "    logits, infos = model(ids, attention_mask=mask, return_info=True, routing_mode=routing_mode)\n",
        "    return logits, infos\n",
        "\n",
        "# ---------------------------\n",
        "# Batch tokenize (RIGHT padding)\n",
        "# ---------------------------\n",
        "enc = tokenizer(\n",
        "    PROMPTS,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=cfg.max_seq_len,\n",
        ")\n",
        "input_ids = enc[\"input_ids\"].to(DEVICE)\n",
        "attention_mask = enc.get(\"attention_mask\", torch.ones_like(input_ids)).to(DEVICE)\n",
        "\n",
        "B, T = input_ids.shape\n",
        "lengths = attention_mask.sum(dim=1).long()\n",
        "t_last = (lengths - 1).clamp(min=0)\n",
        "\n",
        "def late_window_indices_for_b(b: int):\n",
        "    \"\"\"Return list of valid token indices in the late window for prompt b.\"\"\"\n",
        "    last = int(t_last[b].item())\n",
        "    t0 = max(0, last - (LATE_W - 1))\n",
        "    idxs = list(range(t0, last + 1))\n",
        "    if SKIP_T0_IN_PCA:\n",
        "        idxs = [t for t in idxs if t != 0]\n",
        "    return idxs\n",
        "\n",
        "late_idxs = [late_window_indices_for_b(b) for b in range(B)]\n",
        "\n",
        "# ---------------------------\n",
        "# Baseline logits (for Paris margin)\n",
        "# ---------------------------\n",
        "with torch.no_grad():\n",
        "    logits_base, _infos_base = forward_with_info(input_ids, attention_mask, routing_mode=\"softmax\")\n",
        "    logp = F.log_softmax(logits_base, dim=-1)  # [B,T,V]\n",
        "    idx = torch.arange(B, device=DEVICE)\n",
        "    lp_paris = logp[idx, t_last, PARIS_ID].detach().cpu().numpy()\n",
        "    lp_rival = logp[idx, t_last, RIVAL_ID].detach().cpu().numpy()\n",
        "    paris_margin = lp_paris - lp_rival\n",
        "\n",
        "# ---------------------------\n",
        "# PCA fit utilities (PyTorch SVD on GPU; fallback to CPU if needed)\n",
        "# ---------------------------\n",
        "def _finite_mask(x: torch.Tensor) -> torch.Tensor:\n",
        "    return torch.isfinite(x).all(dim=-1)\n",
        "\n",
        "def fit_pca3_from_points(X_points: torch.Tensor):\n",
        "    \"\"\"\n",
        "    X_points: [N,D] float32\n",
        "    returns: mu [D], W3 [D,3], EVR [3], evals [3]\n",
        "    \"\"\"\n",
        "    # sanitize\n",
        "    if SANITIZE_MODE == \"clamp\":\n",
        "        X_points = torch.nan_to_num(X_points, nan=0.0, posinf=CLAMP_VAL, neginf=-CLAMP_VAL)\n",
        "        good = _finite_mask(X_points)\n",
        "    else:\n",
        "        good = _finite_mask(X_points)\n",
        "        X_points = X_points[good]\n",
        "\n",
        "    n_good = int(X_points.shape[0])\n",
        "    if n_good < 10:\n",
        "        raise RuntimeError(f\"Too few finite points to fit PCA (good={n_good}).\")\n",
        "\n",
        "    mu = X_points.mean(dim=0, keepdim=True)\n",
        "    Xc = X_points - mu\n",
        "\n",
        "    # Attempt GPU SVD; fallback to CPU robustly\n",
        "    try:\n",
        "        U, S, Vh = torch.linalg.svd(Xc, full_matrices=False)\n",
        "    except Exception:\n",
        "        Xc_cpu = Xc.detach().cpu()\n",
        "        U, S, Vh = torch.linalg.svd(Xc_cpu, full_matrices=False)\n",
        "        # move back to original device for W/mu usage\n",
        "        Vh = Vh.to(Xc.device)\n",
        "        S = S.to(Xc.device)\n",
        "\n",
        "    W3 = Vh[:3].T.contiguous()  # [D,3]\n",
        "\n",
        "    # explained variance ratio via singular values\n",
        "    s2 = (S ** 2)\n",
        "    evr3 = (s2[:3] / s2.sum().clamp_min(1e-12)).detach().cpu().numpy()\n",
        "    evals3 = s2[:3].detach().cpu().numpy()\n",
        "    return mu.squeeze(0), W3, evr3, evals3, n_good\n",
        "\n",
        "def project_to_sphere(Z: torch.Tensor, mu: torch.Tensor, W3: torch.Tensor, eps=1e-9):\n",
        "    \"\"\"\n",
        "    Z: [N,D]\n",
        "    returns: XYZ [N,3] (raw PCA), XYZs [N,3] (sphere)\n",
        "    \"\"\"\n",
        "    X3 = (Z - mu[None, :]) @ W3  # [N,3]\n",
        "    X3_np = X3.detach().cpu().numpy()\n",
        "    n = np.linalg.norm(X3_np, axis=1, keepdims=True)\n",
        "    X3s_np = X3_np / np.clip(n, eps, None)\n",
        "    return X3_np, X3s_np\n",
        "\n",
        "# ---------------------------\n",
        "# Routing override machinery (forces read_weights at chosen positions)\n",
        "# ---------------------------\n",
        "def clear_routing_override():\n",
        "    for li in range(len(model.blocks)):\n",
        "        model.blocks[li].asa.routing_override = None\n",
        "\n",
        "def set_override_for_layer(layer_idx: int, r_bhk: torch.Tensor, t_targets_cpu: np.ndarray):\n",
        "    \"\"\"\n",
        "    r_bhk: [B,H,K] override distribution for each example at its chosen position (here, t_last)\n",
        "    t_targets_cpu: [B] indices in [0..T-1]\n",
        "    \"\"\"\n",
        "    clear_routing_override()\n",
        "\n",
        "    def _override(t0, t1, read_logits_chunk, read_logits_key, read_logits_content, ctx):\n",
        "        # read_logits_chunk: [B,H,L,K]\n",
        "        Bc, Hc, Lc, Kc = read_logits_chunk.shape\n",
        "        rtemp = float(ctx.get(\"rtemp\", 1.0))\n",
        "        w = torch.softmax(read_logits_chunk / max(1e-6, rtemp), dim=-1)\n",
        "        for b in range(Bc):\n",
        "            tt = int(t_targets_cpu[b])\n",
        "            if t0 <= tt < t1:\n",
        "                lp = tt - t0\n",
        "                w[b, :, lp, :] = r_bhk[b]  # [H,K]\n",
        "        w = w / w.sum(dim=-1, keepdim=True).clamp_min(1e-8)\n",
        "        return w\n",
        "\n",
        "    model.blocks[layer_idx].asa.routing_override = _override\n",
        "\n",
        "@torch.no_grad()\n",
        "def paris_margin_under_forced_routing(layer_idx: int, r_bhk: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Force routing at t_last for the specified layer only, and return mean Paris-margin.\n",
        "    \"\"\"\n",
        "    t_targets_cpu = t_last.detach().cpu().numpy()\n",
        "    set_override_for_layer(layer_idx, r_bhk, t_targets_cpu)\n",
        "    logits, _infos = forward_with_info(input_ids, attention_mask, routing_mode=\"softmax\")\n",
        "    clear_routing_override()\n",
        "    logp = F.log_softmax(logits, dim=-1)\n",
        "    idx = torch.arange(B, device=logits.device)\n",
        "    lpP = logp[idx, t_last, PARIS_ID]\n",
        "    lpR = logp[idx, t_last, RIVAL_ID]\n",
        "    m = (lpP - lpR).mean().item()\n",
        "    return m\n",
        "\n",
        "# ---------------------------\n",
        "# Main per-layer computation\n",
        "# ---------------------------\n",
        "def layer_control_field(layer_idx: int):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      dict with trajectories on sphere, centroids, tangent arrows, PCA EVR, etc.\n",
        "    \"\"\"\n",
        "    # Forward to collect read_logits for this layer\n",
        "    with torch.no_grad():\n",
        "        _logits, infos = forward_with_info(input_ids, attention_mask, routing_mode=\"softmax\")\n",
        "    rl = infos[layer_idx].get(\"read_logits\", None)\n",
        "    if rl is None:\n",
        "        raise KeyError(f\"infos[{layer_idx}] missing 'read_logits'.\")\n",
        "\n",
        "    # rl: [B,H,T,K] -> Z: [B,T,K] (head mean)\n",
        "    Z = rl.mean(dim=1).float()  # [B,T,K]\n",
        "    K = Z.shape[-1]\n",
        "    H = rl.shape[1]\n",
        "\n",
        "    # Collect late-window points for PCA fit: points are in D=K space\n",
        "    pts = []\n",
        "    owner = []  # (b, t) mapping for each point\n",
        "    for b in range(B):\n",
        "        for t in late_idxs[b]:\n",
        "            pts.append(Z[b, t, :])\n",
        "            owner.append((b, t))\n",
        "    Zfit = torch.stack(pts, dim=0)  # [N,D]\n",
        "\n",
        "    # Fit PCA3\n",
        "    mu, W3, evr3, evals3, n_good = fit_pca3_from_points(Zfit)\n",
        "\n",
        "    # Project all late points to sphere for trajectories + centroids\n",
        "    # We'll also project ALL time points for visualization (but PCA fit is late-window)\n",
        "    # For plotting trajectories we project each prompt's valid points.\n",
        "    traj_sphere = []\n",
        "    traj_raw = []\n",
        "    for b in range(B):\n",
        "        valid_t = [t for t in range(T) if int(attention_mask[b, t].item()) == 1]\n",
        "        if SKIP_T0_IN_PCA:\n",
        "            valid_t = [t for t in valid_t if t != 0]\n",
        "        if len(valid_t) == 0:\n",
        "            traj_sphere.append(np.zeros((0, 3)))\n",
        "            traj_raw.append(np.zeros((0, 3)))\n",
        "            continue\n",
        "        Zb = Z[b, valid_t, :]  # [Tv,K]\n",
        "        X3_np, X3s_np = project_to_sphere(Zb, mu, W3)\n",
        "        traj_raw.append(X3_np[::PLOT_STRIDE])\n",
        "        traj_sphere.append(X3s_np[::PLOT_STRIDE])\n",
        "\n",
        "    # Late-window centroids on sphere (mean of sphere points then renormalize)\n",
        "    centroids = np.zeros((B, 3), dtype=np.float32)\n",
        "    late_points_s = [[] for _ in range(B)]\n",
        "    for (b, t), z in zip(owner, pts):\n",
        "        X3_np, X3s_np = project_to_sphere(z[None, :], mu, W3)\n",
        "        late_points_s[b].append(X3s_np[0])\n",
        "    for b in range(B):\n",
        "        arr = np.array(late_points_s[b], dtype=np.float32)\n",
        "        if arr.size == 0:\n",
        "            centroids[b] = np.array([1.0, 0.0, 0.0], dtype=np.float32)\n",
        "        else:\n",
        "            c = arr.mean(axis=0)\n",
        "            c = c / max(1e-9, float(np.linalg.norm(c)))\n",
        "            centroids[b] = c\n",
        "\n",
        "    # ----- FD gradient in PCA coords (3D) per prompt -----\n",
        "    # For each late point (b,t), define a routing distribution from the PCA coordinate:\n",
        "    #   z' = mu + (x3 @ W3^T)   (inverse map into K-space)\n",
        "    #   r = softmax(z')\n",
        "    # Then we apply that same r at t_last across the batch, and measure mean Paris-margin.\n",
        "    #\n",
        "    # Important: This FD is \"global scalar margin sensitivity\" to changing that K-dim routing mixture\n",
        "    # at a chosen layer, expressed in the PCA(3) coordinates used for visualization.\n",
        "    #\n",
        "    # We'll compute per-(b,t) FD grads, then average over late points for each prompt b.\n",
        "    def pca3_to_zk(x3: torch.Tensor):\n",
        "        # x3: [N,3] -> z: [N,K]\n",
        "        return mu.to(x3.device)[None, :] + (x3 @ W3.T.to(x3.device))\n",
        "\n",
        "    def x3_to_rk(x3: torch.Tensor):\n",
        "        zk = pca3_to_zk(x3)  # [N,K]\n",
        "        return torch.softmax(zk, dim=-1)\n",
        "\n",
        "    # baseline r at each late point:\n",
        "    # compute x3 for each late point (owner order)\n",
        "    # We'll do this on GPU\n",
        "    x3_list = []\n",
        "    with torch.no_grad():\n",
        "        for z in pts:\n",
        "            x3 = ((z - mu.to(z.device)) @ W3.to(z.device))  # [3]\n",
        "            x3_list.append(x3)\n",
        "    X3 = torch.stack(x3_list, dim=0).to(DEVICE)  # [N,3]\n",
        "\n",
        "    # FD gradients for each late point\n",
        "    N = X3.shape[0]\n",
        "    grads = torch.zeros((N, 3), device=DEVICE, dtype=torch.float32)\n",
        "\n",
        "    def eval_margin_for_x3(x3_point: torch.Tensor):\n",
        "        # x3_point: [3] -> r_k [K] -> r_bhk [B,H,K] (broadcast same mixture across batch)\n",
        "        r_k = x3_to_rk(x3_point[None, :])[0]  # [K]\n",
        "        r_bhk = r_k.view(1, 1, K).expand(B, H, K).contiguous()\n",
        "        return paris_margin_under_forced_routing(layer_idx, r_bhk)\n",
        "\n",
        "    # Use a progress bar: per-layer FD is N*(2*3) evals in central mode\n",
        "    it = tqdm(range(N), desc=f\"FD grads layer {layer_idx}\", leave=False)\n",
        "    for i in it:\n",
        "        x = X3[i]\n",
        "        if FD_MODE == \"central\":\n",
        "            for d in range(3):\n",
        "                e = torch.zeros_like(x)\n",
        "                e[d] = FD_EPS\n",
        "                m_plus = eval_margin_for_x3(x + e)\n",
        "                m_minus = eval_margin_for_x3(x - e)\n",
        "                grads[i, d] = (m_plus - m_minus) / (2.0 * FD_EPS)\n",
        "        else:\n",
        "            m0 = eval_margin_for_x3(x)\n",
        "            for d in range(3):\n",
        "                e = torch.zeros_like(x)\n",
        "                e[d] = FD_EPS\n",
        "                m_plus = eval_margin_for_x3(x + e)\n",
        "                grads[i, d] = (m_plus - m0) / FD_EPS\n",
        "\n",
        "    # Average per prompt over its late points\n",
        "    g_prompt = torch.zeros((B, 3), device=DEVICE)\n",
        "    counts = torch.zeros((B,), device=DEVICE)\n",
        "    for i, (b, t) in enumerate(owner):\n",
        "        g_prompt[b] += grads[i]\n",
        "        counts[b] += 1.0\n",
        "    g_prompt = g_prompt / counts.clamp_min(1.0).unsqueeze(-1)\n",
        "\n",
        "    # Project gradient to tangent plane at centroid x on sphere (in R^3)\n",
        "    g_np = g_prompt.detach().cpu().numpy()\n",
        "    tangent = np.zeros_like(g_np)\n",
        "    for b in range(B):\n",
        "        x = centroids[b]\n",
        "        g = g_np[b]\n",
        "        # remove radial component\n",
        "        tangent[b] = g - np.dot(g, x) * x\n",
        "        # normalize for plotting (avoid huge variance)\n",
        "        n = np.linalg.norm(tangent[b])\n",
        "        if n > 1e-9:\n",
        "            tangent[b] = tangent[b] / n\n",
        "        else:\n",
        "            tangent[b] = np.zeros(3, dtype=np.float32)\n",
        "\n",
        "    return {\n",
        "        \"layer\": layer_idx,\n",
        "        \"traj_sphere\": traj_sphere,\n",
        "        \"centroids\": centroids,\n",
        "        \"tangent_dir\": tangent,\n",
        "        \"paris_margin\": paris_margin,\n",
        "        \"evr3\": evr3,\n",
        "        \"evals3\": evals3,\n",
        "        \"n_fit\": n_good,\n",
        "    }\n",
        "\n",
        "# ---------------------------\n",
        "# Plotting: sphere + trajectories + centroid points + tangent arrows\n",
        "# ---------------------------\n",
        "def plot_layer_field(res):\n",
        "    layer = res[\"layer\"]\n",
        "    traj_s = res[\"traj_sphere\"]\n",
        "    cent = res[\"centroids\"]\n",
        "    tang = res[\"tangent_dir\"]\n",
        "    pm = res[\"paris_margin\"]\n",
        "    evr3 = res[\"evr3\"]\n",
        "\n",
        "    fig = plt.figure(figsize=(11.0, 8.8))\n",
        "    ax = fig.add_subplot(111, projection=\"3d\")\n",
        "\n",
        "    # faint sphere\n",
        "    u = np.linspace(0, 2*np.pi, 48)\n",
        "    v = np.linspace(0, np.pi, 24)\n",
        "    xs = np.outer(np.cos(u), np.sin(v))\n",
        "    ys = np.outer(np.sin(u), np.sin(v))\n",
        "    zs = np.outer(np.ones_like(u), np.cos(v))\n",
        "    if SPHERE_WIREFRAME:\n",
        "        ax.plot_wireframe(xs, ys, zs, rstride=3, cstride=3, linewidth=0.5, alpha=SPHERE_ALPHA)\n",
        "    else:\n",
        "        ax.plot_surface(xs, ys, zs, alpha=SPHERE_ALPHA, linewidth=0)\n",
        "\n",
        "    # plot trajectories\n",
        "    for b in range(B):\n",
        "        pts = traj_s[b]\n",
        "        if pts.shape[0] >= 2:\n",
        "            ax.plot(pts[:,0], pts[:,1], pts[:,2], linewidth=TRAJ_LW, alpha=TRAJ_ALPHA)\n",
        "\n",
        "    # centroid points colored by Paris-margin\n",
        "    import matplotlib as mpl\n",
        "    normc = mpl.colors.Normalize(vmin=float(np.min(pm)), vmax=float(np.max(pm)))\n",
        "    cmap = plt.cm.viridis\n",
        "\n",
        "    colors = cmap(normc(pm))\n",
        "    ax.scatter(cent[:,0], cent[:,1], cent[:,2], c=colors, s=70, alpha=0.95)\n",
        "\n",
        "    # tangent arrows (quiver)\n",
        "    # draw small arrows starting at centroid, pointing along tangent_dir\n",
        "    for b in range(B):\n",
        "        x = cent[b]\n",
        "        d = tang[b]\n",
        "        if np.linalg.norm(d) < 1e-9:\n",
        "            continue\n",
        "        ax.quiver(\n",
        "            x[0], x[1], x[2],\n",
        "            d[0], d[1], d[2],\n",
        "            length=ARROW_SCALE,\n",
        "            normalize=True,\n",
        "            alpha=ARROW_ALPHA,\n",
        "            linewidth=1.5,\n",
        "        )\n",
        "\n",
        "    # label top prompts by Paris-margin\n",
        "    order = np.argsort(-pm)\n",
        "    for i in order[: min(LABEL_MAX, B)]:\n",
        "        x = cent[i]\n",
        "        ax.text(x[0], x[1], x[2], f\" {i}\", fontsize=10)\n",
        "\n",
        "    # colorbar\n",
        "    sm = mpl.cm.ScalarMappable(cmap=cmap, norm=normc)\n",
        "    sm.set_array([])\n",
        "    cbar = plt.colorbar(sm, ax=ax, fraction=0.03, pad=0.08)\n",
        "    cbar.set_label(f\"logP(' Paris') - logP({RIVAL_TEXT!r})\")\n",
        "\n",
        "    ax.set_title(\n",
        "        f\"Tangent Control Field on Simplex Sphere (late-window centroid)\\n\"\n",
        "        f\"layer={layer} | late_W={LATE_W} | FD_EPS={FD_EPS} ({FD_MODE}) | PCA EVR(PC1..3){np.round(evr3,4)}\"\n",
        "    )\n",
        "    ax.set_xlabel(\"PC1 (sphere)\")\n",
        "    ax.set_ylabel(\"PC2 (sphere)\")\n",
        "    ax.set_zlabel(\"PC3 (sphere)\")\n",
        "    ax.set_box_aspect([1,1,1])\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # quick textual readout\n",
        "    print(f\"[layer {layer}] PCA EVR(PC1..3) {np.round(evr3,4)} | margins: mean={pm.mean():.4f} med={np.median(pm):.4f} min={pm.min():.4f} max={pm.max():.4f}\")\n",
        "    top = order[:5].tolist()\n",
        "    print(\"[top Paris-margin prompts]\", [(i, float(pm[i]), PROMPTS[i][:40]) for i in top])\n",
        "\n",
        "# ---------------------------\n",
        "# Run all requested layers\n",
        "# ---------------------------\n",
        "print(f\"[info] prompts B={B} | seq_len T={T} | late_W={LATE_W} | layers={LAYERS}\")\n",
        "print(\"[note] This cell performs FD evaluations and may take a bit; tqdm shows progress per layer.\")\n",
        "\n",
        "results = {}\n",
        "for L in LAYERS:\n",
        "    res = layer_control_field(int(L))\n",
        "    results[int(L)] = res\n",
        "    plot_layer_field(res)\n",
        "\n",
        "# ---------------------------\n",
        "# Optional: compact comparison summary\n",
        "# ---------------------------\n",
        "print(\"\\n=== Tangent field summary (per layer) ===\")\n",
        "for L in LAYERS:\n",
        "    res = results[int(L)]\n",
        "    # mean resultant length of tangent directions (proxy for \"field coherence\")\n",
        "    tang = res[\"tangent_dir\"]\n",
        "    v = tang.mean(axis=0)\n",
        "    coh = float(np.linalg.norm(v))\n",
        "    print(f\"L{L:2d} | PCA EVR={np.round(res['evr3'],3)} | tangent_coherence={coh:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "q53vFn1oQ5DA"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ================================================================\n",
        "#@title Next Cell (FULL REPLACEMENT): Prompt-Conditional Tangent Control Field on Simplex Sphere (Late-Window Centroid)\n",
        "# For layers [0,5,13]:\n",
        "#   - Extract router read_logits trajectories (head-mean) per prompt\n",
        "#   - Fit PCA->3D on late-window points, project to S^2\n",
        "#   - Compute **PROMPT-CONDITIONAL** FD gradient of each prompt's Paris-margin\n",
        "#       wrt a 3D PCA coordinate at each late-window point\n",
        "#       (override routing ONLY for that prompt b at t_last[b], not the whole batch)\n",
        "#   - Average per-prompt gradients across late-window points\n",
        "#   - Project gradient to tangent plane at each prompt's late-window centroid on the sphere\n",
        "#   - Plot: trajectories + centroid points (colored by Paris-margin) + tangent arrows (control direction)\n",
        "#   - PLUS: alignment diagnostics (control tangent vs late velocity), and centroid-distance vs margin scatter\n",
        "#\n",
        "# Robustness:\n",
        "#   - Enforces RIGHT padding + right padding side (important to avoid NaNs)\n",
        "#   - Sanitizes non-finite values in PCA fit\n",
        "#   - Uses FD (finite difference), no autograd through infos required\n",
        "#   - Includes tqdm progress bars\n",
        "#\n",
        "# Assumptions:\n",
        "#   - model, cfg, DEVICE exist\n",
        "#   - model(..., return_info=True, routing_mode=\"softmax\") -> (logits, infos)\n",
        "#   - infos[layer][\"read_logits\"] exists, shape [B,H,T,K]\n",
        "#   - model.blocks[li].asa.routing_override exists and is consulted by ASA\n",
        "# ================================================================\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ---------------------------\n",
        "# SETTINGS\n",
        "# ---------------------------\n",
        "LAYERS = [0, 5, 13]\n",
        "LATE_W = 5                    # late-window size (last W valid tokens)\n",
        "FD_EPS = 1e-2                 # finite difference step in PCA coords\n",
        "FD_MODE = \"central\"           # \"central\" or \"forward\"\n",
        "PLOT_STRIDE = 1               # downsample trajectory points\n",
        "ARROW_SCALE = 0.22            # arrow length scaling on sphere\n",
        "ARROW_ALPHA = 0.9\n",
        "TRAJ_ALPHA = 0.45\n",
        "TRAJ_LW = 1.8\n",
        "SPHERE_ALPHA = 0.06\n",
        "SPHERE_WIREFRAME = True\n",
        "LABEL_MAX = 10                # label top prompts by Paris-margin\n",
        "SKIP_T0_IN_PCA = True         # skip token index 0 (often special / unstable)\n",
        "SANITIZE_MODE = \"drop\"        # \"drop\" or \"clamp\" for non-finite read_logits in PCA fit\n",
        "CLAMP_VAL = 5.0               # used if SANITIZE_MODE=\"clamp\"\n",
        "MIN_PCA_POINTS = 12           # minimal good points for PCA\n",
        "\n",
        "# Alignment diagnostics\n",
        "DO_ALIGNMENT_PANELS = True\n",
        "VEL_EPS = 1e-9\n",
        "\n",
        "# prompts\n",
        "PROMPTS = [\n",
        "    \"The capital of France is\",\n",
        "    \"France's capital city is\",\n",
        "    \"In France, the capital is\",\n",
        "    \"The seat of government of France is\",\n",
        "    \"The city that is France's capital is\",\n",
        "    \"The Eiffel Tower is located in\",\n",
        "    \"The Louvre is in\",\n",
        "    \"The Seine river flows through\",\n",
        "    \"During the French Revolution, the city of\",\n",
        "    \"Napoleon crowned himself in\",\n",
        "    \"In World War II, the liberation of\",\n",
        "    \"The largest city in France is\",\n",
        "    \"A major European city in France is\",\n",
        "    \"A famous city on the Seine is\",\n",
        "    \"The capital of Germany is\",\n",
        "    \"The capital of England is\",\n",
        "    \"The capital of Italy is\",\n",
        "]\n",
        "\n",
        "RIVAL_TEXT = \" the\"  # single-token rival for Paris margin\n",
        "\n",
        "# ---------------------------\n",
        "# Tokenizer helpers (RIGHT padding enforced)\n",
        "# ---------------------------\n",
        "def _get_tokenizer_from_context():\n",
        "    if \"tokenizer\" in globals():\n",
        "        tok = globals()[\"tokenizer\"]\n",
        "        tok.padding_side = \"right\"\n",
        "        if tok.pad_token is None:\n",
        "            tok.pad_token = tok.eos_token\n",
        "        return tok\n",
        "    from transformers import AutoTokenizer\n",
        "    tok = AutoTokenizer.from_pretrained(getattr(cfg, \"tokenizer_name\", \"gpt2\"))\n",
        "    tok.padding_side = \"right\"\n",
        "    if tok.pad_token is None:\n",
        "        tok.pad_token = tok.eos_token\n",
        "    return tok\n",
        "\n",
        "tokenizer = _get_tokenizer_from_context()\n",
        "\n",
        "def ensure_single_token(text: str) -> int:\n",
        "    ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "    if len(ids) != 1:\n",
        "        raise ValueError(f\"{text!r} must be exactly 1 token. Got ids={ids}\")\n",
        "    return ids[0]\n",
        "\n",
        "PARIS_ID = ensure_single_token(\" Paris\")\n",
        "RIVAL_ID = ensure_single_token(RIVAL_TEXT)\n",
        "\n",
        "# ---------------------------\n",
        "# Forward with info\n",
        "# ---------------------------\n",
        "def forward_with_info(ids, mask, routing_mode=\"softmax\"):\n",
        "    logits, infos = model(ids, attention_mask=mask, return_info=True, routing_mode=routing_mode)\n",
        "    return logits, infos\n",
        "\n",
        "# ---------------------------\n",
        "# Batch tokenize (RIGHT padding)\n",
        "# ---------------------------\n",
        "enc = tokenizer(\n",
        "    PROMPTS,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=cfg.max_seq_len,\n",
        ")\n",
        "input_ids = enc[\"input_ids\"].to(DEVICE)\n",
        "attention_mask = enc.get(\"attention_mask\", torch.ones_like(input_ids)).to(DEVICE)\n",
        "\n",
        "B, T = input_ids.shape\n",
        "lengths = attention_mask.sum(dim=1).long()\n",
        "t_last = (lengths - 1).clamp(min=0)\n",
        "\n",
        "def valid_ts_for_b(b: int):\n",
        "    ts = [t for t in range(T) if int(attention_mask[b, t].item()) == 1]\n",
        "    if SKIP_T0_IN_PCA:\n",
        "        ts = [t for t in ts if t != 0]\n",
        "    return ts\n",
        "\n",
        "def late_window_indices_for_b(b: int):\n",
        "    last = int(t_last[b].item())\n",
        "    t0 = max(0, last - (LATE_W - 1))\n",
        "    idxs = list(range(t0, last + 1))\n",
        "    if SKIP_T0_IN_PCA:\n",
        "        idxs = [t for t in idxs if t != 0]\n",
        "    return idxs\n",
        "\n",
        "late_idxs = [late_window_indices_for_b(b) for b in range(B)]\n",
        "valid_idxs = [valid_ts_for_b(b) for b in range(B)]\n",
        "\n",
        "# ---------------------------\n",
        "# Baseline logits (for Paris margin, per prompt)\n",
        "# ---------------------------\n",
        "with torch.no_grad():\n",
        "    logits_base, _infos_base = forward_with_info(input_ids, attention_mask, routing_mode=\"softmax\")\n",
        "    logp = F.log_softmax(logits_base, dim=-1)  # [B,T,V]\n",
        "    idx = torch.arange(B, device=DEVICE)\n",
        "    lp_paris = logp[idx, t_last, PARIS_ID].detach().cpu().numpy()\n",
        "    lp_rival = logp[idx, t_last, RIVAL_ID].detach().cpu().numpy()\n",
        "    paris_margin = lp_paris - lp_rival\n",
        "baseline_mean_margin = float(np.mean(paris_margin))\n",
        "\n",
        "# ---------------------------\n",
        "# PCA fit utilities (SVD with robust fallback)\n",
        "# ---------------------------\n",
        "def _finite_mask_lastdim(x: torch.Tensor) -> torch.Tensor:\n",
        "    return torch.isfinite(x).all(dim=-1)\n",
        "\n",
        "def fit_pca3_from_points(X_points: torch.Tensor):\n",
        "    \"\"\"\n",
        "    X_points: [N,D] float32\n",
        "    returns: mu [D], W3 [D,3], EVR [3], evals [3], n_good\n",
        "    \"\"\"\n",
        "    if SANITIZE_MODE == \"clamp\":\n",
        "        X_points = torch.nan_to_num(X_points, nan=0.0, posinf=CLAMP_VAL, neginf=-CLAMP_VAL)\n",
        "        good = _finite_mask_lastdim(X_points)\n",
        "    else:\n",
        "        good = _finite_mask_lastdim(X_points)\n",
        "        X_points = X_points[good]\n",
        "\n",
        "    n_good = int(X_points.shape[0])\n",
        "    if n_good < MIN_PCA_POINTS:\n",
        "        raise RuntimeError(f\"Too few finite points to fit PCA (good={n_good}).\")\n",
        "\n",
        "    mu = X_points.mean(dim=0, keepdim=True)\n",
        "    Xc = X_points - mu\n",
        "\n",
        "    # try GPU SVD then fallback to CPU\n",
        "    try:\n",
        "        U, S, Vh = torch.linalg.svd(Xc, full_matrices=False)\n",
        "        Vh = Vh.to(Xc.device)\n",
        "        S = S.to(Xc.device)\n",
        "    except Exception:\n",
        "        Xc_cpu = Xc.detach().cpu()\n",
        "        U, S, Vh = torch.linalg.svd(Xc_cpu, full_matrices=False)\n",
        "        Vh = Vh.to(Xc.device)\n",
        "        S = S.to(Xc.device)\n",
        "\n",
        "    W3 = Vh[:3].T.contiguous()  # [D,3]\n",
        "    s2 = (S ** 2)\n",
        "    evr3 = (s2[:3] / s2.sum().clamp_min(1e-12)).detach().cpu().numpy()\n",
        "    evals3 = s2[:3].detach().cpu().numpy()\n",
        "    return mu.squeeze(0), W3, evr3, evals3, n_good\n",
        "\n",
        "def project_to_sphere_from_Z(Z_points: torch.Tensor, mu: torch.Tensor, W3: torch.Tensor, eps=1e-9):\n",
        "    \"\"\"\n",
        "    Z_points: [N,D] on device\n",
        "    returns: X3_np [N,3] raw, X3s_np [N,3] sphere\n",
        "    \"\"\"\n",
        "    X3 = (Z_points - mu[None, :]) @ W3  # [N,3]\n",
        "    X3_np = X3.detach().cpu().numpy()\n",
        "    n = np.linalg.norm(X3_np, axis=1, keepdims=True)\n",
        "    X3s_np = X3_np / np.clip(n, eps, None)\n",
        "    return X3_np, X3s_np\n",
        "\n",
        "# ---------------------------\n",
        "# Routing override machinery (PROMPT-CONDITIONAL forcing)\n",
        "# ---------------------------\n",
        "def clear_routing_override():\n",
        "    for li in range(len(model.blocks)):\n",
        "        model.blocks[li].asa.routing_override = None\n",
        "\n",
        "def set_override_for_layer_prompt_only(layer_idx: int, b_force: int, t_force: int, r_hk: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Force routing ONLY for example b_force at token position t_force, for this layer.\n",
        "    r_hk: [H,K] distribution per head (typically same across heads)\n",
        "    \"\"\"\n",
        "    clear_routing_override()\n",
        "\n",
        "    def _override(t0, t1, read_logits_chunk, read_logits_key, read_logits_content, ctx):\n",
        "        # read_logits_chunk: [B,H,L,K]\n",
        "        Bc, Hc, Lc, Kc = read_logits_chunk.shape\n",
        "        rtemp = float(ctx.get(\"rtemp\", 1.0))\n",
        "        w = torch.softmax(read_logits_chunk / max(1e-6, rtemp), dim=-1)\n",
        "\n",
        "        # override only if this chunk contains t_force\n",
        "        if t0 <= t_force < t1:\n",
        "            lp = t_force - t0\n",
        "            w[b_force, :, lp, :] = r_hk  # [H,K]\n",
        "\n",
        "        w = w / w.sum(dim=-1, keepdim=True).clamp_min(1e-8)\n",
        "        return w\n",
        "\n",
        "    model.blocks[layer_idx].asa.routing_override = _override\n",
        "\n",
        "@torch.no_grad()\n",
        "def paris_margin_for_prompt_under_forced_routing(layer_idx: int, b_force: int, r_k: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Force routing for prompt b_force at t_last[b_force] in given layer, return that prompt's Paris-margin (scalar).\n",
        "    r_k: [K] distribution; broadcast to [H,K]\n",
        "    \"\"\"\n",
        "    t_force = int(t_last[b_force].item())\n",
        "\n",
        "    # build [H,K] for heads\n",
        "    # (head-specific not needed; copy across heads)\n",
        "    with torch.no_grad():\n",
        "        # Determine H from model/instrumentation by running once? We'll pass H from enclosing scope where needed.\n",
        "        pass\n",
        "\n",
        "# We'll fill the above with H once we know H from read_logits per layer.\n",
        "\n",
        "# ---------------------------\n",
        "# Main per-layer computation\n",
        "# ---------------------------\n",
        "def layer_control_field_prompt_conditional(layer_idx: int):\n",
        "    \"\"\"\n",
        "    Returns dict with trajectories on sphere, per-prompt centroids, tangent arrows,\n",
        "    plus alignment diagnostics and centroid-distance stats.\n",
        "    \"\"\"\n",
        "    # Forward to collect read_logits for this layer\n",
        "    with torch.no_grad():\n",
        "        _logits, infos = forward_with_info(input_ids, attention_mask, routing_mode=\"softmax\")\n",
        "\n",
        "    rl = infos[layer_idx].get(\"read_logits\", None)\n",
        "    if rl is None:\n",
        "        raise KeyError(f\"infos[{layer_idx}] missing 'read_logits'. Keys: {list(infos[layer_idx].keys())}\")\n",
        "\n",
        "    # rl: [B,H,T,K] -> Z: [B,T,K] (head mean)\n",
        "    H = rl.shape[1]\n",
        "    Z = rl.mean(dim=1).float()  # [B,T,K]\n",
        "    K = Z.shape[-1]\n",
        "\n",
        "    # Collect late-window points for PCA fit: points are in D=K space\n",
        "    pts = []\n",
        "    owner = []  # list of (b,t) for each point\n",
        "    for b in range(B):\n",
        "        for t in late_idxs[b]:\n",
        "            pts.append(Z[b, t, :])\n",
        "            owner.append((b, t))\n",
        "    Zfit = torch.stack(pts, dim=0)  # [N,K]\n",
        "\n",
        "    # Fit PCA3\n",
        "    mu, W3, evr3, evals3, n_good = fit_pca3_from_points(Zfit)\n",
        "\n",
        "    # Precompute per-prompt trajectories projected to sphere (for display)\n",
        "    traj_sphere = []\n",
        "    traj_raw = []\n",
        "    for b in range(B):\n",
        "        ts = valid_idxs[b]\n",
        "        if len(ts) == 0:\n",
        "            traj_sphere.append(np.zeros((0, 3)))\n",
        "            traj_raw.append(np.zeros((0, 3)))\n",
        "            continue\n",
        "        Zb = Z[b, ts, :]  # [Tv,K]\n",
        "        X3_np, X3s_np = project_to_sphere_from_Z(Zb, mu, W3)\n",
        "        traj_raw.append(X3_np[::PLOT_STRIDE])\n",
        "        traj_sphere.append(X3s_np[::PLOT_STRIDE])\n",
        "\n",
        "    # Late-window centroids on sphere (mean of late sphere points then renormalize)\n",
        "    centroids = np.zeros((B, 3), dtype=np.float32)\n",
        "    late_points_s = [[] for _ in range(B)]\n",
        "    for (b, t) in owner:\n",
        "        z = Z[b, t, :][None, :]\n",
        "        _, x_s = project_to_sphere_from_Z(z, mu, W3)\n",
        "        late_points_s[b].append(x_s[0])\n",
        "    for b in range(B):\n",
        "        arr = np.array(late_points_s[b], dtype=np.float32)\n",
        "        if arr.size == 0:\n",
        "            centroids[b] = np.array([1.0, 0.0, 0.0], dtype=np.float32)\n",
        "        else:\n",
        "            c = arr.mean(axis=0)\n",
        "            c = c / max(1e-9, float(np.linalg.norm(c)))\n",
        "            centroids[b] = c\n",
        "\n",
        "    # Late-window velocity directions (on sphere): average normalized step directions in late window\n",
        "    vel_dir = np.zeros((B, 3), dtype=np.float32)\n",
        "    for b in range(B):\n",
        "        ts = late_idxs[b]\n",
        "        if len(ts) < 2:\n",
        "            continue\n",
        "        # use the projected sphere points at those times\n",
        "        pts_s = []\n",
        "        for t in ts:\n",
        "            z = Z[b, t, :][None, :]\n",
        "            _, x_s = project_to_sphere_from_Z(z, mu, W3)\n",
        "            pts_s.append(x_s[0])\n",
        "        pts_s = np.array(pts_s, dtype=np.float32)\n",
        "        steps = pts_s[1:] - pts_s[:-1]\n",
        "        n = np.linalg.norm(steps, axis=1, keepdims=True)\n",
        "        steps_u = steps / np.clip(n, VEL_EPS, None)\n",
        "        v = steps_u.mean(axis=0)\n",
        "        nv = float(np.linalg.norm(v))\n",
        "        if nv > 1e-9:\n",
        "            vel_dir[b] = v / nv\n",
        "\n",
        "    # Helper: inverse map PCA3 -> K-space logits -> routing dist\n",
        "    def pca3_to_rk(x3: torch.Tensor):\n",
        "        # x3: [3] -> z_k: [K] -> softmax\n",
        "        z_k = mu.to(x3.device) + (x3 @ W3.T.to(x3.device))  # [K]\n",
        "        return torch.softmax(z_k, dim=-1)  # [K]\n",
        "\n",
        "    # PROMPT-CONDITIONAL FD grads:\n",
        "    # For each late point belonging to prompt b, we:\n",
        "    #   - create routing dist r_k from that point's PCA3 coordinate\n",
        "    #   - override only prompt b at t_last[b] with r (broadcast across heads)\n",
        "    #   - evaluate that prompt b's Paris-margin only (not mean over batch)\n",
        "    #\n",
        "    # We estimate grad wrt PCA3 coords at that point, then average per prompt b.\n",
        "    N = len(owner)\n",
        "    X3_points = []\n",
        "    with torch.no_grad():\n",
        "        for (b, t) in owner:\n",
        "            x3 = (Z[b, t, :] - mu.to(Z.device)) @ W3.to(Z.device)  # [3]\n",
        "            X3_points.append(x3)\n",
        "    X3_points = torch.stack(X3_points, dim=0).to(DEVICE)  # [N,3]\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def eval_prompt_margin_at_x3(layer_idx: int, b_force: int, x3: torch.Tensor):\n",
        "        r_k = pca3_to_rk(x3)  # [K]\n",
        "        r_hk = r_k.view(1, K).expand(H, K).contiguous()  # [H,K]\n",
        "\n",
        "        t_force = int(t_last[b_force].item())\n",
        "        set_override_for_layer_prompt_only(layer_idx, b_force=b_force, t_force=t_force, r_hk=r_hk)\n",
        "\n",
        "        logits, _infos = forward_with_info(input_ids, attention_mask, routing_mode=\"softmax\")\n",
        "        clear_routing_override()\n",
        "\n",
        "        logp_loc = F.log_softmax(logits, dim=-1)\n",
        "        tt = int(t_last[b_force].item())\n",
        "        lpP = float(logp_loc[b_force, tt, PARIS_ID].item())\n",
        "        lpR = float(logp_loc[b_force, tt, RIVAL_ID].item())\n",
        "        return lpP - lpR\n",
        "\n",
        "    grads = torch.zeros((N, 3), device=DEVICE, dtype=torch.float32)\n",
        "\n",
        "    # tqdm over points; inner loop over 3 dims with 2 evals for central\n",
        "    pbar = tqdm(range(N), desc=f\"FD prompt-conditional grads | layer {layer_idx}\", leave=False)\n",
        "    for i in pbar:\n",
        "        b_force, _t_src = owner[i]\n",
        "        x = X3_points[i]\n",
        "\n",
        "        if FD_MODE == \"central\":\n",
        "            for d in range(3):\n",
        "                e = torch.zeros_like(x)\n",
        "                e[d] = FD_EPS\n",
        "                m_plus = eval_prompt_margin_at_x3(layer_idx, b_force, x + e)\n",
        "                m_minus = eval_prompt_margin_at_x3(layer_idx, b_force, x - e)\n",
        "                grads[i, d] = (m_plus - m_minus) / (2.0 * FD_EPS)\n",
        "        else:\n",
        "            m0 = eval_prompt_margin_at_x3(layer_idx, b_force, x)\n",
        "            for d in range(3):\n",
        "                e = torch.zeros_like(x)\n",
        "                e[d] = FD_EPS\n",
        "                m_plus = eval_prompt_margin_at_x3(layer_idx, b_force, x + e)\n",
        "                grads[i, d] = (m_plus - m0) / FD_EPS\n",
        "\n",
        "    # Average per prompt across its late points\n",
        "    g_prompt = torch.zeros((B, 3), device=DEVICE, dtype=torch.float32)\n",
        "    counts = torch.zeros((B,), device=DEVICE, dtype=torch.float32)\n",
        "    for i, (b, t) in enumerate(owner):\n",
        "        g_prompt[b] += grads[i]\n",
        "        counts[b] += 1.0\n",
        "    g_prompt = g_prompt / counts.clamp_min(1.0).unsqueeze(-1)\n",
        "    g_np = g_prompt.detach().cpu().numpy()\n",
        "\n",
        "    # Project to tangent plane at centroid and normalize for plotting\n",
        "    tangent_dir = np.zeros((B, 3), dtype=np.float32)\n",
        "    tangent_mag = np.zeros((B,), dtype=np.float32)\n",
        "    for b in range(B):\n",
        "        x = centroids[b]\n",
        "        g = g_np[b]\n",
        "        g_tan = g - float(np.dot(g, x)) * x\n",
        "        mag = float(np.linalg.norm(g_tan))\n",
        "        tangent_mag[b] = mag\n",
        "        if mag > 1e-9:\n",
        "            tangent_dir[b] = (g_tan / mag).astype(np.float32)\n",
        "\n",
        "    # Tangent coherence: norm of mean unit tangent (skip near-zero)\n",
        "    use = np.linalg.norm(tangent_dir, axis=1) > 0\n",
        "    if use.any():\n",
        "        mean_vec = tangent_dir[use].mean(axis=0)\n",
        "        tangent_coh = float(np.linalg.norm(mean_vec))\n",
        "    else:\n",
        "        tangent_coh = float(\"nan\")\n",
        "\n",
        "    # Alignment cosines: tangent_dir vs velocity_dir (both unit) (skip invalid)\n",
        "    align = np.full((B,), np.nan, dtype=np.float32)\n",
        "    for b in range(B):\n",
        "        if np.linalg.norm(tangent_dir[b]) > 0 and np.linalg.norm(vel_dir[b]) > 0:\n",
        "            align[b] = float(np.dot(tangent_dir[b], vel_dir[b]))\n",
        "\n",
        "    # \"Paris-attractor centroid\": centroid of top-k prompts by Paris margin in this layer's sphere coords\n",
        "    order = np.argsort(-paris_margin)\n",
        "    topk = min(5, B)\n",
        "    attract = centroids[order[:topk]].mean(axis=0)\n",
        "    attract = attract / max(1e-9, float(np.linalg.norm(attract)))\n",
        "\n",
        "    # Distances to attractor (geodesic-ish via dot; also euclidean on sphere is fine)\n",
        "    dot = np.clip((centroids * attract[None, :]).sum(axis=1), -1.0, 1.0)\n",
        "    ang = np.arccos(dot)  # [0,pi]\n",
        "    dist_to_attractor = ang.astype(np.float32)\n",
        "\n",
        "    return {\n",
        "        \"layer\": int(layer_idx),\n",
        "        \"traj_sphere\": traj_sphere,\n",
        "        \"centroids\": centroids,\n",
        "        \"tangent_dir\": tangent_dir,\n",
        "        \"tangent_mag\": tangent_mag,\n",
        "        \"vel_dir\": vel_dir,\n",
        "        \"align\": align,\n",
        "        \"paris_margin\": paris_margin.copy(),\n",
        "        \"evr3\": evr3,\n",
        "        \"evals3\": evals3,\n",
        "        \"n_fit\": int(n_good),\n",
        "        \"attractor\": attract,\n",
        "        \"dist_to_attractor\": dist_to_attractor,\n",
        "        \"tangent_coherence\": tangent_coh,\n",
        "    }\n",
        "\n",
        "# ---------------------------\n",
        "# Plotting helpers\n",
        "# ---------------------------\n",
        "def plot_layer_field(res):\n",
        "    layer = res[\"layer\"]\n",
        "    traj_s = res[\"traj_sphere\"]\n",
        "    cent = res[\"centroids\"]\n",
        "    tang = res[\"tangent_dir\"]\n",
        "    pm = res[\"paris_margin\"]\n",
        "    evr3 = res[\"evr3\"]\n",
        "    align = res[\"align\"]\n",
        "    dist = res[\"dist_to_attractor\"]\n",
        "    attract = res[\"attractor\"]\n",
        "\n",
        "    import matplotlib as mpl\n",
        "    normc = mpl.colors.Normalize(vmin=float(np.min(pm)), vmax=float(np.max(pm)))\n",
        "    cmap = plt.cm.viridis\n",
        "    colors = cmap(normc(pm))\n",
        "\n",
        "    # --- Main globe plot ---\n",
        "    fig = plt.figure(figsize=(11.0, 8.8))\n",
        "    ax = fig.add_subplot(111, projection=\"3d\")\n",
        "\n",
        "    # faint sphere\n",
        "    u = np.linspace(0, 2*np.pi, 48)\n",
        "    v = np.linspace(0, np.pi, 24)\n",
        "    xs = np.outer(np.cos(u), np.sin(v))\n",
        "    ys = np.outer(np.sin(u), np.sin(v))\n",
        "    zs = np.outer(np.ones_like(u), np.cos(v))\n",
        "    if SPHERE_WIREFRAME:\n",
        "        ax.plot_wireframe(xs, ys, zs, rstride=3, cstride=3, linewidth=0.5, alpha=SPHERE_ALPHA)\n",
        "    else:\n",
        "        ax.plot_surface(xs, ys, zs, alpha=SPHERE_ALPHA, linewidth=0)\n",
        "\n",
        "    # trajectories\n",
        "    for b in range(B):\n",
        "        pts = traj_s[b]\n",
        "        if pts.shape[0] >= 2:\n",
        "            ax.plot(pts[:, 0], pts[:, 1], pts[:, 2], linewidth=TRAJ_LW, alpha=TRAJ_ALPHA)\n",
        "\n",
        "    # centroid points\n",
        "    ax.scatter(cent[:, 0], cent[:, 1], cent[:, 2], c=colors, s=70, alpha=0.95)\n",
        "\n",
        "    # attractor marker\n",
        "    ax.scatter([attract[0]], [attract[1]], [attract[2]], s=220, marker=\"X\", alpha=0.95)\n",
        "\n",
        "    # tangent arrows (unit direction, constant length)\n",
        "    for b in range(B):\n",
        "        x = cent[b]\n",
        "        d = tang[b]\n",
        "        if float(np.linalg.norm(d)) < 1e-9:\n",
        "            continue\n",
        "        ax.quiver(\n",
        "            x[0], x[1], x[2],\n",
        "            d[0], d[1], d[2],\n",
        "            length=ARROW_SCALE,\n",
        "            normalize=True,\n",
        "            alpha=ARROW_ALPHA,\n",
        "            linewidth=1.4,\n",
        "        )\n",
        "\n",
        "    # label top prompts by Paris-margin\n",
        "    order = np.argsort(-pm)\n",
        "    for i in order[: min(LABEL_MAX, B)]:\n",
        "        x = cent[i]\n",
        "        ax.text(x[0], x[1], x[2], f\" {i}\", fontsize=10)\n",
        "\n",
        "    # colorbar for margin\n",
        "    sm = mpl.cm.ScalarMappable(cmap=cmap, norm=normc)\n",
        "    sm.set_array([])\n",
        "    cbar = plt.colorbar(sm, ax=ax, fraction=0.03, pad=0.08)\n",
        "    cbar.set_label(f\"logP(' Paris') - logP({RIVAL_TEXT!r})\")\n",
        "\n",
        "    ax.set_title(\n",
        "        f\"Prompt-Conditional Tangent Control Field (late-window centroid)\\n\"\n",
        "        f\"layer={layer} | late_W={LATE_W} | FD_EPS={FD_EPS} ({FD_MODE}) | PCA EVR(PC1..3){np.round(evr3,4)}\"\n",
        "    )\n",
        "    ax.set_xlabel(\"PC1 (sphere)\")\n",
        "    ax.set_ylabel(\"PC2 (sphere)\")\n",
        "    ax.set_zlabel(\"PC3 (sphere)\")\n",
        "    ax.set_box_aspect([1, 1, 1])\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # --- Diagnostics panels ---\n",
        "    if DO_ALIGNMENT_PANELS:\n",
        "        fig = plt.figure(figsize=(14.5, 4.6))\n",
        "\n",
        "        # 1) alignment histogram\n",
        "        ax1 = fig.add_subplot(1, 3, 1)\n",
        "        a = align[np.isfinite(align)]\n",
        "        if a.size > 0:\n",
        "            ax1.hist(a, bins=12)\n",
        "            ax1.axvline(float(np.mean(a)), linestyle=\"--\")\n",
        "            ax1.set_title(f\"cos(tangent, late-velocity)\\nmean={float(np.mean(a)):.3f}  n={a.size}\")\n",
        "        else:\n",
        "            ax1.text(0.5, 0.5, \"no valid alignments\", ha=\"center\", va=\"center\")\n",
        "            ax1.set_title(\"cos(tangent, late-velocity)\")\n",
        "        ax1.set_xlabel(\"cosine\")\n",
        "        ax1.set_ylabel(\"count\")\n",
        "\n",
        "        # 2) dist-to-attractor vs margin\n",
        "        ax2 = fig.add_subplot(1, 3, 2)\n",
        "        ax2.scatter(dist, pm, s=60)\n",
        "        ax2.set_title(\"distance to attractor vs Paris-margin\")\n",
        "        ax2.set_xlabel(\"angle distance (rad)\")\n",
        "        ax2.set_ylabel(\"Paris-margin\")\n",
        "        # correlation (ignore NaNs)\n",
        "        if np.isfinite(dist).all():\n",
        "            corr = float(np.corrcoef(dist, pm)[0, 1])\n",
        "            ax2.text(0.02, 0.98, f\"corr={corr:+.3f}\", transform=ax2.transAxes, va=\"top\")\n",
        "\n",
        "        # 3) 2D centroid map (PC1/PC2 in sphere coords)\n",
        "        ax3 = fig.add_subplot(1, 3, 3)\n",
        "        ax3.scatter(cent[:, 0], cent[:, 1], c=pm, s=70)\n",
        "        ax3.scatter([attract[0]], [attract[1]], s=180, marker=\"X\")\n",
        "        for i in order[: min(LABEL_MAX, B)]:\n",
        "            ax3.text(cent[i, 0], cent[i, 1], f\" {i}\", fontsize=9)\n",
        "        ax3.set_title(\"centroids on sphere (PC1,PC2)\")\n",
        "        ax3.set_xlabel(\"PC1(sphere)\")\n",
        "        ax3.set_ylabel(\"PC2(sphere)\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # quick textual readout\n",
        "    print(f\"[layer {layer}] PCA EVR(PC1..3) {np.round(evr3,4)} | PCA_fit_good={res['n_fit']} points\")\n",
        "    print(f\"  margins: mean={pm.mean():.4f} med={np.median(pm):.4f} min={pm.min():.4f} max={pm.max():.4f}\")\n",
        "    top = order[:5].tolist()\n",
        "    print(\"  [top Paris-margin prompts]\", [(i, float(pm[i]), PROMPTS[i][:50]) for i in top])\n",
        "    print(f\"  tangent_coherence={res['tangent_coherence']:.3f}\")\n",
        "\n",
        "# ---------------------------\n",
        "# Run all requested layers\n",
        "# ---------------------------\n",
        "print(f\"[info] detected layers={len(model.blocks)} | B={B} T={T} | late_W={LATE_W} | layers={LAYERS}\")\n",
        "print(f\"[baseline] mean Paris-margin (no forcing) = {baseline_mean_margin:.4f}\")\n",
        "print(\"[note] This cell runs prompt-conditional FD and can take time; tqdm shows progress per layer.\")\n",
        "\n",
        "results = {}\n",
        "for L in LAYERS:\n",
        "    res = layer_control_field_prompt_conditional(int(L))\n",
        "    results[int(L)] = res\n",
        "    plot_layer_field(res)\n",
        "\n",
        "# ---------------------------\n",
        "# Compact summary across layers\n",
        "# ---------------------------\n",
        "print(\"\\n=== Summary across layers ===\")\n",
        "for L in LAYERS:\n",
        "    res = results[int(L)]\n",
        "    pm = res[\"paris_margin\"]\n",
        "    dist = res[\"dist_to_attractor\"]\n",
        "    a = res[\"align\"]\n",
        "    a_mean = float(np.nanmean(a)) if np.isfinite(a).any() else float(\"nan\")\n",
        "    corr = float(np.corrcoef(dist, pm)[0, 1]) if np.isfinite(dist).all() else float(\"nan\")\n",
        "    print(\n",
        "        f\"L{L:2d} | EVR={np.round(res['evr3'],3)} | coh={res['tangent_coherence']:.3f} | \"\n",
        "        f\"align_mean={a_mean:+.3f} | corr(dist,margin)={corr:+.3f}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "R3RGuk2oV2No"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ================================================================\n",
        "#@title Experiment: Residual-vs-Routing Intervention Parity Test (FD + PCA, late-window centroid)\n",
        "# Tests competing hypothesis:\n",
        "#   Facts retrieved in residual stream; routing gates expression.\n",
        "#\n",
        "# For a chosen layer:\n",
        "#   (A) ROUTING intervention: move in PCA(read_logits) space -> inverse -> softmax -> force routing at t_last\n",
        "#   (B) RESIDUAL intervention: move in PCA(residual) space -> inverse -> add delta to hidden state at t_last\n",
        "#\n",
        "# Compare efficiency: (Paris-margin) per unit .\n",
        "#\n",
        "# Assumptions:\n",
        "#   - model, cfg, DEVICE exist\n",
        "#   - model(input_ids, attention_mask=..., return_info=True, routing_mode=\"softmax\") -> (logits, infos)\n",
        "#   - infos[layer][\"read_logits\"] exists with shape [B,H,T,K]\n",
        "#   - model.blocks[layer] returns hidden states shaped [B,T,D] (or compatible)\n",
        "# ================================================================\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ---------------------------\n",
        "# SETTINGS\n",
        "# ---------------------------\n",
        "LAYER_TEST = 13           # try 13 first; then 0 and 5\n",
        "LATE_W = 5                # late-window size\n",
        "SKIP_T0 = True            # skip token 0 in PCA fits\n",
        "FD_EPS = 1e-2             # FD step in PCA coords for direction estimation\n",
        "FD_MODE = \"central\"       # \"central\" or \"forward\"\n",
        "FORCE_ROUTING_RTEMP = 1.0 # router softmax temp inside override\n",
        "EPS_SWEEP = [0.0, 0.01, 0.02, 0.04, 0.08]  # intervention magnitudes (in PCA coords)\n",
        "N_SLOPE_FIT = 3           # use first N eps points to estimate slope near 0\n",
        "\n",
        "# Prompts (use your current set)\n",
        "PROMPTS = [\n",
        "    \"The capital of France is\",\n",
        "    \"France's capital city is\",\n",
        "    \"In France, the capital is\",\n",
        "    \"The seat of government of France is\",\n",
        "    \"The city that is France's capital is\",\n",
        "    \"The Eiffel Tower is located in\",\n",
        "    \"The Louvre is in\",\n",
        "    \"The Seine river flows through\",\n",
        "    \"During the French Revolution, the city of\",\n",
        "    \"Napoleon crowned himself in\",\n",
        "    \"In World War II, the liberation of\",\n",
        "    \"The largest city in France is\",\n",
        "    \"A major European city in France is\",\n",
        "    \"A famous city on the Seine is\",\n",
        "    \"The capital of Germany is\",\n",
        "    \"The capital of England is\",\n",
        "    \"The capital of Italy is\",\n",
        "]\n",
        "\n",
        "RIVAL_TEXT = \" the\"       # single-token rival for margin\n",
        "\n",
        "# ---------------------------\n",
        "# Tokenizer (RIGHT padding enforced)\n",
        "# ---------------------------\n",
        "def _get_tokenizer_from_context():\n",
        "    if \"tokenizer\" in globals():\n",
        "        tok = globals()[\"tokenizer\"]\n",
        "        tok.padding_side = \"right\"\n",
        "        if tok.pad_token is None:\n",
        "            tok.pad_token = tok.eos_token\n",
        "        return tok\n",
        "    from transformers import AutoTokenizer\n",
        "    tok = AutoTokenizer.from_pretrained(getattr(cfg, \"tokenizer_name\", \"gpt2\"))\n",
        "    tok.padding_side = \"right\"\n",
        "    if tok.pad_token is None:\n",
        "        tok.pad_token = tok.eos_token\n",
        "    return tok\n",
        "\n",
        "tokenizer = _get_tokenizer_from_context()\n",
        "\n",
        "def ensure_single_token(text: str) -> int:\n",
        "    ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "    if len(ids) != 1:\n",
        "        raise ValueError(f\"{text!r} must be exactly 1 token. Got ids={ids}\")\n",
        "    return ids[0]\n",
        "\n",
        "PARIS_ID = ensure_single_token(\" Paris\")\n",
        "RIVAL_ID = ensure_single_token(RIVAL_TEXT)\n",
        "\n",
        "# ---------------------------\n",
        "# Forward with info\n",
        "# ---------------------------\n",
        "def forward_with_info(ids, mask, routing_mode=\"softmax\"):\n",
        "    logits, infos = model(ids, attention_mask=mask, return_info=True, routing_mode=routing_mode)\n",
        "    return logits, infos\n",
        "\n",
        "# ---------------------------\n",
        "# Batch tokenize\n",
        "# ---------------------------\n",
        "enc = tokenizer(\n",
        "    PROMPTS,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=cfg.max_seq_len,\n",
        ")\n",
        "input_ids = enc[\"input_ids\"].to(DEVICE)\n",
        "attention_mask = enc.get(\"attention_mask\", torch.ones_like(input_ids)).to(DEVICE)\n",
        "\n",
        "B, T = input_ids.shape\n",
        "lengths = attention_mask.sum(dim=1).long()\n",
        "t_last = (lengths - 1).clamp(min=0)\n",
        "\n",
        "def late_window_indices(b: int):\n",
        "    last = int(t_last[b].item())\n",
        "    t0 = max(0, last - (LATE_W - 1))\n",
        "    idxs = list(range(t0, last + 1))\n",
        "    if SKIP_T0:\n",
        "        idxs = [t for t in idxs if t != 0]\n",
        "    return idxs\n",
        "\n",
        "late_idxs = [late_window_indices(b) for b in range(B)]\n",
        "\n",
        "# ---------------------------\n",
        "# Baseline Paris margin\n",
        "# ---------------------------\n",
        "@torch.no_grad()\n",
        "def paris_margin_from_logits(logits):\n",
        "    logp = F.log_softmax(logits, dim=-1)\n",
        "    idx = torch.arange(B, device=logits.device)\n",
        "    lpP = logp[idx, t_last, PARIS_ID]\n",
        "    lpR = logp[idx, t_last, RIVAL_ID]\n",
        "    return (lpP - lpR)  # [B]\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits_base, infos_base = forward_with_info(input_ids, attention_mask, routing_mode=\"softmax\")\n",
        "    margin_base_b = paris_margin_from_logits(logits_base).detach().cpu().numpy()\n",
        "    margin_base = float(margin_base_b.mean())\n",
        "\n",
        "print(f\"[info] LAYER_TEST={LAYER_TEST} | B={B} T={T} | baseline mean margin={margin_base:.4f}\")\n",
        "\n",
        "# ================================================================\n",
        "# 1) PCA utilities (robust, CPU fallback)\n",
        "# ================================================================\n",
        "def fit_pca3(X: torch.Tensor):\n",
        "    \"\"\"\n",
        "    X: [N,D] float32\n",
        "    returns: mu [D], W3 [D,3], EVR [3]\n",
        "    \"\"\"\n",
        "    X = X.float()\n",
        "    good = torch.isfinite(X).all(dim=-1)\n",
        "    Xg = X[good]\n",
        "    if Xg.shape[0] < 10:\n",
        "        raise RuntimeError(f\"Too few finite PCA points: {int(Xg.shape[0])}\")\n",
        "    mu = Xg.mean(dim=0, keepdim=True)\n",
        "    Xc = Xg - mu\n",
        "    try:\n",
        "        U, S, Vh = torch.linalg.svd(Xc, full_matrices=False)\n",
        "    except Exception:\n",
        "        Xc_cpu = Xc.detach().cpu()\n",
        "        U, S, Vh = torch.linalg.svd(Xc_cpu, full_matrices=False)\n",
        "        Vh = Vh.to(Xc.device)\n",
        "        S = S.to(Xc.device)\n",
        "    W3 = Vh[:3].T.contiguous()\n",
        "    s2 = S**2\n",
        "    evr3 = (s2[:3] / s2.sum().clamp_min(1e-12)).detach().cpu().numpy()\n",
        "    return mu.squeeze(0), W3, evr3\n",
        "\n",
        "def pca_encode(X: torch.Tensor, mu: torch.Tensor, W3: torch.Tensor):\n",
        "    # X: [N,D] -> [N,3]\n",
        "    return (X - mu[None, :]) @ W3\n",
        "\n",
        "def pca_decode(x3: torch.Tensor, mu: torch.Tensor, W3: torch.Tensor):\n",
        "    # x3: [N,3] -> [N,D]\n",
        "    return mu[None, :] + (x3 @ W3.T)\n",
        "\n",
        "# ================================================================\n",
        "# 2) ROUTING intervention machinery (force read_weights at t_last)\n",
        "# ================================================================\n",
        "def clear_routing_override():\n",
        "    for li in range(len(model.blocks)):\n",
        "        model.blocks[li].asa.routing_override = None\n",
        "\n",
        "def set_override_for_layer(layer_idx: int, r_bhk: torch.Tensor, t_targets_cpu: np.ndarray):\n",
        "    clear_routing_override()\n",
        "\n",
        "    def _override(t0, t1, read_logits_chunk, read_logits_key, read_logits_content, ctx):\n",
        "        # read_logits_chunk: [B,H,L,K]\n",
        "        Bc, Hc, Lc, Kc = read_logits_chunk.shape\n",
        "        rtemp = float(ctx.get(\"rtemp\", FORCE_ROUTING_RTEMP))\n",
        "        w = torch.softmax(read_logits_chunk / max(1e-6, rtemp), dim=-1)\n",
        "        for b in range(Bc):\n",
        "            tt = int(t_targets_cpu[b])\n",
        "            if t0 <= tt < t1:\n",
        "                lp = tt - t0\n",
        "                w[b, :, lp, :] = r_bhk[b]\n",
        "        return w / w.sum(dim=-1, keepdim=True).clamp_min(1e-8)\n",
        "\n",
        "    model.blocks[layer_idx].asa.routing_override = _override\n",
        "\n",
        "@torch.no_grad()\n",
        "def run_with_forced_routing(layer_idx: int, r_bhk: torch.Tensor):\n",
        "    t_targets_cpu = t_last.detach().cpu().numpy()\n",
        "    set_override_for_layer(layer_idx, r_bhk, t_targets_cpu)\n",
        "    logits, _ = forward_with_info(input_ids, attention_mask, routing_mode=\"softmax\")\n",
        "    clear_routing_override()\n",
        "    mb = paris_margin_from_logits(logits).detach().cpu().numpy()\n",
        "    return float(mb.mean())\n",
        "\n",
        "# ================================================================\n",
        "# 3) RESIDUAL intervention machinery (forward hook inject at t_last)\n",
        "# ================================================================\n",
        "class ResidualInjector:\n",
        "    def __init__(self, layer_idx: int):\n",
        "        self.layer_idx = layer_idx\n",
        "        self.delta_btD = None  # [B,T,D] sparse-ish (only t_last filled)\n",
        "        self.hook = None\n",
        "\n",
        "    def install(self):\n",
        "        # Hook the block output: assumed [B,T,D]\n",
        "        def _hook(module, inp, out):\n",
        "            if self.delta_btD is None:\n",
        "                return out\n",
        "            # out could be tuple; handle common patterns\n",
        "            if isinstance(out, tuple):\n",
        "                h = out[0]\n",
        "                rest = out[1:]\n",
        "                h2 = h + self.delta_btD.to(h.device, dtype=h.dtype)\n",
        "                return (h2,) + rest\n",
        "            else:\n",
        "                return out + self.delta_btD.to(out.device, dtype=out.dtype)\n",
        "\n",
        "        self.hook = model.blocks[self.layer_idx].register_forward_hook(_hook)\n",
        "\n",
        "    def uninstall(self):\n",
        "        if self.hook is not None:\n",
        "            self.hook.remove()\n",
        "            self.hook = None\n",
        "        self.delta_btD = None\n",
        "\n",
        "@torch.no_grad()\n",
        "def capture_layer_hidden(layer_idx: int):\n",
        "    \"\"\"\n",
        "    Returns hidden states at output of model.blocks[layer_idx]: [B,T,D] on DEVICE (float32).\n",
        "    \"\"\"\n",
        "    cache = {}\n",
        "    def _cap_hook(module, inp, out):\n",
        "        if isinstance(out, tuple):\n",
        "            cache[\"h\"] = out[0].detach()\n",
        "        else:\n",
        "            cache[\"h\"] = out.detach()\n",
        "\n",
        "    hhook = model.blocks[layer_idx].register_forward_hook(_cap_hook)\n",
        "    logits, _ = forward_with_info(input_ids, attention_mask, routing_mode=\"softmax\")\n",
        "    hhook.remove()\n",
        "    if \"h\" not in cache:\n",
        "        raise RuntimeError(\"Failed to capture hidden state from block hook.\")\n",
        "    return cache[\"h\"].float()\n",
        "\n",
        "@torch.no_grad()\n",
        "def run_with_residual_injection(layer_idx: int, delta_bD: torch.Tensor):\n",
        "    \"\"\"\n",
        "    delta_bD: [B,D] added at each example's t_last position at output of block layer_idx.\n",
        "    \"\"\"\n",
        "    h0 = capture_layer_hidden(layer_idx)  # just to get D safely if needed\n",
        "    D = h0.shape[-1]\n",
        "\n",
        "    inj = ResidualInjector(layer_idx)\n",
        "    inj.install()\n",
        "\n",
        "    delta_btD = torch.zeros((B, T, D), device=DEVICE, dtype=torch.float32)\n",
        "    for b in range(B):\n",
        "        tt = int(t_last[b].item())\n",
        "        delta_btD[b, tt, :] = delta_bD[b].to(DEVICE, dtype=torch.float32)\n",
        "    inj.delta_btD = delta_btD\n",
        "\n",
        "    logits, _ = forward_with_info(input_ids, attention_mask, routing_mode=\"softmax\")\n",
        "    inj.uninstall()\n",
        "\n",
        "    mb = paris_margin_from_logits(logits).detach().cpu().numpy()\n",
        "    return float(mb.mean())\n",
        "\n",
        "# ================================================================\n",
        "# 4) Build PCA bases for routing and residual, using late-window points\n",
        "# ================================================================\n",
        "# ---- Routing read_logits points (K-dim) ----\n",
        "with torch.no_grad():\n",
        "    info = infos_base[LAYER_TEST]\n",
        "    rl = info.get(\"read_logits\", None)\n",
        "    if rl is None:\n",
        "        raise KeyError(f\"infos[{LAYER_TEST}] missing read_logits.\")\n",
        "    # [B,H,T,K] -> [B,T,K]\n",
        "    Z = rl.mean(dim=1).float()\n",
        "    K = Z.shape[-1]\n",
        "    H = rl.shape[1]\n",
        "\n",
        "Z_pts = []\n",
        "for b in range(B):\n",
        "    for t in late_idxs[b]:\n",
        "        Z_pts.append(Z[b, t, :])\n",
        "Z_pts = torch.stack(Z_pts, dim=0).to(DEVICE)  # [N,K]\n",
        "\n",
        "mu_r, W3_r, evr_r = fit_pca3(Z_pts)\n",
        "print(f\"[routing PCA] K={K} | EVR(PC1..3){np.round(evr_r,4)} | points={Z_pts.shape[0]}\")\n",
        "\n",
        "# ---- Residual hidden points (D-dim) ----\n",
        "h_layer = capture_layer_hidden(LAYER_TEST)  # [B,T,D]\n",
        "D = h_layer.shape[-1]\n",
        "\n",
        "H_pts = []\n",
        "for b in range(B):\n",
        "    for t in late_idxs[b]:\n",
        "        H_pts.append(h_layer[b, t, :])\n",
        "H_pts = torch.stack(H_pts, dim=0).to(DEVICE)  # [N,D]\n",
        "\n",
        "mu_h, W3_h, evr_h = fit_pca3(H_pts)\n",
        "print(f\"[residual PCA] D={D} | EVR(PC1..3){np.round(evr_h,4)} | points={H_pts.shape[0]}\")\n",
        "\n",
        "# ================================================================\n",
        "# 5) Late-window centroid per prompt in each PCA space\n",
        "# ================================================================\n",
        "def centroid_pca_per_prompt(points_btD: torch.Tensor, mu: torch.Tensor, W3: torch.Tensor):\n",
        "    \"\"\"\n",
        "    points_btD: [B,T,D] (or [B,T,K]) points in native space\n",
        "    returns centroids: [B,3] in PCA coords (mean over late window)\n",
        "    \"\"\"\n",
        "    cents = torch.zeros((B, 3), device=DEVICE, dtype=torch.float32)\n",
        "    for b in range(B):\n",
        "        idxs = late_idxs[b]\n",
        "        Xb = torch.stack([points_btD[b, t, :] for t in idxs], dim=0)  # [w,D]\n",
        "        x3 = pca_encode(Xb, mu, W3)                                   # [w,3]\n",
        "        cents[b] = x3.mean(dim=0)\n",
        "    return cents\n",
        "\n",
        "cent_r = centroid_pca_per_prompt(Z, mu_r, W3_r)        # [B,3]\n",
        "cent_h = centroid_pca_per_prompt(h_layer, mu_h, W3_h)  # [B,3]\n",
        "\n",
        "# ================================================================\n",
        "# 6) Estimate a GLOBAL control direction via FD at centroids\n",
        "#    - Routing: direction in PCA(read_logits) coords\n",
        "#    - Residual: direction in PCA(hidden) coords\n",
        "# ================================================================\n",
        "def routing_from_x3(x3: torch.Tensor):\n",
        "    \"\"\"\n",
        "    x3: [B,3] in routing PCA coords -> z_k -> softmax -> r_bhk (same per head)\n",
        "    \"\"\"\n",
        "    z_k = pca_decode(x3, mu_r, W3_r)          # [B,K]\n",
        "    r_bk = torch.softmax(z_k, dim=-1)         # [B,K]\n",
        "    r_bhk = r_bk[:, None, :].expand(B, H, K).contiguous()\n",
        "    return r_bhk\n",
        "\n",
        "def residual_delta_from_x3(x3: torch.Tensor):\n",
        "    \"\"\"\n",
        "    x3: [B,3] in residual PCA coords -> delta_h [B,D]\n",
        "    \"\"\"\n",
        "    dh = pca_decode(x3, mu_h, W3_h) - mu_h[None, :]  # convert coords to displacement around mean\n",
        "    return dh\n",
        "\n",
        "def fd_global_dir_routing():\n",
        "    # Compute grad of mean margin w.r.t. x3 centroid (same direction shared)\n",
        "    # We'll average per-prompt grads to get a global direction.\n",
        "    grads = torch.zeros((B, 3), device=DEVICE, dtype=torch.float32)\n",
        "\n",
        "    base_mean = run_with_forced_routing(LAYER_TEST, routing_from_x3(cent_r))\n",
        "\n",
        "    for b in tqdm(range(B), desc=\"FD routing dir (per prompt)\", leave=False):\n",
        "        x = cent_r[b].clone()\n",
        "        for d in range(3):\n",
        "            e = torch.zeros_like(x); e[d] = FD_EPS\n",
        "            if FD_MODE == \"central\":\n",
        "                x_plus = cent_r.clone(); x_plus[b] = x + e\n",
        "                x_minus = cent_r.clone(); x_minus[b] = x - e\n",
        "                m_plus = run_with_forced_routing(LAYER_TEST, routing_from_x3(x_plus))\n",
        "                m_minus = run_with_forced_routing(LAYER_TEST, routing_from_x3(x_minus))\n",
        "                grads[b, d] = (m_plus - m_minus) / (2.0 * FD_EPS)\n",
        "            else:\n",
        "                x_plus = cent_r.clone(); x_plus[b] = x + e\n",
        "                m_plus = run_with_forced_routing(LAYER_TEST, routing_from_x3(x_plus))\n",
        "                grads[b, d] = (m_plus - base_mean) / FD_EPS\n",
        "\n",
        "    g = grads.mean(dim=0)\n",
        "    g = g / g.norm().clamp_min(1e-9)\n",
        "    return g.detach().cpu().numpy()\n",
        "\n",
        "def fd_global_dir_residual():\n",
        "    grads = torch.zeros((B, 3), device=DEVICE, dtype=torch.float32)\n",
        "\n",
        "    # baseline injection = 0\n",
        "    base_mean = float(margin_base)\n",
        "\n",
        "    for b in tqdm(range(B), desc=\"FD residual dir (per prompt)\", leave=False):\n",
        "        x = cent_h[b].clone()\n",
        "        for d in range(3):\n",
        "            e = torch.zeros_like(x); e[d] = FD_EPS\n",
        "            if FD_MODE == \"central\":\n",
        "                x_plus = cent_h.clone(); x_plus[b] = x + e\n",
        "                x_minus = cent_h.clone(); x_minus[b] = x - e\n",
        "                dh_plus = residual_delta_from_x3(x_plus - cent_h)   # interpret as displacement from centroid\n",
        "                dh_minus = residual_delta_from_x3(x_minus - cent_h)\n",
        "                m_plus = run_with_residual_injection(LAYER_TEST, dh_plus)\n",
        "                m_minus = run_with_residual_injection(LAYER_TEST, dh_minus)\n",
        "                grads[b, d] = (m_plus - m_minus) / (2.0 * FD_EPS)\n",
        "            else:\n",
        "                x_plus = cent_h.clone(); x_plus[b] = x + e\n",
        "                dh_plus = residual_delta_from_x3(x_plus - cent_h)\n",
        "                m_plus = run_with_residual_injection(LAYER_TEST, dh_plus)\n",
        "                grads[b, d] = (m_plus - base_mean) / FD_EPS\n",
        "\n",
        "    g = grads.mean(dim=0)\n",
        "    g = g / g.norm().clamp_min(1e-9)\n",
        "    return g.detach().cpu().numpy()\n",
        "\n",
        "print(\"[step] estimating global routing control direction (FD)...\")\n",
        "g_r = fd_global_dir_routing()\n",
        "print(\"  g_r (PCA coords):\", np.round(g_r, 4).tolist())\n",
        "\n",
        "print(\"[step] estimating global residual control direction (FD)...\")\n",
        "g_h = fd_global_dir_residual()\n",
        "print(\"  g_h (PCA coords):\", np.round(g_h, 4).tolist())\n",
        "\n",
        "# ================================================================\n",
        "# 7) Parity sweep: apply  * global_dir in each space\n",
        "#    - Routing: shift all prompt centroids in routing PCA coords by *g_r\n",
        "#    - Residual: inject h from *g_h at each prompt t_last\n",
        "# ================================================================\n",
        "def apply_routing_eps(eps: float):\n",
        "    x3 = cent_r + torch.tensor(eps * g_r, device=DEVICE, dtype=torch.float32)[None, :]\n",
        "    r_bhk = routing_from_x3(x3)\n",
        "    return run_with_forced_routing(LAYER_TEST, r_bhk)\n",
        "\n",
        "def apply_residual_eps(eps: float):\n",
        "    # per-prompt delta in PCA coords = eps*g_h, decode to D space\n",
        "    x3 = torch.tensor(eps * g_h, device=DEVICE, dtype=torch.float32)[None, :].expand(B, 3)\n",
        "    dh = pca_decode(x3, mu_h, W3_h) - mu_h[None, :]  # [B,D]\n",
        "    return run_with_residual_injection(LAYER_TEST, dh)\n",
        "\n",
        "routing_means = []\n",
        "resid_means = []\n",
        "\n",
        "print(\"[run]  sweep (routing vs residual)...\")\n",
        "for eps in tqdm(EPS_SWEEP, desc=\"parity sweep\"):\n",
        "    mr = apply_routing_eps(float(eps))\n",
        "    mh = apply_residual_eps(float(eps))\n",
        "    routing_means.append(mr)\n",
        "    resid_means.append(mh)\n",
        "\n",
        "routing_means = np.array(routing_means, dtype=np.float32)\n",
        "resid_means = np.array(resid_means, dtype=np.float32)\n",
        "\n",
        "d_r = routing_means - margin_base\n",
        "d_h = resid_means - margin_base\n",
        "\n",
        "# slope near 0 (simple least squares on first N points)\n",
        "def fit_slope(eps_list, delta_list, n=3):\n",
        "    x = np.array(eps_list[:n], dtype=np.float32)\n",
        "    y = np.array(delta_list[:n], dtype=np.float32)\n",
        "    # y = a*x (no intercept) is appropriate if eps=0 => delta=0 numerically\n",
        "    denom = float((x*x).sum()) + 1e-12\n",
        "    a = float((x*y).sum() / denom)\n",
        "    return a\n",
        "\n",
        "slope_r = fit_slope(EPS_SWEEP, d_r, n=min(N_SLOPE_FIT, len(EPS_SWEEP)))\n",
        "slope_h = fit_slope(EPS_SWEEP, d_h, n=min(N_SLOPE_FIT, len(EPS_SWEEP)))\n",
        "\n",
        "# ================================================================\n",
        "# 8) Plots + readout\n",
        "# ================================================================\n",
        "plt.figure(figsize=(8.5, 5.8))\n",
        "plt.plot(EPS_SWEEP, d_r, marker=\"o\", label=\"routing override (margin)\")\n",
        "plt.plot(EPS_SWEEP, d_h, marker=\"o\", label=\"residual injection (margin)\")\n",
        "plt.axhline(0.0, linewidth=1)\n",
        "plt.xlabel(\" (step in PCA coord direction)\")\n",
        "plt.ylabel(\" mean Paris-margin\")\n",
        "plt.title(f\"Residual vs Routing Parity Test | layer={LAYER_TEST} | baseline={margin_base:.3f}\\n\"\n",
        "          f\"local slope near 0: routing={slope_r:.3f}  residual={slope_h:.3f}\")\n",
        "plt.grid(True, alpha=0.25)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n=== Summary ===\")\n",
        "print(f\"layer={LAYER_TEST} | baseline mean margin={margin_base:.4f}\")\n",
        "print(f\"routing PCA EVR={np.round(evr_r,4)} | residual PCA EVR={np.round(evr_h,4)}\")\n",
        "print(f\"local slope near 0 (margin/): routing={slope_r:.4f} | residual={slope_h:.4f}\")\n",
        "print(\"eps | mean_margin_routing | mean_margin_residual | r | h\")\n",
        "for eps, mr, mh, dr, dh in zip(EPS_SWEEP, routing_means, resid_means, d_r, d_h):\n",
        "    print(f\"{eps:>4.2f} | {mr:>+8.4f}        | {mh:>+8.4f}         | {dr:>+7.4f} | {dh:>+7.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GCCGq59NXqcB"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ================================================================\n",
        "#@title Experiment 3: Residual controllability with ROUTING FROZEN (decisive A vs B test)\n",
        "#\n",
        "# Goal:\n",
        "#   Test whether residual-state edits can move Paris-margin when routing is held fixed\n",
        "#   to its baseline behavior.\n",
        "#\n",
        "# Method:\n",
        "#   For each layer L in LAYERS:\n",
        "#     1) Run baseline forward, cache baseline routing weights w_base[L] (softmax(read_logits))\n",
        "#     2) Install routing_override that ALWAYS returns w_base[L] (routing frozen)\n",
        "#     3) Capture baseline residual states h_base[L] at the output of block L (B,T,D)\n",
        "#     4) Fit PCA(3) on late-window residual points for that layer\n",
        "#     5) Estimate a \"best\" residual-control direction g_h in PCA coords via small FD\n",
        "#     6) Sweep epsilon along g_h, inject into residual at t_last, and measure mean Paris-margin\n",
        "#\n",
        "# Interpretation:\n",
        "#   If residual injection still cannot move margin (flat response) even with routing frozen,\n",
        "#   that strongly supports \"routing-as-address\" (Hypothesis A).\n",
        "#   If residual injection becomes effective (meaningfully moves margin), that supports\n",
        "#   \"routing-as-decoder-key\" (Hypothesis B).\n",
        "#\n",
        "# Assumptions:\n",
        "#   - model, cfg, DEVICE exist\n",
        "#   - model(..., return_info=True, routing_mode=\"softmax\") -> (logits, infos)\n",
        "#   - infos[L][\"read_logits\"] exists, shape [B,H,T,K]\n",
        "#   - model.blocks is indexable (len gives #layers)\n",
        "#   - Each model.blocks[L](...) returns hidden states as first output or a tensor\n",
        "# ================================================================\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------------------------\n",
        "# SETTINGS\n",
        "# ---------------------------\n",
        "LAYERS = [0, 5, 13]        # edit as desired\n",
        "LATE_W = 5                 # last W valid tokens for PCA fit\n",
        "FD_EPS = 0.02              # FD step in residual PCA coords for direction estimate\n",
        "EPS_SWEEP = [0.00, 0.01, 0.02, 0.04, 0.08]  # injection magnitudes along g_h\n",
        "SKIP_T0 = True             # skip token index 0\n",
        "SANITIZE_MODE = \"drop\"     # \"drop\" or \"clamp\"\n",
        "CLAMP_VAL = 5.0\n",
        "\n",
        "RIVAL_TEXT = \" the\"\n",
        "TARGET_TEXT = \" Paris\"\n",
        "\n",
        "PROMPTS = [\n",
        "    \"The capital of France is\",\n",
        "    \"France's capital city is\",\n",
        "    \"In France, the capital is\",\n",
        "    \"The seat of government of France is\",\n",
        "    \"The city that is France's capital is\",\n",
        "    \"The Eiffel Tower is located in\",\n",
        "    \"The Louvre is in\",\n",
        "    \"The Seine river flows through\",\n",
        "    \"During the French Revolution, the city of\",\n",
        "    \"Napoleon crowned himself in\",\n",
        "    \"In World War II, the liberation of\",\n",
        "    \"The largest city in France is\",\n",
        "    \"A major European city in France is\",\n",
        "    \"A famous city on the Seine is\",\n",
        "    \"The capital of Germany is\",\n",
        "    \"The capital of England is\",\n",
        "    \"The capital of Italy is\",\n",
        "]\n",
        "\n",
        "# ---------------------------\n",
        "# Tokenizer (RIGHT padding enforced)\n",
        "# ---------------------------\n",
        "def _get_tokenizer_from_context():\n",
        "    if \"tokenizer\" in globals():\n",
        "        tok = globals()[\"tokenizer\"]\n",
        "        tok.padding_side = \"right\"\n",
        "        if tok.pad_token is None:\n",
        "            tok.pad_token = tok.eos_token\n",
        "        return tok\n",
        "    from transformers import AutoTokenizer\n",
        "    tok = AutoTokenizer.from_pretrained(getattr(cfg, \"tokenizer_name\", \"gpt2\"))\n",
        "    tok.padding_side = \"right\"\n",
        "    if tok.pad_token is None:\n",
        "        tok.pad_token = tok.eos_token\n",
        "    return tok\n",
        "\n",
        "tokenizer = _get_tokenizer_from_context()\n",
        "\n",
        "def ensure_single_token(text: str) -> int:\n",
        "    ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "    if len(ids) != 1:\n",
        "        raise ValueError(f\"{text!r} must be exactly 1 token under tokenizer. Got ids={ids}\")\n",
        "    return ids[0]\n",
        "\n",
        "PARIS_ID = ensure_single_token(TARGET_TEXT)\n",
        "RIVAL_ID = ensure_single_token(RIVAL_TEXT)\n",
        "\n",
        "# ---------------------------\n",
        "# Batch tokenize\n",
        "# ---------------------------\n",
        "enc = tokenizer(\n",
        "    PROMPTS,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=cfg.max_seq_len,\n",
        ")\n",
        "input_ids = enc[\"input_ids\"].to(DEVICE)\n",
        "attention_mask = enc.get(\"attention_mask\", torch.ones_like(input_ids)).to(DEVICE)\n",
        "\n",
        "B, T = input_ids.shape\n",
        "lengths = attention_mask.sum(dim=1).long()\n",
        "t_last = (lengths - 1).clamp(min=0)\n",
        "\n",
        "def late_window_indices(b: int):\n",
        "    last = int(t_last[b].item())\n",
        "    t0 = max(0, last - (LATE_W - 1))\n",
        "    idxs = list(range(t0, last + 1))\n",
        "    if SKIP_T0:\n",
        "        idxs = [t for t in idxs if t != 0]\n",
        "    return idxs\n",
        "\n",
        "late_idxs = [late_window_indices(b) for b in range(B)]\n",
        "\n",
        "# ---------------------------\n",
        "# Forward helper\n",
        "# ---------------------------\n",
        "def forward_with_info(ids, mask, routing_mode=\"softmax\"):\n",
        "    return model(ids, attention_mask=mask, return_info=True, routing_mode=routing_mode)\n",
        "\n",
        "@torch.no_grad()\n",
        "def mean_paris_margin_from_logits(logits: torch.Tensor) -> float:\n",
        "    logp = F.log_softmax(logits, dim=-1)\n",
        "    idx = torch.arange(B, device=logits.device)\n",
        "    lpP = logp[idx, t_last, PARIS_ID]\n",
        "    lpR = logp[idx, t_last, RIVAL_ID]\n",
        "    return float((lpP - lpR).mean().item())\n",
        "\n",
        "# ---------------------------\n",
        "# Routing freeze: cache baseline weights and override to return them\n",
        "# ---------------------------\n",
        "def clear_all_routing_overrides():\n",
        "    for li in range(len(model.blocks)):\n",
        "        blk = model.blocks[li]\n",
        "        if hasattr(blk, \"asa\") and hasattr(blk.asa, \"routing_override\"):\n",
        "            blk.asa.routing_override = None\n",
        "\n",
        "def set_frozen_routing(layer_idx: int, w_base_bhtk: torch.Tensor):\n",
        "    \"\"\"\n",
        "    w_base_bhtk: [B,H,T,K] baseline routing weights on DEVICE.\n",
        "    Override will return baseline weights for the chunk positions.\n",
        "    \"\"\"\n",
        "    clear_all_routing_overrides()\n",
        "\n",
        "    def _override(t0, t1, read_logits_chunk, read_logits_key, read_logits_content, ctx):\n",
        "        # read_logits_chunk: [B,H,L,K], where L = t1-t0\n",
        "        Bc, Hc, Lc, Kc = read_logits_chunk.shape\n",
        "        w = w_base_bhtk[:, :, t0:t1, :].contiguous()\n",
        "        # safety: ensure correct shape\n",
        "        if w.shape != (Bc, Hc, Lc, Kc):\n",
        "            # fallback: recompute base from current chunk if mismatch (should not happen)\n",
        "            rtemp = float(ctx.get(\"rtemp\", 1.0))\n",
        "            w = torch.softmax(read_logits_chunk / max(1e-6, rtemp), dim=-1)\n",
        "        w = w / w.sum(dim=-1, keepdim=True).clamp_min(1e-8)\n",
        "        return w\n",
        "\n",
        "    model.blocks[layer_idx].asa.routing_override = _override\n",
        "\n",
        "# ---------------------------\n",
        "# Residual capture + injection hooks\n",
        "# ---------------------------\n",
        "def _as_tensor_out(out):\n",
        "    # handle blocks that return (hidden, ...) vs hidden\n",
        "    if isinstance(out, (tuple, list)):\n",
        "        return out[0]\n",
        "    return out\n",
        "\n",
        "class ResidualCapture:\n",
        "    def __init__(self, layer_idx: int):\n",
        "        self.layer_idx = layer_idx\n",
        "        self.h = None\n",
        "        self.hook = None\n",
        "\n",
        "    def __enter__(self):\n",
        "        def fn(module, inp, out):\n",
        "            self.h = _as_tensor_out(out).detach()\n",
        "            return out\n",
        "        self.hook = model.blocks[self.layer_idx].register_forward_hook(fn)\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc, tb):\n",
        "        if self.hook is not None:\n",
        "            self.hook.remove()\n",
        "        self.hook = None\n",
        "\n",
        "class ResidualInjector:\n",
        "    def __init__(self, layer_idx: int, delta_btd: torch.Tensor):\n",
        "        \"\"\"\n",
        "        delta_btd: [B,T,D] additive delta applied to block output at layer_idx.\n",
        "        Typically only nonzero at t_last positions.\n",
        "        \"\"\"\n",
        "        self.layer_idx = layer_idx\n",
        "        self.delta = delta_btd\n",
        "        self.hook = None\n",
        "\n",
        "    def __enter__(self):\n",
        "        def fn(module, inp, out):\n",
        "            h = _as_tensor_out(out)\n",
        "            h2 = h + self.delta.to(h.device, dtype=h.dtype)\n",
        "            if isinstance(out, (tuple, list)):\n",
        "                out = (h2,) + tuple(out[1:])\n",
        "                return out\n",
        "            return h2\n",
        "        self.hook = model.blocks[self.layer_idx].register_forward_hook(fn)\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc, tb):\n",
        "        if self.hook is not None:\n",
        "            self.hook.remove()\n",
        "        self.hook = None\n",
        "\n",
        "# ---------------------------\n",
        "# PCA utilities (torch SVD, robust)\n",
        "# ---------------------------\n",
        "def _finite_mask_rows(X: torch.Tensor) -> torch.Tensor:\n",
        "    return torch.isfinite(X).all(dim=-1)\n",
        "\n",
        "def fit_pca3(X_points: torch.Tensor):\n",
        "    \"\"\"\n",
        "    X_points: [N,D] float\n",
        "    returns mu [D], W3 [D,3], evr3 [3], n_good\n",
        "    \"\"\"\n",
        "    X = X_points\n",
        "    if SANITIZE_MODE == \"clamp\":\n",
        "        X = torch.nan_to_num(X, nan=0.0, posinf=CLAMP_VAL, neginf=-CLAMP_VAL)\n",
        "        good = _finite_mask_rows(X)\n",
        "    else:\n",
        "        good = _finite_mask_rows(X)\n",
        "        X = X[good]\n",
        "\n",
        "    n_good = int(X.shape[0])\n",
        "    if n_good < 10:\n",
        "        raise RuntimeError(f\"Too few good PCA points: {n_good}\")\n",
        "\n",
        "    mu = X.mean(dim=0, keepdim=True)\n",
        "    Xc = X - mu\n",
        "    try:\n",
        "        U, S, Vh = torch.linalg.svd(Xc, full_matrices=False)\n",
        "    except Exception:\n",
        "        Xc_cpu = Xc.detach().cpu()\n",
        "        U, S, Vh = torch.linalg.svd(Xc_cpu, full_matrices=False)\n",
        "        Vh = Vh.to(Xc.device)\n",
        "        S = S.to(Xc.device)\n",
        "\n",
        "    W3 = Vh[:3].T.contiguous()  # [D,3]\n",
        "    s2 = S**2\n",
        "    evr3 = (s2[:3] / s2.sum().clamp_min(1e-12)).detach().cpu().numpy()\n",
        "    return mu.squeeze(0), W3, evr3, n_good\n",
        "\n",
        "def project_pca3(h_btd: torch.Tensor, mu: torch.Tensor, W3: torch.Tensor):\n",
        "    \"\"\"\n",
        "    h_btd: [B,T,D]  -> x_btd3: [B,T,3]\n",
        "    \"\"\"\n",
        "    return (h_btd - mu.view(1, 1, -1)) @ W3  # [B,T,3]\n",
        "\n",
        "# ---------------------------\n",
        "# Direction estimation (FD in residual PCA coords, routing frozen)\n",
        "# ---------------------------\n",
        "@torch.no_grad()\n",
        "def estimate_global_residual_dir(layer_idx: int, mu: torch.Tensor, W3: torch.Tensor, h_base_btd: torch.Tensor, w_base_bhtk: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Estimate FD gradient of mean Paris-margin w.r.t a 3D residual PCA coordinate,\n",
        "    evaluated at the GLOBAL late-point centroid (mean across all late points).\n",
        "    Returns unit vector g_dir (3,) in PCA coords.\n",
        "    \"\"\"\n",
        "    # Build centroid in PCA coords from late-window points\n",
        "    x3 = project_pca3(h_base_btd, mu, W3)  # [B,T,3]\n",
        "    pts = []\n",
        "    for b in range(B):\n",
        "        for t in late_idxs[b]:\n",
        "            pts.append(x3[b, t])\n",
        "    Xc = torch.stack(pts, dim=0)  # [N,3]\n",
        "    x0 = Xc.mean(dim=0)           # [3]\n",
        "\n",
        "    # helper: run model with injection corresponding to x = x0 + delta in PCA coords at each b's t_last\n",
        "    def eval_at_x(x_vec3: torch.Tensor) -> float:\n",
        "        # Convert PCA delta -> residual delta in D\n",
        "        # We inject ONLY at each prompt's t_last: h += (x_vec3 @ W3^T)  (same delta for all prompts)\n",
        "        d_h = (x_vec3 @ W3.T).view(1, 1, -1)  # [1,1,D]\n",
        "        delta = torch.zeros_like(h_base_btd)\n",
        "        for b in range(B):\n",
        "            tt = int(t_last[b].item())\n",
        "            delta[b, tt, :] = d_h[0, 0, :]\n",
        "        set_frozen_routing(layer_idx, w_base_bhtk)\n",
        "        with ResidualInjector(layer_idx, delta):\n",
        "            logits, _infos = forward_with_info(input_ids, attention_mask, routing_mode=\"softmax\")\n",
        "        clear_all_routing_overrides()\n",
        "        return mean_paris_margin_from_logits(logits)\n",
        "\n",
        "    g = torch.zeros(3, device=DEVICE)\n",
        "    for d in range(3):\n",
        "        e = torch.zeros(3, device=DEVICE)\n",
        "        e[d] = FD_EPS\n",
        "        m_plus = eval_at_x(x0 + e)\n",
        "        m_minus = eval_at_x(x0 - e)\n",
        "        g[d] = (m_plus - m_minus) / (2.0 * FD_EPS)\n",
        "\n",
        "    g_norm = g.norm().clamp_min(1e-12)\n",
        "    g_dir = (g / g_norm).detach().cpu().numpy()\n",
        "    return g_dir, g.detach().cpu().numpy()\n",
        "\n",
        "# ---------------------------\n",
        "# Main run\n",
        "# ---------------------------\n",
        "@torch.no_grad()\n",
        "def run_layer(layer_idx: int):\n",
        "    # baseline forward: cache routing weights + capture residual\n",
        "    clear_all_routing_overrides()\n",
        "\n",
        "    # capture residual at layer output\n",
        "    with ResidualCapture(layer_idx) as cap:\n",
        "        logits_base, infos_base = forward_with_info(input_ids, attention_mask, routing_mode=\"softmax\")\n",
        "    base_margin = mean_paris_margin_from_logits(logits_base)\n",
        "\n",
        "    rl = infos_base[layer_idx][\"read_logits\"]            # [B,H,T,K]\n",
        "    w_base = torch.softmax(rl, dim=-1).detach()          # [B,H,T,K]\n",
        "    h_base = cap.h                                       # [B,T,D]\n",
        "    assert h_base is not None, \"Failed to capture residual. Check block output structure.\"\n",
        "\n",
        "    # fit PCA on late-window residual points\n",
        "    pts = []\n",
        "    for b in range(B):\n",
        "        for t in late_idxs[b]:\n",
        "            pts.append(h_base[b, t])\n",
        "    Hfit = torch.stack(pts, dim=0).float()  # [N,D]\n",
        "    mu, W3, evr3, n_good = fit_pca3(Hfit)\n",
        "\n",
        "    # estimate global direction in residual PCA coords, routing frozen\n",
        "    g_dir, g_raw = estimate_global_residual_dir(layer_idx, mu.to(DEVICE), W3.to(DEVICE), h_base.float(), w_base)\n",
        "\n",
        "    # sweep eps along that direction\n",
        "    margins = []\n",
        "    deltas = []\n",
        "    for eps in EPS_SWEEP:\n",
        "        x_vec3 = torch.tensor(g_dir, device=DEVICE, dtype=torch.float32) * float(eps)\n",
        "        d_h = (x_vec3 @ W3.to(DEVICE).T).view(1, 1, -1)  # [1,1,D]\n",
        "        delta = torch.zeros_like(h_base).float()\n",
        "        for b in range(B):\n",
        "            tt = int(t_last[b].item())\n",
        "            delta[b, tt, :] = d_h[0, 0, :]\n",
        "        set_frozen_routing(layer_idx, w_base)\n",
        "        with ResidualInjector(layer_idx, delta):\n",
        "            logits, _infos = forward_with_info(input_ids, attention_mask, routing_mode=\"softmax\")\n",
        "        clear_all_routing_overrides()\n",
        "        m = mean_paris_margin_from_logits(logits)\n",
        "        margins.append(m)\n",
        "        deltas.append(m - base_margin)\n",
        "\n",
        "    # local slope estimate around 0 using first nonzero point\n",
        "    slope = None\n",
        "    if len(EPS_SWEEP) >= 2 and EPS_SWEEP[1] > 0:\n",
        "        slope = (deltas[1] - deltas[0]) / EPS_SWEEP[1]\n",
        "\n",
        "    return {\n",
        "        \"layer\": layer_idx,\n",
        "        \"base_margin\": base_margin,\n",
        "        \"evr3\": evr3,\n",
        "        \"n_good\": n_good,\n",
        "        \"g_raw\": g_raw,\n",
        "        \"g_dir\": g_dir,\n",
        "        \"eps\": np.array(EPS_SWEEP, dtype=np.float32),\n",
        "        \"margins\": np.array(margins, dtype=np.float32),\n",
        "        \"deltas\": np.array(deltas, dtype=np.float32),\n",
        "        \"slope0\": slope,\n",
        "    }\n",
        "\n",
        "results = {}\n",
        "print(f\"[info] B={B} T={T} | detected layers={len(model.blocks)} | testing layers={LAYERS}\")\n",
        "print(\"[note] Routing is frozen per-layer to baseline weights; only residual is perturbed.\\n\")\n",
        "\n",
        "for L in LAYERS:\n",
        "    out = run_layer(int(L))\n",
        "    results[int(L)] = out\n",
        "    print(f\"=== Layer {L} ===\")\n",
        "    print(f\"baseline mean margin = {out['base_margin']:+.4f}\")\n",
        "    print(f\"residual PCA EVR(PC1..3) {np.round(out['evr3'],4)} | PCA_fit_good={out['n_good']}\")\n",
        "    print(f\"FD grad in PCA coords (unnormalized) g_raw = {np.round(out['g_raw'],4)}\")\n",
        "    print(f\"unit dir g_dir = {np.round(out['g_dir'],4)}\")\n",
        "    if out[\"slope0\"] is not None:\n",
        "        print(f\"local slope near 0 (margin/)  {out['slope0']:+.4f}\")\n",
        "    print(\"eps | mean_margin | margin\")\n",
        "    for e, m, d in zip(out[\"eps\"], out[\"margins\"], out[\"deltas\"]):\n",
        "        print(f\"{e:>4.2f} | {m:+.4f}    | {d:+.4f}\")\n",
        "    print()\n",
        "\n",
        "# ---------------------------\n",
        "# Plot summary\n",
        "# ---------------------------\n",
        "plt.figure(figsize=(8.5, 5.4))\n",
        "for L in LAYERS:\n",
        "    out = results[int(L)]\n",
        "    plt.plot(out[\"eps\"], out[\"deltas\"], marker=\"o\", label=f\"L{L} (slope{(out['slope0'] if out['slope0'] is not None else float('nan')):+.2f})\")\n",
        "plt.axhline(0.0, linewidth=1.0)\n",
        "plt.title(\"Residual Injection with Routing FROZEN: (Paris-margin) vs \")\n",
        "plt.xlabel(\" (step along estimated residual-control direction)\")\n",
        "plt.ylabel(\" mean Paris-margin\")\n",
        "plt.grid(True, alpha=0.25)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n[done] If curves are ~flat (all layers), residual is not locally controllable under frozen routing (supports Hypothesis A).\")\n",
        "print(\"[done] If one or more layers show strong monotone margin vs , residual becomes controllable when routing is fixed (supports Hypothesis B).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "yNSN9LdShtn1"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Run WikiText Benchmark\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# ============================\n",
        "# Configuration\n",
        "# ============================\n",
        "BATCH_SIZE = 8  # Adjust based on your GPU memory\n",
        "MAX_LENGTH = cfg.max_seq_len  # Use model's max sequence length\n",
        "STRIDE = MAX_LENGTH // 2  # Sliding window stride for long sequences\n",
        "\n",
        "# ============================\n",
        "# Load WikiText-2 Test Set\n",
        "# ============================\n",
        "#print(\"Loading WikiText-103-raw test set...\")\n",
        "#dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"test\")\n",
        "\n",
        "# Load dataset\n",
        "print(\"Loading WikiText-103 test set...\")\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"test\")\n",
        "print(f\"Number of examples: {len(dataset)}\")\n",
        "\n",
        "# Get tokenizer\n",
        "#tokenizer = ckpt[\"tokenizer\"]\n",
        "#print(f\" Loaded tokenizer (vocab size: {len(tokenizer)})\")\n",
        "if ckpt[\"cfg\"].get(\"tokenizer_name\", None) is not None:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(ckpt[\"cfg\"].get(\"tokenizer_name\", \"gpt2\"), use_fast=True)\n",
        "\n",
        "# Tokenize and concatenate properly\n",
        "print(\"Tokenizing dataset...\")\n",
        "all_tokens = []\n",
        "for example in tqdm(dataset, desc=\"Processing\"):\n",
        "    text = example[\"text\"].strip()\n",
        "    if text:  # Skip empty lines\n",
        "        tokens = tokenizer.encode(text)\n",
        "        all_tokens.extend(tokens)\n",
        "\n",
        "all_tokens = torch.tensor(all_tokens, dtype=torch.long)\n",
        "print(f\"Total tokens: {len(all_tokens):,}\")\n",
        "print(f\"Expected: ~3.4M tokens\")\n",
        "\n",
        "\n",
        "# ============================\n",
        "# Evaluate perplexity\n",
        "# ============================\n",
        "@torch.no_grad()\n",
        "def evaluate_perplexity(model, tokens, batch_size=BATCH_SIZE, max_length=MAX_LENGTH, stride=STRIDE):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "\n",
        "    # Create sliding windows\n",
        "    sequences = []\n",
        "    for i in range(0, len(tokens) - 1, stride):\n",
        "        end_idx = min(i + max_length, len(tokens))\n",
        "        if end_idx - i > 1:  # Need at least 2 tokens (input + target)\n",
        "            sequences.append(tokens[i:end_idx])\n",
        "\n",
        "    print(f\"Created {len(sequences)} sequences with stride {stride}\")\n",
        "\n",
        "    # Process in batches\n",
        "    for batch_start in tqdm(range(0, len(sequences), batch_size), desc=\"Evaluating\"):\n",
        "        batch_seqs = sequences[batch_start:batch_start + batch_size]\n",
        "\n",
        "        # Pad sequences to same length in batch\n",
        "        max_len_in_batch = max(len(s) for s in batch_seqs)\n",
        "        batch_inputs = []\n",
        "        batch_targets = []\n",
        "        batch_masks = []\n",
        "\n",
        "        for seq in batch_seqs:\n",
        "            input_seq = seq[:-1]\n",
        "            target_seq = seq[1:]\n",
        "\n",
        "            # Pad if needed\n",
        "            pad_len = max_len_in_batch - 1 - len(input_seq)\n",
        "            if pad_len > 0:\n",
        "                input_seq = torch.cat([input_seq, torch.zeros(pad_len, dtype=torch.long)])\n",
        "                target_seq = torch.cat([target_seq, torch.zeros(pad_len, dtype=torch.long)])\n",
        "                mask = torch.cat([torch.ones(len(seq) - 1), torch.zeros(pad_len)])\n",
        "            else:\n",
        "                mask = torch.ones(len(input_seq))\n",
        "\n",
        "            batch_inputs.append(input_seq)\n",
        "            batch_targets.append(target_seq)\n",
        "            batch_masks.append(mask)\n",
        "\n",
        "        # Stack into tensors\n",
        "        inputs = torch.stack(batch_inputs).to(DEVICE)\n",
        "        targets = torch.stack(batch_targets).to(DEVICE)\n",
        "        masks = torch.stack(batch_masks).to(DEVICE)\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(inputs)  # Shape: [B, T, vocab_size]\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = F.cross_entropy(\n",
        "            logits.reshape(-1, logits.size(-1)),\n",
        "            targets.reshape(-1),\n",
        "            reduction='none'\n",
        "        )\n",
        "        loss = loss.view(targets.shape)\n",
        "\n",
        "        # Apply mask and accumulate\n",
        "        masked_loss = (loss * masks).sum()\n",
        "        masked_tokens = masks.sum()\n",
        "\n",
        "        total_loss += masked_loss.item()\n",
        "        total_tokens += masked_tokens.item()\n",
        "\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    perplexity = math.exp(avg_loss)\n",
        "\n",
        "    return perplexity, avg_loss\n",
        "\n",
        "# ============================\n",
        "# Run evaluation\n",
        "# ============================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Running WikiText-2 Benchmark\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "perplexity, avg_loss = evaluate_perplexity(model, all_tokens)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"RESULTS\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Perplexity: {perplexity:.2f}\")\n",
        "print(f\"Avg Loss: {avg_loss:.4f}\")\n",
        "print(f\"Model: {CKPT_PATH}\")\n",
        "print_cfg_summary(cfg, model)\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YLdHuFddFfkf"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Experiment 1: Slotspace Ablation Study\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from datasets import load_dataset\n",
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ============================\n",
        "# Configuration\n",
        "# ============================\n",
        "BASE_CONFIG = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"embed_dim\": 384,\n",
        "    \"num_layers\": 12,  # Smaller for faster experiments\n",
        "    \"num_heads\": 8,\n",
        "    \"num_slots\": 16,\n",
        "    \"max_seq_len\": 1024,\n",
        "    \"mlp_ratio\": 4.0,\n",
        "    \"dropout\": 0.1,\n",
        "    \"read_temperature\": 1.0,\n",
        "    \"write_temperature\": 1.0,\n",
        "    \"state_fp32\": True,\n",
        "    \"slot_dropout\": 0.05,\n",
        "    \"normalize_k\": False,\n",
        "    \"tie_weights\": True,\n",
        "    \"use_abs_pos\": False,\n",
        "    \"use_rope_keys\": True,\n",
        "    \"rope_base\": 10000.0,\n",
        "    \"use_alibi_write\": True,\n",
        "    \"alibi_strength_init\": 0.1,\n",
        "    \"learn_alibi_strength\": True,\n",
        "    \"min_strength\": 0.0,\n",
        "    \"use_content_read\": True,\n",
        "    \"content_read_init\": -4.0,\n",
        "    \"content_read_max_gamma\": 3.0,\n",
        "    \"slotspace_dim\": 32,\n",
        "    \"slotspace_gate_init\": -4.0,\n",
        "    \"slotspace_dropout\": 0.05,\n",
        "    \"slotspace_signed_weights\": True,\n",
        "    \"use_rope_slotspace\": True,\n",
        "    \"rope_base_slotspace\": 100000.0,\n",
        "    \"write_chunk_size\": 128,\n",
        "    \"slotspace_chunk_size\": 128,\n",
        "}\n",
        "\n",
        "# Ablation configurations\n",
        "ABLATIONS = {\n",
        "    \"no_slotspace\": {\n",
        "        \"use_slotspace_refine\": False,\n",
        "        \"description\": \"No slotspace refinement (baseline)\"\n",
        "    },\n",
        "    \"full\": {\n",
        "        \"use_slotspace_refine\": True,\n",
        "        \"description\": \"Full model (your current best)\"\n",
        "    },\n",
        "    \"no_signed_weights\": {\n",
        "        \"use_slotspace_refine\": True,\n",
        "        \"slotspace_signed_weights\": False,\n",
        "        \"description\": \"Slotspace with softmax instead of tanh\"\n",
        "    },\n",
        "    \"no_rope_slotspace\": {\n",
        "        \"use_slotspace_refine\": True,\n",
        "        \"use_rope_slotspace\": False,\n",
        "        \"description\": \"Slotspace without RoPE\"\n",
        "    },\n",
        "    \"weak_gate_init\": {\n",
        "        \"use_slotspace_refine\": True,\n",
        "        \"slotspace_gate_init\": -8.0,  # More closed initially\n",
        "        \"description\": \"Slotspace with weaker initial gate\"\n",
        "    },\n",
        "}\n",
        "\n",
        "# Training hyperparameters\n",
        "TRAIN_CONFIG = {\n",
        "    \"batch_size\": 32,\n",
        "    \"learning_rate\": 3e-4,\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"warmup_steps\": 500,\n",
        "    \"total_steps\": 10_000,  # Short training for quick comparison\n",
        "    \"eval_interval\": 500,\n",
        "    \"log_interval\": 100,\n",
        "    \"grad_clip\": 1.0,\n",
        "}\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ============================\n",
        "# Training Loop\n",
        "# ============================\n",
        "def train_ablation(config_name, model_config, train_config, tokenizer):\n",
        "    \"\"\"Train a single ablation\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training: {config_name}\")\n",
        "    print(f\"Description: {ABLATIONS[config_name]['description']}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Build model\n",
        "    cfg = ASMTrainConfig(**{**BASE_CONFIG, **model_config})\n",
        "    model = build_model_from_cfg(cfg).to(DEVICE)\n",
        "\n",
        "    n_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Parameters: {n_params/1e6:.2f}M\")\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=train_config[\"learning_rate\"],\n",
        "        weight_decay=train_config[\"weight_decay\"],\n",
        "        betas=(0.9, 0.95),\n",
        "    )\n",
        "\n",
        "    # Load dataset (use smaller subset for speed)\n",
        "    dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train\")\n",
        "\n",
        "    # Simple training loop\n",
        "    model.train()\n",
        "    losses = []\n",
        "    steps = []\n",
        "\n",
        "    step = 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    pbar = tqdm(total=train_config[\"total_steps\"], desc=f\"Training {config_name}\")\n",
        "\n",
        "    while step < train_config[\"total_steps\"]:\n",
        "        # Get batch (simplified - you'd want proper batching)\n",
        "        batch_texts = []\n",
        "        for _ in range(train_config[\"batch_size\"]):\n",
        "            idx = torch.randint(0, len(dataset), (1,)).item()\n",
        "            text = dataset[idx][\"text\"]\n",
        "            if len(text.strip()) > 0:\n",
        "                batch_texts.append(text)\n",
        "\n",
        "        if len(batch_texts) == 0:\n",
        "            continue\n",
        "\n",
        "        # Tokenize\n",
        "        tokens = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "        input_ids = tokens[\"input_ids\"].to(DEVICE)\n",
        "\n",
        "        if input_ids.size(1) < 2:\n",
        "            continue\n",
        "\n",
        "        # Forward\n",
        "        logits = model(input_ids[:, :-1])\n",
        "        targets = input_ids[:, 1:]\n",
        "\n",
        "        loss = F.cross_entropy(\n",
        "            logits.reshape(-1, logits.size(-1)),\n",
        "            targets.reshape(-1),\n",
        "            ignore_index=tokenizer.pad_token_id,\n",
        "        )\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), train_config[\"grad_clip\"])\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        step += 1\n",
        "\n",
        "        if step % train_config[\"log_interval\"] == 0:\n",
        "            avg_loss = running_loss / train_config[\"log_interval\"]\n",
        "            losses.append(avg_loss)\n",
        "            steps.append(step)\n",
        "            pbar.set_postfix({\"loss\": f\"{avg_loss:.4f}\"})\n",
        "            running_loss = 0.0\n",
        "\n",
        "        pbar.update(1)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    # Quick eval\n",
        "    model.eval()\n",
        "    eval_loss = 0.0\n",
        "    eval_steps = 0\n",
        "\n",
        "    val_dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"validation\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(100):  # Sample 100 validation examples\n",
        "            idx = torch.randint(0, len(val_dataset), (1,)).item()\n",
        "            text = val_dataset[idx][\"text\"]\n",
        "            if len(text.strip()) == 0:\n",
        "                continue\n",
        "\n",
        "            tokens = tokenizer([text], return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "            input_ids = tokens[\"input_ids\"].to(DEVICE)\n",
        "\n",
        "            if input_ids.size(1) < 2:\n",
        "                continue\n",
        "\n",
        "            logits = model(input_ids[:, :-1])\n",
        "            targets = input_ids[:, 1:]\n",
        "\n",
        "            loss = F.cross_entropy(\n",
        "                logits.reshape(-1, logits.size(-1)),\n",
        "                targets.reshape(-1),\n",
        "                reduction='sum',\n",
        "            )\n",
        "\n",
        "            eval_loss += loss.item()\n",
        "            eval_steps += targets.numel()\n",
        "\n",
        "    final_loss = eval_loss / eval_steps if eval_steps > 0 else float('inf')\n",
        "    final_ppl = torch.exp(torch.tensor(final_loss)).item()\n",
        "\n",
        "    print(f\"\\nFinal validation loss: {final_loss:.4f}\")\n",
        "    print(f\"Final validation PPL: {final_ppl:.2f}\")\n",
        "\n",
        "    return {\n",
        "        \"config_name\": config_name,\n",
        "        \"description\": ABLATIONS[config_name]['description'],\n",
        "        \"train_losses\": losses,\n",
        "        \"train_steps\": steps,\n",
        "        \"final_val_loss\": final_loss,\n",
        "        \"final_val_ppl\": final_ppl,\n",
        "        \"n_params\": n_params,\n",
        "    }\n",
        "\n",
        "# ============================\n",
        "# Run All Ablations\n",
        "# ============================\n",
        "# Load tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "results = []\n",
        "\n",
        "for config_name, ablation_config in ABLATIONS.items():\n",
        "    result = train_ablation(config_name, ablation_config, TRAIN_CONFIG, tokenizer)\n",
        "    results.append(result)\n",
        "\n",
        "    # Save intermediate results\n",
        "    torch.save(results, \"ablation_results.pt\")\n",
        "\n",
        "# ============================\n",
        "# Visualize Results\n",
        "# ============================\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Plot 1: Training curves\n",
        "ax = axes[0]\n",
        "for result in results:\n",
        "    ax.plot(result[\"train_steps\"], result[\"train_losses\"], label=result[\"config_name\"], linewidth=2)\n",
        "ax.set_xlabel(\"Training Steps\")\n",
        "ax.set_ylabel(\"Training Loss\")\n",
        "ax.set_title(\"Training Loss Curves\")\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Final validation PPL\n",
        "ax = axes[1]\n",
        "names = [r[\"config_name\"] for r in results]\n",
        "ppls = [r[\"final_val_ppl\"] for r in results]\n",
        "colors = ['red' if 'no_slotspace' in n else 'green' if n == 'full' else 'blue' for n in names]\n",
        "bars = ax.bar(range(len(names)), ppls, color=colors, alpha=0.7)\n",
        "ax.set_xticks(range(len(names)))\n",
        "ax.set_xticklabels(names, rotation=45, ha='right')\n",
        "ax.set_ylabel(\"Validation Perplexity\")\n",
        "ax.set_title(\"Final Validation PPL (lower is better)\")\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, ppl in zip(bars, ppls):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{ppl:.1f}',\n",
        "            ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# Plot 3: Summary table\n",
        "ax = axes[2]\n",
        "ax.axis('tight')\n",
        "ax.axis('off')\n",
        "\n",
        "table_data = []\n",
        "for r in results:\n",
        "    table_data.append([\n",
        "        r[\"config_name\"],\n",
        "        f\"{r['final_val_ppl']:.2f}\",\n",
        "        f\"{r['n_params']/1e6:.1f}M\"\n",
        "    ])\n",
        "\n",
        "table = ax.table(cellText=table_data,\n",
        "                colLabels=['Config', 'Val PPL', 'Params'],\n",
        "                cellLoc='center',\n",
        "                loc='center',\n",
        "                colWidths=[0.4, 0.3, 0.3])\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(10)\n",
        "table.scale(1, 2)\n",
        "\n",
        "# Color code rows\n",
        "for i, (cell_key) in enumerate(table.get_celld().keys()):\n",
        "    cell = table[cell_key]\n",
        "    if cell_key[0] == 0:  # Header\n",
        "        cell.set_facecolor('#40466e')\n",
        "        cell.set_text_props(weight='bold', color='white')\n",
        "    elif 'no_slotspace' in table_data[cell_key[0]-1][0]:\n",
        "        cell.set_facecolor('#ffcccc')\n",
        "    elif table_data[cell_key[0]-1][0] == 'full':\n",
        "        cell.set_facecolor('#ccffcc')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('ablation_study.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ABLATION STUDY COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Print summary\n",
        "df = pd.DataFrame([\n",
        "    {\n",
        "        \"Config\": r[\"config_name\"],\n",
        "        \"Description\": r[\"description\"],\n",
        "        \"Val PPL\": f\"{r['final_val_ppl']:.2f}\",\n",
        "        \"Params\": f\"{r['n_params']/1e6:.1f}M\"\n",
        "    }\n",
        "    for r in results\n",
        "])\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "# Calculate improvements\n",
        "baseline = [r for r in results if r[\"config_name\"] == \"no_slotspace\"][0]\n",
        "full_model = [r for r in results if r[\"config_name\"] == \"full\"][0]\n",
        "\n",
        "improvement = ((baseline[\"final_val_ppl\"] - full_model[\"final_val_ppl\"]) / baseline[\"final_val_ppl\"]) * 100\n",
        "print(f\"\\nSlotspace provides {improvement:.1f}% PPL improvement over baseline\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "v9cV2TJFHu-C"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#@title FAST Paper-Grade Probing with Proper Progress Tracking\n",
        "\n",
        "import re, random\n",
        "from dataclasses import dataclass\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# ===== SAME HELPER FUNCTIONS =====\n",
        "def set_all_seeds(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def make_nonce_word(rng: np.random.Generator, min_len=4, max_len=8):\n",
        "    letters = np.array(list(\"abcdefghijklmnopqrstuvwxyz\"))\n",
        "    L = int(rng.integers(min_len, max_len + 1))\n",
        "    return \"\".join(rng.choice(letters, size=L))\n",
        "\n",
        "def apply_control(words, labels, control: str, seed: int):\n",
        "    if control == \"none\":\n",
        "        return words, labels\n",
        "\n",
        "    n = min(len(words), len(labels))\n",
        "    words = words[:n]\n",
        "    labels = labels[:n]\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    if control == \"nonce\":\n",
        "        return [make_nonce_word(rng) for _ in range(n)], list(labels)\n",
        "    if control == \"shuffle\":\n",
        "        pairs = list(zip(words, labels))\n",
        "        rng.shuffle(pairs)\n",
        "        w2, l2 = zip(*pairs) if pairs else ([], [])\n",
        "        return list(w2), list(l2)\n",
        "    raise ValueError(f\"Unknown control={control!r}\")\n",
        "\n",
        "def depths_for_sent(sent):\n",
        "    root = sent.root\n",
        "    depth = {root.i: 0}\n",
        "    stack = [root]\n",
        "    while stack:\n",
        "        tok = stack.pop()\n",
        "        for child in tok.children:\n",
        "            if child.i < sent.start or child.i >= sent.end:\n",
        "                continue\n",
        "            if child.i not in depth:\n",
        "                depth[child.i] = depth[tok.i] + 1\n",
        "                stack.append(child)\n",
        "    return [int(depth.get(t.i, 0)) for t in sent]\n",
        "\n",
        "def build_spacy_labeled_sentences(\n",
        "    max_sents=2000,\n",
        "    text_source=\"wikitext\",\n",
        "    spacy_model=\"en_core_web_sm\",\n",
        "    seed=42,\n",
        "    min_len=3,\n",
        "    max_len=40,\n",
        "):\n",
        "    set_all_seeds(seed)\n",
        "    from datasets import load_dataset\n",
        "    import spacy\n",
        "\n",
        "    nlp = spacy.load(spacy_model, disable=[\"ner\"])\n",
        "\n",
        "    if text_source == \"wikitext\":\n",
        "        ds = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train\")\n",
        "        texts = ds[\"text\"]\n",
        "    else:\n",
        "        ds = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train[:1%]\")\n",
        "        texts = ds[\"text\"]\n",
        "\n",
        "    idx = np.arange(len(texts))\n",
        "    rng = np.random.default_rng(seed)\n",
        "    rng.shuffle(idx)\n",
        "\n",
        "    examples = []\n",
        "    sent_id = 0\n",
        "\n",
        "    for i in idx:\n",
        "        line = texts[int(i)]\n",
        "        if not isinstance(line, str):\n",
        "            continue\n",
        "        line = line.strip()\n",
        "        if len(line) < 20 or (line.startswith(\"=\") and line.endswith(\"=\")):\n",
        "            continue\n",
        "\n",
        "        doc = nlp(line)\n",
        "        for sent in doc.sents:\n",
        "            words = [t.text for t in sent]\n",
        "            if not (min_len <= len(words) <= max_len):\n",
        "                continue\n",
        "\n",
        "            upos = [t.pos_ for t in sent]\n",
        "            depth = depths_for_sent(sent)\n",
        "\n",
        "            text = \" \".join(words)\n",
        "            examples.append({\n",
        "                \"text\": text,\n",
        "                \"words\": words,\n",
        "                \"upos\": upos,\n",
        "                \"depth\": depth,\n",
        "                \"sent_id\": sent_id\n",
        "            })\n",
        "            sent_id += 1\n",
        "            if sent_id >= max_sents:\n",
        "                return examples\n",
        "\n",
        "    return examples\n",
        "\n",
        "WORD_RE = re.compile(r\"\\S+\")\n",
        "\n",
        "def word_spans(text: str):\n",
        "    return [(m.start(), m.end(), m.group()) for m in WORD_RE.finditer(text)]\n",
        "\n",
        "def token_overlaps(t_start, t_end, w_start, w_end):\n",
        "    return (t_start < w_end) and (t_end > w_start)\n",
        "\n",
        "class ASALayerFeatureExtractor:\n",
        "    def __init__(self, model, tokenizer, device):\n",
        "        self.model = model\n",
        "        self.tok = tokenizer\n",
        "        self.device = device\n",
        "\n",
        "    def _forward_to_layer(self, input_ids, layer_idx):\n",
        "        x = self.model.tok(input_ids)\n",
        "        if getattr(self.model, \"use_abs_pos\", False):\n",
        "            pos = torch.arange(input_ids.size(1), device=input_ids.device).unsqueeze(0)\n",
        "            x = x + self.model.pos(pos)\n",
        "        x = self.model.drop(x)\n",
        "\n",
        "        info_at = None\n",
        "        for i, blk in enumerate(self.model.blocks):\n",
        "            x, info = blk(x, return_info=True)\n",
        "            if i == layer_idx:\n",
        "                info_at = info\n",
        "                break\n",
        "        if info_at is None:\n",
        "            raise ValueError(f\"Layer {layer_idx} not found. blocks={len(self.model.blocks)}\")\n",
        "        return info_at\n",
        "\n",
        "    def extract_word_features(self, text, layer_idx, feature_type):\n",
        "        toks = self.tok(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            return_offsets_mapping=True,\n",
        "            add_special_tokens=False,\n",
        "        )\n",
        "        input_ids = toks[\"input_ids\"].to(self.device)\n",
        "        offsets = toks[\"offset_mapping\"][0].tolist()\n",
        "\n",
        "        spans = word_spans(text)\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            info = self._forward_to_layer(input_ids, layer_idx)\n",
        "\n",
        "            if feature_type == \"content\":\n",
        "                feats = info.get(\"read_logits_content\", None)\n",
        "                if feats is None:\n",
        "                    raise ValueError(\"read_logits_content is None\")\n",
        "            elif feature_type == \"key\":\n",
        "                feats = info.get(\"read_logits_key\", None)\n",
        "                if feats is None:\n",
        "                    raise ValueError(\"read_logits_key missing\")\n",
        "            elif feature_type == \"routing\":\n",
        "                feats = info.get(\"read_weights\", None)\n",
        "                if feats is None:\n",
        "                    raise ValueError(\"read_weights missing\")\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown feature_type={feature_type!r}\")\n",
        "\n",
        "            feats = feats.mean(dim=1).squeeze(0).detach().cpu()\n",
        "            T = feats.shape[0]\n",
        "\n",
        "        word_feats = []\n",
        "        for (w_start, w_end, _) in spans:\n",
        "            idx = []\n",
        "            for t, (t_start, t_end) in enumerate(offsets):\n",
        "                if t_start == t_end == 0:\n",
        "                    continue\n",
        "                if 0 <= t < T and token_overlaps(t_start, t_end, w_start, w_end):\n",
        "                    idx.append(t)\n",
        "            if len(idx) == 0:\n",
        "                word_feats.append(None)\n",
        "            else:\n",
        "                word_feats.append(feats[idx].mean(dim=0).numpy())\n",
        "        return word_feats\n",
        "\n",
        "# ===== CACHE FEATURES (MAJOR SPEEDUP!) =====\n",
        "\n",
        "def cache_all_features(examples, extractor, feature_types, tasks, controls, seed_base=42):\n",
        "    \"\"\"\n",
        "    Pre-extract ALL features for all layers to avoid redundant forward passes.\n",
        "    This is the KEY optimization!\n",
        "    \"\"\"\n",
        "    n_layers = len(extractor.model.blocks)\n",
        "\n",
        "    # Cache structure: cache[sent_id][control][feature_type][layer_idx] = word_features\n",
        "    cache = defaultdict(lambda: defaultdict(lambda: defaultdict(dict)))\n",
        "\n",
        "    total_extractions = len(examples) * len(controls) * len(feature_types) * n_layers\n",
        "\n",
        "    print(f\"Pre-extracting features for {len(examples)} sentences...\")\n",
        "    print(f\"Total forward passes: {total_extractions}\")\n",
        "\n",
        "    with tqdm(total=total_extractions, desc=\"Caching features\") as pbar:\n",
        "        for ex in examples:\n",
        "            sent_id = ex[\"sent_id\"]\n",
        "            words = ex[\"words\"]\n",
        "\n",
        "            for control in controls:\n",
        "                # Apply control once per sentence\n",
        "                words_c, _ = apply_control(words, ex[\"upos\"], control, seed_base + sent_id)\n",
        "                text_c = \" \".join(words_c)\n",
        "\n",
        "                for ft in feature_types:\n",
        "                    for layer_idx in range(n_layers):\n",
        "                        try:\n",
        "                            feats = extractor.extract_word_features(text_c, layer_idx, ft)\n",
        "                            cache[sent_id][control][ft][layer_idx] = feats\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error caching sent={sent_id}, ctrl={control}, ft={ft}, layer={layer_idx}: {e}\")\n",
        "                            cache[sent_id][control][ft][layer_idx] = None\n",
        "\n",
        "                        pbar.update(1)\n",
        "\n",
        "    return cache\n",
        "\n",
        "def build_arrays_from_cache(examples, feature_cache, layer_idx, feature_type, task, control):\n",
        "    \"\"\"Build probe arrays from pre-cached features\"\"\"\n",
        "    X, y, groups = [], [], []\n",
        "\n",
        "    for ex in examples:\n",
        "        sent_id = ex[\"sent_id\"]\n",
        "        labels = ex[task]\n",
        "\n",
        "        feats_word = feature_cache.get(sent_id, {}).get(control, {}).get(feature_type, {}).get(layer_idx, None)\n",
        "\n",
        "        if feats_word is None:\n",
        "            continue\n",
        "\n",
        "        n = min(len(feats_word), len(labels))\n",
        "        for i in range(n):\n",
        "            if feats_word[i] is None:\n",
        "                continue\n",
        "            X.append(feats_word[i])\n",
        "            y.append(labels[i])\n",
        "            groups.append(sent_id)\n",
        "\n",
        "    return X, y, groups\n",
        "\n",
        "# ===== FAST PROBE RUNNER =====\n",
        "\n",
        "def fit_eval_probe(X, y, groups, seed):\n",
        "    X = np.asarray(X)\n",
        "    y = np.asarray(y)\n",
        "    groups = np.asarray(groups)\n",
        "\n",
        "    counts = Counter(y.tolist())\n",
        "    majority = max(counts.values()) / len(y)\n",
        "    uniform = 1.0 / len(counts)\n",
        "\n",
        "    gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=seed)\n",
        "    tr, te = next(gss.split(X, y, groups))\n",
        "    Xtr, Xte = X[tr], X[te]\n",
        "    ytr, yte = y[tr], y[te]\n",
        "\n",
        "    clf = LogisticRegression(\n",
        "        max_iter=3000,\n",
        "        random_state=seed,\n",
        "        class_weight=\"balanced\",\n",
        "    )\n",
        "    clf.fit(Xtr, ytr)\n",
        "    pred = clf.predict(Xte)\n",
        "\n",
        "    acc = accuracy_score(yte, pred)\n",
        "    f1 = f1_score(yte, pred, average=\"macro\", zero_division=0)\n",
        "    return float(acc), float(f1), float(uniform), float(majority)\n",
        "\n",
        "def run_all_layers_fast(examples, feature_cache, feature_types, tasks, controls, seeds):\n",
        "    \"\"\"Fast version using cached features\"\"\"\n",
        "    n_layers = len(extractor.model.blocks)\n",
        "    results = {task: {control: {ft: {\"acc\": [], \"f1\": [], \"uniform\": [], \"majority\": []}\n",
        "                                for ft in feature_types}\n",
        "                      for control in controls}\n",
        "               for task in tasks}\n",
        "\n",
        "    total_probes = n_layers * len(tasks) * len(controls) * len(feature_types)\n",
        "\n",
        "    with tqdm(total=total_probes, desc=\"Running probes\") as pbar:\n",
        "        for task in tasks:\n",
        "            for control in controls:\n",
        "                for ft in feature_types:\n",
        "                    for layer_idx in range(n_layers):\n",
        "                        accs, f1s, unis, majs = [], [], [], []\n",
        "\n",
        "                        for s in seeds:\n",
        "                            X, y, g = build_arrays_from_cache(\n",
        "                                examples, feature_cache, layer_idx, ft, task, control\n",
        "                            )\n",
        "\n",
        "                            if len(X) < 200 or len(set(y)) < 2:\n",
        "                                acc, f1, uni, maj = (np.nan, np.nan, np.nan, np.nan)\n",
        "                            else:\n",
        "                                acc, f1, uni, maj = fit_eval_probe(X, y, g, seed=s)\n",
        "\n",
        "                            accs.append(acc)\n",
        "                            f1s.append(f1)\n",
        "                            unis.append(uni)\n",
        "                            majs.append(maj)\n",
        "\n",
        "                        results[task][control][ft][\"acc\"].append((np.nanmean(accs), np.nanstd(accs)))\n",
        "                        results[task][control][ft][\"f1\"].append((np.nanmean(f1s), np.nanstd(f1s)))\n",
        "                        results[task][control][ft][\"uniform\"].append(np.nanmean(unis))\n",
        "                        results[task][control][ft][\"majority\"].append(np.nanmean(majs))\n",
        "\n",
        "                        pbar.set_postfix({\n",
        "                            'task': task,\n",
        "                            'ctrl': control,\n",
        "                            'ft': ft,\n",
        "                            'layer': layer_idx\n",
        "                        })\n",
        "                        pbar.update(1)\n",
        "\n",
        "    return results\n",
        "\n",
        "def plot_task(results, task, control, feature_types):\n",
        "    n_layers = len(next(iter(results[task][control].values()))[\"acc\"])\n",
        "    layers = np.arange(n_layers)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "    # Accuracy\n",
        "    ax = axes[0]\n",
        "    for ft in feature_types:\n",
        "        m = np.array([x[0] for x in results[task][control][ft][\"acc\"]], dtype=float)\n",
        "        s = np.array([x[1] for x in results[task][control][ft][\"acc\"]], dtype=float)\n",
        "        ax.plot(layers, m, marker=\"o\", linewidth=2, label=f\"{ft}\", markersize=4)\n",
        "        ax.fill_between(layers, m-s, m+s, alpha=0.15)\n",
        "\n",
        "    uni = np.array(results[task][control][\"key\"][\"uniform\"], dtype=float)\n",
        "    maj = np.array(results[task][control][\"key\"][\"majority\"], dtype=float)\n",
        "    ax.plot(layers, uni, linestyle=\"--\", linewidth=2, label=\"Uniform\", alpha=0.7)\n",
        "    ax.plot(layers, maj, linestyle=\":\",  linewidth=2, label=\"Majority\", alpha=0.7)\n",
        "\n",
        "    ax.set_title(f\"{task.upper()} Probing Accuracy (control={control})\", fontsize=13, fontweight='bold')\n",
        "    ax.set_xlabel(\"Layer\", fontsize=11)\n",
        "    ax.set_ylabel(\"Accuracy\", fontsize=11)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.legend(fontsize=10)\n",
        "\n",
        "    # Macro-F1\n",
        "    ax = axes[1]\n",
        "    for ft in feature_types:\n",
        "        m = np.array([x[0] for x in results[task][control][ft][\"f1\"]], dtype=float)\n",
        "        s = np.array([x[1] for x in results[task][control][ft][\"f1\"]], dtype=float)\n",
        "        ax.plot(layers, m, marker=\"o\", linewidth=2, label=f\"{ft}\", markersize=4)\n",
        "        ax.fill_between(layers, m-s, m+s, alpha=0.15)\n",
        "\n",
        "    ax.set_title(f\"{task.upper()} Macro-F1 (control={control})\", fontsize=13, fontweight='bold')\n",
        "    ax.set_xlabel(\"Layer\", fontsize=11)\n",
        "    ax.set_ylabel(\"Macro-F1\", fontsize=11)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.legend(fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'probing_{task}_{control}.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# ===== MAIN =====\n",
        "set_all_seeds(42)\n",
        "\n",
        "print(\"Loading model...\")\n",
        "model, cfg, _, ckpt = load_model_and_cfg(CKPT_PATH)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)\n",
        "extractor = ASALayerFeatureExtractor(model, tokenizer, DEVICE)\n",
        "\n",
        "print(\"\\nBuilding labeled sentences from WikiText with spaCy...\")\n",
        "print(\"(This may take a few minutes on first run...)\")\n",
        "examples = build_spacy_labeled_sentences(max_sents=1000, text_source=\"wikitext\", seed=42)\n",
        "print(f\" Prepared {len(examples)} labeled sentences.\")\n",
        "\n",
        "feature_types = (\"content\", \"key\", \"routing\")\n",
        "tasks = (\"upos\", \"depth\")\n",
        "controls = (\"none\", \"nonce\")  # Skip shuffle for speed\n",
        "seeds = (1, 2, 3)  # Reduced for speed\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PHASE 1: Caching Features (this is the slow part)\")\n",
        "print(\"=\"*70)\n",
        "feature_cache = cache_all_features(examples, extractor, feature_types, tasks, controls)\n",
        "print(\" Feature caching complete!\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PHASE 2: Running Probes (this is fast!)\")\n",
        "print(\"=\"*70)\n",
        "results = run_all_layers_fast(examples, feature_cache, feature_types, tasks, controls, seeds)\n",
        "print(\" All probes complete!\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Generating plots...\")\n",
        "print(\"=\"*70)\n",
        "for task in tasks:\n",
        "    for control in controls:\n",
        "        plot_task(results, task, control, feature_types)\n",
        "\n",
        "print(\"\\n All done! Check the generated plots.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aQPRK3sPFlQS"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Experiment 3: Slot Intervention Experiments\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from collections import defaultdict\n",
        "\n",
        "# ============================\n",
        "# Intervention Functions\n",
        "# ============================\n",
        "\n",
        "def mask_slots(model, slot_indices, layer_idx=6):\n",
        "    \"\"\"\n",
        "    Mask specific slots at a specific layer\n",
        "    Returns a hook that will be registered\n",
        "    \"\"\"\n",
        "    def hook(module, input, output):\n",
        "        # output is (out, info)\n",
        "        out, info = output\n",
        "\n",
        "        # Create slot mask\n",
        "        B, H, T, K = info[\"read_weights\"].shape\n",
        "        mask = torch.ones((K,), device=out.device)\n",
        "        mask[slot_indices] = 0.0\n",
        "\n",
        "        # Apply mask to read weights\n",
        "        masked_weights = info[\"read_weights\"] * mask.view(1, 1, 1, K)\n",
        "        masked_weights = masked_weights / masked_weights.sum(dim=-1, keepdim=True).clamp_min(1e-8)\n",
        "\n",
        "        # Re-run the attention with masked weights\n",
        "        # (This is simplified - you'd need to access slot states)\n",
        "\n",
        "        return out, info\n",
        "\n",
        "    # Register hook on specific layer\n",
        "    handle = model.blocks[layer_idx].asa.register_forward_hook(hook)\n",
        "    return handle\n",
        "\n",
        "def intervene_and_evaluate(model, tokenizer, test_sentences, intervention_fn, device):\n",
        "    \"\"\"\n",
        "    Apply intervention and measure effect on predictions\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for sent in tqdm(test_sentences, desc=\"Evaluating intervention\"):\n",
        "            tokens = tokenizer(sent, return_tensors=\"pt\").to(device)\n",
        "            input_ids = tokens[\"input_ids\"]\n",
        "\n",
        "            if input_ids.size(1) < 2:\n",
        "                continue\n",
        "\n",
        "            # Baseline prediction\n",
        "            logits_base = model(input_ids)\n",
        "            probs_base = F.softmax(logits_base[0, -1], dim=-1)\n",
        "            top_base = torch.topk(probs_base, k=5)\n",
        "\n",
        "            # Intervened prediction\n",
        "            handle = intervention_fn(model)\n",
        "            logits_int = model(input_ids)\n",
        "            handle.remove()\n",
        "\n",
        "            probs_int = F.softmax(logits_int[0, -1], dim=-1)\n",
        "            top_int = torch.topk(probs_int, k=5)\n",
        "\n",
        "            # Measure change\n",
        "            kl_div = F.kl_div(\n",
        "                torch.log(probs_int + 1e-8),\n",
        "                probs_base,\n",
        "                reduction='sum'\n",
        "            ).item()\n",
        "\n",
        "            results.append({\n",
        "                \"sentence\": sent,\n",
        "                \"kl_divergence\": kl_div,\n",
        "                \"top_tokens_base\": [tokenizer.decode([t]) for t in top_base.indices],\n",
        "                \"top_tokens_int\": [tokenizer.decode([t]) for t in top_int.indices],\n",
        "                \"prob_shift\": (probs_base[top_base.indices[0]] - probs_int[top_base.indices[0]]).item()\n",
        "            })\n",
        "\n",
        "    return results\n",
        "\n",
        "# ============================\n",
        "# Specific Intervention Tests\n",
        "# ============================\n",
        "\n",
        "# Load model\n",
        "print(\"Loading model...\")\n",
        "model, cfg, _, ckpt = load_model_and_cfg(CKPT_PATH)\n",
        "tokenizer = ckpt[\"tokenizer\"]\n",
        "\n",
        "test_sentences = [\n",
        "    \"The capital of France is\",\n",
        "    \"The quick brown fox jumps over the\",\n",
        "    \"In the beginning there was\",\n",
        "    \"She walked to the\",\n",
        "    \"The president of the United States is\",\n",
        "    \"Two plus two equals\",\n",
        "    \"The color of the sky is\",\n",
        "    \"The largest planet in our solar system is\",\n",
        "    \"Water boils at\",\n",
        "    \"The opposite of hot is\",\n",
        "]\n",
        "\n",
        "# Test 1: Mask high-usage slots\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"INTERVENTION 1: Masking Top-Used Slots\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# First, identify which slots are used most\n",
        "slot_usage = defaultdict(float)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for sent in test_sentences[:5]:\n",
        "        tokens = tokenizer(sent, return_tensors=\"pt\").to(DEVICE)\n",
        "        logits, infos = model(tokens[\"input_ids\"], return_info=True)\n",
        "\n",
        "        # Aggregate read weights from middle layer\n",
        "        read_w = infos[6][\"read_weights\"]  # [B, H, T, K]\n",
        "        usage = read_w.mean(dim=(0, 1, 2))  # [K]\n",
        "\n",
        "        for k in range(usage.size(0)):\n",
        "            slot_usage[k] += usage[k].item()\n",
        "\n",
        "# Sort by usage\n",
        "sorted_slots = sorted(slot_usage.items(), key=lambda x: x[1], reverse=True)\n",
        "print(\"Top 5 most-used slots:\")\n",
        "for slot_idx, usage in sorted_slots[:5]:\n",
        "    print(f\"  Slot {slot_idx}: {usage:.4f}\")\n",
        "\n",
        "top_slots = [s[0] for s in sorted_slots[:3]]\n",
        "\n",
        "# Intervention: mask top slots\n",
        "intervention_top = lambda m: mask_slots(m, top_slots, layer_idx=6)\n",
        "results_top = intervene_and_evaluate(model, tokenizer, test_sentences, intervention_top, DEVICE)\n",
        "\n",
        "print(\"\\nEffect of masking top-used slots:\")\n",
        "print(f\"Average KL divergence: {np.mean([r['kl_divergence'] for r in results_top]):.4f}\")\n",
        "\n",
        "# Test 2: Mask bottom-used slots (should have less effect)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"INTERVENTION 2: Masking Least-Used Slots\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "bottom_slots = [s[0] for s in sorted_slots[-3:]]\n",
        "intervention_bottom = lambda m: mask_slots(m, bottom_slots, layer_idx=6)\n",
        "results_bottom = intervene_and_evaluate(model, tokenizer, test_sentences, intervention_bottom, DEVICE)\n",
        "\n",
        "print(\"\\nEffect of masking least-used slots:\")\n",
        "print(f\"Average KL divergence: {np.mean([r['kl_divergence'] for r in results_bottom]):.4f}\")\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Plot 1: KL divergence comparison\n",
        "ax = axes[0, 0]\n",
        "conditions = ['Baseline\\n(no masking)', 'Mask Top-3\\nSlots', 'Mask Bottom-3\\nSlots']\n",
        "kls = [\n",
        "    0,\n",
        "    np.mean([r['kl_divergence'] for r in results_top]),\n",
        "    np.mean([r['kl_divergence'] for r in results_bottom])\n",
        "]\n",
        "bars = ax.bar(conditions, kls, color=['green', 'red', 'blue'], alpha=0.7)\n",
        "ax.set_ylabel('KL Divergence')\n",
        "ax.set_title('Effect of Slot Masking on Predictions')\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "for bar, kl in zip(bars, kls):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{kl:.3f}',\n",
        "            ha='center', va='bottom')\n",
        "\n",
        "# Plot 2: Per-sentence effects\n",
        "ax = axes[0, 1]\n",
        "sent_names = [f\"S{i+1}\" for i in range(len(test_sentences))]\n",
        "kls_top = [r['kl_divergence'] for r in results_top]\n",
        "kls_bottom = [r['kl_divergence'] for r in results_bottom]\n",
        "\n",
        "x = np.arange(len(sent_names))\n",
        "width = 0.35\n",
        "\n",
        "ax.bar(x - width/2, kls_top, width, label='Mask Top Slots', color='red', alpha=0.7)\n",
        "ax.bar(x + width/2, kls_bottom, width, label='Mask Bottom Slots', color='blue', alpha=0.7)\n",
        "\n",
        "ax.set_xlabel('Sentence')\n",
        "ax.set_ylabel('KL Divergence')\n",
        "ax.set_title('Intervention Effect Per Sentence')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(sent_names, rotation=45)\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Plot 3: Example predictions\n",
        "ax = axes[1, 0]\n",
        "ax.axis('off')\n",
        "example_idx = 0\n",
        "example = results_top[example_idx]\n",
        "\n",
        "text = f\"Sentence: '{example['sentence']}'\\n\\n\"\n",
        "text += \"Top predictions (baseline):\\n\"\n",
        "for i, tok in enumerate(example['top_tokens_base'][:3]):\n",
        "    text += f\"  {i+1}. {tok}\\n\"\n",
        "text += \"\\nTop predictions (masked top slots):\\n\"\n",
        "for i, tok in enumerate(example['top_tokens_int'][:3]):\n",
        "    text += f\"  {i+1}. {tok}\\n\"\n",
        "text += f\"\\nKL divergence: {example['kl_divergence']:.4f}\"\n",
        "\n",
        "ax.text(0.1, 0.5, text, fontsize=11, family='monospace',\n",
        "        verticalalignment='center')\n",
        "\n",
        "# Plot 4: Slot usage distribution\n",
        "ax = axes[1, 1]\n",
        "slot_ids = list(range(len(sorted_slots)))\n",
        "usages = [sorted_slots[i][1] for i in range(len(sorted_slots))]\n",
        "\n",
        "bars = ax.bar(slot_ids, usages, color='steelblue', alpha=0.7)\n",
        "\n",
        "# Highlight intervened slots\n",
        "for slot_idx in top_slots:\n",
        "    bars[slot_idx].set_color('red')\n",
        "    bars[slot_idx].set_alpha(1.0)\n",
        "\n",
        "for slot_idx in bottom_slots:\n",
        "    bars[slot_idx].set_color('blue')\n",
        "    bars[slot_idx].set_alpha(1.0)\n",
        "\n",
        "ax.set_xlabel('Slot Index')\n",
        "ax.set_ylabel('Average Usage')\n",
        "ax.set_title('Slot Usage (Red=Top, Blue=Bottom)')\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('intervention_results.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"INTERVENTION SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Masking top-used slots causes {np.mean([r['kl_divergence'] for r in results_top]):.3f} KL divergence\")\n",
        "print(f\"Masking bottom-used slots causes {np.mean([r['kl_divergence'] for r in results_bottom]):.3f} KL divergence\")\n",
        "print(f\"\\nRatio: {np.mean([r['kl_divergence'] for r in results_top]) / np.mean([r['kl_divergence'] for r in results_bottom]):.2f}x more effect\")\n",
        "print(\"\\nSlots are causally important! \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WRX5cPoh0cwQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Run WikiText-103 Benchmark on Multiple Models\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from tqdm.auto import tqdm\n",
        "import math\n",
        "\n",
        "# ============================\n",
        "# Configuration\n",
        "# ============================\n",
        "BATCH_SIZE = 8\n",
        "STRIDE = 512\n",
        "\n",
        "class TestResults:\n",
        "    def __init__(self, cfg, avg_loss, ppl, num_params):\n",
        "        self.cfg = cfg\n",
        "        self.avg_loss = avg_loss\n",
        "        self.ppl = ppl\n",
        "        self.num_params = num_params\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"TestResults(ppl={self.ppl:.2f}, loss={self.avg_loss:.4f}, params={self.num_params/1e6:.1f}M)\"\n",
        "\n",
        "CKPT_PATHS = [\n",
        "    \"./drive/MyDrive/asm_outputs/asm_wikitext_1024t_384d_32sd_16s_35l/best.pt\",\n",
        "    \"./drive/MyDrive/asm_outputs/asm_wikitext_1024t_256d_16a_11l_75ksteps/best.pt\",\n",
        "    \"./drive/MyDrive/asm_outputs/asm_wikitext_1024t_384d_16a_15l/best.pt\",\n",
        "    \"./drive/MyDrive/asm_outputs/asm_wikitext_1024t_384d_32a_5l/best.pt\",\n",
        "    \"./drive/MyDrive/asm_outputs/asm_wikitext_3072t_384d_32sd_16s_7l/best.pt\",\n",
        "    \"./drive/MyDrive/asm_outputs/asm_wikitext_1024t_640d_64sd_24s_12l/best.pt\",\n",
        "    \"./drive/MyDrive/asm_outputs/asm_wikitext_1024t_384d_16sd_8s_21l/best.pt\",\n",
        "    \"./drive/MyDrive/asm_outputs/asm_wikitext_1024t_768d_32sd_32cs_17l/best.pt\",\n",
        "    \"./drive/MyDrive/asm_outputs/asm_wikitext_1024t_640d_64sd_24s_17l/best.pt\",\n",
        "]\n",
        "\n",
        "# ============================\n",
        "# Load and tokenize dataset ONCE\n",
        "# ============================\n",
        "print(\"=\"*60)\n",
        "print(\"Loading WikiText-103 test set...\")\n",
        "print(\"=\"*60)\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"test\")\n",
        "print(f\"Number of examples: {len(dataset)}\")\n",
        "\n",
        "# Get tokenizer from first checkpoint\n",
        "print(\"\\nLoading tokenizer from first checkpoint...\")\n",
        "first_ckpt = torch.load(CKPT_PATHS[0], map_location=\"cpu\")\n",
        "if first_ckpt[\"cfg\"].get(\"tokenizer_name\", None) is not None:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(ckpt[\"cfg\"].get(\"tokenizer_name\", \"gpt2\"), use_fast=True)\n",
        "print(f\" Loaded tokenizer (vocab size: {len(tokenizer)})\")\n",
        "\n",
        "# Concatenate ALL text first, then tokenize\n",
        "print(\"\\nConcatenating all text...\")\n",
        "full_text = \"\\n\".join(dataset[\"text\"])\n",
        "print(f\"Total characters: {len(full_text):,}\")\n",
        "\n",
        "print(\"Tokenizing entire dataset...\")\n",
        "all_tokens = torch.tensor(tokenizer.encode(full_text), dtype=torch.long)\n",
        "print(f\"Total tokens: {len(all_tokens):,}\")\n",
        "print(f\"Expected: ~3.4M tokens\")\n",
        "\n",
        "if len(all_tokens) < 3_000_000:\n",
        "    print(\" Warning: Token count seems low. Expected ~3.4M tokens.\")\n",
        "else:\n",
        "    print(\" Token count looks good!\")\n",
        "\n",
        "# ============================\n",
        "# Evaluation function\n",
        "# ============================\n",
        "@torch.no_grad()\n",
        "def evaluate_perplexity(model, tokens, batch_size=BATCH_SIZE, stride=STRIDE):\n",
        "    model.eval()\n",
        "    max_length = model.cfg.max_seq_len if hasattr(model, 'cfg') else 1024\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "\n",
        "    # Create sliding windows\n",
        "    sequences = []\n",
        "    for i in range(0, len(tokens) - 1, stride):\n",
        "        end_idx = min(i + max_length, len(tokens))\n",
        "        if end_idx - i > 1:\n",
        "            sequences.append(tokens[i:end_idx])\n",
        "\n",
        "    print(f\"  Created {len(sequences)} sequences (stride={stride}, max_len={max_length})\")\n",
        "\n",
        "    # Process in batches\n",
        "    for batch_start in tqdm(range(0, len(sequences), batch_size), desc=\"  Evaluating\", leave=False):\n",
        "        batch_seqs = sequences[batch_start:batch_start + batch_size]\n",
        "        max_len_in_batch = max(len(s) for s in batch_seqs)\n",
        "\n",
        "        batch_inputs, batch_targets, batch_masks = [], [], []\n",
        "\n",
        "        for seq in batch_seqs:\n",
        "            input_seq = seq[:-1]\n",
        "            target_seq = seq[1:]\n",
        "            pad_len = max_len_in_batch - 1 - len(input_seq)\n",
        "\n",
        "            if pad_len > 0:\n",
        "                input_seq = torch.cat([input_seq, torch.zeros(pad_len, dtype=torch.long)])\n",
        "                target_seq = torch.cat([target_seq, torch.zeros(pad_len, dtype=torch.long)])\n",
        "                mask = torch.cat([torch.ones(len(seq) - 1), torch.zeros(pad_len)])\n",
        "            else:\n",
        "                mask = torch.ones(len(input_seq))\n",
        "\n",
        "            batch_inputs.append(input_seq)\n",
        "            batch_targets.append(target_seq)\n",
        "            batch_masks.append(mask)\n",
        "\n",
        "        inputs = torch.stack(batch_inputs).to(DEVICE)\n",
        "        targets = torch.stack(batch_targets).to(DEVICE)\n",
        "        masks = torch.stack(batch_masks).to(DEVICE)\n",
        "\n",
        "        logits = model(inputs)\n",
        "        loss = F.cross_entropy(\n",
        "            logits.reshape(-1, logits.size(-1)),\n",
        "            targets.reshape(-1),\n",
        "            reduction='none'\n",
        "        )\n",
        "        loss = loss.view(targets.shape)\n",
        "\n",
        "        total_loss += (loss * masks).sum().item()\n",
        "        total_tokens += masks.sum().item()\n",
        "\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    perplexity = math.exp(avg_loss)\n",
        "    return perplexity, avg_loss\n",
        "\n",
        "# ============================\n",
        "# Evaluate all models\n",
        "# ============================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EVALUATING MODELS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "results = []\n",
        "\n",
        "for i, CKPT_PATH in enumerate(CKPT_PATHS, 1):\n",
        "    print(f\"\\n[{i}/{len(CKPT_PATHS)}] Loading: {CKPT_PATH.split('/')[-2]}\")\n",
        "\n",
        "    try:\n",
        "        model, cfg, cfg_dict, ckpt = load_model_and_cfg(CKPT_PATH)\n",
        "        print_cfg_summary(cfg, model)\n",
        "\n",
        "        print(\"  Running evaluation...\")\n",
        "        perplexity, avg_loss = evaluate_perplexity(model, all_tokens)\n",
        "\n",
        "        num_params, _ = count_params(model)\n",
        "        results.append(TestResults(cfg, avg_loss, perplexity, num_params))\n",
        "\n",
        "        print(f\"   Perplexity: {perplexity:.2f} | Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Free memory\n",
        "        del model, cfg, cfg_dict, ckpt\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   Error: {e}\")\n",
        "        continue\n",
        "\n",
        "# ============================\n",
        "# Summary results\n",
        "# ============================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SUMMARY RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Sort by perplexity\n",
        "results_sorted = sorted(results, key=lambda x: x.ppl)\n",
        "\n",
        "print(f\"\\n{'Rank':<6}{'PPL':<10}{'Loss':<10}{'Params':<12}{'Layers':<8}{'d_model':<10}{'Slots':<8}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "for rank, result in enumerate(results_sorted, 1):\n",
        "    cfg_sum = cfg_summary(result.cfg)\n",
        "    print(f\"{rank:<6}{result.ppl:<10.2f}{result.avg_loss:<10.4f}{result.num_params/1e6:<12.1f}\"\n",
        "          f\"{cfg_sum['layers']:<8}{cfg_sum['d_model']:<10}{cfg_sum['slots']:<8}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"Best model: PPL = {results_sorted[0].ppl:.2f}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i78c8kyHuIim"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "class TestResults:\n",
        "    def __init__(cfg, avg_loss, ppl):\n",
        "        self.cfg = cfg\n",
        "        self.avg_loss = avg_loss\n",
        "        self.ppl = ppl\n",
        "\n",
        "\n",
        "CKPT_PATHS = [\n",
        "    \"./drive/MyDrive/asm_outputs/asm_wikitext_1024t_384d_32sd_16s_35l/best.pt\",\n",
        "    \"./drive/MyDrive/asm_outputs/asm_wikitext_1024t_256d_16a_11l_75ksteps/best.pt\",\n",
        "    \"./drive/MyDrive/asm_outputs/asm_wikitext_1024t_384d_16a_15l/best.pt\",\n",
        "    \"./drive/MyDrive/asm_outputs/asm_wikitext_1024t_384d_32a_5l/best.pt\",\n",
        "    \"./drive/MyDrive/asm_outputs/asm_wikitext_3072t_384d_32sd_16s_7l/best.pt\",\n",
        "    \"./drive/MyDrive/asm_outputs/asm_wikitext_1024t_640d_64sd_24s_12l/best.pt\",\n",
        "    \"./drive/MyDrive/asm_outputs/asm_wikitext_1024t_384d_16sd_8s_21l/best.pt\",\n",
        "    \"./drive/MyDrive/asm_outputs/asm_wikitext_1024t_768d_32sd_32cs_17l/best.pt\",\n",
        "    \"./drive/MyDrive/asm_outputs/asm_wikitext_1024t_640d_64sd_24s_17l/best.pt\",\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n",
        "for CKPT_PATH in CKPT_PATHS:\n",
        "    print(\"loading next model...\")\n",
        "    model, cfg, cfg_dict, ckpt = load_model_and_cfg(CKPT_PATH)\n",
        "    print_cfg_summary(cfg, model)\n",
        "\n",
        "    print(\"testing model...\")\n",
        "    perplexity, avg_loss = evaluate_perplexity(model, all_tokens)\n",
        "    results.append(TestResults(cfg, avg_loss, perplexity))\n",
        "    print(f\"Perplexity: {perplexity:.2f}\")\n",
        "\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bF49CDUJIucT"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Batch Data Generation\n",
        "\n",
        "import os, random, pickle\n",
        "from typing import List, Tuple, Optional, Iterator, Dict\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, IterableDataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Minimal dataset classes (same as training)\n",
        "# -------------------------\n",
        "class StableValidationDataset(Dataset):\n",
        "    def __init__(self, samples: List[Tuple[torch.Tensor, torch.Tensor]]):\n",
        "        self.samples = samples\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx): return self.samples[idx]\n",
        "\n",
        "def _ensure_dir(path: str):\n",
        "    d = os.path.dirname(path)\n",
        "    if d:\n",
        "        os.makedirs(d, exist_ok=True)\n",
        "\n",
        "def build_or_load_token_stream(\n",
        "    *,\n",
        "    cache_path: str,\n",
        "    dataset_name: str,\n",
        "    dataset_config: str,\n",
        "    split: str,\n",
        "    tokenizer_name: str,\n",
        "    min_chars: int = 1,\n",
        "    add_eos_between_rows: bool = True,\n",
        "    max_rows: Optional[int] = None,\n",
        ") -> List[int]:\n",
        "    if os.path.exists(cache_path):\n",
        "        with open(cache_path, \"rb\") as f:\n",
        "            stream = pickle.load(f)\n",
        "        return stream\n",
        "\n",
        "    _ensure_dir(cache_path)\n",
        "    tok = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=True)\n",
        "    eos = tok.eos_token_id\n",
        "    assert eos is not None, \"Tokenizer must have eos_token_id\"\n",
        "\n",
        "    ds = load_dataset(dataset_name, dataset_config, split=split)\n",
        "\n",
        "    stream: List[int] = []\n",
        "    used = 0\n",
        "    for row in ds:\n",
        "        if max_rows is not None and used >= max_rows:\n",
        "            break\n",
        "        text = (row.get(\"text\") or \"\").strip()\n",
        "        if len(text) < min_chars:\n",
        "            continue\n",
        "        ids = tok.encode(text, add_special_tokens=False)\n",
        "        if not ids:\n",
        "            continue\n",
        "        stream.extend(ids)\n",
        "        if add_eos_between_rows:\n",
        "            stream.append(eos)\n",
        "        used += 1\n",
        "\n",
        "    with open(cache_path, \"wb\") as f:\n",
        "        pickle.dump(stream, f)\n",
        "    return stream\n",
        "\n",
        "def build_or_load_validation_windows(\n",
        "    *,\n",
        "    cache_path: str,\n",
        "    token_stream: List[int],\n",
        "    max_seq_len: int,\n",
        "    stride_frac: float,\n",
        "    val_samples_target: int,\n",
        ") -> StableValidationDataset:\n",
        "    if os.path.exists(cache_path):\n",
        "        with open(cache_path, \"rb\") as f:\n",
        "            samples = pickle.load(f)\n",
        "        return StableValidationDataset(samples)\n",
        "\n",
        "    _ensure_dir(cache_path)\n",
        "\n",
        "    T = int(max_seq_len)\n",
        "    stride = max(1, int(T * float(stride_frac)))\n",
        "    need = int(val_samples_target)\n",
        "\n",
        "    max_start = len(token_stream) - (T + 1)\n",
        "    if max_start <= 0:\n",
        "        raise ValueError(\"Validation token stream too small for max_seq_len+1\")\n",
        "\n",
        "    samples: List[Tuple[torch.Tensor, torch.Tensor]] = []\n",
        "    for start in range(0, max_start + 1, stride):\n",
        "        chunk = token_stream[start:start + T + 1]\n",
        "        if len(chunk) < T + 1:\n",
        "            break\n",
        "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
        "        samples.append((x, y))\n",
        "        if len(samples) >= need:\n",
        "            break\n",
        "\n",
        "    with open(cache_path, \"wb\") as f:\n",
        "        pickle.dump(samples, f)\n",
        "    return StableValidationDataset(samples)\n",
        "\n",
        "class WikiTextRandomWindowStream(IterableDataset):\n",
        "    def __init__(self, token_stream: List[int], max_seq_len: int, train_samples_target: int, seed: int):\n",
        "        super().__init__()\n",
        "        self.stream = token_stream\n",
        "        self.T = int(max_seq_len)\n",
        "        self.target = int(train_samples_target)\n",
        "        self.seed = int(seed)\n",
        "        self.max_start = len(self.stream) - (self.T + 1)\n",
        "        if self.max_start <= 0:\n",
        "            raise ValueError(\"Train token stream too small for max_seq_len+1\")\n",
        "\n",
        "    def __iter__(self):\n",
        "        info = torch.utils.data.get_worker_info()\n",
        "        wid = info.id if info is not None else 0\n",
        "        rng = random.Random(self.seed + 17 * wid)\n",
        "\n",
        "        yielded = 0\n",
        "        while yielded < self.target:\n",
        "            start = rng.randint(0, self.max_start)\n",
        "            chunk = self.stream[start:start + self.T + 1]\n",
        "            x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
        "            y = torch.tensor(chunk[1:], dtype=torch.long)\n",
        "            yield x, y\n",
        "            yielded += 1\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Analysis batch generator\n",
        "# -------------------------\n",
        "def make_batch_generator(\n",
        "    cfg,\n",
        "    *,\n",
        "    split: str = \"val\",                 # \"val\" or \"train\"\n",
        "    device: Optional[torch.device] = None,\n",
        "    seq_len: int = 0,\n",
        "    num_workers: int = 0,               # 0 is safest for Colab analysis\n",
        "    pin_memory: bool = True,\n",
        "    batches_per_epoch: Optional[int] = None,  # if set, generator stops after N batches\n",
        "    infinite: bool = True,              # if True, cycles (val) or keeps sampling (train)\n",
        "    seed: Optional[int] = None,         # override cfg.seed for sampling\n",
        ") -> Iterator[Tuple[torch.Tensor, torch.Tensor]]:\n",
        "    \"\"\"\n",
        "    Yields xb,yb batches shaped [B,T] int64.\n",
        "\n",
        "    - split=\"val\": deterministic stable windows (cached) then cycles if infinite=True\n",
        "    - split=\"train\": random windows from cached token stream; infinite is naturally True\n",
        "    \"\"\"\n",
        "    assert split in (\"val\", \"train\")\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Resolve cfg fields with fallback (works for dict-like or dataclass)\n",
        "    def get(name, default=None):\n",
        "        return getattr(cfg, name, cfg.get(name, default) if isinstance(cfg, dict) else default)\n",
        "\n",
        "    cache_dir = get(\"cache_dir\")\n",
        "    dataset_name = get(\"dataset_name\")\n",
        "    dataset_config = get(\"dataset_config\")\n",
        "    tokenizer_name = get(\"tokenizer_name\")\n",
        "    max_seq_len = seq_len if seq_len > 0 else int(get(\"max_seq_len\"))\n",
        "    batch_size = int(get(\"batch_size\"))\n",
        "    stride_frac_val = float(get(\"stride_frac_val\", 0.5))\n",
        "    val_samples_target = int(get(\"val_samples_target\", 25_000))\n",
        "    val_windows_cache = get(\"val_windows_cache\")\n",
        "    train_samples_target = int(get(\"train_samples_target\", 100_000_000))\n",
        "    used_seed = int(seed if seed is not None else get(\"seed\", 1337))\n",
        "\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    train_stream_cache = os.path.join(cache_dir, f\"{dataset_config}_train_stream.pkl\")\n",
        "    val_stream_cache   = os.path.join(cache_dir, f\"{dataset_config}_val_stream.pkl\")\n",
        "\n",
        "    if split == \"train\":\n",
        "        train_stream = build_or_load_token_stream(\n",
        "            cache_path=train_stream_cache,\n",
        "            dataset_name=dataset_name,\n",
        "            dataset_config=dataset_config,\n",
        "            split=\"train\",\n",
        "            tokenizer_name=tokenizer_name,\n",
        "            min_chars=1,\n",
        "            add_eos_between_rows=True,\n",
        "        )\n",
        "        ds = WikiTextRandomWindowStream(\n",
        "            token_stream=train_stream,\n",
        "            max_seq_len=max_seq_len,\n",
        "            train_samples_target=train_samples_target,\n",
        "            seed=used_seed,\n",
        "        )\n",
        "        loader = DataLoader(\n",
        "            ds,\n",
        "            batch_size=batch_size,\n",
        "            num_workers=num_workers,\n",
        "            pin_memory=pin_memory and torch.cuda.is_available(),\n",
        "        )\n",
        "\n",
        "        yielded = 0\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device, non_blocking=True)\n",
        "            yb = yb.to(device, non_blocking=True)\n",
        "            yield xb, yb\n",
        "            yielded += 1\n",
        "            if batches_per_epoch is not None and yielded >= batches_per_epoch:\n",
        "                if infinite:\n",
        "                    yielded = 0\n",
        "                    continue\n",
        "                break\n",
        "\n",
        "    else:\n",
        "        # val\n",
        "        val_stream = build_or_load_token_stream(\n",
        "            cache_path=val_stream_cache,\n",
        "            dataset_name=dataset_name,\n",
        "            dataset_config=dataset_config,\n",
        "            split=\"validation\",\n",
        "            tokenizer_name=tokenizer_name,\n",
        "            min_chars=1,\n",
        "            add_eos_between_rows=True,\n",
        "        )\n",
        "        val_ds = build_or_load_validation_windows(\n",
        "            cache_path=val_windows_cache,\n",
        "            token_stream=val_stream,\n",
        "            max_seq_len=max_seq_len,\n",
        "            stride_frac=stride_frac_val,\n",
        "            val_samples_target=val_samples_target,\n",
        "        )\n",
        "        loader = DataLoader(\n",
        "            val_ds,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=num_workers,\n",
        "            pin_memory=pin_memory and torch.cuda.is_available(),\n",
        "        )\n",
        "\n",
        "        while True:\n",
        "            yielded = 0\n",
        "            for xb, yb in loader:\n",
        "                xb = xb.to(device, non_blocking=True)\n",
        "                yb = yb.to(device, non_blocking=True)\n",
        "                yield xb, yb\n",
        "                yielded += 1\n",
        "                if batches_per_epoch is not None and yielded >= batches_per_epoch:\n",
        "                    break\n",
        "            if not infinite:\n",
        "                break\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Quick smoke test\n",
        "# -------------------------\n",
        "\n",
        "gen = make_batch_generator(cfg, batches_per_epoch=50, split=\"val\", device=torch.device(\"cuda\"))\n",
        "xb, yb = next(gen)\n",
        "print(xb.shape, yb.shape, xb.device, xb.dtype)\n",
        "\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1IfX8jcBIvgE"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Forward Pass, Collect Out Infos\n",
        "\n",
        "@torch.no_grad()\n",
        "def run_with_infos(model, xb: torch.Tensor, attention_mask=None):\n",
        "    out = model(xb, return_info=True, attention_mask=attention_mask)\n",
        "    if isinstance(out, tuple) and len(out) == 2:\n",
        "        logits, infos = out\n",
        "    else:\n",
        "        raise ValueError(\"Model did not return (logits, infos) under return_info=True\")\n",
        "    return logits, infos\n",
        "\n",
        "logits, infos = run_with_infos(model, xb[:4])\n",
        "print(\"logits:\", logits.shape, logits.dtype)\n",
        "print(\"num layers infos:\", len(infos))\n",
        "print(\"info keys example:\", list(infos[0].keys()))\n",
        "print(\"read_weights shape:\", infos[0][\"read_weights\"].shape)\n",
        "\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "c5h6bC4WxFjS"
      },
      "outputs": [],
      "source": [
        "#@title crafted gen\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, Optional, Tuple, Union, List\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Helpers\n",
        "# -----------------------------------------------------------------------------\n",
        "def _top_k_top_p_filtering(\n",
        "    logits: torch.Tensor,\n",
        "    top_k: int = 0,\n",
        "    top_p: float = 1.0,\n",
        "    min_tokens_to_keep: int = 1,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Filter a distribution of logits using top-k and/or nucleus (top-p).\n",
        "    logits: [V]\n",
        "    \"\"\"\n",
        "    if top_k > 0:\n",
        "        top_k = min(max(top_k, min_tokens_to_keep), logits.size(-1))\n",
        "        kth = torch.topk(logits, top_k).values[-1]\n",
        "        logits = logits.masked_fill(logits < kth, float(\"-inf\"))\n",
        "\n",
        "    if top_p < 1.0:\n",
        "        sorted_logits, sorted_idx = torch.sort(logits, descending=True)\n",
        "        probs = F.softmax(sorted_logits, dim=-1)\n",
        "        cumprobs = probs.cumsum(dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative prob above threshold\n",
        "        cutoff = cumprobs > top_p\n",
        "        # Keep at least min_tokens_to_keep\n",
        "        cutoff[:min_tokens_to_keep] = False\n",
        "\n",
        "        sorted_logits = sorted_logits.masked_fill(cutoff, float(\"-inf\"))\n",
        "        logits = logits.scatter(0, sorted_idx, sorted_logits)\n",
        "\n",
        "    return logits\n",
        "\n",
        "\n",
        "def _apply_repetition_penalty(\n",
        "    logits: torch.Tensor,\n",
        "    generated_ids: torch.Tensor,\n",
        "    penalty: float,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Classic repetition penalty (GPT-2 style): penalize logits of previously generated tokens.\n",
        "    logits: [V], generated_ids: [t]\n",
        "    \"\"\"\n",
        "    if penalty is None or penalty == 1.0 or generated_ids.numel() == 0:\n",
        "        return logits\n",
        "    uniq = torch.unique(generated_ids)\n",
        "    # If logit > 0: divide by penalty; else multiply by penalty\n",
        "    l = logits[uniq]\n",
        "    logits[uniq] = torch.where(l > 0, l / penalty, l * penalty)\n",
        "    return logits\n",
        "\n",
        "\n",
        "def _no_repeat_ngram_ban(\n",
        "    logits: torch.Tensor,\n",
        "    generated_ids: torch.Tensor,\n",
        "    no_repeat_ngram_size: int,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Ban tokens that would create a repeated n-gram of size N in the generated sequence.\n",
        "    logits: [V], generated_ids: [t]\n",
        "    \"\"\"\n",
        "    n = int(no_repeat_ngram_size or 0)\n",
        "    if n <= 1 or generated_ids.numel() < n - 1:\n",
        "        return logits\n",
        "\n",
        "    seq = generated_ids.tolist()\n",
        "    prefix = seq[-(n - 1):]  # length n-1\n",
        "    # Build set of next tokens seen after this prefix in the past\n",
        "    banned = set()\n",
        "    for i in range(len(seq) - n + 1):\n",
        "        if seq[i:i + n - 1] == prefix:\n",
        "            banned.add(seq[i + n - 1])\n",
        "\n",
        "    if banned:\n",
        "        banned = torch.tensor(list(banned), device=logits.device, dtype=torch.long)\n",
        "        logits[banned] = float(\"-inf\")\n",
        "    return logits\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# ASA/ASM-specific generation\n",
        "# -----------------------------------------------------------------------------\n",
        "@torch.no_grad()\n",
        "def asa_generate(\n",
        "    prompt: Union[str, List[int], torch.Tensor],\n",
        "    model: torch.nn.Module,\n",
        "    gen: Dict[str, Any],\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Generation crafted for ASA/ASM models:\n",
        "      - Uses soft sampling by default (hard routing variants are unstable per your ablations).\n",
        "      - Optionally uses ASA internal telemetry (return_info=True, return_light_stats=True)\n",
        "        to perform *router-aware fallback* when EOS-risk is high early, or when routing is\n",
        "        pathologically branchy.\n",
        "      - Supports standard sampling controls + mild anti-repetition.\n",
        "      - Keeps inference dropout off.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "    prompt:\n",
        "        - str: requires gen[\"tokenizer\"] providing encode/decode\n",
        "        - List[int] / 1D torch.Tensor: token ids\n",
        "    model:\n",
        "        ASMLanguageModel (or compatible) returning logits or (logits, infos) if return_info=True\n",
        "    gen params (dict):\n",
        "        Required (if prompt is str):\n",
        "          tokenizer: a HF tokenizer with encode/decode\n",
        "        Common:\n",
        "          max_new_tokens: int (default 128)\n",
        "          temperature: float (default 0.8)\n",
        "          top_p: float (default 0.9)\n",
        "          top_k: int (default 50)\n",
        "          min_new_tokens: int (default 0)\n",
        "          eos_token_id: int (default tokenizer.eos_token_id if available)\n",
        "          pad_token_id: int (optional)\n",
        "          do_sample: bool (default True)\n",
        "          repetition_penalty: float (default 1.05)\n",
        "          no_repeat_ngram_size: int (default 3)\n",
        "          device: torch.device or str (default model device)\n",
        "        ASA-aware controls:\n",
        "          asa_info: bool (default True) -> request return_info+light_stats and use them\n",
        "          eos_risk_threshold: float (default 0.25)\n",
        "          early_steps: int (default 24) -> window in which to apply EOS-risk mitigations\n",
        "          branchy_entropy_threshold: float (default None) -> if set, triggers extra sharpening\n",
        "          rescue_mode: str in {\"none\",\"scaffold\",\"resample\"} (default \"resample\")\n",
        "              - \"resample\": if EOS risk triggers, resample with lower temp / higher top_k keep\n",
        "              - \"scaffold\": if tokenizer provided and prompt looks like a known template,\n",
        "                            inject a short scaffold (see below) once at the start\n",
        "          rescue_temp: float (default 0.65)\n",
        "          rescue_top_p: float (default 0.85)\n",
        "          rescue_top_k: int (default 80)\n",
        "          max_resample_tries: int (default 4)\n",
        "        Return:\n",
        "          return_text: bool (default True if tokenizer present else False)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict with:\n",
        "      \"input_ids\": [1, T+new]\n",
        "      \"generated_ids\": [new]\n",
        "      \"text\": optional\n",
        "      \"info_trace\": optional list of per-step ASA stats (if asa_info=True)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    tokenizer = gen.get(\"tokenizer\", None)\n",
        "    device = gen.get(\"device\", None)\n",
        "    if device is None:\n",
        "        device = next(model.parameters()).device\n",
        "\n",
        "    # --- tokenize prompt ---\n",
        "    if isinstance(prompt, str):\n",
        "        if tokenizer is None:\n",
        "            raise ValueError(\"prompt is str but gen['tokenizer'] was not provided.\")\n",
        "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    elif isinstance(prompt, list):\n",
        "        input_ids = torch.tensor(prompt, device=device, dtype=torch.long).unsqueeze(0)\n",
        "    elif isinstance(prompt, torch.Tensor):\n",
        "        if prompt.dim() == 1:\n",
        "            input_ids = prompt.to(device=device, dtype=torch.long).unsqueeze(0)\n",
        "        elif prompt.dim() == 2:\n",
        "            input_ids = prompt.to(device=device, dtype=torch.long)\n",
        "        else:\n",
        "            raise ValueError(\"prompt tensor must be 1D or 2D token ids.\")\n",
        "    else:\n",
        "        raise TypeError(\"prompt must be str, List[int], or torch.Tensor of token ids.\")\n",
        "\n",
        "    max_new = int(gen.get(\"max_new_tokens\", 128))\n",
        "    min_new = int(gen.get(\"min_new_tokens\", 0))\n",
        "    do_sample = bool(gen.get(\"do_sample\", True))\n",
        "\n",
        "    temperature = float(gen.get(\"temperature\", 0.8))\n",
        "    top_p = float(gen.get(\"top_p\", 0.9))\n",
        "    top_k = int(gen.get(\"top_k\", 50))\n",
        "\n",
        "    repetition_penalty = float(gen.get(\"repetition_penalty\", 1.05))\n",
        "    no_repeat_ngram_size = int(gen.get(\"no_repeat_ngram_size\", 3))\n",
        "\n",
        "    eos_token_id = gen.get(\"eos_token_id\", None)\n",
        "    if eos_token_id is None and tokenizer is not None:\n",
        "        eos_token_id = tokenizer.eos_token_id\n",
        "    if eos_token_id is None:\n",
        "        eos_token_id = -1  # disable EOS logic if unknown\n",
        "\n",
        "    asa_info = bool(gen.get(\"asa_info\", True))\n",
        "    eos_risk_threshold = float(gen.get(\"eos_risk_threshold\", 0.25))\n",
        "    early_steps = int(gen.get(\"early_steps\", 24))\n",
        "    branchy_entropy_threshold = gen.get(\"branchy_entropy_threshold\", None)\n",
        "    rescue_mode = str(gen.get(\"rescue_mode\", \"resample\")).lower()\n",
        "    rescue_temp = float(gen.get(\"rescue_temp\", 0.65))\n",
        "    rescue_top_p = float(gen.get(\"rescue_top_p\", 0.85))\n",
        "    rescue_top_k = int(gen.get(\"rescue_top_k\", 80))\n",
        "    max_resample_tries = int(gen.get(\"max_resample_tries\", 4))\n",
        "\n",
        "    # Optional scaffold injection (architecture-aware: helps route trajectory)\n",
        "    if rescue_mode == \"scaffold\" and tokenizer is not None and isinstance(prompt, str):\n",
        "        # Very small, conservative scaffold setextend as you like\n",
        "        scaffolds = [\n",
        "            (\"The capital of\", \" the city of\"),\n",
        "            (\"Albert Einstein was born\", \" in\"),\n",
        "            (\"The scientific method involves\", \" the process of\"),\n",
        "            (\"The algorithm proceeds as follows\", \" 1.\"),\n",
        "        ]\n",
        "        for k, s in scaffolds:\n",
        "            if prompt.strip().startswith(k) and not prompt.strip().endswith(s.strip()):\n",
        "                input_ids = tokenizer.encode(prompt + s, return_tensors=\"pt\").to(device)\n",
        "                break\n",
        "\n",
        "    info_trace: List[Dict[str, float]] = []\n",
        "\n",
        "    # Generation loop\n",
        "    cur_ids = input_ids\n",
        "    for step in range(max_new):\n",
        "        # Model forward\n",
        "        if asa_info:\n",
        "            out = model(cur_ids, return_info=True, return_light_stats=True)\n",
        "            logits, infos = out\n",
        "            # infos is list per layer; take last block's light stats if present\n",
        "            last = infos[-1] if isinstance(infos, list) and len(infos) > 0 else None\n",
        "            stat = {}\n",
        "            if isinstance(last, dict):\n",
        "                # these are CPU tensors in your module; cast to float if present\n",
        "                for k in [\"entropy_mean\", \"top1freq_mean\", \"content_read_gamma_mean\", \"slotspace_gate_mean\", \"slotspace_delta_norm\"]:\n",
        "                    if k in last and last[k] is not None:\n",
        "                        try:\n",
        "                            stat[k] = float(last[k].item())\n",
        "                        except Exception:\n",
        "                            pass\n",
        "            # Store later for debugging\n",
        "        else:\n",
        "            logits = model(cur_ids)\n",
        "            stat = None\n",
        "\n",
        "        next_logits = logits[0, -1, :]  # [V]\n",
        "\n",
        "        # Basic constraints\n",
        "        if step < min_new and eos_token_id >= 0:\n",
        "            next_logits = next_logits.clone()\n",
        "            next_logits[eos_token_id] = float(\"-inf\")\n",
        "\n",
        "        # Anti-repetition (mild, usually good for ASA because content-read is self-referential)\n",
        "        gen_so_far = cur_ids[0, input_ids.shape[1]:]  # only newly generated, if any\n",
        "        next_logits = _apply_repetition_penalty(next_logits, gen_so_far, repetition_penalty)\n",
        "        next_logits = _no_repeat_ngram_ban(next_logits, cur_ids[0], no_repeat_ngram_size)\n",
        "\n",
        "        # Router-aware rescue (early EOS / excessive branchiness)\n",
        "        # Use next-token EOS risk; optionally sharpen if branchy.\n",
        "        tries = 0\n",
        "        used_temp, used_top_p, used_top_k = temperature, top_p, top_k\n",
        "        while True:\n",
        "            l = next_logits\n",
        "            if used_temp and used_temp > 0:\n",
        "                l = l / used_temp\n",
        "\n",
        "            l = _top_k_top_p_filtering(l, top_k=used_top_k, top_p=used_top_p)\n",
        "\n",
        "            probs = F.softmax(l, dim=-1)\n",
        "            p_eos = float(probs[eos_token_id].item()) if eos_token_id >= 0 else 0.0\n",
        "            ent = float(-(probs.clamp_min(1e-12) * probs.clamp_min(1e-12).log()).sum().item())\n",
        "\n",
        "            # Condition: early EOS risk is too high\n",
        "            eos_risky = (eos_token_id >= 0) and (step < early_steps) and (p_eos > eos_risk_threshold)\n",
        "\n",
        "            # Condition: branchy token distribution (optional) -> reduce temperature a bit\n",
        "            branchy = False\n",
        "            if branchy_entropy_threshold is not None and step < early_steps:\n",
        "                branchy = ent > float(branchy_entropy_threshold)\n",
        "\n",
        "            if (eos_risky or branchy) and rescue_mode == \"resample\" and tries < max_resample_tries:\n",
        "                used_temp = min(used_temp, rescue_temp)\n",
        "                used_top_p = min(used_top_p, rescue_top_p)\n",
        "                used_top_k = max(used_top_k, rescue_top_k)\n",
        "                tries += 1\n",
        "                continue\n",
        "\n",
        "            # Choose token\n",
        "            if do_sample:\n",
        "                next_id = torch.multinomial(probs, num_samples=1)\n",
        "            else:\n",
        "                next_id = torch.argmax(probs, dim=-1, keepdim=True)\n",
        "\n",
        "            break\n",
        "\n",
        "        # Log trace\n",
        "        if asa_info:\n",
        "            rec = {\"step\": float(step), \"token_entropy\": float(ent), \"p_eos\": float(p_eos)}\n",
        "            if stat:\n",
        "                for k, v in stat.items():\n",
        "                    rec[k] = float(v)\n",
        "            # record rescue adjustments\n",
        "            rec[\"temp_used\"] = float(used_temp)\n",
        "            rec[\"top_p_used\"] = float(used_top_p)\n",
        "            rec[\"top_k_used\"] = float(used_top_k)\n",
        "            info_trace.append(rec)\n",
        "\n",
        "        # Append token\n",
        "        cur_ids = torch.cat([cur_ids, next_id.view(1, 1)], dim=1)\n",
        "\n",
        "        # Stop on EOS\n",
        "        if eos_token_id >= 0 and int(next_id.item()) == int(eos_token_id) and step >= min_new:\n",
        "            break\n",
        "\n",
        "    generated_ids = cur_ids[:, input_ids.shape[1]:]\n",
        "\n",
        "    out: Dict[str, Any] = {\n",
        "        \"input_ids\": cur_ids,\n",
        "        \"generated_ids\": generated_ids,\n",
        "    }\n",
        "    if asa_info:\n",
        "        out[\"info_trace\"] = info_trace\n",
        "\n",
        "    return_text = bool(gen.get(\"return_text\", tokenizer is not None))\n",
        "    if return_text and tokenizer is not None:\n",
        "        out[\"text\"] = tokenizer.decode(cur_ids[0].tolist(), skip_special_tokens=False)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "# =========================\n",
        "# PATCH 1: wrappers for your crafted asa_generate\n",
        "# =========================\n",
        "\n",
        "@torch.no_grad()\n",
        "def asa_greedy_suffix(\n",
        "    prompt: str,\n",
        "    model: torch.nn.Module,\n",
        "    gen: dict,\n",
        "    max_new_tokens: int = 8,\n",
        "    strip: bool = True,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Runs your asa_generate in greedy mode and returns ONLY the suffix after `prompt`.\n",
        "    This is what you want for exact-match checks / scoring.\n",
        "    \"\"\"\n",
        "    # Copy gen so we can override safely\n",
        "    g = dict(gen)\n",
        "    g[\"do_sample\"] = False\n",
        "    g[\"max_new_tokens\"] = int(max_new_tokens)\n",
        "\n",
        "    out = asa_generate(prompt, model, g)\n",
        "    text = out.get(\"text\", None)\n",
        "    if text is None:\n",
        "        # Fallback: decode manually\n",
        "        tok = g.get(\"tokenizer\", None)\n",
        "        if tok is None:\n",
        "            raise ValueError(\"No decoded text available; provide gen['tokenizer'].\")\n",
        "        text = tok.decode(out[\"input_ids\"][0].tolist(), skip_special_tokens=False)\n",
        "\n",
        "    # Suffix (best-effort): if prompt string matches prefix of decoded text\n",
        "    if text.startswith(prompt):\n",
        "        suf = text[len(prompt):]\n",
        "    else:\n",
        "        # Robust fallback: try to locate the prompt inside the decoded text\n",
        "        idx = text.find(prompt)\n",
        "        suf = text[idx + len(prompt):] if idx >= 0 else text\n",
        "\n",
        "    if strip:\n",
        "        suf = suf.replace(\"\\n\", \" \").strip()\n",
        "    return suf\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def asa_generate_many(\n",
        "    prompts: list,\n",
        "    model: torch.nn.Module,\n",
        "    gen: dict,\n",
        "    do_sample: bool = False,\n",
        "    max_new_tokens: int = 8,\n",
        ") -> list:\n",
        "    \"\"\"\n",
        "    Convenience wrapper: runs asa_generate per prompt (loop) and returns decoded texts.\n",
        "    \"\"\"\n",
        "    g = dict(gen)\n",
        "    g[\"do_sample\"] = bool(do_sample)\n",
        "    g[\"max_new_tokens\"] = int(max_new_tokens)\n",
        "\n",
        "    outs = []\n",
        "    for p in prompts:\n",
        "        out = asa_generate(p, model, g)\n",
        "        text = out.get(\"text\", None)\n",
        "        if text is None:\n",
        "            tok = g.get(\"tokenizer\", None)\n",
        "            if tok is None:\n",
        "                raise ValueError(\"No decoded text available; provide gen['tokenizer'].\")\n",
        "            text = tok.decode(out[\"input_ids\"][0].tolist(), skip_special_tokens=False)\n",
        "        outs.append(text)\n",
        "    return outs\n",
        "\n",
        "@torch.no_grad()\n",
        "def score_next_token_rank(\n",
        "    prompt: str,\n",
        "    target_token: str,\n",
        "    model: torch.nn.Module,\n",
        "    gen: dict,\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Computes P(target) and rank for the *next token* only, matching your printed diagnostics.\n",
        "    \"\"\"\n",
        "    tok = gen[\"tokenizer\"]\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # encode prompt\n",
        "    input_ids = tok.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # encode target token as a single token (best-effort)\n",
        "    target_ids = tok.encode(target_token, add_special_tokens=False)\n",
        "    if len(target_ids) != 1:\n",
        "        return {\"ok\": False, \"reason\": f\"target_token maps to {len(target_ids)} tokens\", \"target_ids\": target_ids}\n",
        "\n",
        "    target_id = target_ids[0]\n",
        "\n",
        "    model.eval()\n",
        "    logits = model(input_ids)  # if your model needs return_info=False default\n",
        "    if isinstance(logits, (tuple, list)):\n",
        "        logits = logits[0]\n",
        "    next_logits = logits[0, -1, :]\n",
        "\n",
        "    probs = torch.softmax(next_logits, dim=-1)\n",
        "    p_t = float(probs[target_id].item())\n",
        "\n",
        "    # rank: 1 = best\n",
        "    sorted_idx = torch.argsort(next_logits, descending=True)\n",
        "    rank = int((sorted_idx == target_id).nonzero(as_tuple=False).item()) + 1\n",
        "\n",
        "    return {\"ok\": True, \"p_target\": p_t, \"rank\": rank, \"target_id\": target_id}\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHoiBcEHuFL6"
      },
      "outputs": [],
      "source": [
        "\n",
        "from contextlib import contextmanager\n",
        "\n",
        "@contextmanager\n",
        "def asa_temperatures(model, read_temperature=None, write_temperature=None):\n",
        "    \"\"\"\n",
        "    Temporarily override ASA read/write temperatures across all ASA modules.\n",
        "    Restores original values afterwards.\n",
        "    \"\"\"\n",
        "    # Collect ASA modules (by attribute presence, avoids needing class import)\n",
        "    asa_mods = []\n",
        "    for m in model.modules():\n",
        "        if hasattr(m, \"read_temperature\") and hasattr(m, \"write_temperature\") and hasattr(m, \"num_slots\"):\n",
        "            # heuristic: ASA-like module\n",
        "            asa_mods.append(m)\n",
        "\n",
        "    old = [(m.read_temperature, m.write_temperature) for m in asa_mods]\n",
        "    try:\n",
        "        for m in asa_mods:\n",
        "            if read_temperature is not None:\n",
        "                m.read_temperature = float(read_temperature)\n",
        "            if write_temperature is not None:\n",
        "                m.write_temperature = float(write_temperature)\n",
        "        yield\n",
        "    finally:\n",
        "        for m, (rt, wt) in zip(asa_mods, old):\n",
        "            m.read_temperature = rt\n",
        "            m.write_temperature = wt\n",
        "\n",
        "\n",
        "def sweep_asa_temps(\n",
        "    model, tokenizer, gener, examples,\n",
        "    read_grid=(0.7, 0.85, 1.0, 1.15),\n",
        "    write_grid=(0.8, 1.0, 1.2),\n",
        "    max_new_tokens=8,\n",
        "    n_gen_samples=6,\n",
        "    seed=1337,\n",
        "):\n",
        "    rng = random.Random(seed)\n",
        "    sample = rng.sample(examples, min(n_gen_samples, len(examples)))\n",
        "\n",
        "    rows = []\n",
        "    for rt in read_grid:\n",
        "        for wt in write_grid:\n",
        "            with asa_temperatures(model, read_temperature=rt, write_temperature=wt):\n",
        "                acc = eval_exact_match(examples, model, gener, max_new_tokens=max_new_tokens)\n",
        "                # next-token diagnostic on a tiny subset (optional)\n",
        "                rows.append((rt, wt, acc))\n",
        "\n",
        "            print(f\"\\n=== read_temperature={rt:.2f} | write_temperature={wt:.2f} | exact_match={acc:.3f} ===\")\n",
        "            with asa_temperatures(model, read_temperature=rt, write_temperature=wt):\n",
        "                for ex in sample:\n",
        "                    pred = greedy_suffix(ex[\"prompt\"], model, gener, max_new_tokens=max_new_tokens)\n",
        "                    print(f\"- {ex['tag']:<18} prompt={ex['prompt']!r} -> {pred[:60]!r}\")\n",
        "\n",
        "    # sort best first\n",
        "    rows.sort(key=lambda x: -x[2])\n",
        "    print(\"\\nTop settings:\")\n",
        "    for rt, wt, acc in rows[:10]:\n",
        "        print(f\"  rt={rt:.2f} wt={wt:.2f} acc={acc:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fd6_QrbnxI2m"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title multigen\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)\n",
        "\n",
        "gener = dict(\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=32,\n",
        "    #min_new_tokens=4,\n",
        "    temperature=0.1,\n",
        "    top_p=0.95,\n",
        "    top_k=80,\n",
        "    repetition_penalty=1.03,\n",
        "    no_repeat_ngram_size=3, # 3\n",
        "    asa_info=False,\n",
        "    rescue_mode=None, # \"resample\", None\n",
        "    #eos_risk_threshold=0.25,\n",
        "    #early_steps=24,\n",
        "    #branchy_entropy_threshold=7.5,   # optional; depends on vocab size and filtering\n",
        ")\n",
        "\n",
        "print(\"#\"*5, \"Countries\", \"#\"*5)\n",
        "finishers = [\"is\", \"sounds like\", \"consists of\", \"is a form of\", \"all changed when\"]\n",
        "qualities = [\"capital\", \"language\", \"geography\", \"government\", \"history\"]\n",
        "countries = [\"France\", \"Spain\", \"the United States\", \"Russia\", \"Italy\", \"Japan\", \"Egypt\", \"Germany\", \"Brazil\", \"Iraq\"]\n",
        "for country in countries:\n",
        "    for quality, finisher in zip(qualities, finishers):\n",
        "        out = asa_generate(f\"The {quality} of {country} {finisher}\", model, gener)\n",
        "        print(out[\"text\"])\n",
        "\n",
        "print(\"#\"*5, \"People\", \"#\"*5)\n",
        "people = [\"Albert Einstein\", \"George Patton\", \"Charles Darwin\", \"George Washington\", \"Winston Churchill\", \"Jesus of Nazareth\"]\n",
        "factoids = [\"was born\", \"contributed\", \"accomplished\", \"had a strong opinion about\", \"died\"]\n",
        "for person in people:\n",
        "    for factoid in factoids:\n",
        "        out = asa_generate(f\"{person} {factoid}\", model, gener)\n",
        "        print(out[\"text\"])\n",
        "\n",
        "\n",
        "# Optionally inspect router-aware trace:\n",
        "# out[\"info_trace\"][:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BbYvFvu2LjNr"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title synthetic fine tune data\n",
        "\n",
        "\n",
        "def is_single_token(s: str, tokenizer) -> bool:\n",
        "    ids = tokenizer.encode(s, add_special_tokens=False)\n",
        "    return len(ids) == 1\n",
        "\n",
        "def build_pairs_expanded(tokenizer):\n",
        "    \"\"\"\n",
        "    Massively expanded dataset generation with WikiText-103 style templates.\n",
        "    Includes geographical, historical, scientific, cultural, and biographical facts.\n",
        "    \"\"\"\n",
        "    pairs = []\n",
        "\n",
        "    # ========================================\n",
        "    # GEOGRAPHY SECTION (Massively Expanded)\n",
        "    # ========================================\n",
        "\n",
        "    # ---- Capitals (comprehensive list)\n",
        "    capitals = {\n",
        "        # Europe\n",
        "        \"France\": \" Paris\",\n",
        "        \"Germany\": \" Berlin\",\n",
        "        \"Italy\": \" Rome\",\n",
        "        \"Spain\": \" Madrid\",\n",
        "        \"Portugal\": \" Lisbon\",\n",
        "        \"Greece\": \" Athens\",\n",
        "        \"Austria\": \" Vienna\",\n",
        "        \"Poland\": \" Warsaw\",\n",
        "        \"Norway\": \" Oslo\",\n",
        "        \"Sweden\": \" Stockholm\",\n",
        "        \"Finland\": \" Helsinki\",\n",
        "        \"Denmark\": \" Copenhagen\",\n",
        "        \"Ireland\": \" Dublin\",\n",
        "        \"Belgium\": \" Brussels\",\n",
        "        \"Netherlands\": \" Amsterdam\",\n",
        "        \"Switzerland\": \" Bern\",\n",
        "        \"Czech Republic\": \" Prague\",\n",
        "        \"Hungary\": \" Budapest\",\n",
        "        \"Romania\": \" Bucharest\",\n",
        "        \"Bulgaria\": \" Sofia\",\n",
        "        \"Croatia\": \" Zagreb\",\n",
        "        \"Serbia\": \" Belgrade\",\n",
        "        \"Slovakia\": \" Bratislava\",\n",
        "        \"Slovenia\": \" Ljubljana\",\n",
        "        \"Lithuania\": \" Vilnius\",\n",
        "        \"Latvia\": \" Riga\",\n",
        "        \"Estonia\": \" Tallinn\",\n",
        "        \"Iceland\": \" Reykjavik\",\n",
        "        \"Luxembourg\": \" Luxembourg\",\n",
        "        \"Malta\": \" Valletta\",\n",
        "        \"Cyprus\": \" Nicosia\",\n",
        "\n",
        "        # Asia\n",
        "        \"Japan\": \" Tokyo\",\n",
        "        \"China\": \" Beijing\",\n",
        "        \"India\": \" Delhi\",\n",
        "        \"South Korea\": \" Seoul\",\n",
        "        \"North Korea\": \" Pyongyang\",\n",
        "        \"Thailand\": \" Bangkok\",\n",
        "        \"Vietnam\": \" Hanoi\",\n",
        "        \"Indonesia\": \" Jakarta\",\n",
        "        \"Philippines\": \" Manila\",\n",
        "        \"Malaysia\": \" Kuala\",\n",
        "        \"Singapore\": \" Singapore\",\n",
        "        \"Myanmar\": \" Naypyidaw\",\n",
        "        \"Cambodia\": \" Phnom\",\n",
        "        \"Laos\": \" Vientiane\",\n",
        "        \"Bangladesh\": \" Dhaka\",\n",
        "        \"Pakistan\": \" Islamabad\",\n",
        "        \"Afghanistan\": \" Kabul\",\n",
        "        \"Iran\": \" Tehran\",\n",
        "        \"Iraq\": \" Baghdad\",\n",
        "        \"Saudi Arabia\": \" Riyadh\",\n",
        "        \"Turkey\": \" Ankara\",\n",
        "        \"Israel\": \" Jerusalem\",\n",
        "        \"Jordan\": \" Amman\",\n",
        "        \"Lebanon\": \" Beirut\",\n",
        "        \"Syria\": \" Damascus\",\n",
        "        \"Yemen\": \" Sanaa\",\n",
        "        \"Oman\": \" Muscat\",\n",
        "        \"Kuwait\": \" Kuwait\",\n",
        "        \"Qatar\": \" Doha\",\n",
        "        \"Bahrain\": \" Manama\",\n",
        "        \"United Arab Emirates\": \" Abu\",\n",
        "        \"Nepal\": \" Kathmandu\",\n",
        "        \"Sri Lanka\": \" Colombo\",\n",
        "        \"Mongolia\": \" Ulaanbaatar\",\n",
        "        \"Kazakhstan\": \" Astana\",\n",
        "        \"Uzbekistan\": \" Tashkent\",\n",
        "\n",
        "        # Africa\n",
        "        \"Egypt\": \" Cairo\",\n",
        "        \"South Africa\": \" Pretoria\",\n",
        "        \"Nigeria\": \" Abuja\",\n",
        "        \"Kenya\": \" Nairobi\",\n",
        "        \"Ethiopia\": \" Addis\",\n",
        "        \"Morocco\": \" Rabat\",\n",
        "        \"Algeria\": \" Algiers\",\n",
        "        \"Tunisia\": \" Tunis\",\n",
        "        \"Libya\": \" Tripoli\",\n",
        "        \"Sudan\": \" Khartoum\",\n",
        "        \"Ghana\": \" Accra\",\n",
        "        \"Tanzania\": \" Dodoma\",\n",
        "        \"Uganda\": \" Kampala\",\n",
        "        \"Angola\": \" Luanda\",\n",
        "        \"Mozambique\": \" Maputo\",\n",
        "        \"Zimbabwe\": \" Harare\",\n",
        "        \"Zambia\": \" Lusaka\",\n",
        "        \"Senegal\": \" Dakar\",\n",
        "        \"Ivory Coast\": \" Yamoussoukro\",\n",
        "        \"Cameroon\": \" Yaounde\",\n",
        "\n",
        "        # Americas\n",
        "        \"United States\": \" Washington\",\n",
        "        \"Canada\": \" Ottawa\",\n",
        "        \"Mexico\": \" Mexico\",\n",
        "        \"Brazil\": \" Brasilia\",\n",
        "        \"Argentina\": \" Buenos\",\n",
        "        \"Chile\": \" Santiago\",\n",
        "        \"Colombia\": \" Bogota\",\n",
        "        \"Peru\": \" Lima\",\n",
        "        \"Venezuela\": \" Caracas\",\n",
        "        \"Ecuador\": \" Quito\",\n",
        "        \"Bolivia\": \" La\",\n",
        "        \"Paraguay\": \" Asuncion\",\n",
        "        \"Uruguay\": \" Montevideo\",\n",
        "        \"Cuba\": \" Havana\",\n",
        "        \"Jamaica\": \" Kingston\",\n",
        "        \"Costa Rica\": \" San\",\n",
        "        \"Panama\": \" Panama\",\n",
        "        \"Guatemala\": \" Guatemala\",\n",
        "        \"Honduras\": \" Tegucigalpa\",\n",
        "        \"Nicaragua\": \" Managua\",\n",
        "\n",
        "        # Oceania\n",
        "        \"Australia\": \" Canberra\",\n",
        "        \"New Zealand\": \" Wellington\",\n",
        "        \"Papua New Guinea\": \" Port\",\n",
        "        \"Fiji\": \" Suva\",\n",
        "\n",
        "        # Former USSR\n",
        "        \"Russia\": \" Moscow\",\n",
        "        \"Ukraine\": \" Kyiv\",\n",
        "        \"Belarus\": \" Minsk\",\n",
        "        \"Georgia\": \" Tbilisi\",\n",
        "        \"Armenia\": \" Yerevan\",\n",
        "        \"Azerbaijan\": \" Baku\",\n",
        "    }\n",
        "\n",
        "    for c, cap in capitals.items():\n",
        "        pairs.append({\"prompt\": f\"The capital of {c} is\", \"completion\": cap, \"tag\": f\"capital:{c}\"})\n",
        "        pairs.append({\"prompt\": f\"{c}'s capital city is\", \"completion\": cap, \"tag\": f\"capital:{c}\"})\n",
        "        pairs.append({\"prompt\": f\"{c} has its capital in\", \"completion\": cap, \"tag\": f\"capital:{c}\"})\n",
        "\n",
        "    # ---- Languages (comprehensive)\n",
        "    languages = {\n",
        "        \"France\": \" French\",\n",
        "        \"Germany\": \" German\",\n",
        "        \"Italy\": \" Italian\",\n",
        "        \"Japan\": \" Japanese\",\n",
        "        \"Spain\": \" Spanish\",\n",
        "        \"Russia\": \" Russian\",\n",
        "        \"Brazil\": \" Portuguese\",\n",
        "        \"Portugal\": \" Portuguese\",\n",
        "        \"Egypt\": \" Arabic\",\n",
        "        \"China\": \" Chinese\",\n",
        "        \"India\": \" Hindi\",\n",
        "        \"Mexico\": \" Spanish\",\n",
        "        \"Argentina\": \" Spanish\",\n",
        "        \"Netherlands\": \" Dutch\",\n",
        "        \"Greece\": \" Greek\",\n",
        "        \"Poland\": \" Polish\",\n",
        "        \"Turkey\": \" Turkish\",\n",
        "        \"Iran\": \" Persian\",\n",
        "        \"Israel\": \" Hebrew\",\n",
        "        \"Sweden\": \" Swedish\",\n",
        "        \"Norway\": \" Norwegian\",\n",
        "        \"Denmark\": \" Danish\",\n",
        "        \"Finland\": \" Finnish\",\n",
        "        \"Czech Republic\": \" Czech\",\n",
        "        \"Hungary\": \" Hungarian\",\n",
        "        \"Romania\": \" Romanian\",\n",
        "        \"Thailand\": \" Thai\",\n",
        "        \"Vietnam\": \" Vietnamese\",\n",
        "        \"South Korea\": \" Korean\",\n",
        "    }\n",
        "    for c, lang in languages.items():\n",
        "        pairs.append({\"prompt\": f\"The language of {c} is\", \"completion\": lang, \"tag\": f\"language:{c}\"})\n",
        "        pairs.append({\"prompt\": f\"The official language of {c} is\", \"completion\": lang, \"tag\": f\"language:{c}\"})\n",
        "        pairs.append({\"prompt\": f\"People in {c} speak\", \"completion\": lang, \"tag\": f\"language:{c}\"})\n",
        "\n",
        "    # ---- Currencies (comprehensive)\n",
        "    currencies = {\n",
        "        \"Japan\": \" yen\",\n",
        "        \"Russia\": \" ruble\",\n",
        "        \"India\": \" rupee\",\n",
        "        \"Mexico\": \" peso\",\n",
        "        \"China\": \" yuan\",\n",
        "        \"United Kingdom\": \" pound\",\n",
        "        \"United States\": \" dollar\",\n",
        "        \"Canada\": \" dollar\",\n",
        "        \"Australia\": \" dollar\",\n",
        "        \"Germany\": \" euro\",\n",
        "        \"France\": \" euro\",\n",
        "        \"Italy\": \" euro\",\n",
        "        \"Spain\": \" euro\",\n",
        "        \"Portugal\": \" euro\",\n",
        "        \"Greece\": \" euro\",\n",
        "        \"Austria\": \" euro\",\n",
        "        \"Netherlands\": \" euro\",\n",
        "        \"Belgium\": \" euro\",\n",
        "        \"Poland\": \" zloty\",\n",
        "        \"Czech Republic\": \" koruna\",\n",
        "        \"Sweden\": \" krona\",\n",
        "        \"Norway\": \" krone\",\n",
        "        \"Denmark\": \" krone\",\n",
        "        \"Switzerland\": \" franc\",\n",
        "        \"Brazil\": \" real\",\n",
        "        \"South Africa\": \" rand\",\n",
        "        \"Turkey\": \" lira\",\n",
        "        \"Thailand\": \" baht\",\n",
        "        \"Indonesia\": \" rupiah\",\n",
        "    }\n",
        "    for c, cur in currencies.items():\n",
        "        pairs.append({\"prompt\": f\"The currency of {c} is the\", \"completion\": cur, \"tag\": f\"currency:{c}\"})\n",
        "        pairs.append({\"prompt\": f\"{c} uses the\", \"completion\": cur, \"tag\": f\"currency:{c}\"})\n",
        "\n",
        "    # ---- Continents (expanded with variations)\n",
        "    continents = {\n",
        "        \"France\": \" Europe\",\n",
        "        \"Germany\": \" Europe\",\n",
        "        \"Italy\": \" Europe\",\n",
        "        \"Spain\": \" Europe\",\n",
        "        \"Poland\": \" Europe\",\n",
        "        \"Greece\": \" Europe\",\n",
        "        \"Sweden\": \" Europe\",\n",
        "        \"Norway\": \" Europe\",\n",
        "        \"Russia\": \" Europe\",\n",
        "        \"Egypt\": \" Africa\",\n",
        "        \"Nigeria\": \" Africa\",\n",
        "        \"Kenya\": \" Africa\",\n",
        "        \"South Africa\": \" Africa\",\n",
        "        \"Morocco\": \" Africa\",\n",
        "        \"Japan\": \" Asia\",\n",
        "        \"China\": \" Asia\",\n",
        "        \"India\": \" Asia\",\n",
        "        \"Thailand\": \" Asia\",\n",
        "        \"Vietnam\": \" Asia\",\n",
        "        \"Indonesia\": \" Asia\",\n",
        "        \"Brazil\": \" South\",\n",
        "        \"Argentina\": \" South\",\n",
        "        \"Chile\": \" South\",\n",
        "        \"Peru\": \" South\",\n",
        "        \"Colombia\": \" South\",\n",
        "        \"Canada\": \" North\",\n",
        "        \"United States\": \" North\",\n",
        "        \"Mexico\": \" North\",\n",
        "        \"Australia\": \" Oceania\",\n",
        "        \"New Zealand\": \" Oceania\",\n",
        "    }\n",
        "    for c, cont in continents.items():\n",
        "        pairs.append({\"prompt\": f\"{c} is in\", \"completion\": cont, \"tag\": f\"continent:{c}\"})\n",
        "        pairs.append({\"prompt\": f\"{c} is located in\", \"completion\": cont, \"tag\": f\"continent:{c}\"})\n",
        "\n",
        "    # ---- Major Rivers\n",
        "    rivers = {\n",
        "        \"The Nile flows through\": \" Egypt\",\n",
        "        \"The Amazon flows through\": \" Brazil\",\n",
        "        \"The Thames flows through\": \" London\",\n",
        "        \"The Seine flows through\": \" Paris\",\n",
        "        \"The Danube flows through\": \" Europe\",\n",
        "        \"The Rhine flows through\": \" Germany\",\n",
        "        \"The Ganges flows through\": \" India\",\n",
        "        \"The Yangtze flows through\": \" China\",\n",
        "        \"The Mississippi flows through\": \" America\",\n",
        "        \"The Nile is located in\": \" Africa\",\n",
        "        \"The Amazon is in\": \" South\",\n",
        "        \"The Rhine is in\": \" Europe\",\n",
        "    }\n",
        "    for p, comp in rivers.items():\n",
        "        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"rivers\"})\n",
        "\n",
        "    # ---- Mountain ranges and peaks\n",
        "    mountains = {\n",
        "        \"Mount Everest is in\": \" Nepal\",\n",
        "        \"The Alps are in\": \" Europe\",\n",
        "        \"The Himalayas are in\": \" Asia\",\n",
        "        \"The Andes are in\": \" South\",\n",
        "        \"The Rocky Mountains are in\": \" North\",\n",
        "        \"Mount Fuji is in\": \" Japan\",\n",
        "        \"The Pyrenees are between France and\": \" Spain\",\n",
        "    }\n",
        "    for p, comp in mountains.items():\n",
        "        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"mountains\"})\n",
        "\n",
        "    # ---- Oceans and seas\n",
        "    oceans = {\n",
        "        \"The Pacific Ocean is the\": \" largest\",\n",
        "        \"The Atlantic Ocean is the\": \" second\",\n",
        "        \"The Mediterranean Sea is in\": \" Europe\",\n",
        "        \"The Caribbean Sea is in\": \" Central\",\n",
        "        \"The Baltic Sea is in\": \" Northern\",\n",
        "    }\n",
        "    for p, comp in oceans.items():\n",
        "        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"oceans\"})\n",
        "\n",
        "    # ========================================\n",
        "    # HISTORICAL FACTS (Massively Expanded)\n",
        "    # ========================================\n",
        "\n",
        "    # ---- Birth locations (expanded)\n",
        "    born_in = {\n",
        "        \"Albert Einstein was born in\": \" Germany\",\n",
        "        \"Charles Darwin was born in\": \" England\",\n",
        "        \"George Washington was born in\": \" Virginia\",\n",
        "        \"Winston Churchill was born in\": \" England\",\n",
        "        \"Napoleon Bonaparte was born in\": \" Corsica\",\n",
        "        \"Leonardo da Vinci was born in\": \" Italy\",\n",
        "        \"William Shakespeare was born in\": \" England\",\n",
        "        \"Isaac Newton was born in\": \" England\",\n",
        "        \"Marie Curie was born in\": \" Poland\",\n",
        "        \"Galileo Galilei was born in\": \" Italy\",\n",
        "        \"Aristotle was born in\": \" Greece\",\n",
        "        \"Plato was born in\": \" Athens\",\n",
        "        \"Confucius was born in\": \" China\",\n",
        "        \"Buddha was born in\": \" Nepal\",\n",
        "        \"Muhammad Ali was born in\": \" Kentucky\",\n",
        "        \"Martin Luther King was born in\": \" Georgia\",\n",
        "        \"Abraham Lincoln was born in\": \" Kentucky\",\n",
        "        \"Thomas Edison was born in\": \" Ohio\",\n",
        "        \"Nikola Tesla was born in\": \" Croatia\",\n",
        "        \"Sigmund Freud was born in\": \" Czech\",\n",
        "    }\n",
        "    for p, comp in born_in.items():\n",
        "        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"born_in\"})\n",
        "\n",
        "    # ---- Death years (single token years)\n",
        "    death_years = {\n",
        "        \"Albert Einstein died in\": \" 1955\",\n",
        "        \"Isaac Newton died in\": \" 1727\",\n",
        "        \"Charles Darwin died in\": \" 1882\",\n",
        "        \"Leonardo da Vinci died in\": \" 1519\",\n",
        "        \"William Shakespeare died in\": \" 1616\",\n",
        "        \"George Washington died in\": \" 1799\",\n",
        "        \"Napoleon Bonaparte died in\": \" 1821\",\n",
        "        \"Abraham Lincoln died in\": \" 1865\",\n",
        "        \"Marie Curie died in\": \" 1934\",\n",
        "        \"Nikola Tesla died in\": \" 1943\",\n",
        "    }\n",
        "    for p, comp in death_years.items():\n",
        "        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"death_year\"})\n",
        "\n",
        "    # ---- Historical events and dates\n",
        "    historical_events = {\n",
        "        \"World War I began in\": \" 1914\",\n",
        "        \"World War II began in\": \" 1939\",\n",
        "        \"World War II ended in\": \" 1945\",\n",
        "        \"The American Revolution began in\": \" 1775\",\n",
        "        \"The French Revolution began in\": \" 1789\",\n",
        "        \"The Russian Revolution was in\": \" 1917\",\n",
        "        \"The fall of the Berlin Wall was in\": \" 1989\",\n",
        "        \"The September 11 attacks occurred in\": \" 2001\",\n",
        "        \"The moon landing was in\": \" 1969\",\n",
        "        \"Christopher Columbus sailed in\": \" 1492\",\n",
        "        \"The Declaration of Independence was signed in\": \" 1776\",\n",
        "        \"The Civil War began in\": \" 1861\",\n",
        "        \"The Great Depression began in\": \" 1929\",\n",
        "        \"The Cold War began after\": \" 1945\",\n",
        "    }\n",
        "    for p, comp in historical_events.items():\n",
        "        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"historical_event\"})\n",
        "\n",
        "    # ---- Century associations\n",
        "    centuries = {\n",
        "        \"The Renaissance occurred in the\": \" 15th\",\n",
        "        \"The Industrial Revolution began in the\": \" 18th\",\n",
        "        \"The Enlightenment was in the\": \" 18th\",\n",
        "        \"The Victorian Era was in the\": \" 19th\",\n",
        "        \"World War I was in the\": \" 20th\",\n",
        "    }\n",
        "    for p, comp in centuries.items():\n",
        "        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"century\"})\n",
        "\n",
        "    # ---- Leaders and rulers\n",
        "    leaders = {\n",
        "        \"Julius Caesar was a\": \" Roman\",\n",
        "        \"Alexander the Great was a\": \" Macedonian\",\n",
        "        \"Cleopatra was the queen of\": \" Egypt\",\n",
        "        \"Queen Victoria ruled\": \" Britain\",\n",
        "        \"Napoleon was the emperor of\": \" France\",\n",
        "        \"Peter the Great ruled\": \" Russia\",\n",
        "        \"Elizabeth I was queen of\": \" England\",\n",
        "        \"Henry VIII was king of\": \" England\",\n",
        "        \"Louis XIV was king of\": \" France\",\n",
        "    }\n",
        "    for p, comp in leaders.items():\n",
        "        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"leader\"})\n",
        "\n",
        "    # ========================================\n",
        "    # SCIENTIFIC FACTS (Massively Expanded)\n",
        "    # ========================================\n",
        "\n",
        "    # ---- Physics facts\n",
        "    physics = {\n",
        "        \"The speed of light is approximately\": \" 300\",\n",
        "        \"Gravity was discovered by\": \" Newton\",\n",
        "        \"Einstein developed the theory of\": \" relativity\",\n",
        "        \"The atomic bomb was developed during\": \" World\",\n",
        "        \"Newton's laws describe\": \" motion\",\n",
        "        \"Electrons have a\": \" negative\",\n",
        "        \"Protons have a\": \" positive\",\n",
        "        \"The Earth orbits the\": \" Sun\",\n",
        "        \"The Moon orbits the\": \" Earth\",\n",
        "    }\n",
        "    for p, comp in physics.items():\n",
        "        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"physics\"})\n",
        "\n",
        "    # ---- Chemistry facts\n",
        "    chemistry = {\n",
        "        \"Water is composed of hydrogen and\": \" oxygen\",\n",
        "        \"The symbol for gold is\": \" Au\",\n",
        "        \"The symbol for silver is\": \" Ag\",\n",
        "        \"The symbol for iron is\": \" Fe\",\n",
        "        \"The periodic table was created by\": \" Mendeleev\",\n",
        "        \"Oxygen has atomic number\": \" 8\",\n",
        "        \"Carbon has atomic number\": \" 6\",\n",
        "        \"Hydrogen has atomic number\": \" 1\",\n",
        "        \"Salt is composed of sodium and\": \" chloride\",\n",
        "    }\n",
        "    for p, comp in chemistry.items():\n",
        "        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"chemistry\"})\n",
        "\n",
        "    # ---- Biology facts\n",
        "    biology = {\n",
        "        \"DNA stands for deoxyribonucleic\": \" acid\",\n",
        "        \"Photosynthesis occurs in\": \" plants\",\n",
        "        \"The heart pumps\": \" blood\",\n",
        "        \"Humans have\": \" 46\",\n",
        "        \"Evolution was proposed by\": \" Darwin\",\n",
        "        \"Cells are the basic unit of\": \" life\",\n",
        "        \"Mitochondria produce\": \" energy\",\n",
        "        \"The largest organ is the\": \" skin\",\n",
        "    }\n",
        "    for p, comp in biology.items():\n",
        "        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"biology\"})\n",
        "\n",
        "    # ---- Mathematics facts\n",
        "    mathematics = {\n",
        "        \"Pi is approximately\": \" 3\",\n",
        "        \"A triangle has\": \" three\",\n",
        "        \"A square has\": \" four\",\n",
        "        \"A circle has\": \" 360\",\n",
        "        \"The Pythagorean theorem relates\": \" triangles\",\n",
        "        \"Calculus was invented by\": \" Newton\",\n",
        "        \"Algebra originated in\": \" ancient\",\n",
        "    }\n",
        "    for p, comp in mathematics.items():\n",
        "        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"mathematics\"})\n",
        "\n",
        "    # ---- Astronomy facts\n",
        "    astronomy = {\n",
        "        \"The Sun is a\": \" star\",\n",
        "        \"Jupiter is a\": \" gas\",\n",
        "        \"Mars is the\": \" red\",\n",
        "        \"Saturn has\": \" rings\",\n",
        "        \"The Solar System has\": \" eight\",\n",
        "        \"The Milky Way is a\": \" galaxy\",\n",
        "        \"A light year measures\": \" distance\",\n",
        "        \"The nearest star to Earth is the\": \" Sun\",\n",
        "        \"Pluto was reclassified as a\": \" dwarf\",\n",
        "    }\n",
        "    for p, comp in astronomy.items():\n",
        "        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"astronomy\"})\n",
        "\n",
        "    # ========================================\n",
        "    # CULTURAL FACTS (Massively Expanded)\n",
        "    # ========================================\n",
        "\n",
        "    # ---- Literature and authors\n",
        "    literature = {\n",
        "        \"Shakespeare wrote\": \" Hamlet\",\n",
        "        \"Homer wrote the\": \" Odyssey\",\n",
        "        \"Tolkien wrote The Lord of the\": \" Rings\",\n",
        "        \"George Orwell wrote\": \" 1984\",\n",
        "        \"Jane Austen wrote Pride and\": \" Prejudice\",\n",
        "        \"Mark Twain wrote The Adventures of\": \" Tom\",\n",
        "        \"Charles Dickens wrote A Tale of\": \" Two\",\n",
        "        \"Ernest Hemingway wrote The Old Man and the\": \" Sea\",\n",
        "        \"F. Scott Fitzgerald wrote The Great\": \" Gatsby\",\n",
        "        \"Leo Tolstoy wrote War and\": \" Peace\",\n",
        "        \"Fyodor Dostoevsky wrote Crime and\": \" Punishment\",\n",
        "        \"Victor Hugo wrote Les\": \" Miserables\",\n",
        "        \"Miguel de Cervantes wrote Don\": \" Quixote\",\n",
        "        \"Dante wrote The Divine\": \" Comedy\",\n",
        "        \"Virgil wrote the\": \" Aeneid\",\n",
        "    }\n",
        "    for p, comp in literature.items():\n",
        "        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"literature\"})\n",
        "\n",
        "    # ---- Art and artists\n",
        "    art = {\n",
        "        \"Leonardo da Vinci painted the Mona\": \" Lisa\",\n",
        "        \"Vincent van Gogh painted Starry\": \" Night\",\n",
        "        \"Pablo Picasso was a\": \" Spanish\",\n",
        "        \"Michelangelo painted the Sistine\": \" Chapel\",\n",
        "        \"Claude Monet was an\": \" Impressionist\",\n",
        "        \"Salvador Dali was a\": \" Surrealist\",\n",
        "        \"Rembrandt was a\": \" Dutch\",\n",
        "        \"Andy Warhol was a\": \" Pop\",\n",
        "    }\n",
        "    for p, comp in art.items():\n",
        "        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"art\"})\n",
        "\n",
        "    # ---- Music and composers\n",
        "    music = {\n",
        "        \"Mozart was a\": \" composer\",\n",
        "        \"Beethoven wrote\": \" symphonies\",\n",
        "        \"Bach was a\": \" Baroque\",\n",
        "        \"Chopin was a\": \" Polish\",\n",
        "        \"Tchaikovsky was a\": \" Russian\",\n",
        "        \"Wagner was a\": \" German\",\n",
        "        \"Vivaldi wrote The Four\": \" Seasons\",\n",
        "        \"Handel wrote\": \" Messiah\",\n",
        "    }\n",
        "    for p, comp in music.items():\n",
        "        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"music\"})\n",
        "\n",
        "    # ---- Sports facts\n",
        "    sports = {\n",
        "        \"The Olympics originated in\": \" Greece\",\n",
        "        \"Soccer is called football in\": \" Europe\",\n",
        "        \"Basketball was invented in\": \" America\",\n",
        "        \"Baseball is popular in\": \" America\",\n",
        "        \"Cricket is popular in\": \" India\",\n",
        "        \"The World Cup is held every\": \" four\",\n",
        "        \"Tennis is played on a\": \" court\",\n",
        "        \"Golf is played on a\": \" course\",\n",
        "    }\n",
        "    for p, comp in sports.items():\n",
        "        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"sports\"})\n",
        "\n",
        "    # ========================================\n",
        "    # TECHNOLOGY AND INVENTIONS\n",
        "    # ========================================\n",
        "\n",
        "    technology = {\n",
        "        \"The telephone was invented by\": \" Bell\",\n",
        "        \"The light bulb was invented by\": \" Edison\",\n",
        "        \"The airplane was invented by the Wright\": \" Brothers\",\n",
        "        \"The printing press was invented by\": \" Gutenberg\",\n",
        "        \"The steam engine was invented by\": \" Watt\",\n",
        "        \"The radio was invented by\": \" Marconi\",\n",
        "        \"The computer was invented in the\": \" 20th\",\n",
        "        \"The internet was developed in\": \" America\",\n",
        "        \"Apple was founded by Steve\": \" Jobs\",\n",
        "        \"Microsoft was founded by Bill\": \" Gates\",\n",
        "        \"Facebook was founded by Mark\": \" Zuckerberg\",\n",
        "    }\n",
        "    for p, comp in technology.items():\n",
        "        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"technology\"})\n",
        "\n",
        "    # ========================================\n",
        "    # ARCHITECTURE AND LANDMARKS\n",
        "    # ========================================\n",
        "\n",
        "    landmarks = {\n",
        "        \"The Eiffel Tower is in\": \" Paris\",\n",
        "        \"The Colosseum is in\": \" Rome\",\n",
        "        \"The Taj Mahal is in\": \" India\",\n",
        "        \"The Great Wall is in\": \" China\",\n",
        "        \"The Statue of Liberty is in\": \" New\",\n",
        "        \"Big Ben is in\": \" London\",\n",
        "        \"The Pyramids are in\": \" Egypt\",\n",
        "        \"The Parthenon is in\": \" Athens\",\n",
        "        \"The Kremlin is in\": \" Moscow\",\n",
        "        \"Machu Picchu is in\": \" Peru\",\n",
        "        \"Petra is in\": \" Jordan\",\n",
        "        \"Angkor Wat is in\": \" Cambodia\",\n",
        "        \"The Sydney Opera House is in\": \" Australia\",\n",
        "    }\n",
        "    for p, comp in landmarks.items():\n",
        "        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"landmarks\"})\n",
        "\n",
        "    # ========================================\n",
        "    # ANIMALS AND NATURE\n",
        "    # ========================================\n",
        "\n",
        "    animals = {\n",
        "        \"The largest animal is the blue\": \" whale\",\n",
        "        \"The fastest land animal is the\": \" cheetah\",\n",
        "        \"The tallest animal is the\": \" giraffe\",\n",
        "        \"Lions are found in\": \" Africa\",\n",
        "        \"Pandas are native to\": \" China\",\n",
        "        \"Kangaroos are native to\": \" Australia\",\n",
        "        \"Penguins live in\": \" Antarctica\",\n",
        "        \"Tigers are native to\": \" Asia\",\n",
        "        \"Elephants are found in\": \" Africa\",\n",
        "        \"Polar bears live in the\": \" Arctic\",\n",
        "    }\n",
        "    for p, comp in animals.items():\n",
        "        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"animals\"})\n",
        "\n",
        "    # ========================================\n",
        "    # RELIGIONS AND MYTHOLOGY\n",
        "    # ========================================\n",
        "\n",
        "    religions = {\n",
        "        \"Christianity originated in\": \" Israel\",\n",
        "        \"Islam originated in\": \" Saudi\",\n",
        "        \"Buddhism originated in\": \" India\",\n",
        "        \"Hinduism originated in\": \" India\",\n",
        "        \"Judaism originated in\": \" Israel\",\n",
        "        \"The Bible is the holy book of\": \" Christianity\",\n",
        "        \"The Quran is the holy book of\": \" Islam\",\n",
        "        \"Zeus was the king of the\": \" Greek\",\n",
        "        \"Thor was a\": \" Norse\",\n",
        "        \"Ra was an\": \" Egyptian\",\n",
        "    }\n",
        "    for p, comp in religions.items():\n",
        "        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"religion\"})\n",
        "\n",
        "    # ========================================\n",
        "    # FOOD AND CUISINE\n",
        "    # ========================================\n",
        "\n",
        "    cuisine = {\n",
        "        \"Pizza originated in\": \" Italy\",\n",
        "        \"Sushi originated in\": \" Japan\",\n",
        "        \"Tacos originated in\": \" Mexico\",\n",
        "        \"Hamburgers are popular in\": \" America\",\n",
        "        \"Pasta is from\": \" Italy\",\n",
        "        \"Croissants are from\": \" France\",\n",
        "        \"Curry is from\": \" India\",\n",
        "        \"Paella is from\": \" Spain\",\n",
        "        \"Kimchi is from\": \" Korea\",\n",
        "    }\n",
        "    for p, comp in cuisine.items():\n",
        "        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"cuisine\"})\n",
        "\n",
        "    # ========================================\n",
        "    # ECONOMIC AND POLITICAL FACTS\n",
        "    # ========================================\n",
        "\n",
        "    economics = {\n",
        "        \"The largest economy is\": \" America\",\n",
        "        \"The European Union uses the\": \" euro\",\n",
        "        \"OPEC stands for Organization of\": \" Petroleum\",\n",
        "        \"The World Bank is headquartered in\": \" Washington\",\n",
        "        \"The United Nations is headquartered in\": \" New\",\n",
        "        \"NATO stands for North Atlantic\": \" Treaty\",\n",
        "        \"GDP stands for Gross Domestic\": \" Product\",\n",
        "    }\n",
        "    for p, comp in economics.items():\n",
        "        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"economics\"})\n",
        "\n",
        "    # ---- Filter to single-token completions\n",
        "    kept = []\n",
        "    dropped = []\n",
        "    for ex in pairs:\n",
        "        if is_single_token(ex[\"completion\"], tokenizer):\n",
        "            kept.append(ex)\n",
        "        else:\n",
        "            dropped.append(ex)\n",
        "\n",
        "    # ---- Basic reporting\n",
        "    from collections import Counter\n",
        "    counts = Counter(ex[\"tag\"].split(\":\")[0] for ex in kept)\n",
        "    print(\"Kept counts by task:\", dict(counts))\n",
        "    print(f\"\\nTotal generated pairs: {len(pairs)}\")\n",
        "    print(f\"Single-token completions: {len(kept)}\")\n",
        "    print(f\"Multi-token completions (dropped): {len(dropped)}\")\n",
        "\n",
        "    if dropped:\n",
        "        print(f\"\\nShowing first 20 dropped (multi-token) examples:\")\n",
        "        for ex in dropped[:20]:\n",
        "            ids = tokenizer.encode(ex[\"completion\"], add_special_tokens=False)\n",
        "            print(f\"  {ex['tag']:<20} {ex['prompt']:<50} -> {repr(ex['completion']):<20} token_ids={ids}\")\n",
        "\n",
        "    return kept\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)\n",
        "pairs = build_pairs_expanded(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "VWhgBD9RKuhj"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ==========================================\n",
        "#@title Expanded Mini-alignment dataset + WikiText mix + optional slot-attn-only finetune + rerun generations\n",
        "# (Aligned to ASMLanguageModel + your asa_generate)\n",
        "# ==========================================\n",
        "\n",
        "import random\n",
        "import math\n",
        "import re\n",
        "import itertools\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# -----------------------\n",
        "# 0) Repro & device\n",
        "# -----------------------\n",
        "SEED = 1337\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "device = next(model.parameters()).device\n",
        "\n",
        "# -----------------------\n",
        "# 0.5) Config knobs (NEW)\n",
        "# -----------------------\n",
        "CFG = {\n",
        "    # mix in WikiText\n",
        "    \"use_wiki\": True,\n",
        "    \"wiki_dataset_name\": \"wikitext\",\n",
        "    \"wiki_config_candidates\": [\"wikitext-103-raw-v1\", \"wikitext-2-raw-v1\"],  # fallback\n",
        "    \"wiki_num_samples\": 1536,         # number of wiki chunks (not lines)\n",
        "    \"wiki_chunk_chars_min\": 400,      # filter small chunks\n",
        "    \"wiki_chunk_chars_max\": 1200,     # chunk size (chars) before tokenization\n",
        "\n",
        "    # training\n",
        "    \"max_len\": 128,                   # increased since wiki chunks are longer\n",
        "    \"batch_size\": 16,\n",
        "    \"steps\": 777,\n",
        "    \"lr\": 7e-6,\n",
        "    \"weight_decay\": 0.007,\n",
        "    \"grad_clip\": 1.0,\n",
        "\n",
        "    # finetune mode\n",
        "    # \"all\" trains everything; \"slot_attn_only\" freezes everything except slot-space attention op\n",
        "    \"finetune_mode\": \"all\",  # or \"all\" or  \"slot_attn_only\"\n",
        "\n",
        "    # which params count as \"slot attention\" (adjust to your module names)\n",
        "    #\"slot_train_name_regex\": r\"(slot|slots).*(attn|attention)|((attn|attention).*(slot|slots))\",\n",
        "\n",
        "    \"slot_train_name_regex\":r\"(^|\\.)(slot_in|slot_q|slot_k|slot_v|slot_out)\\.weight$|(^|\\.)(_slotspace_gate_raw)$\",\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "# -----------------------\n",
        "# 1) Utilities (aligned to your model call style)\n",
        "# -----------------------\n",
        "@torch.no_grad()\n",
        "def next_token_stats(prompt: str, target_token_str: str, model, tokenizer):\n",
        "    model.eval()\n",
        "    inp = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    tgt_ids = tokenizer.encode(target_token_str, add_special_tokens=False)\n",
        "    if len(tgt_ids) != 1:\n",
        "        return {\"ok\": False, \"reason\": f\"target string maps to {len(tgt_ids)} tokens\", \"target_ids\": tgt_ids}\n",
        "\n",
        "    tgt = tgt_ids[0]\n",
        "\n",
        "    out = model(inp)\n",
        "    logits = out[0] if isinstance(out, (tuple, list)) else out\n",
        "    last = logits[0, -1, :]\n",
        "    probs = torch.softmax(last, dim=-1)\n",
        "\n",
        "    p = float(probs[tgt].item())\n",
        "    rank = int((torch.argsort(last, descending=True) == tgt).nonzero(as_tuple=False).item()) + 1\n",
        "    top1_id = int(torch.argmax(last).item())\n",
        "    top1 = tokenizer.decode([top1_id])\n",
        "\n",
        "    return {\"ok\": True, \"p_target\": p, \"rank\": rank, \"top1\": top1, \"target_id\": tgt}\n",
        "\n",
        "@torch.no_grad()\n",
        "def greedy_suffix(prompt: str, model, gen, max_new_tokens=8):\n",
        "    g = dict(gen)\n",
        "    g[\"do_sample\"] = False\n",
        "    g[\"max_new_tokens\"] = int(max_new_tokens)\n",
        "    out = asa_generate(prompt, model, g)\n",
        "    text = out[\"text\"]\n",
        "    if text.startswith(prompt):\n",
        "        return text[len(prompt):].replace(\"\\n\", \" \").strip()\n",
        "    idx = text.find(prompt)\n",
        "    if idx >= 0:\n",
        "        return text[idx+len(prompt):].replace(\"\\n\", \" \").strip()\n",
        "    return text.replace(\"\\n\", \" \").strip()\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_exact_match(examples, model, gen, max_new_tokens=8):\n",
        "    model.eval()\n",
        "    ok = 0\n",
        "    for ex in examples:\n",
        "        pred = greedy_suffix(ex[\"prompt\"], model, gen, max_new_tokens=max_new_tokens)\n",
        "        gold = ex[\"completion\"].replace(\"\\n\", \" \").strip()\n",
        "        ok += int(pred.startswith(gold))\n",
        "    return ok / max(1, len(examples))\n",
        "\n",
        "# -----------------------\n",
        "# 2) Dataset builders\n",
        "# -----------------------\n",
        "\n",
        "\n",
        "def load_wikitext_chunks(tokenizer, num_samples=2048, chunk_chars_min=400, chunk_chars_max=1200):\n",
        "    \"\"\"\n",
        "    Produces wiki training examples as plain LM text chunks:\n",
        "      ex = {\"prompt\": \"\", \"completion\": \"<wiki chunk>\", \"tag\": \"wiki\"}\n",
        "    We chunk by chars first, then token-truncate later in dataset.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from datasets import load_dataset\n",
        "    except Exception as e:\n",
        "        print(\"[WikiText] datasets not available; skipping WikiText mix.\")\n",
        "        return []\n",
        "\n",
        "    ds = None\n",
        "    used_cfg = None\n",
        "    for cfg in CFG[\"wiki_config_candidates\"]:\n",
        "        try:\n",
        "            ds = load_dataset(CFG[\"wiki_dataset_name\"], cfg, split=\"train\")\n",
        "            used_cfg = cfg\n",
        "            break\n",
        "        except Exception:\n",
        "            ds = None\n",
        "\n",
        "    if ds is None:\n",
        "        print(\"[WikiText] Could not load WikiText (tried configs:\", CFG[\"wiki_config_candidates\"], \"). Skipping.\")\n",
        "        return []\n",
        "\n",
        "    print(f\"[WikiText] Loaded {CFG['wiki_dataset_name']} / {used_cfg} train split with {len(ds)} rows.\")\n",
        "\n",
        "    # Pull raw text field (wikitext uses 'text')\n",
        "    texts = [t for t in ds[\"text\"] if isinstance(t, str) and len(t.strip()) > 0]\n",
        "\n",
        "    # Make chunks: concatenate consecutive lines until size bound, filter small chunks\n",
        "    chunks = []\n",
        "    buf = []\n",
        "    buf_len = 0\n",
        "\n",
        "    # shuffle deterministically\n",
        "    rng = random.Random(SEED)\n",
        "    rng.shuffle(texts)\n",
        "\n",
        "    for line in texts:\n",
        "        line = line.strip()\n",
        "        # skip headings markup lines; keep normal prose\n",
        "        if line.startswith(\"=\") and line.endswith(\"=\"):\n",
        "            continue\n",
        "        if not line:\n",
        "            continue\n",
        "\n",
        "        # add line to buffer\n",
        "        if buf_len + len(line) + 1 <= chunk_chars_max:\n",
        "            buf.append(line)\n",
        "            buf_len += len(line) + 1\n",
        "        else:\n",
        "            chunk = \" \".join(buf).strip()\n",
        "            if len(chunk) >= chunk_chars_min:\n",
        "                chunks.append(chunk)\n",
        "            buf = [line]\n",
        "            buf_len = len(line) + 1\n",
        "\n",
        "        if len(chunks) >= num_samples:\n",
        "            break\n",
        "\n",
        "    # flush\n",
        "    if len(chunks) < num_samples:\n",
        "        chunk = \" \".join(buf).strip()\n",
        "        if len(chunk) >= chunk_chars_min:\n",
        "            chunks.append(chunk)\n",
        "\n",
        "    # create examples\n",
        "    wiki_examples = [{\"prompt\": \"\", \"completion\": c, \"tag\": \"wiki\"} for c in chunks[:num_samples]]\n",
        "    print(f\"[WikiText] Prepared {len(wiki_examples)} wiki chunks.\")\n",
        "\n",
        "    return wiki_examples\n",
        "\n",
        "# -----------------------\n",
        "# 3) Split synthetic (entity-holdout) + build mixed train set\n",
        "# -----------------------\n",
        "#pairs = build_pairs_expanded(tokenizer)\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "holdout_capitals = {\n",
        "    \"Spain\", \"Canada\", \"Poland\", \"Portugal\", \"Greece\", \"Austria\",\n",
        "    \"Norway\", \"Ireland\", \"Romania\", \"Croatia\", \"Argentina\", \"Chile\"\n",
        "}\n",
        "holdout_languages = {\"Brazil\", \"Mexico\", \"Netherlands\", \"Sweden\", \"Finland\"}\n",
        "holdout_currencies = {\"Japan\", \"Switzerland\", \"South Africa\", \"Thailand\"}\n",
        "holdout_continents = {\"Kenya\", \"Vietnam\", \"Peru\", \"New Zealand\"}\n",
        "\n",
        "train_examples, holdout_examples = [], []\n",
        "for ex in pairs:\n",
        "    task = ex[\"tag\"].split(\":\")[0]\n",
        "\n",
        "    if task == \"capital\":\n",
        "        country = ex[\"tag\"].split(\":\", 1)[1]\n",
        "        (holdout_examples if country in holdout_capitals else train_examples).append(ex)\n",
        "    elif task == \"language\":\n",
        "        country = ex[\"tag\"].split(\":\", 1)[1]\n",
        "        (holdout_examples if country in holdout_languages else train_examples).append(ex)\n",
        "    elif task == \"currency\":\n",
        "        country = ex[\"tag\"].split(\":\", 1)[1]\n",
        "        (holdout_examples if country in holdout_currencies else train_examples).append(ex)\n",
        "    elif task == \"continent\":\n",
        "        country = ex[\"tag\"].split(\":\", 1)[1]\n",
        "        (holdout_examples if country in holdout_continents else train_examples).append(ex)\n",
        "    else:\n",
        "        if random.random() < 0.2:\n",
        "            holdout_examples.append(ex)\n",
        "        else:\n",
        "            train_examples.append(ex)\n",
        "\n",
        "print(f\"\\n[Synthetic] Total kept pairs: {len(pairs)} | Train: {len(train_examples)} | Holdout: {len(holdout_examples)}\")\n",
        "print(f\"[Synthetic] Split: {len(train_examples)/len(pairs)*100:.1f}% / {len(holdout_examples)/len(pairs)*100:.1f}%\")\n",
        "holdout_by_cat = Counter(ex[\"tag\"].split(\":\")[0] for ex in holdout_examples)\n",
        "print(\"[Synthetic] Holdout by category:\", dict(holdout_by_cat))\n",
        "\n",
        "# NEW: load wiki and mix into TRAIN ONLY\n",
        "wiki_examples = []\n",
        "if CFG[\"use_wiki\"]:\n",
        "    wiki_examples = load_wikitext_chunks(\n",
        "        tokenizer,\n",
        "        num_samples=CFG[\"wiki_num_samples\"],\n",
        "        chunk_chars_min=CFG[\"wiki_chunk_chars_min\"],\n",
        "        chunk_chars_max=CFG[\"wiki_chunk_chars_max\"],\n",
        "    )\n",
        "\n",
        "mixed_train_examples = train_examples + wiki_examples\n",
        "print(f\"\\n[Mix] Train synthetic={len(train_examples)} + wiki={len(wiki_examples)} => mixed_train={len(mixed_train_examples)}\")\n",
        "print(f\"[Mix] Holdout (synthetic only) = {len(holdout_examples)}\")\n",
        "\n",
        "# -----------------------\n",
        "# 4) Tiny finetune dataset (teacher forcing)\n",
        "#    Works for BOTH: prompt+completion pairs and raw wiki chunks (prompt=\"\")\n",
        "# -----------------------\n",
        "class PromptCompletionDataset(Dataset):\n",
        "    def __init__(self, examples, tokenizer, max_len=128):\n",
        "        self.examples = examples\n",
        "        self.tok = tokenizer\n",
        "        self.max_len = int(max_len)\n",
        "\n",
        "    def __len__(self): return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ex = self.examples[idx]\n",
        "        text = ex[\"prompt\"] + ex[\"completion\"]\n",
        "        ids = self.tok.encode(text)\n",
        "\n",
        "        # keep the tail; for wiki, this acts like \"random suffix LM\"\n",
        "        ids = ids[-self.max_len:]\n",
        "\n",
        "        x = torch.tensor(ids[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(ids[1:], dtype=torch.long)\n",
        "        return x, y, ex\n",
        "\n",
        "def collate_pad(batch):\n",
        "    xs, ys, exs = zip(*batch)\n",
        "    maxT = max(x.size(0) for x in xs)\n",
        "    pad_id = tokenizer.eos_token_id  # GPT-2 no pad token\n",
        "\n",
        "    X = torch.full((len(xs), maxT), pad_id, dtype=torch.long)\n",
        "    Y = torch.full((len(xs), maxT), -100, dtype=torch.long)\n",
        "\n",
        "    for i, (x, y) in enumerate(zip(xs, ys)):\n",
        "        T = x.size(0)\n",
        "        X[i, :T] = x\n",
        "        Y[i, :T] = y\n",
        "    return X.to(device), Y.to(device), exs\n",
        "\n",
        "train_ds = PromptCompletionDataset(mixed_train_examples, tokenizer, max_len=CFG[\"max_len\"])\n",
        "train_dl = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=min(CFG[\"batch_size\"], len(train_ds)),\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_pad\n",
        ")\n",
        "\n",
        "# -----------------------\n",
        "# 5) Optional: freeze everything except slot-space attention (NEW)\n",
        "# -----------------------\n",
        "def configure_finetune_mode(model, mode: str, name_regex: str):\n",
        "    \"\"\"\n",
        "    mode:\n",
        "      - \"all\": train everything\n",
        "      - \"slot_attn_only\": only train parameters whose full name matches `name_regex`\n",
        "    \"\"\"\n",
        "    if mode == \"all\":\n",
        "        for p in model.parameters():\n",
        "            p.requires_grad = True\n",
        "        trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "        total = sum(p.numel() for p in model.parameters())\n",
        "        print(f\"[Finetune] mode=all trainable={trainable}/{total} ({trainable/total*100:.2f}%)\")\n",
        "        return\n",
        "\n",
        "    if mode != \"slot_attn_only\":\n",
        "        raise ValueError(f\"Unknown finetune_mode={mode}\")\n",
        "\n",
        "    rx = re.compile(name_regex, flags=re.IGNORECASE)\n",
        "\n",
        "    # freeze everything\n",
        "    for _, p in model.named_parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # unfreeze matching params\n",
        "    matched = []\n",
        "    for n, p in model.named_parameters():\n",
        "        if rx.search(n) is not None:\n",
        "            p.requires_grad = True\n",
        "            matched.append((n, p.numel()))\n",
        "\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"[Finetune] mode=slot_attn_only regex={name_regex!r}\")\n",
        "    print(f\"[Finetune] trainable={trainable}/{total} ({trainable/total*100:.4f}%) matched_tensors={len(matched)}\")\n",
        "\n",
        "    # show top matches by size\n",
        "    matched.sort(key=lambda x: -x[1])\n",
        "    for n, k in matched[:25]:\n",
        "        print(f\"  [trainable] {k:>10}  {n}\")\n",
        "\n",
        "configure_finetune_mode(model, CFG[\"finetune_mode\"], CFG[\"slot_train_name_regex\"])\n",
        "\n",
        "# -----------------------\n",
        "# 6) Pre-eval (synthetic only, as before)\n",
        "# -----------------------\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PRE-TRAINING EVALUATION (synthetic only)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "pre_acc_train = eval_exact_match(train_examples, model, gener, max_new_tokens=8)\n",
        "pre_acc_hold  = eval_exact_match(holdout_examples, model, gener, max_new_tokens=8)\n",
        "\n",
        "print(f\"\\n[PRE] Exact-match accuracy:\")\n",
        "print(f\"  Train:   {pre_acc_train:.3f} ({int(pre_acc_train*len(train_examples))}/{len(train_examples)})\")\n",
        "print(f\"  Holdout: {pre_acc_hold:.3f} ({int(pre_acc_hold*len(holdout_examples))}/{len(holdout_examples)})\")\n",
        "\n",
        "print(\"\\n[PRE] Next-token stats for sample of single-token targets (synthetic only):\")\n",
        "sample_for_stats = random.sample(pairs, min(30, len(pairs)))\n",
        "for ex in sample_for_stats:\n",
        "    stats = next_token_stats(ex[\"prompt\"], ex[\"completion\"], model, tokenizer)\n",
        "    if stats[\"ok\"]:\n",
        "        print(f\"  {ex['tag']:<25} P={stats['p_target']:.4f} rank={stats['rank']:>5} top1={stats['top1']!r}\")\n",
        "    else:\n",
        "        print(f\"  {ex['tag']:<25} (skip) {stats['reason']}\")\n",
        "\n",
        "# -----------------------\n",
        "# 7) Light training (mixed: synthetic + wiki)\n",
        "# -----------------------\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAINING (mixed synthetic + wiki)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "model.train()\n",
        "\n",
        "# IMPORTANT: optimizer must only see trainable params (esp for slot_attn_only)\n",
        "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "if len(trainable_params) == 0:\n",
        "    raise RuntimeError(\"No trainable parameters. Check CFG['finetune_mode'] and regex.\")\n",
        "\n",
        "opt = torch.optim.AdamW(\n",
        "    trainable_params,\n",
        "    lr=CFG[\"lr\"],\n",
        "    betas=(0.9, 0.95),\n",
        "    weight_decay=CFG[\"weight_decay\"]\n",
        ")\n",
        "\n",
        "steps = int(CFG[\"steps\"])\n",
        "grad_clip = float(CFG[\"grad_clip\"])\n",
        "\n",
        "print(f\"Training for {steps} steps with batch_size={train_dl.batch_size}\")\n",
        "print(f\"Total mixed_train_examples={len(mixed_train_examples)} | synthetic={len(train_examples)} | wiki={len(wiki_examples)}\\n\")\n",
        "\n",
        "# stable batch stream (avoid re-instantiating iter(train_dl) each step)\n",
        "batch_iter = itertools.cycle(train_dl)\n",
        "\n",
        "for step in range(steps):\n",
        "    X, Y, _ = next(batch_iter)\n",
        "    opt.zero_grad(set_to_none=True)\n",
        "\n",
        "    logits = model(X)\n",
        "    logits = logits[0] if isinstance(logits, (tuple, list)) else logits\n",
        "\n",
        "    loss = F.cross_entropy(\n",
        "        logits.view(-1, logits.size(-1)),\n",
        "        Y.view(-1),\n",
        "        ignore_index=-100\n",
        "    )\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(trainable_params, grad_clip)\n",
        "    opt.step()\n",
        "\n",
        "    if (step + 1) % 50 == 0:\n",
        "        print(f\"  [train] step {step+1:>4}/{steps} loss={float(loss.item()):.4f}\")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# -----------------------\n",
        "# 8) Post-eval (synthetic only, as before)\n",
        "# -----------------------\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"POST-TRAINING EVALUATION (synthetic only)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "post_acc_train = eval_exact_match(train_examples, model, gener, max_new_tokens=8)\n",
        "post_acc_hold  = eval_exact_match(holdout_examples, model, gener, max_new_tokens=8)\n",
        "\n",
        "print(f\"\\n[POST] Exact-match accuracy:\")\n",
        "print(f\"  Train:   {post_acc_train:.3f} ({int(post_acc_train*len(train_examples))}/{len(train_examples)})\")\n",
        "print(f\"  Holdout: {post_acc_hold:.3f} ({int(post_acc_hold*len(holdout_examples))}/{len(holdout_examples)})\")\n",
        "\n",
        "print(f\"\\n[DELTA] Accuracy change:\")\n",
        "print(f\"  Train:   {pre_acc_train:.3f} -> {post_acc_train:.3f} (={post_acc_train-pre_acc_train:+.3f})\")\n",
        "print(f\"  Holdout: {pre_acc_hold:.3f} -> {post_acc_hold:.3f} (={post_acc_hold-pre_acc_hold:+.3f})\")\n",
        "\n",
        "print(\"\\n[POST] Next-token stats for same sample (synthetic only):\")\n",
        "for ex in sample_for_stats:\n",
        "    stats = next_token_stats(ex[\"prompt\"], ex[\"completion\"], model, tokenizer)\n",
        "    if stats[\"ok\"]:\n",
        "        print(f\"  {ex['tag']:<25} P={stats['p_target']:.4f} rank={stats['rank']:>5} top1={stats['top1']!r}\")\n",
        "\n",
        "# -----------------------\n",
        "# 9) Generations (synthetic categories only)\n",
        "# -----------------------\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GENERATION SAMPLES (greedy decoding) (synthetic only)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "generation_samples = []\n",
        "by_category = {}\n",
        "for ex in pairs:\n",
        "    cat = ex[\"tag\"].split(\":\")[0]\n",
        "    by_category.setdefault(cat, []).append(ex)\n",
        "\n",
        "for cat, exs in sorted(by_category.items()):\n",
        "    generation_samples.extend(exs[:2])\n",
        "\n",
        "generation_samples = generation_samples[:25]\n",
        "\n",
        "for ex in generation_samples:\n",
        "    raw = greedy_suffix(ex[\"prompt\"], model, gener, max_new_tokens=12)\n",
        "\n",
        "    tag_base = ex[\"tag\"].split(\":\")[0]\n",
        "    if tag_base == \"capital\":\n",
        "        scaffold_prompt = ex[\"prompt\"] + \" the city of\"\n",
        "    elif tag_base == \"language\":\n",
        "        scaffold_prompt = ex[\"prompt\"] + \" primarily\"\n",
        "    elif tag_base == \"currency\":\n",
        "        scaffold_prompt = ex[\"prompt\"]\n",
        "    else:\n",
        "        scaffold_prompt = ex[\"prompt\"]\n",
        "\n",
        "    sca = greedy_suffix(scaffold_prompt, model, gener, max_new_tokens=12)\n",
        "\n",
        "    print(f\"\\n{''*80}\")\n",
        "    print(f\"CATEGORY: {ex['tag']:<25} TARGET: {ex['completion']!r}\")\n",
        "    print(f\"PROMPT:   {ex['prompt']!r}\")\n",
        "    print(f\"RAW:      {raw[:100]}\")\n",
        "    if scaffold_prompt != ex[\"prompt\"]:\n",
        "        print(f\"SCAFFOLD: {sca[:100]}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EVALUATION COMPLETE\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDzP5ZcxuVee"
      },
      "outputs": [],
      "source": [
        "\n",
        "sweep_asa_temps(\n",
        "    model, tokenizer, gener,\n",
        "    holdout_examples,\n",
        "    read_grid=(0.4, 0.1, 1.0, 0.05),\n",
        "    write_grid=(1.0,),\n",
        "    max_new_tokens=8,\n",
        "    n_gen_samples=8,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nuLwxAMsrIZ"
      },
      "source": [
        "# Final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "p6wJtrFjL6pq"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "#@title Vanilla Transformer (GPT-style) baseline with matching training loop + comparable metrics\n",
        "\n",
        "import math\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Optional, Tuple, Dict, List\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# =========================================================\n",
        "# RoPE (same helper style as your ASA)\n",
        "# =========================================================\n",
        "def _rotate_half(x: torch.Tensor) -> torch.Tensor:\n",
        "    x1 = x[..., ::2]\n",
        "    x2 = x[..., 1::2]\n",
        "    return torch.stack((-x2, x1), dim=-1).flatten(-2)\n",
        "\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    def __init__(self, dim: int, base: float = 10000.0):\n",
        "        super().__init__()\n",
        "        assert dim % 2 == 0, \"RoPE requires even dim\"\n",
        "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
        "        self._cos_cached = None\n",
        "        self._sin_cached = None\n",
        "        self._t_cached = None\n",
        "        self._device_cached = None\n",
        "\n",
        "    def get_cos_sin(self, T: int, device, dtype):\n",
        "        if (\n",
        "            self._t_cached == T\n",
        "            and self._cos_cached is not None\n",
        "            and self._device_cached == device\n",
        "        ):\n",
        "            return self._cos_cached.to(dtype=dtype), self._sin_cached.to(dtype=dtype)\n",
        "\n",
        "        t = torch.arange(T, device=device, dtype=self.inv_freq.dtype)\n",
        "        freqs = torch.einsum(\"t,f->tf\", t, self.inv_freq)  # [T, d/2]\n",
        "        emb = torch.cat([freqs, freqs], dim=-1)            # [T, d]\n",
        "        cos = emb.cos()[None, None, :, :]                  # [1,1,T,d]\n",
        "        sin = emb.sin()[None, None, :, :]                  # [1,1,T,d]\n",
        "\n",
        "        self._t_cached = T\n",
        "        self._device_cached = device\n",
        "        self._cos_cached = cos\n",
        "        self._sin_cached = sin\n",
        "        return cos.to(dtype=dtype), sin.to(dtype=dtype)\n",
        "\n",
        "def apply_rope(x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> torch.Tensor:\n",
        "    return (x * cos) + (_rotate_half(x) * sin)\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Config\n",
        "# =========================================================\n",
        "@dataclass\n",
        "class TFTrainConfig:\n",
        "    # Data\n",
        "    dataset_name: str = \"wikitext\"\n",
        "    dataset_config: str = \"wikitext-103-raw-v1\"\n",
        "    tokenizer_name: str = \"gpt2\"\n",
        "\n",
        "    max_seq_len: int = 1024\n",
        "    stride_frac_val: float = 0.50\n",
        "    seed: int = 1337\n",
        "\n",
        "    # Sample budgets\n",
        "    train_samples_target: int = 100_000_000\n",
        "    val_samples_target: int = 25_000\n",
        "\n",
        "    # Training\n",
        "    batch_size: int = 32\n",
        "    learning_rate: float = 3e-4\n",
        "    weight_decay: float = 0.01\n",
        "    betas: Tuple[float, float] = (0.9, 0.95)\n",
        "    grad_clip: float = 1.0\n",
        "    warmup_steps: int = 1_000\n",
        "    total_steps: int = 75_000\n",
        "    eval_interval: int = 1_000\n",
        "    log_interval: int = 100\n",
        "\n",
        "    # Model\n",
        "    vocab_size: int = 50257\n",
        "    embed_dim: int = 384\n",
        "    num_layers: int = 13\n",
        "    num_heads: int = 8\n",
        "    mlp_ratio: float = 4.0\n",
        "    dropout: float = 0.1\n",
        "    tie_weights: bool = True\n",
        "\n",
        "    # Positions\n",
        "    use_abs_pos: bool = False          # keep False to mirror your ASA runs\n",
        "    use_rope: bool = True\n",
        "    rope_base: float = 10000.0\n",
        "\n",
        "    # Analytics\n",
        "    eval_max_batches: int = 150\n",
        "    analytics_last_k: int = 32\n",
        "\n",
        "    # IO / caches\n",
        "    output_dir: str = \"./drive/MyDrive/tf_outputs\"\n",
        "    tag: str = \"tf_wikitext\"\n",
        "    cache_dir: str = \"./drive/MyDrive/asm_caches\"  # reuse your cached streams\n",
        "    val_windows_cache: str = \"./drive/MyDrive/asm_nlp/val_cache_wikitext_windows_512.pkl\"  # reuse\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Model: GPT-style Transformer\n",
        "# =========================================================\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int,\n",
        "        num_heads: int,\n",
        "        dropout: float,\n",
        "        use_rope: bool = True,\n",
        "        rope_base: float = 10000.0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.use_rope = bool(use_rope)\n",
        "\n",
        "        self.qkv = nn.Linear(embed_dim, 3 * embed_dim, bias=False)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.attn_drop = nn.Dropout(dropout)\n",
        "        self.resid_drop = nn.Dropout(dropout)\n",
        "\n",
        "        self.rope = RotaryEmbedding(self.head_dim, base=float(rope_base)) if self.use_rope else None\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,  # [B,T] bool/int (1=valid)\n",
        "        return_info: bool = False,\n",
        "    ):\n",
        "        B, T, C = x.shape\n",
        "        H, d = self.num_heads, self.head_dim\n",
        "\n",
        "        qkv = self.qkv(x)  # [B,T,3C]\n",
        "        q, k, v = qkv.split(C, dim=-1)\n",
        "\n",
        "        # [B,H,T,d]\n",
        "        q = q.view(B, T, H, d).transpose(1, 2)\n",
        "        k = k.view(B, T, H, d).transpose(1, 2)\n",
        "        v = v.view(B, T, H, d).transpose(1, 2)\n",
        "\n",
        "        if self.use_rope:\n",
        "            cos, sin = self.rope.get_cos_sin(T, device=x.device, dtype=q.dtype)\n",
        "            q = apply_rope(q, cos, sin)\n",
        "            k = apply_rope(k, cos, sin)\n",
        "\n",
        "        # scaled dot-product attention logits: [B,H,T,T]\n",
        "        att = torch.einsum(\"bhid,bhjd->bhij\", q, k) / math.sqrt(d)\n",
        "\n",
        "        # causal mask\n",
        "        causal = torch.tril(torch.ones((T, T), device=x.device, dtype=torch.bool))\n",
        "        att = att.masked_fill(~causal.view(1, 1, T, T), float(\"-inf\"))\n",
        "\n",
        "        # optional key padding mask (mask keys that are padding)\n",
        "        if attention_mask is not None:\n",
        "            valid = attention_mask.to(dtype=torch.bool)  # [B,T]\n",
        "            att = att.masked_fill(~valid.view(B, 1, 1, T), float(\"-inf\"))\n",
        "\n",
        "        w = torch.softmax(att, dim=-1)  # [B,H,T,T]\n",
        "        w = self.attn_drop(w)\n",
        "\n",
        "        y = torch.einsum(\"bhij,bhjd->bhid\", w, v)  # [B,H,T,d]\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.resid_drop(self.out_proj(y))\n",
        "\n",
        "        info = None\n",
        "        if return_info:\n",
        "            # Store attention weights (can be big); use eval_max_batches to control cost.\n",
        "            info = {\n",
        "                \"attn_weights\": w.detach(),               # [B,H,T,T]\n",
        "                \"attn_logits\": att.detach().to(torch.float32),  # [B,H,T,T] for COM/diagnostics if needed\n",
        "            }\n",
        "        return y, info\n",
        "\n",
        "\n",
        "class TFBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int,\n",
        "        num_heads: int,\n",
        "        mlp_ratio: float,\n",
        "        dropout: float,\n",
        "        use_rope: bool,\n",
        "        rope_base: float,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = CausalSelfAttention(\n",
        "            embed_dim=embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            use_rope=use_rope,\n",
        "            rope_base=rope_base,\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        hidden = int(embed_dim * float(mlp_ratio))\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden, bias=False),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden, embed_dim, bias=False),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, attention_mask=None, return_info=False):\n",
        "        a, info = self.attn(self.norm1(x), attention_mask=attention_mask, return_info=return_info)\n",
        "        x = x + a\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x, info\n",
        "\n",
        "\n",
        "class TransformerLM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        embed_dim: int,\n",
        "        num_layers: int,\n",
        "        num_heads: int,\n",
        "        max_seq_len: int,\n",
        "        mlp_ratio: float,\n",
        "        dropout: float,\n",
        "        tie_weights: bool,\n",
        "        use_abs_pos: bool,\n",
        "        use_rope: bool,\n",
        "        rope_base: float,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.use_abs_pos = bool(use_abs_pos)\n",
        "\n",
        "        self.tok = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos = nn.Embedding(max_seq_len, embed_dim) if self.use_abs_pos else None\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TFBlock(\n",
        "                embed_dim=embed_dim,\n",
        "                num_heads=num_heads,\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                dropout=dropout,\n",
        "                use_rope=use_rope,\n",
        "                rope_base=rope_base,\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
        "        if tie_weights:\n",
        "            self.lm_head.weight = self.tok.weight\n",
        "\n",
        "        self.apply(self._init)\n",
        "\n",
        "    def _init(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.normal_(m.weight, std=0.02)\n",
        "        elif isinstance(m, nn.Embedding):\n",
        "            nn.init.normal_(m.weight, std=0.02)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.ones_(m.weight)\n",
        "            nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, return_info=False):\n",
        "        B, T = input_ids.shape\n",
        "        #assert T <= self.max_seq_len, f\"T={T} exceeds max_seq_len={self.max_seq_len}\"\n",
        "\n",
        "        x = self.tok(input_ids)\n",
        "        if self.use_abs_pos:\n",
        "            pos = torch.arange(T, device=input_ids.device).unsqueeze(0).expand(B, -1)\n",
        "            x = x + self.pos(pos)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        infos = []\n",
        "        for blk in self.blocks:\n",
        "            x, info = blk(x, attention_mask=attention_mask, return_info=return_info)\n",
        "            if return_info:\n",
        "                infos.append(info)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        logits = self.lm_head(x)\n",
        "        return (logits, infos) if return_info else logits\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Comparable metrics for Transformer attention\n",
        "# =========================================================\n",
        "@torch.no_grad()\n",
        "def _attn_entropy_mean(attn_w: torch.Tensor) -> float:\n",
        "    # attn_w: [B,H,T,T] entropy over keys for each query\n",
        "    eps = 1e-8\n",
        "    p = attn_w.clamp_min(eps)\n",
        "    ent = -(p * p.log()).sum(dim=-1)  # [B,H,T]\n",
        "    return float(ent.mean().cpu().item())\n",
        "\n",
        "@torch.no_grad()\n",
        "def _attn_top1freq_mean(attn_w: torch.Tensor) -> float:\n",
        "    # \"most common top1 key index\" frequency, averaged over batch+heads\n",
        "    top1 = attn_w.argmax(dim=-1)  # [B,H,T]\n",
        "    flat = top1.reshape(-1).cpu()\n",
        "    T = attn_w.shape[-1]\n",
        "    hist = torch.bincount(flat, minlength=T).float()\n",
        "    denom = hist.sum().clamp_min(1.0)\n",
        "    return float((hist.max() / denom).item())\n",
        "\n",
        "@torch.no_grad()\n",
        "def _attn_com_and_lastk(attn_w: torch.Tensor, last_k: int) -> Tuple[float, float]:\n",
        "    # center-of-mass over keys (0..1) and last-k mass, averaged over B,H,T\n",
        "    B, H, Tq, Tk = attn_w.shape\n",
        "    assert Tq == Tk\n",
        "    pos = torch.arange(Tk, device=attn_w.device, dtype=attn_w.dtype).view(1, 1, 1, Tk)\n",
        "    com = (attn_w * pos).sum(dim=-1) / max(1.0, float(Tk - 1))  # [B,H,T]\n",
        "    last_k = min(max(1, int(last_k)), Tk)\n",
        "    lastk_mass = attn_w[..., -last_k:].sum(dim=-1)              # [B,H,T]\n",
        "    return float(com.mean().cpu().item()), float(lastk_mass.mean().cpu().item())\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Eval for Transformer (same CE loss, similar printed stats)\n",
        "# =========================================================\n",
        "@torch.no_grad()\n",
        "def evaluate_tf(model, val_loader, max_batches=50, last_k=32):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "\n",
        "    ent_acc = 0.0\n",
        "    top1_acc = 0.0\n",
        "    com_acc = 0.0\n",
        "    lastk_acc = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    for i, (xb, yb) in enumerate(val_loader):\n",
        "        if i >= max_batches:\n",
        "            break\n",
        "        xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
        "\n",
        "        with torch.autocast(device_type=device.type, dtype=amp_dtype, enabled=use_amp):\n",
        "            logits, infos = model(xb, return_info=True)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), yb.view(-1))\n",
        "        losses.append(float(loss.item()))\n",
        "\n",
        "        # layer-avg attention diagnostics\n",
        "        eL, tL, cL, lL = [], [], [], []\n",
        "        for info in infos:\n",
        "            w = info.get(\"attn_weights\", None)\n",
        "            if w is None:\n",
        "                continue\n",
        "            eL.append(_attn_entropy_mean(w))\n",
        "            tL.append(_attn_top1freq_mean(w))\n",
        "            com, lastm = _attn_com_and_lastk(w, last_k=last_k)\n",
        "            cL.append(com); lL.append(lastm)\n",
        "\n",
        "        if eL:\n",
        "            ent_acc += sum(eL) / len(eL)\n",
        "            top1_acc += sum(tL) / len(tL)\n",
        "            com_acc += sum(cL) / len(cL)\n",
        "            lastk_acc += sum(lL) / len(lL)\n",
        "            n_batches += 1\n",
        "\n",
        "    mean = sum(losses) / max(1, len(losses))\n",
        "    ppl = float(math.exp(min(20.0, mean)))\n",
        "\n",
        "    stats = {}\n",
        "    if n_batches > 0:\n",
        "        stats[\"attn_entropy_mean\"] = ent_acc / n_batches\n",
        "        stats[\"attn_top1freq_mean\"] = top1_acc / n_batches\n",
        "        stats[\"attn_com_mean\"] = com_acc / n_batches\n",
        "        stats[\"attn_lastk_mass_mean\"] = lastk_acc / n_batches\n",
        "\n",
        "    model.train()\n",
        "    return mean, ppl, stats\n",
        "\n",
        "def _fmt_stats_tf(stats: Dict[str, float], last_k: int) -> str:\n",
        "    keys = [\"attn_entropy_mean\", \"attn_top1freq_mean\", \"attn_com_mean\", \"attn_lastk_mass_mean\"]\n",
        "    parts = []\n",
        "    for k in keys:\n",
        "        if k in stats:\n",
        "            if k == \"attn_lastk_mass_mean\":\n",
        "                parts.append(f\"{k}(last_k={last_k})={stats[k]:.4f}\")\n",
        "            else:\n",
        "                parts.append(f\"{k}={stats[k]:.4f}\")\n",
        "    return \" | \".join(parts)\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Train loop (reuses your cached stream/window builders + WarmupCosine + save_ckpt)\n",
        "# Requires: build_or_load_token_stream, build_or_load_validation_windows, WikiTextRandomWindowStream,\n",
        "#           WarmupCosine, save_ckpt from your notebook (already defined above).\n",
        "# =========================================================\n",
        "def train_tf(cfg: TFTrainConfig, bail=False):\n",
        "    random.seed(cfg.seed)\n",
        "    torch.manual_seed(cfg.seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(cfg.seed)\n",
        "\n",
        "    os.makedirs(cfg.cache_dir, exist_ok=True)\n",
        "    train_stream_cache = os.path.join(cfg.cache_dir, f\"{cfg.dataset_config}_train_stream.pkl\")\n",
        "    val_stream_cache   = os.path.join(cfg.cache_dir, f\"{cfg.dataset_config}_val_stream.pkl\")\n",
        "\n",
        "    train_stream = build_or_load_token_stream(\n",
        "        cache_path=train_stream_cache,\n",
        "        dataset_name=cfg.dataset_name,\n",
        "        dataset_config=cfg.dataset_config,\n",
        "        split=\"train\",\n",
        "        tokenizer_name=cfg.tokenizer_name,\n",
        "        min_chars=1,\n",
        "        add_eos_between_rows=True,\n",
        "    )\n",
        "    val_stream = build_or_load_token_stream(\n",
        "        cache_path=val_stream_cache,\n",
        "        dataset_name=cfg.dataset_name,\n",
        "        dataset_config=cfg.dataset_config,\n",
        "        split=\"validation\",\n",
        "        tokenizer_name=cfg.tokenizer_name,\n",
        "        min_chars=1,\n",
        "        add_eos_between_rows=True,\n",
        "    )\n",
        "\n",
        "    val_dataset = build_or_load_validation_windows(\n",
        "        cache_path=cfg.val_windows_cache,\n",
        "        token_stream=val_stream,\n",
        "        max_seq_len=cfg.max_seq_len,\n",
        "        stride_frac=cfg.stride_frac_val,\n",
        "        val_samples_target=cfg.val_samples_target,\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=cfg.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "        pin_memory=torch.cuda.is_available(),\n",
        "    )\n",
        "\n",
        "    train_ds = WikiTextRandomWindowStream(\n",
        "        token_stream=train_stream,\n",
        "        max_seq_len=cfg.max_seq_len,\n",
        "        train_samples_target=cfg.train_samples_target,\n",
        "        seed=cfg.seed,\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=cfg.batch_size,\n",
        "        num_workers=3,\n",
        "        pin_memory=torch.cuda.is_available(),\n",
        "    )\n",
        "\n",
        "    model = TransformerLM(\n",
        "        vocab_size=cfg.vocab_size,\n",
        "        embed_dim=cfg.embed_dim,\n",
        "        num_layers=cfg.num_layers,\n",
        "        num_heads=cfg.num_heads,\n",
        "        max_seq_len=cfg.max_seq_len,\n",
        "        mlp_ratio=cfg.mlp_ratio,\n",
        "        dropout=cfg.dropout,\n",
        "        tie_weights=cfg.tie_weights,\n",
        "        use_abs_pos=cfg.use_abs_pos,\n",
        "        use_rope=cfg.use_rope,\n",
        "        rope_base=cfg.rope_base,\n",
        "    ).to(device)\n",
        "\n",
        "    if bail:\n",
        "        return model\n",
        "\n",
        "    out_dir = os.path.join(cfg.output_dir, cfg.tag)\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    n_params = sum(p.numel() for p in model.parameters())\n",
        "    print(\"=\" * 108)\n",
        "    print(f\"Training [TF {cfg.tag}] on {cfg.dataset_name}/{cfg.dataset_config}\")\n",
        "    print(f\"Params: {n_params:,}\")\n",
        "    print(f\"Train tokens: {len(train_stream):,} | Val tokens: {len(val_stream):,} | Val windows: {len(val_dataset):,}\")\n",
        "    print(f\"T={cfg.max_seq_len} | val_stride_frac={cfg.stride_frac_val} | last_k={cfg.analytics_last_k}\")\n",
        "    print(f\"amp={use_amp}({amp_dtype}) | abs_pos={cfg.use_abs_pos} | rope={cfg.use_rope}(base={cfg.rope_base:g})\")\n",
        "    print(\"=\" * 108)\n",
        "\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay, betas=cfg.betas)\n",
        "    sched = WarmupCosine(opt, cfg.warmup_steps, cfg.total_steps, cfg.learning_rate)\n",
        "\n",
        "    # initial eval + ckpt\n",
        "    best_val = float(\"inf\")\n",
        "    vloss, vppl, vstats = evaluate_tf(model, val_loader, max_batches=cfg.eval_max_batches, last_k=cfg.analytics_last_k)\n",
        "    best_val = vloss\n",
        "    save_ckpt(os.path.join(out_dir, \"best.pt\"), cfg, model, opt, 0, best_val)\n",
        "\n",
        "    print(f\"[VAL step 0] loss={vloss:.3f} ppl={vppl:.2f}\")\n",
        "    if vstats:\n",
        "        print(\"  \" + _fmt_stats_tf(vstats, last_k=cfg.analytics_last_k))\n",
        "\n",
        "    running = 0.0\n",
        "    step = 0\n",
        "    t_last = time.time()\n",
        "\n",
        "    pbar = tqdm(total=cfg.total_steps, desc=f\"[TF {cfg.tag}]\")\n",
        "    for xb, yb in train_loader:\n",
        "        xb = xb.to(device, non_blocking=True)\n",
        "        yb = yb.to(device, non_blocking=True)\n",
        "\n",
        "        with torch.autocast(device_type=device.type, dtype=amp_dtype, enabled=use_amp):\n",
        "            logits = model(xb)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), yb.view(-1))\n",
        "\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
        "        opt.step()\n",
        "        lr = sched.step()\n",
        "\n",
        "        step += 1\n",
        "        running += float(loss.item())\n",
        "        pbar.update(1)\n",
        "\n",
        "        if step % cfg.log_interval == 0:\n",
        "            avg = running / cfg.log_interval\n",
        "            running = 0.0\n",
        "            it_s = cfg.log_interval / max(1e-9, (time.time() - t_last))\n",
        "            t_last = time.time()\n",
        "            pbar.set_postfix({\n",
        "                \"loss\": f\"{avg:.3f}\",\n",
        "                \"ppl\": f\"{math.exp(min(20.0, avg)):.2f}\",\n",
        "                \"lr\": f\"{lr:.2e}\",\n",
        "                \"it/s\": f\"{it_s:.2f}\",\n",
        "            })\n",
        "            print(f\"[step {step}] train_loss={avg:.3f} ppl={math.exp(min(20.0, avg)):.2f} lr={lr:.2e} it/s={it_s:.2f}\")\n",
        "\n",
        "        if step % cfg.eval_interval == 0:\n",
        "            vloss, vppl, vstats = evaluate_tf(model, val_loader, max_batches=cfg.eval_max_batches, last_k=cfg.analytics_last_k)\n",
        "            print(f\"\\n[VAL step {step}] loss={vloss:.3f} ppl={vppl:.2f}\")\n",
        "            if vstats:\n",
        "                print(\"  \" + _fmt_stats_tf(vstats, last_k=cfg.analytics_last_k))\n",
        "            if vloss < best_val:\n",
        "                best_val = vloss\n",
        "                save_ckpt(os.path.join(out_dir, \"best.pt\"), cfg, model, opt, step, best_val)\n",
        "\n",
        "        if step >= cfg.total_steps:\n",
        "            break\n",
        "\n",
        "    save_ckpt(os.path.join(out_dir, \"final.pt\"), cfg, model, opt, step, best_val)\n",
        "    print(f\"[TF {cfg.tag}] Done. Best val loss: {best_val:.4f}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def count_params(model):\n",
        "    n = sum(p.numel() for p in model.parameters())\n",
        "    n_train = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return n, n_train\n",
        "\n",
        "# =========================================================\n",
        "# Quick start snippet (edit tag + total_steps to 25000 if you want parity)\n",
        "# =========================================================\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "use_amp = torch.cuda.is_available()\n",
        "amp_dtype = torch.bfloat16  # A100-friendly\n",
        "\n",
        "print(\"Using device:\", device)\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory/1e9:.1f} GB\")\n",
        "\n",
        "\n",
        "tf_cfg = TFTrainConfig(\n",
        "    max_seq_len=1024,\n",
        "    batch_size=32,\n",
        "    embed_dim=384,\n",
        "    num_layers=13,\n",
        "    num_heads=8,\n",
        "    total_steps=25000,  # match your interrupted ASA point\n",
        "    tag=\"tf_wikitext_1024t_384d_8h_13l_25ksteps\",\n",
        ")\n",
        "\n",
        "print(tf_cfg)\n",
        "tf_model = train_tf(tf_cfg, True)\n",
        "n, n_trainable = count_params(tf_model)\n",
        "print(f\"Params: {n/1e6:.2f}M\")\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FrE2F1htCNpn"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Batch Data Generation (UPDATED): supports variable seq_len for extrapolation\n",
        "\n",
        "import os, random, pickle\n",
        "from typing import List, Tuple, Optional, Iterator\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, IterableDataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Minimal dataset classes (same as training)\n",
        "# -------------------------\n",
        "class StableValidationDataset(Dataset):\n",
        "    def __init__(self, samples: List[Tuple[torch.Tensor, torch.Tensor]]):\n",
        "        self.samples = samples\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx): return self.samples[idx]\n",
        "\n",
        "def _ensure_dir(path: str):\n",
        "    d = os.path.dirname(path)\n",
        "    if d:\n",
        "        os.makedirs(d, exist_ok=True)\n",
        "\n",
        "def build_or_load_token_stream(\n",
        "    *,\n",
        "    cache_path: str,\n",
        "    dataset_name: str,\n",
        "    dataset_config: str,\n",
        "    split: str,\n",
        "    tokenizer_name: str,\n",
        "    min_chars: int = 1,\n",
        "    add_eos_between_rows: bool = True,\n",
        "    max_rows: Optional[int] = None,\n",
        ") -> List[int]:\n",
        "    if os.path.exists(cache_path):\n",
        "        with open(cache_path, \"rb\") as f:\n",
        "            return pickle.load(f)\n",
        "\n",
        "    _ensure_dir(cache_path)\n",
        "    tok = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=True)\n",
        "    eos = tok.eos_token_id\n",
        "    assert eos is not None, \"Tokenizer must have eos_token_id\"\n",
        "\n",
        "    ds = load_dataset(dataset_name, dataset_config, split=split)\n",
        "\n",
        "    stream: List[int] = []\n",
        "    used = 0\n",
        "    for row in ds:\n",
        "        if max_rows is not None and used >= max_rows:\n",
        "            break\n",
        "        text = (row.get(\"text\") or \"\").strip()\n",
        "        if len(text) < min_chars:\n",
        "            continue\n",
        "        ids = tok.encode(text, add_special_tokens=False)\n",
        "        if not ids:\n",
        "            continue\n",
        "        stream.extend(ids)\n",
        "        if add_eos_between_rows:\n",
        "            stream.append(eos)\n",
        "        used += 1\n",
        "\n",
        "    with open(cache_path, \"wb\") as f:\n",
        "        pickle.dump(stream, f)\n",
        "    return stream\n",
        "\n",
        "def build_or_load_validation_windows(\n",
        "    *,\n",
        "    cache_path: str,\n",
        "    token_stream: List[int],\n",
        "    max_seq_len: int,\n",
        "    stride_frac: float,\n",
        "    val_samples_target: int,\n",
        ") -> StableValidationDataset:\n",
        "    \"\"\"\n",
        "    Same as before: stable deterministic val windows at a specific max_seq_len.\n",
        "    \"\"\"\n",
        "    if os.path.exists(cache_path):\n",
        "        with open(cache_path, \"rb\") as f:\n",
        "            samples = pickle.load(f)\n",
        "        return StableValidationDataset(samples)\n",
        "\n",
        "    _ensure_dir(cache_path)\n",
        "\n",
        "    T = int(max_seq_len)\n",
        "    stride = max(1, int(T * float(stride_frac)))\n",
        "    need = int(val_samples_target)\n",
        "\n",
        "    max_start = len(token_stream) - (T + 1)\n",
        "    if max_start <= 0:\n",
        "        raise ValueError(\"Validation token stream too small for max_seq_len+1\")\n",
        "\n",
        "    samples: List[Tuple[torch.Tensor, torch.Tensor]] = []\n",
        "    for start in range(0, max_start + 1, stride):\n",
        "        chunk = token_stream[start:start + T + 1]\n",
        "        if len(chunk) < T + 1:\n",
        "            break\n",
        "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
        "        samples.append((x, y))\n",
        "        if len(samples) >= need:\n",
        "            break\n",
        "\n",
        "    with open(cache_path, \"wb\") as f:\n",
        "        pickle.dump(samples, f)\n",
        "    return StableValidationDataset(samples)\n",
        "\n",
        "class WikiTextRandomWindowStream(IterableDataset):\n",
        "    def __init__(self, token_stream: List[int], seq_len: int, train_samples_target: int, seed: int):\n",
        "        super().__init__()\n",
        "        self.stream = token_stream\n",
        "        self.T = int(seq_len)\n",
        "        self.target = int(train_samples_target)\n",
        "        self.seed = int(seed)\n",
        "        self.max_start = len(self.stream) - (self.T + 1)\n",
        "        if self.max_start <= 0:\n",
        "            raise ValueError(f\"Train token stream too small for seq_len+1 (seq_len={self.T})\")\n",
        "\n",
        "    def __iter__(self):\n",
        "        info = torch.utils.data.get_worker_info()\n",
        "        wid = info.id if info is not None else 0\n",
        "        rng = random.Random(self.seed + 17 * wid)\n",
        "\n",
        "        yielded = 0\n",
        "        while yielded < self.target:\n",
        "            start = rng.randint(0, self.max_start)\n",
        "            chunk = self.stream[start:start + self.T + 1]\n",
        "            x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
        "            y = torch.tensor(chunk[1:], dtype=torch.long)\n",
        "            yield x, y\n",
        "            yielded += 1\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# New: stable val windows for arbitrary seq_len (no cache explosion)\n",
        "# -------------------------\n",
        "class StableValWindowsFromStream(Dataset):\n",
        "    \"\"\"\n",
        "    Deterministic windows for *any* seq_len using a token stream.\n",
        "    This avoids writing a giant cache file for 8k+ windows.\n",
        "    \"\"\"\n",
        "    def __init__(self, token_stream: List[int], seq_len: int, stride_frac: float, n_samples: int):\n",
        "        self.stream = token_stream\n",
        "        self.T = int(seq_len)\n",
        "        self.stride = max(1, int(self.T * float(stride_frac)))\n",
        "        self.n = int(n_samples)\n",
        "\n",
        "        max_start = len(self.stream) - (self.T + 1)\n",
        "        if max_start <= 0:\n",
        "            raise ValueError(f\"Val token stream too small for seq_len+1 (seq_len={self.T})\")\n",
        "\n",
        "        # deterministically pick starts: 0, stride, 2*stride, ...\n",
        "        starts = list(range(0, max_start + 1, self.stride))\n",
        "        if len(starts) == 0:\n",
        "            raise ValueError(\"No valid starts computed.\")\n",
        "        # truncate deterministically\n",
        "        self.starts = starts[: self.n]\n",
        "\n",
        "    def __len__(self): return len(self.starts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s = self.starts[idx]\n",
        "        chunk = self.stream[s:s + self.T + 1]\n",
        "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Analysis batch generator (UPDATED)\n",
        "# -------------------------\n",
        "def make_batch_generator(\n",
        "    cfg,\n",
        "    *,\n",
        "    split: str = \"val\",                 # \"val\" or \"train\"\n",
        "    device: Optional[torch.device] = None,\n",
        "    num_workers: int = 0,               # 0 is safest for Colab analysis\n",
        "    pin_memory: bool = True,\n",
        "    batches_per_epoch: Optional[int] = None,\n",
        "    set_batch_size: Optional[int] = -1,\n",
        "    infinite: bool = True,\n",
        "    seed: Optional[int] = None,\n",
        "    seq_len: Optional[int] = None,      # <-- NEW: override cfg.max_seq_len\n",
        ") -> Iterator[Tuple[torch.Tensor, torch.Tensor]]:\n",
        "    \"\"\"\n",
        "    Yields xb,yb batches shaped [B,T] int64.\n",
        "\n",
        "    NEW:\n",
        "      - seq_len lets you request longer contexts (e.g., 8192) for extrapolation.\n",
        "      - For val:\n",
        "          * if seq_len == cfg.max_seq_len: uses cached stable windows (as before)\n",
        "          * else: uses deterministic StableValWindowsFromStream (no cache explosion)\n",
        "      - For train: uses random windows at seq_len.\n",
        "    \"\"\"\n",
        "    assert split in (\"val\", \"train\")\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Resolve cfg fields with fallback (works for dict-like or dataclass)\n",
        "    def get(name, default=None):\n",
        "        return getattr(cfg, name, cfg.get(name, default) if isinstance(cfg, dict) else default)\n",
        "\n",
        "    cache_dir = get(\"cache_dir\")\n",
        "    dataset_name = get(\"dataset_name\")\n",
        "    dataset_config = get(\"dataset_config\")\n",
        "    tokenizer_name = get(\"tokenizer_name\")\n",
        "    train_T = int(get(\"max_seq_len\"))\n",
        "    T = int(seq_len) if seq_len is not None else train_T\n",
        "\n",
        "    batch_size = set_batch_size if set_batch_size != -1 else int(get(\"batch_size\"))\n",
        "    stride_frac_val = float(get(\"stride_frac_val\", 0.5))\n",
        "    val_samples_target = int(get(\"val_samples_target\", 25_000))\n",
        "    val_windows_cache = get(\"val_windows_cache\")\n",
        "    train_samples_target = int(get(\"train_samples_target\", 100_000_000))\n",
        "    used_seed = int(seed if seed is not None else get(\"seed\", 1337))\n",
        "\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    train_stream_cache = os.path.join(cache_dir, f\"{dataset_config}_train_stream.pkl\")\n",
        "    val_stream_cache   = os.path.join(cache_dir, f\"{dataset_config}_val_stream.pkl\")\n",
        "\n",
        "    if split == \"train\":\n",
        "        train_stream = build_or_load_token_stream(\n",
        "            cache_path=train_stream_cache,\n",
        "            dataset_name=dataset_name,\n",
        "            dataset_config=dataset_config,\n",
        "            split=\"train\",\n",
        "            tokenizer_name=tokenizer_name,\n",
        "            min_chars=1,\n",
        "            add_eos_between_rows=True,\n",
        "        )\n",
        "        ds = WikiTextRandomWindowStream(\n",
        "            token_stream=train_stream,\n",
        "            seq_len=T,\n",
        "            train_samples_target=train_samples_target,\n",
        "            seed=used_seed,\n",
        "        )\n",
        "        loader = DataLoader(\n",
        "            ds,\n",
        "            batch_size=batch_size,\n",
        "            num_workers=num_workers,\n",
        "            pin_memory=pin_memory and torch.cuda.is_available(),\n",
        "        )\n",
        "\n",
        "        yielded = 0\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device, non_blocking=True)\n",
        "            yb = yb.to(device, non_blocking=True)\n",
        "            yield xb, yb\n",
        "            yielded += 1\n",
        "            if batches_per_epoch is not None and yielded >= batches_per_epoch:\n",
        "                if infinite:\n",
        "                    yielded = 0\n",
        "                    continue\n",
        "                break\n",
        "\n",
        "    else:\n",
        "        # val stream\n",
        "        val_stream = build_or_load_token_stream(\n",
        "            cache_path=val_stream_cache,\n",
        "            dataset_name=dataset_name,\n",
        "            dataset_config=dataset_config,\n",
        "            split=\"validation\",\n",
        "            tokenizer_name=tokenizer_name,\n",
        "            min_chars=1,\n",
        "            add_eos_between_rows=True,\n",
        "        )\n",
        "\n",
        "        # If we're at training length, preserve exact old behavior (cached stable windows)\n",
        "        if T == train_T:\n",
        "            val_ds = build_or_load_validation_windows(\n",
        "                cache_path=val_windows_cache,\n",
        "                token_stream=val_stream,\n",
        "                max_seq_len=T,\n",
        "                stride_frac=stride_frac_val,\n",
        "                val_samples_target=val_samples_target,\n",
        "            )\n",
        "            loader = DataLoader(\n",
        "                val_ds,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=False,\n",
        "                num_workers=num_workers,\n",
        "                pin_memory=pin_memory and torch.cuda.is_available(),\n",
        "            )\n",
        "        else:\n",
        "            # NEW: deterministic stable windows directly from stream for arbitrary T\n",
        "            # We only need a small number of batches for analysis, so keep it modest.\n",
        "            # If batches_per_epoch is None, default to enough samples for ~50 batches.\n",
        "            if batches_per_epoch is None:\n",
        "                target_batches = 50\n",
        "            else:\n",
        "                target_batches = int(batches_per_epoch)\n",
        "\n",
        "            n_samples = target_batches * batch_size\n",
        "            val_ds = StableValWindowsFromStream(\n",
        "                token_stream=val_stream,\n",
        "                seq_len=T,\n",
        "                stride_frac=stride_frac_val,\n",
        "                n_samples=n_samples,\n",
        "            )\n",
        "            loader = DataLoader(\n",
        "                val_ds,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=False,  # deterministic\n",
        "                num_workers=num_workers,\n",
        "                pin_memory=pin_memory and torch.cuda.is_available(),\n",
        "            )\n",
        "\n",
        "        while True:\n",
        "            yielded = 0\n",
        "            for xb, yb in loader:\n",
        "                xb = xb.to(device, non_blocking=True)\n",
        "                yb = yb.to(device, non_blocking=True)\n",
        "                yield xb, yb\n",
        "                yielded += 1\n",
        "                if batches_per_epoch is not None and yielded >= batches_per_epoch:\n",
        "                    break\n",
        "            if not infinite:\n",
        "                break\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Quick smoke tests\n",
        "# -------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# training-length batch\n",
        "gen = make_batch_generator(cfg, split=\"val\", device=device, batches_per_epoch=1, set_batch_size=4, infinite=False)\n",
        "xb, yb = next(gen)\n",
        "print(\"train-len val:\", xb.shape, yb.shape, xb.device, xb.dtype)\n",
        "\n",
        "# extrapolation-length batch (example: 8192)  will work as long as val_stream is long enough\n",
        "try:\n",
        "    gen2 = make_batch_generator(cfg, split=\"val\", device=device, seq_len=8192, batches_per_epoch=1, set_batch_size=4, infinite=False)\n",
        "    xb2, yb2 = next(gen2)\n",
        "    print(\"8192-len val:\", xb2.shape, yb2.shape, xb2.device, xb2.dtype)\n",
        "except Exception as e:\n",
        "    print(\" 8192 smoke test failed (usually stream too short or RAM constraints):\", repr(e))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "KxIBaCzkIwI0"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Throughput & Scaling Benchmark: Verify O(TK) Complexity\n",
        "\n",
        "import time\n",
        "import gc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List, Dict, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ============================\n",
        "# Configuration\n",
        "# ============================\n",
        "BENCHMARK_CONFIG = {\n",
        "    # Sequence lengths to test\n",
        "    'seq_lengths': [512, 1024, 2048, 4096, 8192],\n",
        "\n",
        "    # Batch sizes (can vary by seq_len to fit in memory)\n",
        "    'batch_size_map': {\n",
        "        256: 16,\n",
        "        512: 16,\n",
        "        1024: 8,\n",
        "        2048: 4,\n",
        "        4096: 2,\n",
        "        8192: 1,\n",
        "    },\n",
        "\n",
        "    # Number of warmup iterations (for stable timing)\n",
        "    'warmup_iters': 10,\n",
        "\n",
        "    # Number of measurement iterations\n",
        "    'measure_iters': 50,\n",
        "\n",
        "    # Whether to test backward pass (memory intensive)\n",
        "    'test_backward': True,\n",
        "\n",
        "    # Whether to measure peak memory\n",
        "    'measure_memory': True,\n",
        "}\n",
        "\n",
        "# ============================\n",
        "# Helper Functions\n",
        "# ============================\n",
        "\n",
        "def clear_gpu_memory():\n",
        "    \"\"\"Clear GPU memory cache.\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "def get_gpu_memory_mb() -> float:\n",
        "    \"\"\"Get current GPU memory usage in MB.\"\"\"\n",
        "    if not torch.cuda.is_available():\n",
        "        return 0.0\n",
        "    return torch.cuda.memory_allocated() / 1024**2\n",
        "\n",
        "def get_peak_gpu_memory_mb() -> float:\n",
        "    \"\"\"Get peak GPU memory usage in MB.\"\"\"\n",
        "    if not torch.cuda.is_available():\n",
        "        return 0.0\n",
        "    return torch.cuda.max_memory_allocated() / 1024**2\n",
        "\n",
        "def reset_peak_memory():\n",
        "    \"\"\"Reset peak memory stats.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "def benchmark_forward_pass(\n",
        "    model: torch.nn.Module,\n",
        "    seq_len: int,\n",
        "    batch_size: int,\n",
        "    vocab_size: int,\n",
        "    device: torch.device,\n",
        "    warmup_iters: int = 3,\n",
        "    measure_iters: int = 10,\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Benchmark forward pass only.\n",
        "    Returns: {\n",
        "        'mean_time_ms': float,\n",
        "        'std_time_ms': float,\n",
        "        'tokens_per_sec': float,\n",
        "        'memory_mb': float,\n",
        "    }\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    clear_gpu_memory()\n",
        "    reset_peak_memory()\n",
        "\n",
        "    # Create dummy input\n",
        "    input_ids = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)\n",
        "\n",
        "    # Warmup\n",
        "    with torch.no_grad():\n",
        "        for _ in range(warmup_iters):\n",
        "            _ = model(input_ids)\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.synchronize()\n",
        "\n",
        "    # Measure\n",
        "    times = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(measure_iters):\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.synchronize()\n",
        "\n",
        "            start = time.perf_counter()\n",
        "            _ = model(input_ids)\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.synchronize()\n",
        "\n",
        "            end = time.perf_counter()\n",
        "            times.append((end - start) * 1000)  # Convert to ms\n",
        "\n",
        "    mean_time_ms = np.mean(times)\n",
        "    std_time_ms = np.std(times)\n",
        "\n",
        "    # Calculate throughput\n",
        "    total_tokens = batch_size * seq_len\n",
        "    tokens_per_sec = total_tokens / (mean_time_ms / 1000)\n",
        "\n",
        "    # Memory\n",
        "    memory_mb = get_peak_gpu_memory_mb()\n",
        "\n",
        "    return {\n",
        "        'mean_time_ms': mean_time_ms,\n",
        "        'std_time_ms': std_time_ms,\n",
        "        'tokens_per_sec': tokens_per_sec,\n",
        "        'memory_mb': memory_mb,\n",
        "    }\n",
        "\n",
        "def benchmark_forward_backward(\n",
        "    model: torch.nn.Module,\n",
        "    seq_len: int,\n",
        "    batch_size: int,\n",
        "    vocab_size: int,\n",
        "    device: torch.device,\n",
        "    warmup_iters: int = 3,\n",
        "    measure_iters: int = 10,\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Benchmark forward + backward pass.\n",
        "    Returns: {\n",
        "        'mean_time_ms': float,\n",
        "        'std_time_ms': float,\n",
        "        'tokens_per_sec': float,\n",
        "        'memory_mb': float,\n",
        "    }\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    clear_gpu_memory()\n",
        "    reset_peak_memory()\n",
        "\n",
        "    # Create dummy input\n",
        "    input_ids = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)\n",
        "    targets = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)\n",
        "\n",
        "    # Warmup\n",
        "    for _ in range(warmup_iters):\n",
        "        logits = model(input_ids)\n",
        "        loss = F.cross_entropy(\n",
        "            logits.view(-1, vocab_size),\n",
        "            targets.view(-1)\n",
        "        )\n",
        "        loss.backward()\n",
        "        model.zero_grad()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "    # Measure\n",
        "    times = []\n",
        "    for _ in range(measure_iters):\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "        start = time.perf_counter()\n",
        "\n",
        "        logits = model(input_ids)\n",
        "        loss = F.cross_entropy(\n",
        "            logits.view(-1, vocab_size),\n",
        "            targets.view(-1)\n",
        "        )\n",
        "        loss.backward()\n",
        "        model.zero_grad()\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "        end = time.perf_counter()\n",
        "        times.append((end - start) * 1000)  # Convert to ms\n",
        "\n",
        "    mean_time_ms = np.mean(times)\n",
        "    std_time_ms = np.std(times)\n",
        "\n",
        "    # Calculate throughput\n",
        "    total_tokens = batch_size * seq_len\n",
        "    tokens_per_sec = total_tokens / (mean_time_ms / 1000)\n",
        "\n",
        "    # Memory\n",
        "    memory_mb = get_peak_gpu_memory_mb()\n",
        "\n",
        "    model.eval()  # Reset to eval\n",
        "\n",
        "    return {\n",
        "        'mean_time_ms': mean_time_ms,\n",
        "        'std_time_ms': std_time_ms,\n",
        "        'tokens_per_sec': tokens_per_sec,\n",
        "        'memory_mb': memory_mb,\n",
        "    }\n",
        "\n",
        "# ============================\n",
        "# Main Benchmark\n",
        "# ============================\n",
        "\n",
        "def run_scaling_benchmark(\n",
        "    model: torch.nn.Module,\n",
        "    config: Dict,\n",
        "    device: torch.device,\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Run comprehensive scaling benchmark.\n",
        "\n",
        "    Returns:\n",
        "        forward_df: DataFrame with forward pass results\n",
        "        backward_df: DataFrame with forward+backward results (if enabled)\n",
        "    \"\"\"\n",
        "    vocab_size = model.vocab_size\n",
        "    K = cfg.num_slots\n",
        "    H = cfg.num_heads\n",
        "\n",
        "    forward_results = []\n",
        "    backward_results = []\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(f\"SCALING BENCHMARK: {cfg.num_layers}L, {cfg.embed_dim}D, {H}H, {K} slots\")\n",
        "    print(f\"Expected complexity: O(T  K) = O(T  {K})\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for seq_len in config['seq_lengths']:\n",
        "        batch_size = config['batch_size_map'].get(seq_len, 1)\n",
        "\n",
        "        print(f\"\\n{''*80}\")\n",
        "        print(f\"Testing T={seq_len:5d}, B={batch_size:2d}\")\n",
        "        print(f\"{''*80}\")\n",
        "\n",
        "        try:\n",
        "            # Forward pass benchmark\n",
        "            print(\"  Forward pass...\", end=\" \", flush=True)\n",
        "            fwd_stats = benchmark_forward_pass(\n",
        "                model=model,\n",
        "                seq_len=seq_len,\n",
        "                batch_size=batch_size,\n",
        "                vocab_size=vocab_size,\n",
        "                device=device,\n",
        "                warmup_iters=config['warmup_iters'],\n",
        "                measure_iters=config['measure_iters'],\n",
        "            )\n",
        "\n",
        "            forward_results.append({\n",
        "                'seq_len': seq_len,\n",
        "                'batch_size': batch_size,\n",
        "                'time_ms': fwd_stats['mean_time_ms'],\n",
        "                'time_std_ms': fwd_stats['std_time_ms'],\n",
        "                'tokens_per_sec': fwd_stats['tokens_per_sec'],\n",
        "                'memory_mb': fwd_stats['memory_mb'],\n",
        "                'time_per_token_us': (fwd_stats['mean_time_ms'] * 1000) / (batch_size * seq_len),\n",
        "            })\n",
        "\n",
        "            print(f\" {fwd_stats['mean_time_ms']:.1f}ms  {fwd_stats['std_time_ms']:.1f}ms \"\n",
        "                  f\"({fwd_stats['tokens_per_sec']:.0f} tok/s, {fwd_stats['memory_mb']:.0f} MB)\")\n",
        "\n",
        "            # Forward + Backward benchmark\n",
        "            if config['test_backward']:\n",
        "                print(\"  Fwd+Bwd pass...\", end=\" \", flush=True)\n",
        "                bwd_stats = benchmark_forward_backward(\n",
        "                    model=model,\n",
        "                    seq_len=seq_len,\n",
        "                    batch_size=batch_size,\n",
        "                    vocab_size=vocab_size,\n",
        "                    device=device,\n",
        "                    warmup_iters=config['warmup_iters'],\n",
        "                    measure_iters=config['measure_iters'],\n",
        "                )\n",
        "\n",
        "                backward_results.append({\n",
        "                    'seq_len': seq_len,\n",
        "                    'batch_size': batch_size,\n",
        "                    'time_ms': bwd_stats['mean_time_ms'],\n",
        "                    'time_std_ms': bwd_stats['std_time_ms'],\n",
        "                    'tokens_per_sec': bwd_stats['tokens_per_sec'],\n",
        "                    'memory_mb': bwd_stats['memory_mb'],\n",
        "                    'time_per_token_us': (bwd_stats['mean_time_ms'] * 1000) / (batch_size * seq_len),\n",
        "                })\n",
        "\n",
        "                print(f\" {bwd_stats['mean_time_ms']:.1f}ms  {bwd_stats['std_time_ms']:.1f}ms \"\n",
        "                      f\"({bwd_stats['tokens_per_sec']:.0f} tok/s, {bwd_stats['memory_mb']:.0f} MB)\")\n",
        "\n",
        "            clear_gpu_memory()\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            if \"out of memory\" in str(e).lower():\n",
        "                print(f\" OOM at T={seq_len}, B={batch_size}\")\n",
        "                clear_gpu_memory()\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "    forward_df = pd.DataFrame(forward_results)\n",
        "    backward_df = pd.DataFrame(backward_results) if backward_results else None\n",
        "\n",
        "    return forward_df, backward_df\n",
        "\n",
        "# ============================\n",
        "# Visualization\n",
        "# ============================\n",
        "\n",
        "def plot_scaling_analysis(\n",
        "    forward_df: pd.DataFrame,\n",
        "    backward_df: Optional[pd.DataFrame],\n",
        "    K: int,\n",
        "    save_path: Optional[str] = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Create comprehensive scaling analysis plots.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "    fig.suptitle(f'ASA Scaling Analysis (K={K} slots per head)', fontsize=16, fontweight='bold')\n",
        "\n",
        "    T = forward_df['seq_len'].values\n",
        "\n",
        "    # --- Plot 1: Time vs Sequence Length (log-log) ---\n",
        "    ax = axes[0, 0]\n",
        "    ax.errorbar(T, forward_df['time_ms'], yerr=forward_df['time_std_ms'],\n",
        "                marker='o', label='Forward', capsize=4, linewidth=2)\n",
        "    if backward_df is not None:\n",
        "        ax.errorbar(T, backward_df['time_ms'], yerr=backward_df['time_std_ms'],\n",
        "                    marker='s', label='Forward+Backward', capsize=4, linewidth=2)\n",
        "\n",
        "    # Add theoretical curves\n",
        "    T_theory = np.array([T.min(), T.max()])\n",
        "\n",
        "    # O(T) reference\n",
        "    t_linear_ref = T_theory / T_theory[0] * forward_df['time_ms'].iloc[0]\n",
        "    ax.plot(T_theory, t_linear_ref, 'k--', alpha=0.5, linewidth=1.5, label='O(T) reference')\n",
        "\n",
        "    # O(T) reference\n",
        "    t_quadratic_ref = (T_theory / T_theory[0])**2 * forward_df['time_ms'].iloc[0]\n",
        "    ax.plot(T_theory, t_quadratic_ref, 'r:', alpha=0.5, linewidth=1.5, label='O(T) reference')\n",
        "\n",
        "    ax.set_xscale('log', base=2)\n",
        "    ax.set_yscale('log')\n",
        "    ax.set_xlabel('Sequence Length (T)', fontsize=11)\n",
        "    ax.set_ylabel('Time (ms)', fontsize=11)\n",
        "    ax.set_title('Latency vs Sequence Length', fontweight='bold')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # --- Plot 2: Throughput vs Sequence Length ---\n",
        "    ax = axes[0, 1]\n",
        "    ax.plot(T, forward_df['tokens_per_sec'], marker='o', linewidth=2, label='Forward')\n",
        "    if backward_df is not None:\n",
        "        ax.plot(T, backward_df['tokens_per_sec'], marker='s', linewidth=2, label='Forward+Backward')\n",
        "\n",
        "    ax.set_xscale('log', base=2)\n",
        "    ax.set_xlabel('Sequence Length (T)', fontsize=11)\n",
        "    ax.set_ylabel('Throughput (tokens/sec)', fontsize=11)\n",
        "    ax.set_title('Throughput vs Sequence Length', fontweight='bold')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # --- Plot 3: Memory vs Sequence Length ---\n",
        "    ax = axes[0, 2]\n",
        "    ax.plot(T, forward_df['memory_mb'], marker='o', linewidth=2, label='Forward')\n",
        "    if backward_df is not None:\n",
        "        ax.plot(T, backward_df['memory_mb'], marker='s', linewidth=2, label='Forward+Backward')\n",
        "\n",
        "    # O(T) memory reference\n",
        "    mem_linear_ref = T_theory / T_theory[0] * forward_df['memory_mb'].iloc[0]\n",
        "    ax.plot(T_theory, mem_linear_ref, 'k--', alpha=0.5, linewidth=1.5, label='O(T) reference')\n",
        "\n",
        "    ax.set_xscale('log', base=2)\n",
        "    ax.set_yscale('log')\n",
        "    ax.set_xlabel('Sequence Length (T)', fontsize=11)\n",
        "    ax.set_ylabel('Peak Memory (MB)', fontsize=11)\n",
        "    ax.set_title('Memory Usage vs Sequence Length', fontweight='bold')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # --- Plot 4: Time per Token (should be constant for O(TK)) ---\n",
        "    ax = axes[1, 0]\n",
        "    ax.plot(T, forward_df['time_per_token_us'], marker='o', linewidth=2, label='Forward')\n",
        "    if backward_df is not None:\n",
        "        ax.plot(T, backward_df['time_per_token_us'], marker='s', linewidth=2, label='Forward+Backward')\n",
        "\n",
        "    # Add mean reference line\n",
        "    fwd_mean = forward_df['time_per_token_us'].mean()\n",
        "    ax.axhline(fwd_mean, color='blue', linestyle='--', alpha=0.5, label=f'Fwd mean: {fwd_mean:.2f}s')\n",
        "\n",
        "    ax.set_xscale('log', base=2)\n",
        "    ax.set_xlabel('Sequence Length (T)', fontsize=11)\n",
        "    ax.set_ylabel('Time per Token (s)', fontsize=11)\n",
        "    ax.set_title('Time per Token (should be ~constant for O(TK))', fontweight='bold')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # --- Plot 5: Scaling Factor Analysis ---\n",
        "    ax = axes[1, 1]\n",
        "\n",
        "    # Calculate actual scaling exponent\n",
        "    T_ratios = T[1:] / T[:-1]\n",
        "    time_ratios_fwd = forward_df['time_ms'].values[1:] / forward_df['time_ms'].values[:-1]\n",
        "\n",
        "    # Theoretical: for O(T^n), time_ratio = T_ratio^n, so n = log(time_ratio)/log(T_ratio)\n",
        "    scaling_exponents_fwd = np.log(time_ratios_fwd) / np.log(T_ratios)\n",
        "\n",
        "    ax.plot(T[1:], scaling_exponents_fwd, marker='o', linewidth=2, label='Forward (measured)')\n",
        "\n",
        "    if backward_df is not None:\n",
        "        time_ratios_bwd = backward_df['time_ms'].values[1:] / backward_df['time_ms'].values[:-1]\n",
        "        scaling_exponents_bwd = np.log(time_ratios_bwd) / np.log(T_ratios)\n",
        "        ax.plot(T[1:], scaling_exponents_bwd, marker='s', linewidth=2, label='Fwd+Bwd (measured)')\n",
        "\n",
        "    ax.axhline(1.0, color='green', linestyle='--', alpha=0.7, linewidth=2, label='O(T) target')\n",
        "    ax.axhline(2.0, color='red', linestyle=':', alpha=0.7, linewidth=2, label='O(T) reference')\n",
        "\n",
        "    ax.set_xscale('log', base=2)\n",
        "    ax.set_xlabel('Sequence Length (T)', fontsize=11)\n",
        "    ax.set_ylabel('Scaling Exponent', fontsize=11)\n",
        "    ax.set_title('Empirical Scaling Exponent (target: 1.0)', fontweight='bold')\n",
        "    ax.set_ylim(0.5, 2.5)\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # --- Plot 6: Summary Statistics Table ---\n",
        "    ax = axes[1, 2]\n",
        "    ax.axis('off')\n",
        "\n",
        "    # Calculate summary stats\n",
        "    mean_exponent_fwd = scaling_exponents_fwd.mean()\n",
        "    std_exponent_fwd = scaling_exponents_fwd.std()\n",
        "\n",
        "    summary_text = f\"\"\"\n",
        "    SUMMARY STATISTICS\n",
        "    {''*35}\n",
        "\n",
        "    Model Configuration:\n",
        "       Layers: {cfg.num_layers}\n",
        "       Embed dim: {cfg.embed_dim}\n",
        "       Heads: {cfg.num_heads}\n",
        "       Slots/head: {K}\n",
        "       Total params: {sum(p.numel() for p in model.parameters())/1e6:.1f}M\n",
        "\n",
        "    Scaling Analysis (Forward):\n",
        "       Mean exponent: {mean_exponent_fwd:.3f}  {std_exponent_fwd:.3f}\n",
        "       Target (O(TK)): 1.000\n",
        "       Deviation: {abs(mean_exponent_fwd - 1.0):.3f}\n",
        "\n",
        "    Throughput (T=1024):\n",
        "       Forward: {forward_df[forward_df['seq_len']==1024]['tokens_per_sec'].values[0]:.0f} tok/s\n",
        "    \"\"\"\n",
        "\n",
        "    if backward_df is not None:\n",
        "        mean_exponent_bwd = scaling_exponents_bwd.mean()\n",
        "        std_exponent_bwd = scaling_exponents_bwd.std()\n",
        "        summary_text += f\"\"\"\n",
        "    Scaling Analysis (Fwd+Bwd):\n",
        "       Mean exponent: {mean_exponent_bwd:.3f}  {std_exponent_bwd:.3f}\n",
        "       Deviation: {abs(mean_exponent_bwd - 1.0):.3f}\n",
        "\n",
        "    Throughput (T=1024):\n",
        "       Fwd+Bwd: {backward_df[backward_df['seq_len']==1024]['tokens_per_sec'].values[0]:.0f} tok/s\n",
        "        \"\"\"\n",
        "\n",
        "    ax.text(0.05, 0.95, summary_text, transform=ax.transAxes,\n",
        "            fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
        "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"\\n Saved plot to {save_path}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    return fig\n",
        "\n",
        "# ============================\n",
        "# Run Benchmark\n",
        "# ============================\n",
        "\n",
        "print(f\"\\n Starting benchmark on {DEVICE}\")\n",
        "print(f\"Model: {cfg.num_layers}L  {cfg.embed_dim}D  {cfg.num_heads}H  {cfg.num_slots}K\")\n",
        "\n",
        "forward_df, backward_df = run_scaling_benchmark(\n",
        "    model=model,\n",
        "    config=BENCHMARK_CONFIG,\n",
        "    device=DEVICE,\n",
        ")\n",
        "\n",
        "# ============================\n",
        "# Display Results\n",
        "# ============================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FORWARD PASS RESULTS\")\n",
        "print(\"=\"*80)\n",
        "print(forward_df.to_string(index=False))\n",
        "\n",
        "if backward_df is not None:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"FORWARD + BACKWARD RESULTS\")\n",
        "    print(\"=\"*80)\n",
        "    print(backward_df.to_string(index=False))\n",
        "\n",
        "# ============================\n",
        "# Complexity Verification\n",
        "# ============================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPLEXITY VERIFICATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "T_vals = forward_df['seq_len'].values\n",
        "time_vals = forward_df['time_ms'].values\n",
        "\n",
        "# Fit to power law: time = a * T^b\n",
        "log_T = np.log(T_vals)\n",
        "log_time = np.log(time_vals)\n",
        "b, log_a = np.polyfit(log_T, log_time, 1)\n",
        "a = np.exp(log_a)\n",
        "\n",
        "print(f\"\\nFitted power law: time = {a:.4f}  T^{b:.4f}\")\n",
        "print(f\"Expected for O(TK): exponent  1.0\")\n",
        "print(f\"Expected for O(T): exponent  2.0\")\n",
        "print(f\"\\nActual exponent: {b:.4f}\")\n",
        "\n",
        "if abs(b - 1.0) < 0.15:\n",
        "    print(\" CONFIRMED: Empirical scaling matches O(T) / O(TK) complexity!\")\n",
        "elif abs(b - 2.0) < 0.15:\n",
        "    print(\" WARNING: Empirical scaling suggests O(T) complexity\")\n",
        "else:\n",
        "    print(f\" NOTICE: Empirical scaling exponent is {b:.3f}\")\n",
        "\n",
        "# ============================\n",
        "# Visualization\n",
        "# ============================\n",
        "\n",
        "fig = plot_scaling_analysis(\n",
        "    forward_df=forward_df,\n",
        "    backward_df=backward_df,\n",
        "    K=cfg.num_slots,\n",
        "    save_path=None,  # Set to path if you want to save\n",
        ")\n",
        "\n",
        "print(\"\\n Benchmark complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "n77ViURo0v3u"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Hungarian Alignment: table + summary + plots\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ----------------------------\n",
        "# Info capture policy for this experiment\n",
        "# ----------------------------\n",
        "# We only need read_weights.\n",
        "ASA_ALIGN_INFO_CFG = dict(\n",
        "    store_read_weights=True,\n",
        "    store_read_logits=False,\n",
        "    store_write_logits=False,\n",
        "    store_slot_state_norm=False,\n",
        "    detach_to_cpu=False,\n",
        "    time_stride=1,\n",
        "    batch_stride=1,\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# Core: cosine + Hungarian\n",
        "# ----------------------------\n",
        "\n",
        "def _cosine_sim_matrix(A_kd: torch.Tensor, B_kd: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n",
        "    \"\"\"Return S [K,K] where S[i,j] = cos(A[i], B[j]).\"\"\"\n",
        "    A = A_kd.float()\n",
        "    B = B_kd.float()\n",
        "    A = A / A.norm(dim=-1, keepdim=True).clamp_min(eps)\n",
        "    B = B / B.norm(dim=-1, keepdim=True).clamp_min(eps)\n",
        "    return A @ B.t()\n",
        "\n",
        "def hungarian_match_cos(A_kd: torch.Tensor, B_kd: torch.Tensor, eps: float = 1e-8):\n",
        "    \"\"\"\n",
        "    A_kd, B_kd: [K,D] slot feature matrices.\n",
        "    Returns diag_mean, best_mean, perm (LongTensor[K] where perm[i] = matched j in B for i in A).\n",
        "    \"\"\"\n",
        "    if A_kd.shape != B_kd.shape:\n",
        "        raise ValueError(f\"Shape mismatch: A={tuple(A_kd.shape)} B={tuple(B_kd.shape)}\")\n",
        "\n",
        "    S = _cosine_sim_matrix(A_kd, B_kd, eps=eps).detach().cpu().numpy()  # [K,K]\n",
        "    diag = float(np.diag(S).mean())\n",
        "\n",
        "    try:\n",
        "        from scipy.optimize import linear_sum_assignment\n",
        "        row, col = linear_sum_assignment(-S)  # maximize\n",
        "        best = float(S[row, col].mean())\n",
        "        perm = np.empty((S.shape[0],), dtype=np.int64)\n",
        "        perm[row] = col\n",
        "        perm = torch.from_numpy(perm)\n",
        "    except Exception:\n",
        "        # deterministic greedy fallback\n",
        "        K = S.shape[0]\n",
        "        used = np.zeros((K,), dtype=bool)\n",
        "        perm_g = np.full((K,), -1, dtype=np.int64)\n",
        "        vals = []\n",
        "        for i in range(K):\n",
        "            j = int(np.argmax(np.where(used, -1e9, S[i])))\n",
        "            used[j] = True\n",
        "            perm_g[i] = j\n",
        "            vals.append(S[i, j])\n",
        "        best = float(np.mean(vals))\n",
        "        perm = torch.from_numpy(perm_g)\n",
        "\n",
        "    return diag, best, perm\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Feature builders (all -> [K,D])\n",
        "# ----------------------------\n",
        "\n",
        "def feat_basic_role_mass(read_weights_bhtk: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    read_weights: [B,H,T,K] -> feature per slot = mean role mass across (B,T) as a vector over heads.\n",
        "    Returns [K,H].\n",
        "    \"\"\"\n",
        "    hk = read_weights_bhtk.float().mean(dim=(0, 2))   # [H,K]\n",
        "    kd = hk.t().contiguous()                          # [K,H]\n",
        "    return kd / kd.norm(dim=-1, keepdim=True).clamp_min(eps)\n",
        "\n",
        "def feat_timebins(read_weights_bhtk: torch.Tensor, num_bins: int = 8, eps: float = 1e-8) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    [B,H,T,K] -> [K, H*num_bins]\n",
        "    \"\"\"\n",
        "    B, H, T, K = read_weights_bhtk.shape\n",
        "    p = read_weights_bhtk.float()\n",
        "    edges = torch.linspace(0, T, num_bins + 1, device=p.device).round().long().clamp(0, T)\n",
        "\n",
        "    chunks = []\n",
        "    for b in range(num_bins):\n",
        "        t0, t1 = int(edges[b]), int(edges[b + 1])\n",
        "        if t1 <= t0:\n",
        "            m = torch.zeros((H, K), device=p.device)\n",
        "        else:\n",
        "            m = p[:, :, t0:t1, :].mean(dim=(0, 2))  # [H,K]\n",
        "        chunks.append(m)\n",
        "\n",
        "    feats = torch.stack(chunks, dim=0).permute(2, 1, 0).reshape(K, H * num_bins).contiguous()\n",
        "    return feats / feats.norm(dim=-1, keepdim=True).clamp_min(eps)\n",
        "\n",
        "def feat_inertia(read_weights_bhtk: torch.Tensor, max_lag: int = 32, per_head: bool = True, eps: float = 1e-8) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Autocorr signature of routing weights vs lag.\n",
        "    If per_head: feature is [K, H*L] else [K, L].\n",
        "    \"\"\"\n",
        "    B, H, T, K = read_weights_bhtk.shape\n",
        "    L = min(int(max_lag), T - 1)\n",
        "    p = read_weights_bhtk.float()\n",
        "\n",
        "    if per_head:\n",
        "        s = p.mean(dim=0)  # [H,T,K]\n",
        "        feats = torch.zeros((K, H * L), device=p.device)\n",
        "        for k in range(K):\n",
        "            rows = []\n",
        "            for h in range(H):\n",
        "                v = s[h, :, k]  # [T]\n",
        "                sims = []\n",
        "                for lag in range(1, L + 1):\n",
        "                    a = v[:-lag]\n",
        "                    b = v[lag:]\n",
        "                    sim = (a @ b) / (a.norm() * b.norm().clamp_min(eps))\n",
        "                    sims.append(sim)\n",
        "                rows.append(torch.stack(sims))\n",
        "            feats[k] = torch.cat(rows)\n",
        "    else:\n",
        "        s = p.mean(dim=(0, 1))  # [T,K]\n",
        "        feats = torch.zeros((K, L), device=p.device)\n",
        "        for k in range(K):\n",
        "            v = s[:, k]\n",
        "            sims = []\n",
        "            for lag in range(1, L + 1):\n",
        "                a = v[:-lag]\n",
        "                b = v[lag:]\n",
        "                sim = (a @ b) / (a.norm() * b.norm().clamp_min(eps))\n",
        "                sims.append(sim)\n",
        "            feats[k] = torch.stack(sims)\n",
        "\n",
        "    return feats / feats.norm(dim=-1, keepdim=True).clamp_min(eps)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Experiment runner\n",
        "# ----------------------------\n",
        "\n",
        "@torch.no_grad()\n",
        "def compute_adjacent_layer_alignment(\n",
        "    infos,\n",
        "    *,\n",
        "    num_bins: int = 8,\n",
        "    max_lag: int = 32,\n",
        "    per_head_inertia: bool = True,\n",
        "):\n",
        "    rows = []\n",
        "    for l in range(len(infos) - 1):\n",
        "        rwA = infos[l].get(\"read_weights\", None) if infos[l] is not None else None\n",
        "        rwB = infos[l + 1].get(\"read_weights\", None) if infos[l + 1] is not None else None\n",
        "        if rwA is None or rwB is None:\n",
        "            continue\n",
        "\n",
        "        A_basic = feat_basic_role_mass(rwA)\n",
        "        B_basic = feat_basic_role_mass(rwB)\n",
        "        d, b, _ = hungarian_match_cos(A_basic, B_basic)\n",
        "        rows.append({\"method\": \"basic\", \"layer\": l, \"diag\": d, \"best\": b, \"improvement\": b - d})\n",
        "\n",
        "        A_tb = feat_timebins(rwA, num_bins=num_bins)\n",
        "        B_tb = feat_timebins(rwB, num_bins=num_bins)\n",
        "        d, b, _ = hungarian_match_cos(A_tb, B_tb)\n",
        "        rows.append({\"method\": \"timebins\", \"layer\": l, \"diag\": d, \"best\": b, \"improvement\": b - d})\n",
        "\n",
        "        A_in = feat_inertia(rwA, max_lag=max_lag, per_head=per_head_inertia)\n",
        "        B_in = feat_inertia(rwB, max_lag=max_lag, per_head=per_head_inertia)\n",
        "        d, b, _ = hungarian_match_cos(A_in, B_in)\n",
        "        rows.append({\"method\": \"inertia\", \"layer\": l, \"diag\": d, \"best\": b, \"improvement\": b - d})\n",
        "\n",
        "        A_c = torch.cat([A_tb, A_in], dim=-1)\n",
        "        B_c = torch.cat([B_tb, B_in], dim=-1)\n",
        "        A_c = A_c / A_c.norm(dim=-1, keepdim=True).clamp_min(1e-8)\n",
        "        B_c = B_c / B_c.norm(dim=-1, keepdim=True).clamp_min(1e-8)\n",
        "        d, b, _ = hungarian_match_cos(A_c, B_c)\n",
        "        rows.append({\"method\": \"combined\", \"layer\": l, \"diag\": d, \"best\": b, \"improvement\": b - d})\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    if not df.empty:\n",
        "        df[\"transition\"] = df[\"layer\"].astype(str) + \"\" + (df[\"layer\"] + 1).astype(str)\n",
        "        df = df[[\"method\", \"layer\", \"transition\", \"diag\", \"best\", \"improvement\"]]\n",
        "    return df\n",
        "\n",
        "def summarize_alignment_df(df: pd.DataFrame):\n",
        "    if df.empty:\n",
        "        print(\"No alignment rows computed.\")\n",
        "        return\n",
        "    g = df.groupby(\"method\")[[\"best\", \"improvement\"]].mean().sort_values(\"best\", ascending=False)\n",
        "    print(\"Average (over adjacent layer transitions)\")\n",
        "    print(g.to_string(float_format=lambda x: f\"{x:.4f}\"))\n",
        "\n",
        "def plot_alignment_suite(df: pd.DataFrame, *, title=\"Adjacent-layer slot alignment\", figsize=(14, 9)):\n",
        "    if df.empty:\n",
        "        print(\"No data to plot.\")\n",
        "        return\n",
        "\n",
        "    methods = [\"basic\", \"timebins\", \"inertia\", \"combined\"]\n",
        "\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    gs = fig.add_gridspec(2, 2, hspace=0.35, wspace=0.25)\n",
        "\n",
        "    ax1 = fig.add_subplot(gs[0, 0])  # best vs layer\n",
        "    ax2 = fig.add_subplot(gs[0, 1])  # improvement vs layer\n",
        "    ax3 = fig.add_subplot(gs[1, 0])  # distribution of best by method\n",
        "    ax4 = fig.add_subplot(gs[1, 1])  # scatter: diag vs best\n",
        "\n",
        "    for ax in (ax1, ax2, ax3, ax4):\n",
        "        ax.grid(True, alpha=0.2)\n",
        "\n",
        "    # best cosine across depth\n",
        "    for method in methods:\n",
        "        sub = df[df[\"method\"] == method].sort_values(\"layer\")\n",
        "        if sub.empty:\n",
        "            continue\n",
        "        ax1.plot(sub[\"layer\"].values, sub[\"best\"].values, marker=\"o\", linewidth=2, label=method)\n",
        "    ax1.set_title(\"Alignment quality across depth\")\n",
        "    ax1.set_xlabel(\"Layer transition index (l  l+1)\")\n",
        "    ax1.set_ylabel(\"Mean cosine (optimal matching)\")\n",
        "    ax1.set_ylim(0, 1.05)\n",
        "    ax1.legend(title=\"Feature space\", frameon=True)\n",
        "\n",
        "    # improvement across depth\n",
        "    for method in methods:\n",
        "        sub = df[df[\"method\"] == method].sort_values(\"layer\")\n",
        "        if sub.empty:\n",
        "            continue\n",
        "        ax2.plot(sub[\"layer\"].values, sub[\"improvement\"].values, marker=\"o\", linewidth=2, label=method)\n",
        "    ax2.set_title(\"Gain from remapping across depth\")\n",
        "    ax2.set_xlabel(\"Layer transition index (l  l+1)\")\n",
        "    ax2.set_ylabel(\"best  diagonal\")\n",
        "    ax2.axhline(0, linewidth=1)\n",
        "    ax2.legend(title=\"Feature space\", frameon=True)\n",
        "\n",
        "    # distribution of best across transitions\n",
        "    best_lists, labels = [], []\n",
        "    for method in methods:\n",
        "        vals = df.loc[df[\"method\"] == method, \"best\"].values\n",
        "        if len(vals) == 0:\n",
        "            continue\n",
        "        best_lists.append(vals)\n",
        "        labels.append(method)\n",
        "    ax3.boxplot(best_lists, labels=labels, showmeans=True)\n",
        "    ax3.set_title(\"Distribution of alignment quality\")\n",
        "    ax3.set_ylabel(\"Mean cosine (optimal matching)\")\n",
        "    ax3.set_ylim(0, 1.05)\n",
        "\n",
        "    # diag vs best scatter\n",
        "    for method in methods:\n",
        "        sub = df[df[\"method\"] == method]\n",
        "        if sub.empty:\n",
        "            continue\n",
        "        ax4.scatter(sub[\"diag\"].values, sub[\"best\"].values, s=40, label=method, alpha=0.85)\n",
        "\n",
        "    lo = float(df[\"diag\"].min())\n",
        "    hi = float(df[\"best\"].max())\n",
        "    ax4.plot([lo, hi], [lo, hi], linewidth=1)  # y=x\n",
        "\n",
        "    ax4.set_title(\"Diagonal vs optimal matching\")\n",
        "    ax4.set_xlabel(\"Mean cosine (diagonal / identity)\")\n",
        "    ax4.set_ylabel(\"Mean cosine (optimal matching)\")\n",
        "    ax4.set_xlim(max(0, lo - 0.02), min(1.05, hi + 0.02))\n",
        "    ax4.set_ylim(max(0, lo - 0.02), 1.05)\n",
        "    ax4.legend(title=\"Feature space\", frameon=True)\n",
        "\n",
        "    fig.suptitle(title, y=1.02)\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Run (directly uses model(...) new args)\n",
        "# ----------------------------\n",
        "\n",
        "NUM_BINS = 8\n",
        "MAX_LAG = 32\n",
        "PER_HEAD_INERTIA = True\n",
        "\n",
        "# Recompute infos with the new ASA capture controls\n",
        "with torch.no_grad():\n",
        "    logits, infos = model(\n",
        "        xb[:4],\n",
        "        return_info=True,\n",
        "        attention_mask=None,\n",
        "        info_level=\"basic\",          # \"basic\" is sufficient here\n",
        "        info_cfg=ASA_ALIGN_INFO_CFG, # only read_weights\n",
        "    )\n",
        "\n",
        "df_align = compute_adjacent_layer_alignment(\n",
        "    infos,\n",
        "    num_bins=NUM_BINS,\n",
        "    max_lag=MAX_LAG,\n",
        "    per_head_inertia=PER_HEAD_INERTIA,\n",
        ")\n",
        "\n",
        "display(df_align)\n",
        "\n",
        "print()\n",
        "summarize_alignment_df(df_align)\n",
        "\n",
        "print()\n",
        "plot_alignment_suite(\n",
        "    df_align,\n",
        "    title=f\"Adjacent-layer slot alignment (bins={NUM_BINS}, lag{MAX_LAG})\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7qMLUsvJ21-a"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Policy Inertia: curves + uncertainty + half-life summary\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ----------------------------\n",
        "# Capture policy (new ASA plumbing)\n",
        "# ----------------------------\n",
        "ASA_INERTIA_INFO_CFG = dict(\n",
        "    store_read_weights=True,\n",
        "    store_read_logits=False,\n",
        "    store_write_logits=False,\n",
        "    store_slot_state_norm=False,\n",
        "    detach_to_cpu=False,   # keep on GPU for speed; df/plots go to cpu anyway\n",
        "    time_stride=1,\n",
        "    batch_stride=1,\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# Core: inertia curve\n",
        "# ----------------------------\n",
        "@torch.no_grad()\n",
        "def routing_inertia_curve(\n",
        "    t2a: torch.Tensor,\n",
        "    max_lag: int = 64,\n",
        "    *,\n",
        "    reduce: str = \"mean\",          # \"mean\" | \"per_head\" | \"none\"\n",
        "    eps: float = 1e-8,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    t2a: [B,H,T,K] probabilities (or weights)\n",
        "    returns:\n",
        "      - reduce==\"mean\":     [L]     mean over (B,H,T')\n",
        "      - reduce==\"per_head\": [H,L]   mean over (B,T')\n",
        "      - reduce==\"none\":     [B,H,L] mean over (T')\n",
        "    where L = min(max_lag, T-1) and T' = T-L\n",
        "    \"\"\"\n",
        "    if t2a is None:\n",
        "        raise ValueError(\"routing_inertia_curve: got t2a=None\")\n",
        "    if t2a.dim() != 4:\n",
        "        raise ValueError(f\"routing_inertia_curve expects [B,H,T,K], got shape={tuple(t2a.shape)}\")\n",
        "\n",
        "    B, H, T, K = t2a.shape\n",
        "    if T < 2:\n",
        "        if reduce == \"mean\":\n",
        "            return torch.empty((0,), device=t2a.device, dtype=torch.float32)\n",
        "        if reduce == \"per_head\":\n",
        "            return torch.empty((H, 0), device=t2a.device, dtype=torch.float32)\n",
        "        if reduce == \"none\":\n",
        "            return torch.empty((B, H, 0), device=t2a.device, dtype=torch.float32)\n",
        "        raise ValueError(f\"Unknown reduce={reduce}\")\n",
        "\n",
        "    L = int(min(max_lag, T - 1))\n",
        "    p = t2a.float()\n",
        "\n",
        "    # normalize per (B,H,T) vector over K\n",
        "    p = p / (p.norm(dim=-1, keepdim=True).clamp_min(eps))\n",
        "\n",
        "    # windows: [B,H, T-L, K, L+1] -> [B,H, T-L, L+1, K]\n",
        "    windows = p.unfold(dimension=2, size=L + 1, step=1).permute(0, 1, 2, 4, 3).contiguous()\n",
        "\n",
        "    # anchor t vs t+\n",
        "    a = windows[:, :, :, :1, :]      # [B,H,T-L,1,K]\n",
        "    b = windows[:, :, :, 1:, :]      # [B,H,T-L,L,K]\n",
        "    sim = (a * b).sum(dim=-1)        # [B,H,T-L,L]\n",
        "\n",
        "    if reduce == \"mean\":\n",
        "        return sim.mean(dim=(0, 1, 2))      # [L]\n",
        "    elif reduce == \"per_head\":\n",
        "        return sim.mean(dim=(0, 2))         # [H,L]\n",
        "    elif reduce == \"none\":\n",
        "        return sim.mean(dim=2)              # [B,H,L]\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown reduce={reduce}\")\n",
        "\n",
        "\n",
        "def inertia_half_life(curve_L: torch.Tensor, *, threshold: float = 0.90) -> float:\n",
        "    \"\"\"\n",
        "    curve_L: [L] mean cosine at lags 1..L\n",
        "    returns the smallest  where curve() <= threshold (linear interp), else L+.\n",
        "    \"\"\"\n",
        "    y = curve_L.detach().float().cpu().numpy()\n",
        "    if y.size == 0:\n",
        "        return float(\"nan\")\n",
        "    # Find first index where below threshold\n",
        "    idx = np.where(y <= threshold)[0]\n",
        "    if len(idx) == 0:\n",
        "        return float(len(y) + 1)  # \"beyond range\"\n",
        "    i = int(idx[0])\n",
        "    if i == 0:\n",
        "        return 1.0\n",
        "    # linear interpolation between (i, y[i]) and (i-1, y[i-1])\n",
        "    y0, y1 = y[i - 1], y[i]\n",
        "    if abs(y1 - y0) < 1e-12:\n",
        "        return float(i + 1)\n",
        "    frac = (threshold - y0) / (y1 - y0)\n",
        "    # lag values are 1..L, and i corresponds to lag=i+1\n",
        "    return float((i) + 1 + frac)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Run model with new plumbing\n",
        "# ----------------------------\n",
        "MAX_LAG = 64\n",
        "BATCH_FOR_PAPER = 8  # bump to 16/32 if you want smoother SEM; still cheap on A100\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits, infos = model(\n",
        "        xb[:BATCH_FOR_PAPER],\n",
        "        return_info=True,\n",
        "        info_level=\"basic\",\n",
        "        info_cfg=ASA_INERTIA_INFO_CFG,\n",
        "    )\n",
        "\n",
        "# ----------------------------\n",
        "# Compute curves + uncertainty + half-life\n",
        "# ----------------------------\n",
        "curves_mean = {}\n",
        "curves_sem  = {}\n",
        "rows = []\n",
        "\n",
        "for li, info in enumerate(infos):\n",
        "    rw = None if (info is None) else info.get(\"read_weights\", None)\n",
        "    if rw is None:\n",
        "        continue\n",
        "\n",
        "    # mean curve [L]\n",
        "    mean_L = routing_inertia_curve(rw, max_lag=MAX_LAG, reduce=\"mean\")  # [L]\n",
        "\n",
        "    # per (B,H) curve [B,H,L] -> meanSEM over BH for uncertainty bands\n",
        "    bh_L = routing_inertia_curve(rw, max_lag=MAX_LAG, reduce=\"none\")    # [B,H,L]\n",
        "    bh = bh_L.reshape(-1, bh_L.shape[-1]).detach().float().cpu().numpy() # [BH,L]\n",
        "\n",
        "    mu = bh.mean(axis=0)\n",
        "    sem = bh.std(axis=0, ddof=1) / max(1.0, np.sqrt(bh.shape[0]))\n",
        "\n",
        "    curves_mean[li] = mean_L.detach().float().cpu()\n",
        "    curves_sem[li]  = torch.from_numpy(sem).float()\n",
        "\n",
        "    # Half-lives at a couple thresholds (paper-friendly)\n",
        "    hl_90 = inertia_half_life(curves_mean[li], threshold=0.90)\n",
        "    hl_80 = inertia_half_life(curves_mean[li], threshold=0.80)\n",
        "\n",
        "    # Fixed-lag summaries (helps interpret how inertial?)\n",
        "    def at_lag(lag: int) -> float:\n",
        "        if lag <= 0: return float(\"nan\")\n",
        "        if lag > curves_mean[li].numel(): return float(\"nan\")\n",
        "        return float(curves_mean[li][lag - 1])\n",
        "\n",
        "    rows.append(dict(\n",
        "        layer=li,\n",
        "        sim_d1=at_lag(1),\n",
        "        sim_d8=at_lag(8),\n",
        "        sim_d32=at_lag(32),\n",
        "        half_life_0p90=hl_90,\n",
        "        half_life_0p80=hl_80,\n",
        "        L=int(curves_mean[li].numel()),\n",
        "        BH=int(bh.shape[0]),\n",
        "    ))\n",
        "\n",
        "df_inertia = pd.DataFrame(rows).sort_values(\"layer\")\n",
        "display(df_inertia)\n",
        "\n",
        "# ----------------------------\n",
        "# Plots (paper-facing)\n",
        "# ----------------------------\n",
        "# Choose a small set of layers to avoid a spaghetti plot.\n",
        "def pick_layers(n_layers: int, picks=(0, 1, 2, -1)):\n",
        "    out = []\n",
        "    for p in picks:\n",
        "        idx = (n_layers + p) if p < 0 else p\n",
        "        if 0 <= idx < n_layers:\n",
        "            out.append(idx)\n",
        "    # de-dup while preserving order\n",
        "    seen = set()\n",
        "    out2 = []\n",
        "    for x in out:\n",
        "        if x not in seen:\n",
        "            out2.append(x)\n",
        "            seen.add(x)\n",
        "    return out2\n",
        "\n",
        "layer_list = sorted(list(curves_mean.keys()))\n",
        "show_layers = pick_layers(len(layer_list), picks=(0, 1, 2, -1))\n",
        "show_layers = [layer_list[i] for i in show_layers] if len(layer_list) > 0 else []\n",
        "\n",
        "# 1) Inertia curves with uncertainty bands\n",
        "plt.figure(figsize=(8, 4.5))\n",
        "for li in show_layers:\n",
        "    y = curves_mean[li].numpy()\n",
        "    s = curves_sem[li].numpy()\n",
        "    x = np.arange(1, len(y) + 1)\n",
        "    plt.plot(x, y, linewidth=2, label=f\"layer {li}\")\n",
        "    plt.fill_between(x, y - s, y + s, alpha=0.15)\n",
        "\n",
        "plt.axhline(0.90, linewidth=1, linestyle=\"--\")\n",
        "plt.axhline(0.80, linewidth=1, linestyle=\"--\")\n",
        "plt.xlabel(\"Lag \")\n",
        "plt.ylabel(\"Mean cosine(p_t, p_{t+})\")\n",
        "plt.title(f\"Policy inertia curves (mean  SEM), max_lag={MAX_LAG}\")\n",
        "plt.legend(title=\"Layer\", frameon=True)\n",
        "plt.grid(True, alpha=0.25)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2) Half-life vs depth (two thresholds)\n",
        "if not df_inertia.empty:\n",
        "    plt.figure(figsize=(7.5, 4))\n",
        "    plt.plot(df_inertia[\"layer\"].values, df_inertia[\"half_life_0p90\"].values, marker=\"o\", linewidth=2, label=\"threshold 0.90\")\n",
        "    plt.plot(df_inertia[\"layer\"].values, df_inertia[\"half_life_0p80\"].values, marker=\"o\", linewidth=2, label=\"threshold 0.80\")\n",
        "    plt.xlabel(\"Layer\")\n",
        "    plt.ylabel(\"Lag  at threshold crossing\")\n",
        "    plt.title(\"Routing inertia half-life vs depth\")\n",
        "    plt.legend(frameon=True)\n",
        "    plt.grid(True, alpha=0.25)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 3) Optional: similarity at fixed lags across depth (quick how inertial? view)\n",
        "if not df_inertia.empty:\n",
        "    plt.figure(figsize=(7.5, 4))\n",
        "    plt.plot(df_inertia[\"layer\"].values, df_inertia[\"sim_d1\"].values, marker=\"o\", linewidth=2, label=\"=1\")\n",
        "    plt.plot(df_inertia[\"layer\"].values, df_inertia[\"sim_d8\"].values, marker=\"o\", linewidth=2, label=\"=8\")\n",
        "    plt.plot(df_inertia[\"layer\"].values, df_inertia[\"sim_d32\"].values, marker=\"o\", linewidth=2, label=\"=32\")\n",
        "    plt.xlabel(\"Layer\")\n",
        "    plt.ylabel(\"Mean cosine similarity\")\n",
        "    plt.title(\"Routing similarity at fixed lags vs depth\")\n",
        "    plt.ylim(0, 1.05)\n",
        "    plt.legend(frameon=True)\n",
        "    plt.grid(True, alpha=0.25)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "c7z-rEMwstZ8"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Slot Write Half-Life: capture + timescales + ESS + plots (FIXED for new ASA info_level)\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ----------------------------\n",
        "# Capture policy (new ASA plumbing)\n",
        "# ----------------------------\n",
        "# NOTE: With your ASA class, write logits are only stored when info_level=\"full\".\n",
        "ASA_WRITE_INFO_CFG = dict(\n",
        "    store_read_weights=False,\n",
        "    store_read_logits=False,\n",
        "    store_write_logits=True,     # <-- required\n",
        "    store_slot_state_norm=False,\n",
        "    detach_to_cpu=False,         # keep on GPU for speed; derived values go to CPU\n",
        "    time_stride=1,               # you can set >1 for cheaper capture\n",
        "    batch_stride=1,\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# Prefix stats from write logits\n",
        "# ----------------------------\n",
        "@torch.no_grad()\n",
        "def _prefix_tail_halflife_and_ess_at_t(\n",
        "    logits_bhkt: torch.Tensor,     # [B,H,K,T]\n",
        "    t_points: torch.Tensor,        # [P] int\n",
        "    *,\n",
        "    threshold: float = 0.5,\n",
        "    eps: float = 1e-12,\n",
        "):\n",
        "    \"\"\"\n",
        "    For each requested t in t_points, define p_i  exp(logits[..., i]) over i<=t (prefix softmax).\n",
        "\n",
        "    Returns:\n",
        "      hl  : [B,H,K,P] int32   (smallest h s.t. sum_{i=t-h+1..t} p_i >= threshold)\n",
        "      ess : [B,H,K,P] float32 (ESS = 1 / sum_i p_i^2 over i<=t)\n",
        "      t_points_used: [P] cpu long\n",
        "    \"\"\"\n",
        "    if logits_bhkt is None or logits_bhkt.dim() != 4:\n",
        "        raise ValueError(f\"Expected logits [B,H,K,T], got {None if logits_bhkt is None else tuple(logits_bhkt.shape)}\")\n",
        "\n",
        "    B, H, K, T = logits_bhkt.shape\n",
        "    device = logits_bhkt.device\n",
        "\n",
        "    t_points = t_points.to(device=device, dtype=torch.long)\n",
        "    t_points = torch.unique(t_points.clamp(0, T - 1))\n",
        "    P = int(t_points.numel())\n",
        "    thr = float(threshold)\n",
        "\n",
        "    hl_out  = torch.empty((B, H, K, P), device=device, dtype=torch.int32)\n",
        "    ess_out = torch.empty((B, H, K, P), device=device, dtype=torch.float32)\n",
        "\n",
        "    for pi, t in enumerate(t_points.tolist()):\n",
        "        L = int(t) + 1\n",
        "        x = logits_bhkt[..., :L].float()                 # [B,H,K,L]\n",
        "        m = x.max(dim=-1, keepdim=True).values           # [B,H,K,1]\n",
        "        z = torch.exp(x - m)                             # [B,H,K,L]\n",
        "        Z = z.sum(dim=-1).clamp_min(eps)                 # [B,H,K]\n",
        "        p = z / Z.unsqueeze(-1)                          # [B,H,K,L]\n",
        "\n",
        "        ess_out[..., pi] = 1.0 / p.square().sum(dim=-1).clamp_min(eps)\n",
        "\n",
        "        tail = torch.flip(p, dims=[-1]).cumsum(dim=-1)   # [B,H,K,L]\n",
        "        crossed = (tail >= thr)\n",
        "        first = crossed.float().argmax(dim=-1)           # [B,H,K]\n",
        "        never = ~crossed.any(dim=-1)\n",
        "        first = torch.where(never, torch.full_like(first, L - 1), first)\n",
        "        hl_out[..., pi] = (first + 1).to(torch.int32)\n",
        "\n",
        "    return hl_out, ess_out, t_points.detach().cpu()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def slot_write_timescale_analysis(\n",
        "    model,\n",
        "    xb: torch.Tensor,\n",
        "    attention_mask=None,\n",
        "    *,\n",
        "    threshold: float = 0.5,\n",
        "    n_timepoints: int = 32,\n",
        "    min_t: int = 32,\n",
        "    max_t: int | None = None,\n",
        "    per_head: bool = True,\n",
        "    max_sample_per_layer: int = 20000,  # keeps pooled plots light\n",
        "):\n",
        "    \"\"\"\n",
        "    Computes per-layer write tail half-life + ESS using ASA write logits.\n",
        "\n",
        "    IMPORTANT for your ASA:\n",
        "      - must call model(..., return_info=True, info_level=\"full\", info_cfg with store_write_logits=True)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    B, T = xb.shape\n",
        "    if T < 2:\n",
        "        raise ValueError(\"Need T>=2\")\n",
        "\n",
        "    if max_t is None:\n",
        "        max_t = T - 1\n",
        "    max_t = min(int(max_t), T - 1)\n",
        "    min_t = min(int(min_t), max_t)\n",
        "\n",
        "    tp = torch.linspace(min_t, max_t, steps=int(n_timepoints), device=xb.device).round().long()\n",
        "    tp = torch.unique(tp)\n",
        "\n",
        "    # --- CRITICAL FIX: info_level must be \"full\" for write_logits to be stored in your ASA.\n",
        "    logits, infos = model(\n",
        "        xb,\n",
        "        attention_mask=attention_mask,\n",
        "        return_info=True,\n",
        "        info_level=\"full\",\n",
        "        info_cfg=ASA_WRITE_INFO_CFG,\n",
        "    )\n",
        "\n",
        "    out = dict(\n",
        "        threshold=float(threshold),\n",
        "        t_points=tp.detach().cpu().tolist(),\n",
        "        per_layer=[],\n",
        "    )\n",
        "\n",
        "    # quick contract sanity\n",
        "    if not infos:\n",
        "        print(\" No infos returned by model.\")\n",
        "        return out\n",
        "    if infos[0] is not None:\n",
        "        k0 = sorted(list(infos[0].keys()))\n",
        "        if (\"write_logits\" not in k0) and (\"write_logits_raw\" not in k0):\n",
        "            print(\" info dict has no write logits keys. Keys:\", k0)\n",
        "\n",
        "    for layer, info in enumerate(infos):\n",
        "        if info is None:\n",
        "            continue\n",
        "\n",
        "        wl = info.get(\"write_logits\", None)\n",
        "        if wl is None:\n",
        "            # Some runs may only store raw; accept that too.\n",
        "            wl = info.get(\"write_logits_raw\", None)\n",
        "        if wl is None:\n",
        "            continue\n",
        "\n",
        "        if wl.dim() != 4:\n",
        "            print(f\" layer {layer}: write_logits shape {tuple(wl.shape)} (expected [B,H,K,T])\")\n",
        "            continue\n",
        "        if wl.shape[-1] != T:\n",
        "            print(f\" layer {layer}: write_logits T={wl.shape[-1]} != xb T={T} (skipping)\")\n",
        "            continue\n",
        "\n",
        "        hl, ess, _ = _prefix_tail_halflife_and_ess_at_t(wl, tp, threshold=threshold)\n",
        "        hl_f = hl.float()\n",
        "\n",
        "        rec = dict(\n",
        "            layer=int(layer),\n",
        "            B=int(wl.shape[0]),\n",
        "            H=int(wl.shape[1]),\n",
        "            K=int(wl.shape[2]),\n",
        "            T=int(wl.shape[3]),\n",
        "            P=int(hl.shape[-1]),\n",
        "            mean_tail_half_life=float(hl_f.mean().detach().cpu()),\n",
        "            median_tail_half_life=float(hl_f.median().detach().cpu()),\n",
        "            mean_ess=float(ess.mean().detach().cpu()),\n",
        "            median_ess=float(ess.median().detach().cpu()),\n",
        "        )\n",
        "\n",
        "        # pooled samples for distribution plots (true pooled values, not headslot means)\n",
        "        flat = hl_f.reshape(-1).detach().cpu().numpy()\n",
        "        if flat.size > max_sample_per_layer:\n",
        "            idx = np.random.choice(flat.size, size=max_sample_per_layer, replace=False)\n",
        "            flat = flat[idx]\n",
        "        rec[\"half_life_sample\"] = flat\n",
        "\n",
        "        if per_head:\n",
        "            rec[\"per_head_mean_tail_half_life\"] = hl_f.mean(dim=(0, 2, 3)).detach().cpu().numpy()  # [H]\n",
        "            rec[\"per_head_mean_ess\"] = ess.mean(dim=(0, 2, 3)).detach().cpu().numpy()              # [H]\n",
        "            rec[\"head_slot_half_life\"] = hl_f.mean(dim=(0, 3)).detach().cpu().numpy()               # [H,K]\n",
        "            rec[\"head_slot_ess\"] = ess.mean(dim=(0, 3)).detach().cpu().numpy()                      # [H,K]\n",
        "\n",
        "        out[\"per_layer\"].append(rec)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# DataFrame + plots\n",
        "# ----------------------------\n",
        "def _as_df(res):\n",
        "    rows = []\n",
        "    for r in res.get(\"per_layer\", []):\n",
        "        rows.append(dict(\n",
        "            layer=r[\"layer\"],\n",
        "            mean_tail_half_life=r[\"mean_tail_half_life\"],\n",
        "            median_tail_half_life=r[\"median_tail_half_life\"],\n",
        "            mean_ess=r[\"mean_ess\"],\n",
        "            median_ess=r[\"median_ess\"],\n",
        "        ))\n",
        "    df = pd.DataFrame(rows)\n",
        "    if df.empty:\n",
        "        return pd.DataFrame(columns=[\"layer\",\"mean_tail_half_life\",\"median_tail_half_life\",\"mean_ess\",\"median_ess\"])\n",
        "    return df.sort_values(\"layer\")\n",
        "\n",
        "\n",
        "def plot_write_timescale_paper(res, figsize=(13, 4.5)):\n",
        "    df = _as_df(res)\n",
        "    if df.empty:\n",
        "        print(\"No layers found with write logits (df empty).\")\n",
        "        return\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize, sharex=True)\n",
        "\n",
        "    ax1.plot(df[\"layer\"], df[\"mean_tail_half_life\"], marker=\"o\", linewidth=2, label=\"mean\")\n",
        "    ax1.plot(df[\"layer\"], df[\"median_tail_half_life\"], marker=\"s\", linewidth=2, linestyle=\"--\", alpha=0.85, label=\"median\")\n",
        "    ax1.set_title(\"Write tail half-life vs depth\")\n",
        "    ax1.set_xlabel(\"Layer\")\n",
        "    ax1.set_ylabel(f\"Tokens to cover {int(res['threshold']*100)}% mass (tail)\")\n",
        "    ax1.grid(True, alpha=0.25)\n",
        "    ax1.legend(frameon=True)\n",
        "\n",
        "    ax2.plot(df[\"layer\"], df[\"mean_ess\"], marker=\"o\", linewidth=2, label=\"mean\")\n",
        "    ax2.plot(df[\"layer\"], df[\"median_ess\"], marker=\"s\", linewidth=2, linestyle=\"--\", alpha=0.85, label=\"median\")\n",
        "    ax2.set_title(\"Write distribution ESS vs depth\")\n",
        "    ax2.set_xlabel(\"Layer\")\n",
        "    ax2.set_ylabel(\"ESS  1 /  p (prefix)\")\n",
        "    ax2.grid(True, alpha=0.25)\n",
        "    ax2.legend(frameon=True)\n",
        "\n",
        "    fig.suptitle(f\"Slot write timescales (threshold={res['threshold']}, P={len(res['t_points'])} timepoints)\", y=1.02)\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_tail_half_life_distribution(res, *, log_x=True, bins=45, figsize=(7.5, 4.5)):\n",
        "    samples = []\n",
        "    for r in res.get(\"per_layer\", []):\n",
        "        if \"half_life_sample\" in r:\n",
        "            samples.append(np.asarray(r[\"half_life_sample\"]))\n",
        "    if not samples:\n",
        "        print(\"No pooled samples available for half-life distribution.\")\n",
        "        return\n",
        "\n",
        "    vals = np.concatenate(samples)\n",
        "    vals = vals[np.isfinite(vals) & (vals > 0)]\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.hist(vals, bins=bins, density=True, alpha=0.75)\n",
        "    if log_x:\n",
        "        plt.xscale(\"log\")\n",
        "    plt.xlabel(\"Tail half-life (tokens)\")\n",
        "    plt.ylabel(\"Density\")\n",
        "    plt.title(\"Pooled distribution of write tail half-life\")\n",
        "    plt.grid(True, alpha=0.25)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\n",
        "        f\"Pooled tail half-life stats: \"\n",
        "        f\"mean={vals.mean():.1f}, median={np.median(vals):.1f}, \"\n",
        "        f\"p90={np.percentile(vals, 90):.1f}, p99={np.percentile(vals, 99):.1f}, \"\n",
        "        f\"n={vals.size}\"\n",
        "    )\n",
        "\n",
        "\n",
        "def plot_head_slot_heatmaps(res, *, which=\"half_life\", figsize=(13, 2.8)):\n",
        "    layers = res.get(\"per_layer\", [])\n",
        "    if not layers:\n",
        "        print(\"No data.\")\n",
        "        return\n",
        "\n",
        "    if which == \"half_life\":\n",
        "        k = \"head_slot_half_life\"\n",
        "        title = \"Mean write tail half-life (head  slot)\"\n",
        "    elif which == \"ess\":\n",
        "        k = \"head_slot_ess\"\n",
        "        title = \"Mean write ESS (head  slot)\"\n",
        "    else:\n",
        "        raise ValueError(\"which must be 'half_life' or 'ess'\")\n",
        "\n",
        "    if k not in layers[0]:\n",
        "        print(f\"Missing heatmap key: {k}\")\n",
        "        return\n",
        "\n",
        "    for r in layers:\n",
        "        mat = np.asarray(r[k])\n",
        "        plt.figure(figsize=figsize)\n",
        "        plt.imshow(mat, aspect=\"auto\", interpolation=\"nearest\")\n",
        "        plt.colorbar()\n",
        "        plt.xlabel(\"Slot\")\n",
        "        plt.ylabel(\"Head\")\n",
        "        plt.title(f\"Layer {r['layer']}  {title}\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Run\n",
        "# ----------------------------\n",
        "print(\"Computing slot write half-life + ESS...\\n\")\n",
        "\n",
        "BATCH_FOR_PAPER = 8\n",
        "res_write = slot_write_timescale_analysis(\n",
        "    model,\n",
        "    xb[:BATCH_FOR_PAPER],\n",
        "    attention_mask=None,\n",
        "    threshold=0.50,\n",
        "    n_timepoints=32,\n",
        "    min_t=32,\n",
        "    max_t=None,\n",
        "    per_head=True,\n",
        ")\n",
        "\n",
        "df_write = _as_df(res_write)\n",
        "display(df_write)\n",
        "\n",
        "if df_write.empty:\n",
        "    print(\"\\n Still no write logits captured.\")\n",
        "    print(\"Sanity checks:\")\n",
        "    print(\"  1) Are ASMBlock / ASMLanguageModel forwarding info_level and info_cfg into ASA?\")\n",
        "    print(\"  2) Does each layer's info dict contain write_logits/write_logits_raw keys at all?\")\n",
        "    if res_write.get(\"per_layer\") is None:\n",
        "        print(\"  (no per_layer records were produced)\")\n",
        "else:\n",
        "    print(\"\\nPaper plots...\\n\")\n",
        "    plot_write_timescale_paper(res_write)\n",
        "\n",
        "    print(\"\\nPooled distribution...\\n\")\n",
        "    plot_tail_half_life_distribution(res_write)\n",
        "\n",
        "    print(\"\\nAppendix: headslot half-life maps...\\n\")\n",
        "    plot_head_slot_heatmaps(res_write, which=\"half_life\")\n",
        "    plot_head_slot_heatmaps(res_write, which=\"ess\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DJlyQdtU-aG_"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Big-picture head stratification: quantile fans (+ optional stratification index)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def _stack_per_head(res, key):\n",
        "    layers = res.get(\"per_layer\", [])\n",
        "    if not layers:\n",
        "        raise ValueError(\"res has no per_layer records.\")\n",
        "    mats = []\n",
        "    layer_ids = []\n",
        "    for r in layers:\n",
        "        if key not in r:\n",
        "            raise KeyError(f\"Missing key '{key}' in per_layer records.\")\n",
        "        v = np.asarray(r[key], dtype=np.float64)  # [H]\n",
        "        mats.append(v)\n",
        "        layer_ids.append(int(r[\"layer\"]))\n",
        "    M = np.stack(mats, axis=0)  # [L,H]\n",
        "    return np.asarray(layer_ids, dtype=int), M\n",
        "\n",
        "def _quantiles(M_LH, qs=(0.10, 0.25, 0.50, 0.75, 0.90)):\n",
        "    # returns dict q -> [L]\n",
        "    out = {}\n",
        "    for q in qs:\n",
        "        out[q] = np.quantile(M_LH, q, axis=1)\n",
        "    return out\n",
        "\n",
        "def _strat_index(M_LH, eps=1e-12):\n",
        "    \"\"\"\n",
        "    Simple, interpretable stratification score per layer:\n",
        "      log10(p90/p10). Larger = more stratified across heads.\n",
        "    Works for positive quantities (ESS, half-life).\n",
        "    \"\"\"\n",
        "    p10 = np.quantile(M_LH, 0.10, axis=1)\n",
        "    p90 = np.quantile(M_LH, 0.90, axis=1)\n",
        "    return np.log10((p90 + eps) / (p10 + eps))\n",
        "\n",
        "def plot_quantile_fan(layer_ids, M_LH, *, title, ylabel, logy=True, show_index=True, figsize=(8.8, 4.8)):\n",
        "    Q = _quantiles(M_LH, qs=(0.10, 0.25, 0.50, 0.75, 0.90))\n",
        "    med = Q[0.50]\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
        "    ax.grid(True, alpha=0.25)\n",
        "\n",
        "    # Bands: 1090 (light) and 2575 (darker)\n",
        "    ax.fill_between(layer_ids, Q[0.10], Q[0.90], alpha=0.15, label=\"1090% across heads\")\n",
        "    ax.fill_between(layer_ids, Q[0.25], Q[0.75], alpha=0.25, label=\"2575% across heads\")\n",
        "    ax.plot(layer_ids, med, marker=\"o\", linewidth=2.2, label=\"median head\")\n",
        "\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(\"Layer\")\n",
        "    ax.set_ylabel(ylabel)\n",
        "    if logy:\n",
        "        ax.set_yscale(\"log\")\n",
        "\n",
        "    ax.legend(frameon=True, loc=\"best\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Optional: stratification index (single number per layer)\n",
        "    if show_index:\n",
        "        s = _strat_index(M_LH)\n",
        "        plt.figure(figsize=(7.6, 3.8))\n",
        "        plt.plot(layer_ids, s, marker=\"o\", linewidth=2.2)\n",
        "        plt.grid(True, alpha=0.25)\n",
        "        plt.xlabel(\"Layer\")\n",
        "        plt.ylabel(\"log10(p90/p10)\")\n",
        "        plt.title(f\"Stratification index vs depth (derived from: {title})\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# ---- build matrices from res_write ----\n",
        "layer_ids, HL = _stack_per_head(res_write, \"per_head_mean_tail_half_life\")  # [L,H]\n",
        "_, ESS = _stack_per_head(res_write, \"per_head_mean_ess\")                   # [L,H]\n",
        "\n",
        "# ---- Panel E: big-picture quantile fans ----\n",
        "plot_quantile_fan(\n",
        "    layer_ids, HL,\n",
        "    title=\"(E) Head population structure: write tail half-life distribution vs depth\",\n",
        "    ylabel=\"Tail half-life (tokens)\",\n",
        "    logy=True,\n",
        "    show_index=True,\n",
        ")\n",
        "\n",
        "plot_quantile_fan(\n",
        "    layer_ids, ESS,\n",
        "    title=\"(E2) Head population structure: write ESS distribution vs depth\",\n",
        "    ylabel=\"ESS (prefix)\",\n",
        "    logy=True,\n",
        "    show_index=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hw41O8tz_a4C"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Big-picture: head stratification across layers (rank stability panels)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def _stack_per_head(res, key):\n",
        "    layers = res.get(\"per_layer\", [])\n",
        "    if not layers:\n",
        "        raise ValueError(\"res has no per_layer records.\")\n",
        "    mats = []\n",
        "    layer_ids = []\n",
        "    for r in layers:\n",
        "        if key not in r:\n",
        "            raise KeyError(f\"Missing key '{key}' in per_layer records.\")\n",
        "        mats.append(np.asarray(r[key], dtype=np.float64))  # [H]\n",
        "        layer_ids.append(int(r[\"layer\"]))\n",
        "    M = np.stack(mats, axis=0)  # [L,H]\n",
        "    return np.asarray(layer_ids, dtype=int), M\n",
        "\n",
        "def _rankdata_ordinal(x):\n",
        "    \"\"\"\n",
        "    Ordinal ranks (1..H) with average for ties (rare here).\n",
        "    Higher values => rank 1 (so rank order corresponds to \"largest is best\").\n",
        "    \"\"\"\n",
        "    # sort descending\n",
        "    order = np.argsort(-x, kind=\"mergesort\")\n",
        "    ranks = np.empty_like(order, dtype=np.float64)\n",
        "    ranks[order] = np.arange(1, len(x) + 1, dtype=np.float64)\n",
        "\n",
        "    # tie handling (average ranks)\n",
        "    # if exact ties exist, average their ranks\n",
        "    xs = x[order]\n",
        "    i = 0\n",
        "    while i < len(xs):\n",
        "        j = i + 1\n",
        "        while j < len(xs) and xs[j] == xs[i]:\n",
        "            j += 1\n",
        "        if j - i > 1:\n",
        "            avg = ranks[order[i:j]].mean()\n",
        "            ranks[order[i:j]] = avg\n",
        "        i = j\n",
        "    return ranks\n",
        "\n",
        "def _spearman_corr(a, b, eps=1e-12):\n",
        "    # a,b are ranks (floats)\n",
        "    a = a - a.mean()\n",
        "    b = b - b.mean()\n",
        "    denom = (np.sqrt((a*a).sum()) * np.sqrt((b*b).sum())) + eps\n",
        "    return float((a*b).sum() / denom)\n",
        "\n",
        "def rank_correlation_matrix(M_LH):\n",
        "    \"\"\"\n",
        "    M_LH: [L,H] metric values per layer per head.\n",
        "    Returns:\n",
        "      R_LL: [L,L] Spearman correlation of head ranks between layers.\n",
        "      ranks_LH: [L,H] ranks per layer (1..H, 1 = highest metric).\n",
        "    \"\"\"\n",
        "    L, H = M_LH.shape\n",
        "    ranks = np.stack([_rankdata_ordinal(M_LH[i]) for i in range(L)], axis=0)  # [L,H]\n",
        "    R = np.zeros((L, L), dtype=np.float64)\n",
        "    for i in range(L):\n",
        "        for j in range(L):\n",
        "            R[i, j] = _spearman_corr(ranks[i], ranks[j])\n",
        "    return R, ranks\n",
        "\n",
        "def consensus_order_and_error(ranks_LH):\n",
        "    \"\"\"\n",
        "    ranks_LH: [L,H], 1 = highest.\n",
        "    Returns:\n",
        "      consensus_order: [H] head indices sorted by mean rank (ascending).\n",
        "      mean_abs_err_L: [L] mean absolute rank error per layer vs consensus ranks.\n",
        "    \"\"\"\n",
        "    mean_rank = ranks_LH.mean(axis=0)  # [H]\n",
        "    consensus_order = np.argsort(mean_rank, kind=\"mergesort\")  # best (low rank) first\n",
        "\n",
        "    # consensus ranks: map head -> rank position 1..H\n",
        "    consensus_rank = np.empty_like(consensus_order, dtype=np.float64)\n",
        "    consensus_rank[consensus_order] = np.arange(1, len(consensus_order)+1, dtype=np.float64)\n",
        "\n",
        "    mean_abs_err = np.mean(np.abs(ranks_LH - consensus_rank[None, :]), axis=1)  # [L]\n",
        "    return consensus_order, mean_abs_err, mean_rank\n",
        "\n",
        "def plot_rank_stability_panels(layer_ids, M_LH, *, metric_name, figsize=(13.5, 4.8)):\n",
        "    R, ranks = rank_correlation_matrix(M_LH)\n",
        "    consensus_order, mean_abs_err, mean_rank = consensus_order_and_error(ranks)\n",
        "\n",
        "    L, H = M_LH.shape\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
        "\n",
        "    # ---- Panel F: Spearman rank-correlation matrix ----\n",
        "    im = ax1.imshow(R, vmin=0.0, vmax=1.0, interpolation=\"nearest\", aspect=\"auto\")\n",
        "    ax1.set_title(f\"(F) Rank consistency across layers ({metric_name})\\nSpearman( rank(head, layer i), rank(head, layer j) )\")\n",
        "    ax1.set_xticks(range(L)); ax1.set_yticks(range(L))\n",
        "    ax1.set_xticklabels([f\"L{l}\" for l in layer_ids], rotation=45, ha=\"right\")\n",
        "    ax1.set_yticklabels([f\"L{l}\" for l in layer_ids])\n",
        "    cbar = plt.colorbar(im, ax=ax1, fraction=0.046, pad=0.04)\n",
        "    cbar.set_label(\"Spearman  (1 = identical ordering)\")\n",
        "\n",
        "    # ---- Panel G: agreement to consensus ordering ----\n",
        "    ax2.plot(layer_ids, mean_abs_err, marker=\"o\", linewidth=2.2)\n",
        "    ax2.grid(True, alpha=0.25)\n",
        "    ax2.set_title(f\"(G) Agreement with consensus head ordering ({metric_name})\")\n",
        "    ax2.set_xlabel(\"Layer\")\n",
        "    ax2.set_ylabel(\"Mean |rank_layer  rank_consensus|  (lower = more stable)\")\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print a compact consensus head order for the paper notebook text output\n",
        "    # (best heads first according to average rank)\n",
        "    print(f\"\\nConsensus head order by {metric_name} (best  worst):\")\n",
        "    print(\"  \" + \" \".join([f\"H{h}\" for h in consensus_order.tolist()]))\n",
        "\n",
        "    # Also show mean rank per head (optional, useful for debugging)\n",
        "    # print(\"Mean rank per head:\", {f\"H{i}\": float(mean_rank[i]) for i in range(H)})\n",
        "\n",
        "# ---- Build matrices from res_write ----\n",
        "layer_ids, HL  = _stack_per_head(res_write, \"per_head_mean_tail_half_life\")  # [L,H]\n",
        "_,        ESS = _stack_per_head(res_write, \"per_head_mean_ess\")              # [L,H]\n",
        "\n",
        "# ---- Run panels for both metrics ----\n",
        "plot_rank_stability_panels(layer_ids, HL,  metric_name=\"write tail half-life\")\n",
        "plot_rank_stability_panels(layer_ids, ESS, metric_name=\"write ESS\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CegT1cMX8KQs"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Build event_toks + ctrl_toks (paper-facing): model-driven events + matched controls\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# ==========================================================\n",
        "# Produces:\n",
        "#   event_toks: np.ndarray[int64]  token indices in [0, T)\n",
        "#   ctrl_toks : np.ndarray[int64]  matched control indices in [0, T)\n",
        "#\n",
        "# Requires:\n",
        "#   model, xb   (xb: [B,T] input ids)\n",
        "#\n",
        "# Options:\n",
        "#   event_mode=\"model\"  -> derive \"event\" tokens from ASA routing dynamics\n",
        "#   event_mode=\"given\"  -> use provided EVENT_TOKS_GIVEN list/array\n",
        "#\n",
        "# Control matching:\n",
        "#   - always matches position bins (prevents trivial \"late tokens differ\")\n",
        "#   - optionally matches token id distribution (stronger control)\n",
        "# ==========================================================\n",
        "\n",
        "# ----------------------------\n",
        "# Capture policy for event discovery (new ASA plumbing)\n",
        "# ----------------------------\n",
        "ASA_EVENT_INFO_CFG = dict(\n",
        "    store_read_weights=True,      # needed to compute routing dynamics\n",
        "    store_read_logits=False,\n",
        "    store_write_logits=False,\n",
        "    store_slot_state_norm=False,\n",
        "    detach_to_cpu=False,\n",
        "    time_stride=1,\n",
        "    batch_stride=1,\n",
        ")\n",
        "\n",
        "def _sanitize_unique(idxs, T):\n",
        "    idxs = np.asarray(idxs, dtype=np.int64)\n",
        "    idxs = idxs[(idxs >= 0) & (idxs < T)]\n",
        "    idxs = np.unique(idxs)\n",
        "    return idxs\n",
        "\n",
        "def _position_bins(T, n_bins):\n",
        "    # returns bin index for each t in [0..T-1]\n",
        "    edges = np.linspace(0, T, n_bins + 1).astype(np.int64)\n",
        "    # ensure monotonic\n",
        "    edges[0] = 0\n",
        "    edges[-1] = T\n",
        "    bins = np.zeros((T,), dtype=np.int64)\n",
        "    for b in range(n_bins):\n",
        "        t0, t1 = edges[b], edges[b + 1]\n",
        "        if t1 > t0:\n",
        "            bins[t0:t1] = b\n",
        "    return bins, n_bins\n",
        "\n",
        "@torch.no_grad()\n",
        "def _event_scores_from_read_weights(rw_bhtk: torch.Tensor, eps=1e-8):\n",
        "    \"\"\"\n",
        "    rw: [B,H,T,K]  (probabilities over slots)\n",
        "    Returns three per-token scores (length T), computed on mean-over-B,H routing:\n",
        "      - delta_l1: mean L1 change in routing between t and t-1\n",
        "      - delta_cos: 1 - cosine(r_t, r_{t-1})\n",
        "      - entropy: entropy(r_t)  (peaks when routing is diffuse/uncertain)\n",
        "    \"\"\"\n",
        "    # mean policy across batch+heads: [T,K]\n",
        "    p = rw_bhtk.float().mean(dim=(0, 1))\n",
        "    p = p / p.sum(dim=-1, keepdim=True).clamp_min(eps)\n",
        "\n",
        "    # deltas vs previous step (pad t=0 with 0)\n",
        "    p0 = p[:-1]\n",
        "    p1 = p[1:]\n",
        "\n",
        "    delta_l1 = (p1 - p0).abs().sum(dim=-1)  # [T-1]\n",
        "    delta_l1 = torch.cat([torch.zeros((1,), device=p.device), delta_l1], dim=0)\n",
        "\n",
        "    # cosine distance (routing normalized)\n",
        "    pn = p / p.norm(dim=-1, keepdim=True).clamp_min(eps)\n",
        "    cos = (pn[1:] * pn[:-1]).sum(dim=-1).clamp(-1, 1)\n",
        "    delta_cos = 1.0 - cos\n",
        "    delta_cos = torch.cat([torch.zeros((1,), device=p.device), delta_cos], dim=0)\n",
        "\n",
        "    # entropy\n",
        "    ent = -(p * torch.log(p.clamp_min(eps))).sum(dim=-1)\n",
        "\n",
        "    return delta_l1, delta_cos, ent\n",
        "\n",
        "def build_event_and_control_tokens(\n",
        "    model,\n",
        "    xb: torch.Tensor,\n",
        "    *,\n",
        "    event_mode: str = \"model\",          # \"model\" | \"given\"\n",
        "    EVENT_TOKS_GIVEN=None,              # used if event_mode=\"given\"\n",
        "    n_events: int = 256,                # target number of event tokens\n",
        "    min_t: int = 8,                     # ignore earliest positions (often special tokens)\n",
        "    max_t: int | None = None,\n",
        "    score: str = \"delta_cos\",           # \"delta_cos\" | \"delta_l1\" | \"entropy\"\n",
        "    position_bins: int = 16,            # for matched controls\n",
        "    match_token_id: bool = True,        # stronger control: match token ids too\n",
        "    seed: int = 1337,\n",
        "):\n",
        "    \"\"\"\n",
        "    Returns (event_toks, ctrl_toks).\n",
        "    \"\"\"\n",
        "    assert xb.dim() == 2, \"xb must be [B,T]\"\n",
        "    B, T = xb.shape\n",
        "    if max_t is None:\n",
        "        max_t = T - 1\n",
        "    max_t = min(int(max_t), T - 1)\n",
        "    min_t = min(int(min_t), max_t)\n",
        "\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    # ------------------------\n",
        "    # Build event_toks\n",
        "    # ------------------------\n",
        "    if event_mode == \"given\":\n",
        "        if EVENT_TOKS_GIVEN is None:\n",
        "            raise ValueError(\"event_mode='given' requires EVENT_TOKS_GIVEN\")\n",
        "        event_toks = _sanitize_unique(EVENT_TOKS_GIVEN, T)\n",
        "\n",
        "    elif event_mode == \"model\":\n",
        "        with torch.no_grad():\n",
        "            _, infos = model(\n",
        "                xb,\n",
        "                return_info=True,\n",
        "                info_level=\"basic\",\n",
        "                info_cfg=ASA_EVENT_INFO_CFG,\n",
        "            )\n",
        "\n",
        "        # stack per-layer scores, then take a robust aggregate (median across layers)\n",
        "        scores_by_layer = []\n",
        "        for info in infos:\n",
        "            rw = None if (info is None) else info.get(\"read_weights\", None)\n",
        "            if rw is None:\n",
        "                continue\n",
        "            d_l1, d_cos, ent = _event_scores_from_read_weights(rw)\n",
        "            if score == \"delta_l1\":\n",
        "                s = d_l1\n",
        "            elif score == \"delta_cos\":\n",
        "                s = d_cos\n",
        "            elif score == \"entropy\":\n",
        "                s = ent\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown score={score}\")\n",
        "            scores_by_layer.append(s.detach())\n",
        "\n",
        "        if not scores_by_layer:\n",
        "            raise RuntimeError(\"No read_weights captured for event discovery. Check ASA_EVENT_INFO_CFG / info_level.\")\n",
        "\n",
        "        S = torch.stack(scores_by_layer, dim=0)   # [L,T]\n",
        "        s_med = S.median(dim=0).values            # [T]\n",
        "\n",
        "        # restrict to range\n",
        "        mask = torch.zeros((T,), device=s_med.device, dtype=torch.bool)\n",
        "        mask[min_t:max_t+1] = True\n",
        "        s_med = s_med.masked_fill(~mask, float(\"-inf\"))\n",
        "\n",
        "        # pick top-n_events\n",
        "        n_events = int(min(n_events, (max_t - min_t + 1)))\n",
        "        top = torch.topk(s_med, k=n_events, dim=0).indices.detach().cpu().numpy()\n",
        "        event_toks = np.sort(np.unique(top.astype(np.int64)))\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"event_mode must be 'model' or 'given'\")\n",
        "\n",
        "    # apply min/max window\n",
        "    event_toks = event_toks[(event_toks >= min_t) & (event_toks <= max_t)]\n",
        "    if event_toks.size == 0:\n",
        "        raise RuntimeError(\"No event tokens after applying [min_t, max_t].\")\n",
        "\n",
        "    # ------------------------\n",
        "    # Build matched controls\n",
        "    # ------------------------\n",
        "    # Build position bins and match per-bin counts\n",
        "    bins, nb = _position_bins(T, position_bins)\n",
        "    event_bins = bins[event_toks]\n",
        "\n",
        "    # Candidate pool excludes event tokens\n",
        "    all_t = np.arange(T, dtype=np.int64)\n",
        "    is_event = np.zeros((T,), dtype=bool)\n",
        "    is_event[event_toks] = True\n",
        "\n",
        "    ctrl = []\n",
        "    # Optional token-id matching uses xb[0] token IDs as the \"position token id\".\n",
        "    # (You can switch to a specific example in batch; for paper this is fine.)\n",
        "    toks_ids = xb[0].detach().cpu().numpy()  # [T]\n",
        "\n",
        "    for b in range(nb):\n",
        "        e_in_bin = event_toks[event_bins == b]\n",
        "        if e_in_bin.size == 0:\n",
        "            continue\n",
        "\n",
        "        cand = all_t[(bins == b) & (~is_event)]\n",
        "        if cand.size == 0:\n",
        "            continue\n",
        "\n",
        "        if match_token_id:\n",
        "            # For each event position, try to pick a control position in same bin with same token id.\n",
        "            # Fall back to random in-bin if none exists.\n",
        "            cand_by_id = {}\n",
        "            for t in cand:\n",
        "                cand_by_id.setdefault(int(toks_ids[t]), []).append(int(t))\n",
        "\n",
        "            # Shuffle each list deterministically\n",
        "            for k in cand_by_id:\n",
        "                rng.shuffle(cand_by_id[k])\n",
        "\n",
        "            used = set()\n",
        "            for t_ev in e_in_bin:\n",
        "                tid = int(toks_ids[t_ev])\n",
        "                picked = None\n",
        "                lst = cand_by_id.get(tid, [])\n",
        "                while lst:\n",
        "                    t = lst.pop()\n",
        "                    if t not in used:\n",
        "                        picked = t\n",
        "                        break\n",
        "                if picked is None:\n",
        "                    # fallback: pick any unused cand\n",
        "                    avail = [int(t) for t in cand if int(t) not in used]\n",
        "                    if not avail:\n",
        "                        break\n",
        "                    picked = int(rng.choice(avail))\n",
        "                used.add(picked)\n",
        "                ctrl.append(picked)\n",
        "        else:\n",
        "            # simpler: random sample same count in this bin\n",
        "            k = min(len(e_in_bin), len(cand))\n",
        "            picked = rng.choice(cand, size=k, replace=False)\n",
        "            ctrl.extend([int(x) for x in picked])\n",
        "\n",
        "    ctrl_toks = np.asarray(ctrl, dtype=np.int64)\n",
        "\n",
        "    # Make lengths match (if token-id constraints limited controls)\n",
        "    n = min(len(event_toks), len(ctrl_toks))\n",
        "    event_toks = event_toks[:n]\n",
        "    ctrl_toks  = ctrl_toks[:n]\n",
        "\n",
        "    return event_toks, ctrl_toks\n",
        "\n",
        "# ----------------------------\n",
        "# Run (recommended defaults)\n",
        "# ----------------------------\n",
        "event_toks, ctrl_toks = build_event_and_control_tokens(\n",
        "    model,\n",
        "    xb,\n",
        "    event_mode=\"model\", # or given\n",
        "    n_events=256,\n",
        "    min_t=8,\n",
        "    max_t=None,\n",
        "    score=\"delta_cos\",       # tends to be very interpretable as policy shift\n",
        "    position_bins=16,\n",
        "    match_token_id=True,\n",
        "    seed=1337,\n",
        ")\n",
        "\n",
        "print(f\"Built event/control tokens: event={len(event_toks)} ctrl={len(ctrl_toks)}\")\n",
        "print(\"First 20 event_toks:\", event_toks[:20])\n",
        "print(\"First 20 ctrl_toks :\", ctrl_toks[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RZkHaOrA6Idt"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Canonicalized Head Semantic Role Alignment (paper-facing): event tokens vs matched controls\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "# ==========================================================\n",
        "# Expected inputs:\n",
        "#   model, xb\n",
        "#   event_toks: 1D list/np array of token indices in [0, T-1]\n",
        "#   ctrl_toks : 1D list/np array of matched control indices in [0, T-1]\n",
        "#\n",
        "# Notes:\n",
        "#   - This cell runs model(...) with ASA info plumbing (no precomputed infos).\n",
        "#   - It captures read_weights only (OOM-friendly).\n",
        "# ==========================================================\n",
        "\n",
        "# ----------------------------\n",
        "# Capture policy (new ASA plumbing)\n",
        "# ----------------------------\n",
        "ASA_CANON_INFO_CFG = dict(\n",
        "    store_read_weights=True,\n",
        "    store_read_logits=False,\n",
        "    store_write_logits=False,\n",
        "    store_slot_state_norm=False,\n",
        "    detach_to_cpu=False,    # keep on GPU for speed; we move reduced tensors to CPU\n",
        "    time_stride=1,\n",
        "    batch_stride=1,\n",
        ")\n",
        "\n",
        "# ----------------\n",
        "# Small helpers\n",
        "# ----------------\n",
        "def _safe_log(x, eps=1e-9):\n",
        "    return torch.log(x.clamp_min(eps))\n",
        "\n",
        "def kl_divergence(p, q, eps=1e-9):\n",
        "    # KL(p||q) over last dim\n",
        "    return (p * (_safe_log(p, eps) - _safe_log(q, eps))).sum(dim=-1)\n",
        "\n",
        "def cosine_sim(a, b, eps=1e-9):\n",
        "    an = a.norm(dim=-1).clamp_min(eps)\n",
        "    bn = b.norm(dim=-1).clamp_min(eps)\n",
        "    return (a * b).sum(dim=-1) / (an * bn)\n",
        "\n",
        "def hungarian_align(A_hd: np.ndarray, B_hd: np.ndarray):\n",
        "    \"\"\"\n",
        "    Align rows of B to rows of A by maximizing cosine similarity.\n",
        "    A,B: [H,D] numpy arrays\n",
        "    Returns:\n",
        "      perm: array length H where perm[i] is B-row matched to A-row i\n",
        "      q   : mean matched cosine similarity\n",
        "    \"\"\"\n",
        "    A = A_hd.astype(np.float64)\n",
        "    B = B_hd.astype(np.float64)\n",
        "    A = A / (np.linalg.norm(A, axis=1, keepdims=True) + 1e-9)\n",
        "    B = B / (np.linalg.norm(B, axis=1, keepdims=True) + 1e-9)\n",
        "    S = A @ B.T  # [H,H]\n",
        "    row, col = linear_sum_assignment(-S)\n",
        "    perm = np.empty((A.shape[0],), dtype=np.int64)\n",
        "    perm[row] = col\n",
        "    return perm, float(S[row, col].mean())\n",
        "\n",
        "def apply_perm_head0(x_h, perm, device=None):\n",
        "    \"\"\"\n",
        "    x_h: torch tensor with head axis at dim=0: [H,...]\n",
        "    perm maps canonical head i -> x head perm[i]\n",
        "    output[i] = x[perm[i]]\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = x_h.device\n",
        "    idx = torch.tensor(perm, device=device, dtype=torch.long)\n",
        "    return x_h.index_select(0, idx)\n",
        "\n",
        "# -------------------------------------------\n",
        "# 0) Run model to get per-layer read_weights\n",
        "# -------------------------------------------\n",
        "with torch.no_grad():\n",
        "    logits, infos = model(\n",
        "        xb,\n",
        "        return_info=True,\n",
        "        info_level=\"basic\",          # read_weights allowed in \"basic\"\n",
        "        info_cfg=ASA_CANON_INFO_CFG,\n",
        "    )\n",
        "\n",
        "# Collect valid layers\n",
        "rw_by_layer = {}\n",
        "valid_layers = []\n",
        "for l, info in enumerate(infos):\n",
        "    rw = None if (info is None) else info.get(\"read_weights\", None)\n",
        "    if rw is None or rw.dim() != 4:\n",
        "        continue\n",
        "    # rw: [B,H,T,K]\n",
        "    rw_by_layer[l] = rw.float()\n",
        "    valid_layers.append(l)\n",
        "\n",
        "assert len(valid_layers) >= 2, \"Need at least 2 layers with read_weights captured.\"\n",
        "\n",
        "L0 = valid_layers[0]\n",
        "B, H, T, K = rw_by_layer[L0].shape\n",
        "\n",
        "# ----------------------------\n",
        "# 1) sanitize token sets\n",
        "# ----------------------------\n",
        "event_toks = np.asarray(event_toks, dtype=np.int64)\n",
        "ctrl_toks  = np.asarray(ctrl_toks, dtype=np.int64)\n",
        "\n",
        "event_toks = event_toks[(event_toks >= 0) & (event_toks < T)]\n",
        "ctrl_toks  = ctrl_toks[(ctrl_toks  >= 0) & (ctrl_toks  < T)]\n",
        "\n",
        "n = min(len(event_toks), len(ctrl_toks))\n",
        "if n == 0:\n",
        "    raise ValueError(\"No valid event/control tokens after clipping to [0, T).\")\n",
        "if len(event_toks) != len(ctrl_toks):\n",
        "    print(f\" event/control count mismatch after clipping: event={len(event_toks)} ctrl={len(ctrl_toks)}. Using n={n}.\")\n",
        "    event_toks = event_toks[:n]\n",
        "    ctrl_toks  = ctrl_toks[:n]\n",
        "\n",
        "print(f\"Valid layers: {valid_layers}\")\n",
        "print(f\"Shapes: B={B}, H={H}, T={T}, K={K}\")\n",
        "print(f\"Event tokens: {len(event_toks)} | Controls: {len(ctrl_toks)}\")\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 2) Head feature vectors for cross-layer canonicalization\n",
        "# ----------------------------------------------------\n",
        "def build_head_features_from_rw(rw_bhtk: torch.Tensor, max_lag=16):\n",
        "    \"\"\"\n",
        "    rw: [B,H,T,K] probabilities over slots\n",
        "    Return [H,D] numpy features capturing:\n",
        "      - inertia_lag1 (mean cosine p_t vs p_{t+1})\n",
        "      - inertia_mean (mean cosine across lags)\n",
        "      - inertia_slope (slope of cosine vs lag)\n",
        "      - entropy_mean (mean H(p_t))\n",
        "      - eff_slots (exp(entropy_mean))\n",
        "      - top4_mass (mean sum of top4 probs)\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        p = rw_bhtk.mean(dim=0)  # [H,T,K]\n",
        "        p = p / p.sum(dim=-1, keepdim=True).clamp_min(1e-9)\n",
        "\n",
        "        ent = -(p * _safe_log(p)).sum(dim=-1)     # [H,T]\n",
        "        ent_mean = ent.mean(dim=-1)               # [H]\n",
        "        eff_slots = torch.exp(ent_mean)           # [H]\n",
        "\n",
        "        top4 = torch.topk(p, k=min(4, K), dim=-1).values.sum(dim=-1)  # [H,T]\n",
        "        top4_mean = top4.mean(dim=-1)                                 # [H]\n",
        "\n",
        "        L = min(int(max_lag), T - 1)\n",
        "        cos_lags = []\n",
        "        for lag in range(1, L + 1):\n",
        "            a = p[:, :-lag, :]\n",
        "            b = p[:, lag:, :]\n",
        "            c = cosine_sim(a, b).mean(dim=-1)  # [H]\n",
        "            cos_lags.append(c)\n",
        "        cos_lags = torch.stack(cos_lags, dim=-1)  # [H,L]\n",
        "\n",
        "        inertia_lag1 = cos_lags[:, 0]\n",
        "        inertia_mean = cos_lags.mean(dim=-1)\n",
        "\n",
        "        xs = torch.arange(1, L + 1, device=rw_bhtk.device, dtype=torch.float32)\n",
        "        xs = (xs - xs.mean()) / xs.std().clamp_min(1e-9)\n",
        "        ys = (cos_lags - cos_lags.mean(dim=-1, keepdim=True)) / cos_lags.std(dim=-1, keepdim=True).clamp_min(1e-9)\n",
        "        inertia_slope = (ys * xs).mean(dim=-1)\n",
        "\n",
        "        feats = torch.stack(\n",
        "            [inertia_lag1, inertia_mean, inertia_slope, ent_mean, eff_slots, top4_mean], dim=-1\n",
        "        )\n",
        "        feats = feats / feats.norm(dim=-1, keepdim=True).clamp_min(1e-9)\n",
        "        return feats.detach().cpu().numpy()\n",
        "\n",
        "head_features_by_layer = {l: build_head_features_from_rw(rw_by_layer[l], max_lag=16) for l in valid_layers}\n",
        "D = head_features_by_layer[L0].shape[1]\n",
        "print(f\"Head feature dim for canonicalization: D={D}\")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 3) Canonicalize heads across layers (compose Hungarian permutations)\n",
        "# -------------------------------------------------------------------\n",
        "canon_perm_to_layer = {L0: np.arange(H, dtype=np.int64)}\n",
        "align_quality = {}\n",
        "\n",
        "for i in range(len(valid_layers) - 1):\n",
        "    la = valid_layers[i]\n",
        "    lb = valid_layers[i + 1]\n",
        "    A = head_features_by_layer[la]  # [H,D]\n",
        "    Bf = head_features_by_layer[lb] # [H,D]\n",
        "\n",
        "    perm_ab, q = hungarian_align(A, Bf)          # la-head -> lb-head\n",
        "    align_quality[(la, lb)] = q\n",
        "\n",
        "    canon_to_la = canon_perm_to_layer[la]\n",
        "    canon_to_lb = perm_ab[canon_to_la]           # compose\n",
        "    canon_perm_to_layer[lb] = canon_to_lb\n",
        "\n",
        "print(\"\\nAdjacent-layer head alignment quality (mean cosine under matching):\")\n",
        "for (la, lb), q in align_quality.items():\n",
        "    print(f\"  {la:>2} -> {lb:<2}: {q:.4f}\")\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# 4) Token-conditioned head metrics (event vs control)\n",
        "# ---------------------------------------------------\n",
        "def per_layer_head_metrics(rw_bhtk: torch.Tensor, toks):\n",
        "    \"\"\"\n",
        "    rw: [B,H,T,K]\n",
        "    toks: list/array token indices\n",
        "    Returns dict of [H] tensors:\n",
        "      - kl_to_mean: mean KL(p_h(t) || mean_h(p(t))) over tokens\n",
        "      - entropy: mean entropy(p_h(t)) over tokens\n",
        "      - top1: mean max prob over tokens\n",
        "      - arg_conflict: fraction tokens where head argmax != mean-head argmax\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        p = rw_bhtk.mean(dim=0)  # [H,T,K]\n",
        "        p = p / p.sum(dim=-1, keepdim=True).clamp_min(1e-9)\n",
        "\n",
        "        toks = torch.as_tensor(toks, device=p.device, dtype=torch.long)\n",
        "        pt = p.index_select(1, toks)  # [H,|t|,K]\n",
        "\n",
        "        pmean = pt.mean(dim=0, keepdim=True)  # [1,|t|,K]\n",
        "        pmean = pmean / pmean.sum(dim=-1, keepdim=True).clamp_min(1e-9)\n",
        "\n",
        "        kl = kl_divergence(pt, pmean).mean(dim=1)                              # [H]\n",
        "        ent = (-(pt * _safe_log(pt)).sum(dim=-1)).mean(dim=1)                  # [H]\n",
        "        top1 = pt.max(dim=-1).values.mean(dim=1)                                # [H]\n",
        "        head_arg = pt.argmax(dim=-1)                                            # [H,|t|]\n",
        "        mean_arg = pmean.argmax(dim=-1).squeeze(0)                              # [|t|]\n",
        "        arg_conflict = (head_arg != mean_arg.unsqueeze(0)).float().mean(dim=1)  # [H]\n",
        "\n",
        "        return {\"kl_to_mean\": kl, \"entropy\": ent, \"top1\": top1, \"arg_conflict\": arg_conflict}\n",
        "\n",
        "layers = valid_layers\n",
        "metrics = [\"kl_to_mean\", \"entropy\", \"top1\", \"arg_conflict\"]\n",
        "\n",
        "event_mat = {m: torch.zeros((len(layers), H)) for m in metrics}\n",
        "ctrl_mat  = {m: torch.zeros((len(layers), H)) for m in metrics}\n",
        "\n",
        "for li, l in enumerate(layers):\n",
        "    rw = rw_by_layer[l]\n",
        "    ev = per_layer_head_metrics(rw, event_toks)\n",
        "    ct = per_layer_head_metrics(rw, ctrl_toks)\n",
        "\n",
        "    perm = canon_perm_to_layer[l]  # canonical head i corresponds to layer head perm[i]\n",
        "    for m in metrics:\n",
        "        event_mat[m][li] = apply_perm_head0(ev[m], perm, device=ev[m].device).detach().cpu()\n",
        "        ctrl_mat[m][li]  = apply_perm_head0(ct[m], perm, device=ct[m].device).detach().cpu()\n",
        "\n",
        "diff_mat = {m: (event_mat[m] - ctrl_mat[m]) for m in metrics}\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# 5) Falsifiable tests (paper-friendly readouts)\n",
        "# ---------------------------------------------------\n",
        "decision_window = [l for l in layers if (l >= 2 and l <= 5)]\n",
        "dw_idx = [layers.index(l) for l in decision_window] if decision_window else list(range(len(layers)))\n",
        "\n",
        "branch_score = diff_mat[\"kl_to_mean\"][dw_idx].mean(dim=0).numpy()  # [H]\n",
        "rank = np.argsort(-branch_score)\n",
        "\n",
        "print(\"\\nCanonical head 'brancher' ranking (higher = more event-specific divergence):\")\n",
        "for i in rank[:min(H, 12)]:\n",
        "    print(f\"  Hcanon {i:2d}: KL_to_mean (decision-window avg) = {branch_score[i]:+.4f}\")\n",
        "\n",
        "ref = diff_mat[\"kl_to_mean\"][dw_idx].mean(dim=0)\n",
        "ref = (ref - ref.mean()) / ref.std().clamp_min(1e-9)\n",
        "\n",
        "layer_corrs = []\n",
        "for li in range(len(layers)):\n",
        "    v = diff_mat[\"kl_to_mean\"][li]\n",
        "    v = (v - v.mean()) / v.std().clamp_min(1e-9)\n",
        "    layer_corrs.append(float((v * ref).mean()))\n",
        "\n",
        "print(\"\\nLayerwise correlation of KL_to_mean head profile with reference profile:\")\n",
        "for l, c in zip(layers, layer_corrs):\n",
        "    print(f\"  Layer {l:2d}: corr = {c:+.3f}\")\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# 6) Plots (heatmaps + role curves + signature scatter)\n",
        "# ---------------------------------------------------\n",
        "def plot_heatmap(mat, title, *, vmin=None, vmax=None):\n",
        "    plt.figure(figsize=(12, 5.5))\n",
        "    plt.imshow(mat, aspect=\"auto\", interpolation=\"nearest\", vmin=vmin, vmax=vmax)\n",
        "    plt.colorbar()\n",
        "    plt.yticks(range(len(layers)), [f\"L{l}\" for l in layers])\n",
        "    plt.xticks(range(H), [f\"H{i}\" for i in range(H)], rotation=0)\n",
        "    plt.xlabel(\"Canonical head\")\n",
        "    plt.ylabel(\"Layer\")\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_heatmap(diff_mat[\"kl_to_mean\"].numpy(),\n",
        "             \" KL(head || mean): event  control (canonicalized heads)\")\n",
        "\n",
        "plot_heatmap(diff_mat[\"entropy\"].numpy(),\n",
        "             \" head entropy: event  control (canonicalized heads)\")\n",
        "\n",
        "plot_heatmap(diff_mat[\"arg_conflict\"].numpy(),\n",
        "             \" argmax conflict vs mean: event  control (canonicalized heads)\")\n",
        "\n",
        "# Role curves: top-k branchers\n",
        "topk = min(4, H)\n",
        "plt.figure(figsize=(12, 4.8))\n",
        "for j in range(topk):\n",
        "    h = rank[j]\n",
        "    plt.plot(layers, diff_mat[\"kl_to_mean\"][:, h].numpy(), marker=\"o\", label=f\"Hcanon {h}\")\n",
        "if decision_window:\n",
        "    plt.axvspan(min(decision_window), max(decision_window), alpha=0.15)\n",
        "plt.axhline(0, lw=1)\n",
        "plt.xlabel(\"Layer\")\n",
        "plt.ylabel(\" KL(head || mean)  (event  control)\")\n",
        "plt.title(\"Event-specific head divergence across depth (canonical heads)\")\n",
        "plt.grid(True, alpha=0.25)\n",
        "plt.legend(title=\"Top branchers\", frameon=True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# \"confident split\" signature: Entropy vs KL in decision window\n",
        "sig_dkl  = diff_mat[\"kl_to_mean\"][dw_idx].mean(dim=0).numpy()\n",
        "sig_dent = diff_mat[\"entropy\"][dw_idx].mean(dim=0).numpy()\n",
        "sig_top1 = diff_mat[\"top1\"][dw_idx].mean(dim=0).numpy()\n",
        "\n",
        "plt.figure(figsize=(7.2, 6.2))\n",
        "plt.scatter(sig_dent, sig_dkl, s=45, alpha=0.9)\n",
        "for h in range(H):\n",
        "    plt.text(sig_dent[h], sig_dkl[h], str(h), fontsize=9)\n",
        "plt.axhline(0, lw=1); plt.axvline(0, lw=1)\n",
        "plt.xlabel(\" entropy (eventcontrol)\")\n",
        "plt.ylabel(\" KL(head||mean) (eventcontrol)\")\n",
        "plt.title(\"Canonical heads: confident divergence signature (decision window)\")\n",
        "plt.grid(True, alpha=0.25)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nDone.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gUGPCsJ3UmTq"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Position remapping WITHOUT changing forward signatures (monkeypatch RoPE + optional ALiBi disable)\n",
        "# This cell:\n",
        "#  1) Reuses the same deterministic long window (T_MAX)\n",
        "#  2) Computes chunked NLL vs absolute position\n",
        "#  3) Compares variants that DO NOT require model(..., position_ids=...)\n",
        "#     - baseline\n",
        "#     - ALiBi disabled (strength -> 0)   [optional but strongly recommended]\n",
        "#     - RoPE remap: reset every TRAIN_T  (t -> t % TRAIN_T)\n",
        "#     - RoPE remap: random block offset (blockwise t -> (t+off_b)%TRAIN_T)\n",
        "#\n",
        "# Works by monkeypatching RotaryEmbedding.get_cos_sin() for all modules that look like RoPE,\n",
        "# plus (optionally) forcing ALiBi strength to zero via safe param override.\n",
        "\n",
        "import math\n",
        "import inspect\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ----------------------------\n",
        "# Config\n",
        "# ----------------------------\n",
        "BATCH = 8\n",
        "T_MAX = 12288\n",
        "CHUNK = 2048\n",
        "SEED  = 1337\n",
        "SPLIT = \"val\"\n",
        "\n",
        "TRAIN_T = int(getattr(cfg, \"max_seq_len\", getattr(model, \"training_max_seq_len\", getattr(model, \"max_seq_len\", 256))))\n",
        "print(f\"Training context length (reference): {TRAIN_T}\")\n",
        "\n",
        "# ----------------------------\n",
        "# Deterministic long window\n",
        "# ----------------------------\n",
        "@torch.no_grad()\n",
        "def get_same_window(cfg, *, T: int, B: int, split=SPLIT, device=DEVICE):\n",
        "    gen = make_batch_generator(\n",
        "        cfg,\n",
        "        split=split,\n",
        "        device=device,\n",
        "        seq_len=int(T),\n",
        "        batches_per_epoch=1,\n",
        "        infinite=False,\n",
        "        num_workers=0,\n",
        "    )\n",
        "    xb, yb = next(gen)\n",
        "    xb = xb[:B].contiguous()\n",
        "    yb = yb[:B].contiguous()\n",
        "    if xb.shape[1] != T:\n",
        "        raise ValueError(f\"Expected xb length {T}, got {xb.shape[1]}\")\n",
        "    return xb, yb\n",
        "\n",
        "xb_long, yb_long = get_same_window(cfg, T=T_MAX, B=BATCH)\n",
        "print(f\"Using same window: B={xb_long.shape[0]} T={xb_long.shape[1]}\")\n",
        "\n",
        "# ----------------------------\n",
        "# Forward wrapper (no return_info)\n",
        "# ----------------------------\n",
        "@torch.no_grad()\n",
        "def run_logits(model, xb):\n",
        "    model.eval()\n",
        "    out = model(xb, attention_mask=None, return_info=False)\n",
        "    logits = out[0] if isinstance(out, (tuple, list)) else out\n",
        "    return logits\n",
        "\n",
        "# ----------------------------\n",
        "# Chunked NLL curve\n",
        "# ----------------------------\n",
        "@torch.no_grad()\n",
        "def chunk_nll_curve(logits_btv, y_bt, *, chunk=CHUNK):\n",
        "    B, T, V = logits_btv.shape\n",
        "    losses = []\n",
        "    centers = []\n",
        "    for start in range(0, T, chunk):\n",
        "        end = min(T, start + chunk)\n",
        "        l = F.cross_entropy(\n",
        "            logits_btv[:, start:end, :].reshape(-1, V),\n",
        "            y_bt[:, start:end].reshape(-1),\n",
        "            reduction=\"mean\"\n",
        "        ).item()\n",
        "        losses.append(l)\n",
        "        centers.append((start + end - 1) / 2.0)\n",
        "    return np.asarray(centers), np.asarray(losses)\n",
        "\n",
        "# ============================================================\n",
        "# 1) Monkeypatch RoPE: patch *all* modules that look like RotaryEmbedding\n",
        "# ============================================================\n",
        "def _is_rope_module(m) -> bool:\n",
        "    # robust duck typing: has inv_freq buffer + get_cos_sin method\n",
        "    return hasattr(m, \"inv_freq\") and hasattr(m, \"get_cos_sin\") and callable(getattr(m, \"get_cos_sin\"))\n",
        "\n",
        "def _make_pos_map(T: int, period: int, mode: str, seed: int, device):\n",
        "    \"\"\"\n",
        "    Returns int64 positions [T] to use in RoPE instead of arange(T).\n",
        "      - mode=\"baseline\": t\n",
        "      - mode=\"reset\": t % period\n",
        "      - mode=\"rand_block\": for each block of size period, t -> (t+off_b)%period\n",
        "    \"\"\"\n",
        "    t = torch.arange(T, device=device, dtype=torch.long)\n",
        "    if mode == \"baseline\":\n",
        "        return t\n",
        "    if period <= 0:\n",
        "        raise ValueError(\"period must be > 0\")\n",
        "    if mode == \"reset\":\n",
        "        return t % (period//2)\n",
        "    if mode == \"rand_block\":\n",
        "        rng = np.random.default_rng(seed)\n",
        "        out = torch.empty((T,), device=device, dtype=torch.long)\n",
        "        n_blocks = (T + period - 1) // period\n",
        "        for b in range(n_blocks):\n",
        "            off = int(rng.integers(low=0, high=period))\n",
        "            s = b * period\n",
        "            e = min(T, (b + 1) * period)\n",
        "            out[s:e] = (torch.arange(e - s, device=device, dtype=torch.long) + off) % period\n",
        "        return out\n",
        "    raise ValueError(f\"Unknown RoPE remap mode: {mode}\")\n",
        "\n",
        "class RopeRemapPatch:\n",
        "    \"\"\"\n",
        "    Context manager: temporarily replaces get_cos_sin(T, device, dtype) on all RoPE modules.\n",
        "    \"\"\"\n",
        "    def __init__(self, model, *, mode: str, period: int, seed: int):\n",
        "        self.model = model\n",
        "        self.mode = mode\n",
        "        self.period = int(period)\n",
        "        self.seed = int(seed)\n",
        "        self._saved = []\n",
        "\n",
        "    def __enter__(self):\n",
        "        # Find and patch\n",
        "        rope_modules = [m for m in self.model.modules() if _is_rope_module(m)]\n",
        "        if len(rope_modules) == 0:\n",
        "            print(\" No RoPE-like modules found to patch.\")\n",
        "            return self\n",
        "\n",
        "        for rm in rope_modules:\n",
        "            old = rm.get_cos_sin\n",
        "\n",
        "            def make_wrapped(rm_ref, old_fn):\n",
        "                # rm_ref: module with inv_freq\n",
        "                def wrapped(T: int, device, dtype):\n",
        "                    # Build mapped positions\n",
        "                    t_map = _make_pos_map(T, self.period, self.mode, self.seed, device=device)\n",
        "                    # Use rm_ref.inv_freq dtype for freqs, but final cos/sin in requested dtype\n",
        "                    inv = rm_ref.inv_freq.to(device=device)\n",
        "                    # freqs: [T, d/2]\n",
        "                    freqs = torch.einsum(\"t,f->tf\", t_map.to(inv.dtype), inv)\n",
        "                    emb = torch.cat([freqs, freqs], dim=-1)            # [T, d]\n",
        "                    cos = emb.cos()[None, None, :, :].to(dtype=dtype)  # [1,1,T,d]\n",
        "                    sin = emb.sin()[None, None, :, :].to(dtype=dtype)  # [1,1,T,d]\n",
        "                    return cos, sin\n",
        "                return wrapped\n",
        "\n",
        "            rm.get_cos_sin = make_wrapped(rm, old)\n",
        "            self._saved.append((rm, old))\n",
        "\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc, tb):\n",
        "        for rm, old in self._saved:\n",
        "            rm.get_cos_sin = old\n",
        "        self._saved.clear()\n",
        "        return False\n",
        "\n",
        "# ============================================================\n",
        "# 2) Optional ALiBi disable (no signature changes)\n",
        "#    - We try common patterns:\n",
        "#        a) modules with _alibi_strength_param (softplus param)\n",
        "#        b) modules with alibi_strength float attribute\n",
        "# ============================================================\n",
        "class TempSetAlibiZero:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.saved = []\n",
        "\n",
        "    def __enter__(self):\n",
        "        for m in self.model.modules():\n",
        "            if hasattr(m, \"_alibi_strength_param\") and isinstance(getattr(m, \"_alibi_strength_param\"), torch.nn.Parameter):\n",
        "                p = m._alibi_strength_param\n",
        "                self.saved.append((\"param\", p, p.data.clone()))\n",
        "                # make softplus(param)+min_strength  0 by setting very negative\n",
        "                p.data.fill_(-30.0)\n",
        "            elif hasattr(m, \"alibi_strength\") and isinstance(getattr(m, \"alibi_strength\"), (float, int)):\n",
        "                self.saved.append((\"attr\", m, float(m.alibi_strength)))\n",
        "                m.alibi_strength = 0.0\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc, tb):\n",
        "        for kind, obj, old in self.saved:\n",
        "            if kind == \"param\":\n",
        "                obj.data.copy_(old)\n",
        "            else:\n",
        "                obj.alibi_strength = old\n",
        "        self.saved.clear()\n",
        "        return False\n",
        "\n",
        "# ============================================================\n",
        "# 3) Evaluate variants\n",
        "# ============================================================\n",
        "def eval_variant(name, *, rope_mode=None, alibi_zero=False):\n",
        "    if rope_mode is None:\n",
        "        rope_ctx = nullcontext()\n",
        "    else:\n",
        "        rope_ctx = RopeRemapPatch(model, mode=rope_mode, period=TRAIN_T, seed=SEED)\n",
        "\n",
        "    alibi_ctx = TempSetAlibiZero(model) if alibi_zero else nullcontext()\n",
        "\n",
        "    with rope_ctx, alibi_ctx:\n",
        "        logits = run_logits(model, xb_long)\n",
        "        x, nll = chunk_nll_curve(logits, yb_long, chunk=CHUNK)\n",
        "\n",
        "    mean_nll = float(nll.mean())\n",
        "    ppl = float(np.exp(min(20.0, mean_nll)))\n",
        "    print(f\"{name:>22}: mean NLL={mean_nll:.3f} | ppl={ppl:.2f}\")\n",
        "    return x, nll, mean_nll, ppl\n",
        "\n",
        "# small helper\n",
        "from contextlib import nullcontext\n",
        "\n",
        "variants = []\n",
        "series = {}\n",
        "\n",
        "# Baseline\n",
        "x, nll, mn, ppl = eval_variant(\"baseline\", rope_mode=None, alibi_zero=False)\n",
        "variants.append(dict(variant=\"baseline\", mean_nll=mn, ppl=ppl))\n",
        "series[\"baseline\"] = (x, nll)\n",
        "\n",
        "# ALiBi off only\n",
        "x, nll, mn, ppl = eval_variant(\"alibi_zero\", rope_mode=None, alibi_zero=True)\n",
        "variants.append(dict(variant=\"alibi_zero\", mean_nll=mn, ppl=ppl))\n",
        "series[\"alibi_zero\"] = (x, nll)\n",
        "\n",
        "# RoPE reset only\n",
        "x, nll, mn, ppl = eval_variant(\"rope_reset_mod\", rope_mode=\"reset\", alibi_zero=False)\n",
        "variants.append(dict(variant=\"rope_reset_mod\", mean_nll=mn, ppl=ppl))\n",
        "series[\"rope_reset_mod\"] = (x, nll)\n",
        "\n",
        "# RoPE reset + ALiBi off\n",
        "x, nll, mn, ppl = eval_variant(\"rope_reset+alibi0\", rope_mode=\"reset\", alibi_zero=True)\n",
        "variants.append(dict(variant=\"rope_reset+alibi0\", mean_nll=mn, ppl=ppl))\n",
        "series[\"rope_reset+alibi0\"] = (x, nll)\n",
        "\n",
        "# RoPE random block offset only\n",
        "x, nll, mn, ppl = eval_variant(\"rope_rand_block\", rope_mode=\"rand_block\", alibi_zero=False)\n",
        "variants.append(dict(variant=\"rope_rand_block\", mean_nll=mn, ppl=ppl))\n",
        "series[\"rope_rand_block\"] = (x, nll)\n",
        "\n",
        "# RoPE random block offset + ALiBi off\n",
        "x, nll, mn, ppl = eval_variant(\"rope_rand+alibi0\", rope_mode=\"rand_block\", alibi_zero=True)\n",
        "variants.append(dict(variant=\"rope_rand+alibi0\", mean_nll=mn, ppl=ppl))\n",
        "series[\"rope_rand+alibi0\"] = (x, nll)\n",
        "\n",
        "df = pd.DataFrame(variants).sort_values(\"mean_nll\")\n",
        "display(df)\n",
        "\n",
        "# ============================================================\n",
        "# 4) Plot: NLL vs absolute position (chunk centers)\n",
        "# ============================================================\n",
        "plt.figure(figsize=(10.5, 4.8))\n",
        "for name, (x, y) in series.items():\n",
        "    plt.plot(x, y, marker=\"o\", linewidth=2, label=name)\n",
        "plt.axvline(TRAIN_T, linestyle=\"--\", linewidth=1)\n",
        "plt.xlabel(\"Absolute position (chunk center)\")\n",
        "plt.ylabel(f\"Mean NLL over chunk (size={CHUNK})\")\n",
        "plt.title(f\"RoPE/ALiBi position-remap diagnostics @ T={T_MAX}  (TRAIN_T={TRAIN_T})\")\n",
        "plt.grid(True, alpha=0.25)\n",
        "plt.legend(frameon=True, ncol=3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "m0HJJSfmV2uO"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Context Extrapolation  RoPE position remap sweeps + ALiBi strength sweeps (DROP-IN CELL)\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ----------------------------\n",
        "# Config\n",
        "# ----------------------------\n",
        "TRAIN_T = int(getattr(cfg, \"max_seq_len\", getattr(model, \"training_max_seq_len\", getattr(model, \"max_seq_len\", 256))))\n",
        "print(f\"Training context length (reference): {TRAIN_T}\")\n",
        "\n",
        "BATCH_FOR_PAPER = 1\n",
        "T_LONG = 12288\n",
        "\n",
        "# Chunked loss-vs-position diagnostic (optional, but very informative)\n",
        "CHUNK = 4096\n",
        "\n",
        "# --- RoPE remap sweeps ---\n",
        "# effective position: t' = floor(t / s)   (smooth dilation/compression)\n",
        "ROPE_SCALES = [2, 3, 4, 6, 8, 12]\n",
        "\n",
        "# --- ALiBi strength sweeps ---\n",
        "# multiplier on ALiBi term\n",
        "ALIBI_MULTS = [0.25, 0.5, 1.0, 2.0, 4.0, 16.0]\n",
        "\n",
        "# ASA capture knobs (keep light; we only need logits here)\n",
        "INFO_LEVEL = \"basic\"\n",
        "ASA_EXTRAP_INFO_CFG = dict(\n",
        "    store_read_weights=False,  # keep light for sweeps\n",
        "    store_read_logits=False,\n",
        "    store_write_logits=False,\n",
        "    store_slot_state_norm=False,\n",
        "    detach_to_cpu=False,\n",
        "    time_stride=4,\n",
        "    batch_stride=1,\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# Helpers\n",
        "# ----------------------------\n",
        "@torch.no_grad()\n",
        "def lm_loss(logits_btv: torch.Tensor, y_bt: torch.Tensor) -> float:\n",
        "    return F.cross_entropy(logits_btv.reshape(-1, logits_btv.size(-1)), y_bt.reshape(-1)).item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def run_model(model, xb, *, info_level=INFO_LEVEL, info_cfg=ASA_EXTRAP_INFO_CFG):\n",
        "    model.eval()\n",
        "    try:\n",
        "        logits, infos = model(\n",
        "            xb,\n",
        "            attention_mask=None,\n",
        "            return_info=True,\n",
        "            info_level=info_level,\n",
        "            info_cfg=info_cfg,\n",
        "        )\n",
        "        return logits, infos\n",
        "    except TypeError:\n",
        "        logits, infos = model(xb, attention_mask=None, return_info=True)\n",
        "        return logits, infos\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_same_window(cfg, *, T: int, B: int, split=\"val\", device=DEVICE):\n",
        "    gen = make_batch_generator(\n",
        "        cfg,\n",
        "        split=split,\n",
        "        device=device,\n",
        "        seq_len=int(T),\n",
        "        batches_per_epoch=1,\n",
        "        set_batch_size=BATCH_FOR_PAPER,\n",
        "        infinite=False,\n",
        "        num_workers=0,\n",
        "    )\n",
        "    xb, yb = next(gen)\n",
        "    xb = xb[:B].contiguous()\n",
        "    yb = yb[:B].contiguous()\n",
        "    if xb.shape[1] != T:\n",
        "        raise ValueError(f\"Expected xb length {T}, got {xb.shape[1]}\")\n",
        "    return xb, yb\n",
        "\n",
        "def _softplus_inv(y: float) -> float:\n",
        "    # inverse softplus for y>0: x = log(exp(y)-1)\n",
        "    y = float(max(1e-12, y))\n",
        "    return float(np.log(np.expm1(y)))\n",
        "\n",
        "def _try_get_alibi_param(model):\n",
        "    \"\"\"\n",
        "    Returns a list of (asa_module, attr_name, tensor_param) for ALiBi strength raw params if present.\n",
        "    Supports common patterns:\n",
        "      - asa._alibi_strength_param (raw, softplus'd in forward)\n",
        "    \"\"\"\n",
        "    found = []\n",
        "    if not hasattr(model, \"blocks\"):\n",
        "        return found\n",
        "    for blk in model.blocks:\n",
        "        asa = getattr(blk, \"asa\", None)\n",
        "        if asa is None:\n",
        "            continue\n",
        "        if hasattr(asa, \"_alibi_strength_param\") and isinstance(getattr(asa, \"_alibi_strength_param\"), torch.nn.Parameter):\n",
        "            found.append((asa, \"_alibi_strength_param\", getattr(asa, \"_alibi_strength_param\")))\n",
        "    return found\n",
        "\n",
        "def _try_get_alibi_min_strength(model):\n",
        "    # optional min_strength used in forward\n",
        "    mins = []\n",
        "    if not hasattr(model, \"blocks\"):\n",
        "        return mins\n",
        "    for blk in model.blocks:\n",
        "        asa = getattr(blk, \"asa\", None)\n",
        "        if asa is None:\n",
        "            continue\n",
        "        mins.append(float(getattr(asa, \"min_strength\", 0.0)))\n",
        "    return mins\n",
        "\n",
        "def _try_get_alibi_current_strength(model):\n",
        "    \"\"\"\n",
        "    Best-effort: compute current effective alibi strength if the module exposes it.\n",
        "    If asa._alibi_strength(dtype, device) exists, use that (in eval).\n",
        "    Else return None.\n",
        "    \"\"\"\n",
        "    vals = []\n",
        "    if not hasattr(model, \"blocks\"):\n",
        "        return None\n",
        "    for blk in model.blocks:\n",
        "        asa = getattr(blk, \"asa\", None)\n",
        "        if asa is None:\n",
        "            continue\n",
        "        fn = getattr(asa, \"_alibi_strength\", None)\n",
        "        if callable(fn):\n",
        "            with torch.no_grad():\n",
        "                v = float(fn(dtype=torch.float32, device=DEVICE).detach().cpu().item())\n",
        "            vals.append(v)\n",
        "        else:\n",
        "            return None\n",
        "    if not vals:\n",
        "        return None\n",
        "    return float(np.mean(vals))\n",
        "\n",
        "class _TempOverride:\n",
        "    def __init__(self):\n",
        "        self.saved = []\n",
        "\n",
        "    def set_tensor_(self, tensor, new_data: torch.Tensor):\n",
        "        self.saved.append((tensor, tensor.data.clone()))\n",
        "        tensor.data.copy_(new_data)\n",
        "\n",
        "    def fill_(self, tensor, value: float):\n",
        "        self.saved.append((tensor, tensor.data.clone()))\n",
        "        tensor.data.fill_(value)\n",
        "\n",
        "    def restore_(self):\n",
        "        for tensor, old in self.saved:\n",
        "            tensor.data.copy_(old)\n",
        "\n",
        "# ----------------------------\n",
        "# RoPE remap monkeypatch\n",
        "# ----------------------------\n",
        "def _patch_rope_get_cos_sin(rope_module, pos_map: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Patch rope_module.get_cos_sin(T, device, dtype) to return cos/sin for mapped positions.\n",
        "    pos_map: [T] long tensor mapping absolute t -> t'\n",
        "    \"\"\"\n",
        "    if rope_module is None or not hasattr(rope_module, \"inv_freq\"):\n",
        "        return None\n",
        "\n",
        "    orig = rope_module.get_cos_sin\n",
        "\n",
        "    def get_cos_sin_mapped(T: int, device, dtype):\n",
        "        # pos_map is defined for the specific T\n",
        "        assert pos_map.numel() == T, \"pos_map length mismatch\"\n",
        "        t = pos_map.to(device=device, dtype=rope_module.inv_freq.dtype)  # [T]\n",
        "        freqs = torch.einsum(\"t,f->tf\", t, rope_module.inv_freq)         # [T, d/2]\n",
        "        emb = torch.cat([freqs, freqs], dim=-1)                          # [T, d]\n",
        "        cos = emb.cos()[None, None, :, :].to(dtype=dtype)                # [1,1,T,d]\n",
        "        sin = emb.sin()[None, None, :, :].to(dtype=dtype)\n",
        "        return cos, sin\n",
        "\n",
        "    rope_module.get_cos_sin = get_cos_sin_mapped\n",
        "    return orig\n",
        "\n",
        "def _patch_all_asa_ropes(model, pos_map: torch.Tensor, also_slotspace: bool = True):\n",
        "    \"\"\"\n",
        "    Patch both write-side rope (asa.rope) and optionally slotspace rope (asa.rope_slotspace).\n",
        "    Returns list of (module, original_fn) to restore.\n",
        "    \"\"\"\n",
        "    patches = []\n",
        "    if not hasattr(model, \"blocks\"):\n",
        "        return patches\n",
        "    for blk in model.blocks:\n",
        "        asa = getattr(blk, \"asa\", None)\n",
        "        if asa is None:\n",
        "            continue\n",
        "        # write-side rope\n",
        "        if getattr(asa, \"rope\", None) is not None:\n",
        "            orig = _patch_rope_get_cos_sin(asa.rope, pos_map)\n",
        "            if orig is not None:\n",
        "                patches.append((asa.rope, orig))\n",
        "        # slotspace rope\n",
        "        if also_slotspace and getattr(asa, \"rope_slotspace\", None) is not None:\n",
        "            orig = _patch_rope_get_cos_sin(asa.rope_slotspace, pos_map)\n",
        "            if orig is not None:\n",
        "                patches.append((asa.rope_slotspace, orig))\n",
        "    return patches\n",
        "\n",
        "def _restore_rope_patches(patches):\n",
        "    for rope_module, orig in patches:\n",
        "        rope_module.get_cos_sin = orig\n",
        "\n",
        "# ----------------------------\n",
        "# ALiBi strength scaling (best-effort)\n",
        "# ----------------------------\n",
        "def _set_alibi_multiplier(model, mult: float):\n",
        "    \"\"\"\n",
        "    Best effort:\n",
        "      - if asa._alibi_strength_param exists (raw), compute current effective strength s,\n",
        "        then set raw so new strength ~= mult*s (respecting min_strength).\n",
        "    Returns override object to restore.\n",
        "    \"\"\"\n",
        "    ov = _TempOverride()\n",
        "    alibi_params = _try_get_alibi_param(model)\n",
        "    if not alibi_params:\n",
        "        return ov, False\n",
        "\n",
        "    # estimate a per-layer effective strength; if accessible, use it, else approximate from param directly\n",
        "    # We'll compute per-asa:\n",
        "    for asa, name, p in alibi_params:\n",
        "        with torch.no_grad():\n",
        "            # Try calling _alibi_strength to get current effective s\n",
        "            fn = getattr(asa, \"_alibi_strength\", None)\n",
        "            if callable(fn):\n",
        "                s0 = float(fn(dtype=torch.float32, device=DEVICE).detach().cpu().item())\n",
        "            else:\n",
        "                # fallback: assume softplus(raw)+min_strength\n",
        "                min_strength = float(getattr(asa, \"min_strength\", 0.0))\n",
        "                s0 = float((torch.nn.functional.softplus(p.detach().cpu()) + min_strength).item())\n",
        "\n",
        "            s1 = float(max(1e-12, mult * s0))\n",
        "            min_strength = float(getattr(asa, \"min_strength\", 0.0))\n",
        "            y = max(1e-12, s1 - min_strength)\n",
        "            raw_new = torch.tensor(_softplus_inv(y), device=p.device, dtype=p.dtype)\n",
        "            ov.set_tensor_(p, raw_new)\n",
        "\n",
        "    return ov, True\n",
        "\n",
        "# ----------------------------\n",
        "# Metrics & diagnostics\n",
        "# ----------------------------\n",
        "@torch.no_grad()\n",
        "def eval_mean_nll(model, xb, yb):\n",
        "    logits, _ = run_model(model, xb)\n",
        "    return float(lm_loss(logits, yb))\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_chunk_nll_curve(model, xb, yb, chunk=256):\n",
        "    \"\"\"\n",
        "    Returns centers, mean_nll_per_chunk for a given xb/yb (same length).\n",
        "    \"\"\"\n",
        "    logits, _ = run_model(model, xb)\n",
        "    B, T, V = logits.shape\n",
        "    losses = []\n",
        "    centers = []\n",
        "    for t0 in range(0, T, chunk):\n",
        "        t1 = min(T, t0 + chunk)\n",
        "        if t1 - t0 < 8:\n",
        "            continue\n",
        "        l = F.cross_entropy(\n",
        "            logits[:, t0:t1, :].reshape(-1, V),\n",
        "            yb[:, t0:t1].reshape(-1),\n",
        "            reduction=\"mean\",\n",
        "        ).item()\n",
        "        losses.append(l)\n",
        "        centers.append((t0 + t1) / 2.0)\n",
        "    return np.asarray(centers), np.asarray(losses)\n",
        "\n",
        "def _ppl(nll): return float(np.exp(min(20.0, float(nll))))\n",
        "\n",
        "# ----------------------------\n",
        "# Run\n",
        "# ----------------------------\n",
        "xb_long, yb_long = get_same_window(cfg, T=T_LONG, B=BATCH_FOR_PAPER, split=\"val\", device=DEVICE)\n",
        "print(f\"Using same window: B={xb_long.shape[0]} T={xb_long.shape[1]}\")\n",
        "\n",
        "# Baseline\n",
        "base_nll = eval_mean_nll(model, xb_long, yb_long)\n",
        "print(f\"baseline: mean NLL={base_nll:.3f} | ppl={_ppl(base_nll):.2f}\")\n",
        "\n",
        "rows = []\n",
        "rows.append(dict(kind=\"baseline\", variant=\"baseline\", mean_nll=base_nll, ppl=_ppl(base_nll)))\n",
        "\n",
        "# ----------------------------\n",
        "# 1) RoPE scale sweep (smooth dilation)\n",
        "# ----------------------------\n",
        "print(\"\\nRoPE scale sweep: t' = floor(t / s)\")\n",
        "for s in ROPE_SCALES:\n",
        "    pos_map = (torch.arange(T_LONG, dtype=torch.long) // int(s)).clamp_min(0)\n",
        "    patches = _patch_all_asa_ropes(model, pos_map, also_slotspace=True)\n",
        "    try:\n",
        "        nll = eval_mean_nll(model, xb_long, yb_long)\n",
        "        rows.append(dict(kind=\"rope_scale\", variant=f\"rope_scale_s={s}\", mean_nll=nll, ppl=_ppl(nll)))\n",
        "        print(f\"  s={s:>2}: mean NLL={nll:.3f} | ppl={_ppl(nll):.2f}\")\n",
        "    finally:\n",
        "        _restore_rope_patches(patches)\n",
        "\n",
        "# ----------------------------\n",
        "# 2) RoPE reset baseline (t % TRAIN_T)\n",
        "# ----------------------------\n",
        "print(\"\\nRoPE reset: t' = t % TRAIN_T\")\n",
        "pos_map = (torch.arange(T_LONG, dtype=torch.long) % int(TRAIN_T//2)).clamp_min(0)\n",
        "patches = _patch_all_asa_ropes(model, pos_map, also_slotspace=True)\n",
        "try:\n",
        "    nll = eval_mean_nll(model, xb_long, yb_long)\n",
        "    rows.append(dict(kind=\"rope_reset\", variant=f\"rope_reset_mod_{TRAIN_T}//2\", mean_nll=nll, ppl=_ppl(nll)))\n",
        "    print(f\"  reset: mean NLL={nll:.3f} | ppl={_ppl(nll):.2f}\")\n",
        "finally:\n",
        "    _restore_rope_patches(patches)\n",
        "\n",
        "# ----------------------------\n",
        "# 3) ALiBi strength sweep (multiplier)\n",
        "# ----------------------------\n",
        "print(\"\\nALiBi strength sweep (multiplier on effective strength):\")\n",
        "alibi_supported = bool(_try_get_alibi_param(model))\n",
        "if not alibi_supported:\n",
        "    print(\"   No _alibi_strength_param found in model.blocks[*].asa; skipping ALiBi sweep.\")\n",
        "else:\n",
        "    s_now = _try_get_alibi_current_strength(model)\n",
        "    if s_now is not None:\n",
        "        print(f\"  Current mean effective ALiBi strength  {s_now:.6f}\")\n",
        "    for a in ALIBI_MULTS:\n",
        "        ov, ok = _set_alibi_multiplier(model, a)\n",
        "        if not ok:\n",
        "            print(\"   Failed to apply ALiBi scaling (unexpected).\")\n",
        "            break\n",
        "        try:\n",
        "            nll = eval_mean_nll(model, xb_long, yb_long)\n",
        "            rows.append(dict(kind=\"alibi_mult\", variant=f\"alibi_mult={a:g}\", mean_nll=nll, ppl=_ppl(nll)))\n",
        "            print(f\"  ={a:g}: mean NLL={nll:.3f} | ppl={_ppl(nll):.2f}\")\n",
        "        finally:\n",
        "            ov.restore_()\n",
        "\n",
        "# ----------------------------\n",
        "# 4) Best-of diagnostics: chunk loss vs absolute position\n",
        "# ----------------------------\n",
        "df = pd.DataFrame(rows).sort_values([\"mean_nll\"])\n",
        "display(df)\n",
        "\n",
        "# pick best rope_scale and best alibi_mult (if present)\n",
        "best_rope = df[df[\"kind\"].isin([\"rope_scale\",\"rope_reset\"])].iloc[0] if np.any(df[\"kind\"].isin([\"rope_scale\",\"rope_reset\"])) else None\n",
        "best_alibi = df[df[\"kind\"].isin([\"alibi_mult\"])].iloc[0] if np.any(df[\"kind\"].isin([\"alibi_mult\"])) else None\n",
        "\n",
        "def _run_variant_for_curve(variant_row):\n",
        "    name = variant_row[\"variant\"]\n",
        "    kind = variant_row[\"kind\"]\n",
        "\n",
        "    if kind == \"baseline\":\n",
        "        centers, losses = eval_chunk_nll_curve(model, xb_long, yb_long, chunk=CHUNK)\n",
        "        return name, centers, losses\n",
        "\n",
        "    if kind in [\"rope_scale\",\"rope_reset\"]:\n",
        "        # parse s or reset\n",
        "        if \"rope_scale_s=\" in name:\n",
        "            s = int(name.split(\"rope_scale_s=\")[-1])\n",
        "            pos_map = (torch.arange(T_LONG, dtype=torch.long) // s).clamp_min(0)\n",
        "        else:\n",
        "            pos_map = (torch.arange(T_LONG, dtype=torch.long) % int(TRAIN_T)).clamp_min(0)\n",
        "\n",
        "        patches = _patch_all_asa_ropes(model, pos_map, also_slotspace=True)\n",
        "        try:\n",
        "            centers, losses = eval_chunk_nll_curve(model, xb_long, yb_long, chunk=CHUNK)\n",
        "            return name, centers, losses\n",
        "        finally:\n",
        "            _restore_rope_patches(patches)\n",
        "\n",
        "    if kind == \"alibi_mult\":\n",
        "        a = float(name.split(\"=\")[-1])\n",
        "        ov, ok = _set_alibi_multiplier(model, a)\n",
        "        if not ok:\n",
        "            return None\n",
        "        try:\n",
        "            centers, losses = eval_chunk_nll_curve(model, xb_long, yb_long, chunk=CHUNK)\n",
        "            return name, centers, losses\n",
        "        finally:\n",
        "            ov.restore_()\n",
        "\n",
        "    return None\n",
        "\n",
        "# Make curves: baseline vs best_rope vs best_alibi (if any)\n",
        "curves = []\n",
        "curves.append(_run_variant_for_curve(pd.Series(dict(kind=\"baseline\", variant=\"baseline\"))))\n",
        "if best_rope is not None:\n",
        "    curves.append(_run_variant_for_curve(best_rope))\n",
        "if best_alibi is not None:\n",
        "    curves.append(_run_variant_for_curve(best_alibi))\n",
        "\n",
        "plt.figure(figsize=(10.5, 4.6))\n",
        "for item in curves:\n",
        "    if item is None:\n",
        "        continue\n",
        "    name, centers, losses = item\n",
        "    plt.plot(centers, losses, marker=\"o\", linewidth=2, label=name)\n",
        "plt.axvline(TRAIN_T, linestyle=\"--\", linewidth=1)\n",
        "plt.xlabel(f\"Absolute position (chunk center, chunk={CHUNK})\")\n",
        "plt.ylabel(\"Mean NLL over chunk\")\n",
        "plt.title(f\"Position diagnostics @ T={T_LONG}: baseline vs best remap / ALiBi sweep\")\n",
        "plt.grid(True, alpha=0.25)\n",
        "plt.legend(frameon=True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "on2RwiXzYFU1"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Analysis 1  Cyclic RoPE induces *stationary* routing (periodicity + drift diagnostics)\n",
        "# This drills into: \"cyclic is a feature\" by showing that routing stats stop drifting with absolute position.\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ----------------------------\n",
        "# Config\n",
        "# ----------------------------\n",
        "TRAIN_T = int(getattr(cfg, \"max_seq_len\", getattr(model, \"training_max_seq_len\", getattr(model, \"max_seq_len\", 256))))\n",
        "T_LONG  = 12288\n",
        "BATCH   = 2\n",
        "\n",
        "# Capture knobs (keep light; we only need read_weights summaries)\n",
        "INFO_LEVEL = \"basic\"\n",
        "ASA_INFO_CFG = dict(\n",
        "    store_read_weights=True,\n",
        "    store_read_logits=False,\n",
        "    store_write_logits=False,\n",
        "    store_slot_state_norm=False,\n",
        "    detach_to_cpu=False,\n",
        "    time_stride=4,   # downsample time for long sequences\n",
        "    batch_stride=1,\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# Helpers: loss + runner\n",
        "# ----------------------------\n",
        "@torch.no_grad()\n",
        "def lm_loss(logits_btv: torch.Tensor, y_bt: torch.Tensor) -> float:\n",
        "    return F.cross_entropy(logits_btv.reshape(-1, logits_btv.size(-1)), y_bt.reshape(-1)).item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def run_model(model, xb, *, info_level=INFO_LEVEL, info_cfg=ASA_INFO_CFG):\n",
        "    model.eval()\n",
        "    try:\n",
        "        return model(xb, attention_mask=None, return_info=True, info_level=info_level, info_cfg=info_cfg)\n",
        "    except TypeError:\n",
        "        return model(xb, attention_mask=None, return_info=True)\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_same_window(cfg, *, T: int, B: int, split=\"val\", device=DEVICE):\n",
        "    gen = make_batch_generator(\n",
        "        cfg, split=split, device=device,\n",
        "        seq_len=int(T), batches_per_epoch=1, set_batch_size=2, infinite=False,\n",
        "        num_workers=0\n",
        "    )\n",
        "    xb, yb = next(gen)\n",
        "    xb = xb[:B].contiguous()\n",
        "    yb = yb[:B].contiguous()\n",
        "    if xb.shape[1] != T:\n",
        "        raise ValueError(f\"Expected T={T}, got {xb.shape[1]}\")\n",
        "    return xb, yb\n",
        "\n",
        "# ----------------------------\n",
        "# RoPE patcher (cyclic or scaled positions) for ASA blocks\n",
        "# Works without position_ids support.\n",
        "# ----------------------------\n",
        "class RopePosRemap:\n",
        "    def __init__(self, model, *, mode=\"reset\", train_T=1024, scale=1):\n",
        "        self.model = model\n",
        "        self.mode = str(mode)\n",
        "        self.train_T = int(train_T)\n",
        "        self.scale = int(scale)\n",
        "        self.saved = []  # (rope_module, original_get_cos_sin)\n",
        "\n",
        "    def _remap_positions(self, T: int, device, dtype, inv_freq: torch.Tensor):\n",
        "        # returns cos,sin with shape [1,1,T,d]\n",
        "        t = torch.arange(T, device=device, dtype=inv_freq.dtype)\n",
        "        if self.mode == \"reset\":\n",
        "            t = t % self.train_T\n",
        "        elif self.mode == \"scale\":\n",
        "            s = max(1, self.scale)\n",
        "            t = torch.floor_divide(t, s)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown mode={self.mode}\")\n",
        "\n",
        "        freqs = torch.einsum(\"t,f->tf\", t, inv_freq)     # [T, d/2]\n",
        "        emb   = torch.cat([freqs, freqs], dim=-1)        # [T, d]\n",
        "        cos   = emb.cos()[None, None, :, :].to(dtype=dtype)\n",
        "        sin   = emb.sin()[None, None, :, :].to(dtype=dtype)\n",
        "        return cos, sin\n",
        "\n",
        "    def __enter__(self):\n",
        "        # Patch every ASA rope used for write keys + (optionally) slotspace rope\n",
        "        for blk in getattr(self.model, \"blocks\", []):\n",
        "            asa = getattr(blk, \"asa\", None)\n",
        "            if asa is None:\n",
        "                continue\n",
        "\n",
        "            for rope_name in (\"rope\", \"rope_slotspace\"):\n",
        "                rope = getattr(asa, rope_name, None)\n",
        "                if rope is None:\n",
        "                    continue\n",
        "                if not hasattr(rope, \"inv_freq\") or not hasattr(rope, \"get_cos_sin\"):\n",
        "                    continue\n",
        "\n",
        "                orig = rope.get_cos_sin\n",
        "\n",
        "                def patched_get_cos_sin(T, device, dtype, *, _rope=rope, _orig=orig):\n",
        "                    inv = _rope.inv_freq\n",
        "                    return self._remap_positions(T=int(T), device=device, dtype=dtype, inv_freq=inv)\n",
        "\n",
        "                self.saved.append((rope, orig))\n",
        "                rope.get_cos_sin = patched_get_cos_sin\n",
        "\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc, tb):\n",
        "        for rope, orig in self.saved:\n",
        "            rope.get_cos_sin = orig\n",
        "        self.saved = []\n",
        "\n",
        "# ----------------------------\n",
        "# Routing summaries: per-chunk stationarity + drift vs chunk0\n",
        "# ----------------------------\n",
        "@torch.no_grad()\n",
        "def summarize_chunks(infos, *, chunk_len: int, time_stride: int):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      df_chunk: per (layer, chunk) scalar routing metrics\n",
        "      df_drift: per chunk drift stats aggregated across layers\n",
        "    \"\"\"\n",
        "    eps = 1e-9\n",
        "    rows = []\n",
        "\n",
        "    for li, info in enumerate(infos):\n",
        "        if info is None:\n",
        "            continue\n",
        "        rw = info.get(\"read_weights\", None)\n",
        "        if rw is None:\n",
        "            continue\n",
        "\n",
        "        # rw is [B,H,T_eff,K] after time_stride downsample\n",
        "        rw = rw.float()\n",
        "        B, H, T_eff, K = rw.shape\n",
        "\n",
        "        # Effective chunk length in downsampled time\n",
        "        chunk_eff = max(1, int(chunk_len // max(1, time_stride)))\n",
        "        n_chunks = int(math.ceil(T_eff / chunk_eff))\n",
        "\n",
        "        # Per-token entropy + maxp\n",
        "        ent = -(rw * rw.clamp_min(eps).log()).sum(dim=-1)  # [B,H,T_eff]\n",
        "        maxp = rw.max(dim=-1).values                        # [B,H,T_eff]\n",
        "\n",
        "        for c in range(n_chunks):\n",
        "            t0 = c * chunk_eff\n",
        "            t1 = min(T_eff, (c + 1) * chunk_eff)\n",
        "            if t1 <= t0:\n",
        "                continue\n",
        "\n",
        "            # Chunk routing scalars\n",
        "            ent_norm = (ent[:, :, t0:t1].mean() / math.log(K)).item()\n",
        "            maxp_mu  = maxp[:, :, t0:t1].mean().item()\n",
        "\n",
        "            # Slot usage distribution in this chunk\n",
        "            usage = rw[:, :, t0:t1, :].mean(dim=(0, 1, 2))              # [K]\n",
        "            usage = usage / usage.sum().clamp_min(eps)\n",
        "            usage_ent = float(-(usage * usage.clamp_min(eps).log()).sum().cpu())\n",
        "            usage_ent_norm = usage_ent / float(math.log(K))\n",
        "\n",
        "            rows.append(dict(\n",
        "                layer=int(li),\n",
        "                chunk=int(c),\n",
        "                ent_norm=float(ent_norm),\n",
        "                maxp=float(maxp_mu),\n",
        "                usage_ent_norm=float(usage_ent_norm),\n",
        "                T_eff=int(T_eff),\n",
        "                K=int(K),\n",
        "            ))\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    if df.empty:\n",
        "        return df, df\n",
        "\n",
        "    # Drift vs chunk0: compare usage distributions (approx via entropy proxy + Jensen gap)\n",
        "    # We also report stationarity of ent_norm/maxp: std across chunks.\n",
        "    drift_rows = []\n",
        "    for c in sorted(df[\"chunk\"].unique()):\n",
        "        sub = df[df[\"chunk\"] == c]\n",
        "        drift_rows.append(dict(\n",
        "            chunk=int(c),\n",
        "            ent_norm_mean=float(sub[\"ent_norm\"].mean()),\n",
        "            ent_norm_std=float(sub[\"ent_norm\"].std(ddof=0)),\n",
        "            maxp_mean=float(sub[\"maxp\"].mean()),\n",
        "            maxp_std=float(sub[\"maxp\"].std(ddof=0)),\n",
        "            usage_ent_mean=float(sub[\"usage_ent_norm\"].mean()),\n",
        "            usage_ent_std=float(sub[\"usage_ent_norm\"].std(ddof=0)),\n",
        "        ))\n",
        "    df_drift = pd.DataFrame(drift_rows).sort_values(\"chunk\")\n",
        "    return df.sort_values([\"layer\", \"chunk\"]), df_drift\n",
        "\n",
        "def plot_stationarity(df_drift, *, title, train_T=1024):\n",
        "    if df_drift.empty:\n",
        "        print(\"No chunk summaries to plot.\")\n",
        "        return\n",
        "    plt.figure(figsize=(10.8, 4.6))\n",
        "    plt.plot(df_drift[\"chunk\"], df_drift[\"ent_norm_mean\"], marker=\"o\", linewidth=2, label=\"mean routing entropy (norm)\")\n",
        "    plt.plot(df_drift[\"chunk\"], df_drift[\"maxp_mean\"], marker=\"o\", linewidth=2, label=\"mean routing maxp\")\n",
        "    plt.plot(df_drift[\"chunk\"], df_drift[\"usage_ent_mean\"], marker=\"o\", linewidth=2, label=\"mean slot usage entropy (norm)\")\n",
        "    plt.xlabel(f\"Chunk index (each  {train_T} tokens)\")\n",
        "    plt.ylabel(\"Layer-avg metric\")\n",
        "    plt.title(title)\n",
        "    plt.grid(True, alpha=0.25)\n",
        "    plt.legend(frameon=True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ----------------------------\n",
        "# Run: baseline vs cyclic RoPE reset\n",
        "# ----------------------------\n",
        "xb_long, yb_long = get_same_window(cfg, T=T_LONG, B=BATCH, split=\"val\", device=DEVICE)\n",
        "print(f\"Same window: B={xb_long.shape[0]} T={xb_long.shape[1]} | TRAIN_T={TRAIN_T}\")\n",
        "\n",
        "# Baseline\n",
        "logits0, infos0 = run_model(model, xb_long, info_level=INFO_LEVEL, info_cfg=ASA_INFO_CFG)\n",
        "loss0 = lm_loss(logits0, yb_long)\n",
        "ppl0  = float(np.exp(min(20.0, loss0)))\n",
        "df0, d0 = summarize_chunks(infos0, chunk_len=TRAIN_T, time_stride=ASA_INFO_CFG[\"time_stride\"])\n",
        "print(f\"baseline: loss={loss0:.4f} ppl={ppl0:.2f} | chunks={len(d0)}\")\n",
        "\n",
        "# Cyclic RoPE reset\n",
        "with RopePosRemap(model, mode=\"reset\", train_T=TRAIN_T):\n",
        "    logits1, infos1 = run_model(model, xb_long, info_level=INFO_LEVEL, info_cfg=ASA_INFO_CFG)\n",
        "loss1 = lm_loss(logits1, yb_long)\n",
        "ppl1  = float(np.exp(min(20.0, loss1)))\n",
        "df1, d1 = summarize_chunks(infos1, chunk_len=TRAIN_T, time_stride=ASA_INFO_CFG[\"time_stride\"])\n",
        "print(f\"rope_reset (t%{TRAIN_T}): loss={loss1:.4f} ppl={ppl1:.2f} | chunks={len(d1)}\")\n",
        "\n",
        "# Plot: stationarity across chunks\n",
        "plot_stationarity(d0, title=f\"Baseline routing drift across chunks @ T={T_LONG}\", train_T=TRAIN_T)\n",
        "plot_stationarity(d1, title=f\"Cyclic RoPE routing stationarity across chunks @ T={T_LONG}\", train_T=TRAIN_T)\n",
        "\n",
        "# Quick numeric: how stationary?\n",
        "def stationarity_score(df_drift):\n",
        "    # lower is more stationary: sum of stds\n",
        "    if df_drift.empty:\n",
        "        return np.nan\n",
        "    return float(df_drift[\"ent_norm_std\"].mean() + df_drift[\"maxp_std\"].mean() + df_drift[\"usage_ent_std\"].mean())\n",
        "\n",
        "s0 = stationarity_score(d0)\n",
        "s1 = stationarity_score(d1)\n",
        "pd.DataFrame([\n",
        "    dict(variant=\"baseline\", ppl=ppl0, stationarity_score=s0),\n",
        "    dict(variant=f\"rope_reset_mod_{TRAIN_T}\", ppl=ppl1, stationarity_score=s1),\n",
        "]).sort_values(\"ppl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "iTRqymMmeH3U"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Analysis 2 (DROP-IN, OOM-safe + RELIABLE canonical head+slot matching)  \"cyclic is a feature\" & \"internals behave as intended\"\n",
        "# Complete replacement. Fixes:\n",
        "#   - OOM: streamed loss uses log-softmax + gather (no giant CE temp); optional CPU eval.\n",
        "#   - Head match bug: schema-locked head feature vectors (fixed D) so no (11 vs 12) mismatch.\n",
        "#   - Robust canonicalization: head matching across depth + cross-condition head match at ref layer.\n",
        "#   - Slot matching: per-head slot descriptor uses time-marginal usage AND inertia signatures, then exact match for K<=8.\n",
        "#\n",
        "# Outputs:\n",
        "#   df_scores: NLL/PPL per condition\n",
        "#   df_align : head+slot alignment quality trainbaseline and traincyclic at ref layer\n",
        "#   df_depth : adjacent-layer head alignment quality per condition\n",
        "#\n",
        "# Notes:\n",
        "#   - Assumes ASA info plumbing exposes read_weights in info_level=\"basic\" and obeys info_cfg time_stride/detach_to_cpu.\n",
        "#   - Uses RoPE remap patcher (no position_ids required) by monkeypatching rope.get_cos_sin.\n",
        "\n",
        "import math\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ----------------------------\n",
        "# Config knobs\n",
        "# ----------------------------\n",
        "TRAIN_T = int(getattr(cfg, \"max_seq_len\", getattr(model, \"training_max_seq_len\", getattr(model, \"max_seq_len\", 256))))\n",
        "T_LONG  = 12288\n",
        "BATCH   = 4\n",
        "\n",
        "# Streamed loss knobs\n",
        "LOSS_CHUNK_T = 256           # token chunk size for loss\n",
        "LOSS_CHUNK_B = 1             # microbatch for loss (reduces peak)\n",
        "LOSS_ON_CPU  = False         # set True if you keep hitting GPU fragmentation\n",
        "\n",
        "INFO_LEVEL = \"basic\"\n",
        "INFO_CFG = dict(\n",
        "    store_read_weights=True,\n",
        "    store_read_logits=False,\n",
        "    store_write_logits=False,\n",
        "    store_slot_state_norm=False,\n",
        "    detach_to_cpu=True,      # IMPORTANT: frees GPU\n",
        "    time_stride=16,          # IMPORTANT: huge memory win\n",
        "    batch_stride=1,\n",
        ")\n",
        "\n",
        "# Descriptor knobs (applied after time_stride)\n",
        "MAX_LAG = 64\n",
        "USE_LAGS = (1, 2, 4, 8, 16, 32, 64)\n",
        "\n",
        "# ----------------------------\n",
        "# Helpers: data\n",
        "# ----------------------------\n",
        "@torch.no_grad()\n",
        "def get_same_window(cfg, *, T: int, B: int, split=\"val\", device=DEVICE):\n",
        "    gen = make_batch_generator(\n",
        "        cfg, split=split, device=device, seq_len=int(T),\n",
        "        batches_per_epoch=1, infinite=False, num_workers=0\n",
        "    )\n",
        "    xb, yb = next(gen)\n",
        "    xb = xb[:B].contiguous()\n",
        "    yb = yb[:B].contiguous()\n",
        "    if xb.shape[1] != T:\n",
        "        raise ValueError(f\"Expected T={T}, got {xb.shape[1]}\")\n",
        "    return xb, yb\n",
        "\n",
        "# ----------------------------\n",
        "# Helpers: OOM-safe streamed NLL (log-softmax + gather)\n",
        "# ----------------------------\n",
        "@torch.no_grad()\n",
        "def streamed_nll_from_logits(logits_btv: torch.Tensor, y_bt: torch.Tensor,\n",
        "                             *, chunk_t: int = LOSS_CHUNK_T, chunk_b: int = LOSS_CHUNK_B,\n",
        "                             compute_on_cpu: bool = LOSS_ON_CPU) -> float:\n",
        "    \"\"\"\n",
        "    Computes mean NLL without allocating a huge temporary (like CE sometimes does).\n",
        "    Uses log_softmax + gather in chunks over time and microbatches over batch.\n",
        "    \"\"\"\n",
        "    if compute_on_cpu:\n",
        "        logits_btv = logits_btv.detach().to(\"cpu\")\n",
        "        y_bt = y_bt.detach().to(\"cpu\")\n",
        "\n",
        "    B, T, V = logits_btv.shape\n",
        "    total = 0.0\n",
        "    count = 0\n",
        "\n",
        "    for b0 in range(0, B, chunk_b):\n",
        "        b1 = min(B, b0 + chunk_b)\n",
        "        lg_b = logits_btv[b0:b1]  # [b,T,V]\n",
        "        y_b  = y_bt[b0:b1]        # [b,T]\n",
        "\n",
        "        for t0 in range(0, T, chunk_t):\n",
        "            t1 = min(T, t0 + chunk_t)\n",
        "            lg = lg_b[:, t0:t1, :]                 # [b,dt,V]\n",
        "            yy = y_b[:, t0:t1]                     # [b,dt]\n",
        "            lsm = F.log_softmax(lg, dim=-1)         # [b,dt,V]\n",
        "            nll = -lsm.gather(-1, yy.unsqueeze(-1)).squeeze(-1)  # [b,dt]\n",
        "            total += float(nll.sum().item())\n",
        "            count += int(nll.numel())\n",
        "\n",
        "    return float(total / max(1, count))\n",
        "\n",
        "# ----------------------------\n",
        "# Helpers: model runner\n",
        "# ----------------------------\n",
        "@torch.no_grad()\n",
        "def run_model(model, xb, *, info_level=INFO_LEVEL, info_cfg=INFO_CFG):\n",
        "    model.eval()\n",
        "    try:\n",
        "        return model(xb, attention_mask=None, return_info=True, info_level=info_level, info_cfg=info_cfg)\n",
        "    except TypeError:\n",
        "        return model(xb, attention_mask=None, return_info=True)\n",
        "\n",
        "# ----------------------------\n",
        "# RoPE remap patcher (works without position_ids)\n",
        "# ----------------------------\n",
        "class RopePosRemap:\n",
        "    def __init__(self, model, *, mode=\"reset\", train_T=1024, scale=1):\n",
        "        self.model = model\n",
        "        self.mode = str(mode)\n",
        "        self.train_T = int(train_T)\n",
        "        self.scale = int(scale)\n",
        "        self.saved = []\n",
        "\n",
        "    def _remap_positions(self, T: int, device, dtype, inv_freq: torch.Tensor):\n",
        "        t = torch.arange(T, device=device, dtype=inv_freq.dtype)\n",
        "        if self.mode == \"reset\":\n",
        "            t = t % self.train_T\n",
        "        elif self.mode == \"scale\":\n",
        "            s = max(1, self.scale)\n",
        "            t = torch.floor_divide(t, s)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown mode={self.mode}\")\n",
        "        freqs = torch.einsum(\"t,f->tf\", t, inv_freq)   # [T, d/2]\n",
        "        emb = torch.cat([freqs, freqs], dim=-1)        # [T, d]\n",
        "        cos = emb.cos()[None, None, :, :].to(dtype=dtype)\n",
        "        sin = emb.sin()[None, None, :, :].to(dtype=dtype)\n",
        "        return cos, sin\n",
        "\n",
        "    def __enter__(self):\n",
        "        for blk in getattr(self.model, \"blocks\", []):\n",
        "            asa = getattr(blk, \"asa\", None)\n",
        "            if asa is None:\n",
        "                continue\n",
        "            for rope_name in (\"rope\", \"rope_slotspace\"):\n",
        "                rope = getattr(asa, rope_name, None)\n",
        "                if rope is None or (not hasattr(rope, \"inv_freq\")) or (not hasattr(rope, \"get_cos_sin\")):\n",
        "                    continue\n",
        "                orig = rope.get_cos_sin\n",
        "\n",
        "                def patched_get_cos_sin(T, device, dtype, *, _rope=rope):\n",
        "                    return self._remap_positions(int(T), device, dtype, _rope.inv_freq)\n",
        "\n",
        "                self.saved.append((rope, orig))\n",
        "                rope.get_cos_sin = patched_get_cos_sin\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc, tb):\n",
        "        for rope, orig in self.saved:\n",
        "            rope.get_cos_sin = orig\n",
        "        self.saved = []\n",
        "\n",
        "# ----------------------------\n",
        "# Canonical assignment (exact for N<=8)\n",
        "# ----------------------------\n",
        "def _cosine_sim_matrix(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n",
        "    A = A.astype(np.float64); B = B.astype(np.float64)\n",
        "    A = A / (np.linalg.norm(A, axis=1, keepdims=True) + 1e-9)\n",
        "    B = B / (np.linalg.norm(B, axis=1, keepdims=True) + 1e-9)\n",
        "    return A @ B.T\n",
        "\n",
        "def _exact_assignment_maxsim(S: np.ndarray) -> np.ndarray:\n",
        "    import itertools\n",
        "    N = S.shape[0]\n",
        "    best = None\n",
        "    best_val = -1e18\n",
        "    for perm in itertools.permutations(range(N)):\n",
        "        val = 0.0\n",
        "        for i in range(N):\n",
        "            val += S[i, perm[i]]\n",
        "        if val > best_val:\n",
        "            best_val = val\n",
        "            best = perm\n",
        "    return np.asarray(best, dtype=np.int64)\n",
        "\n",
        "def _greedy_assignment_maxsim(S: np.ndarray) -> np.ndarray:\n",
        "    N = S.shape[0]\n",
        "    used_r = set(); used_c = set()\n",
        "    perm = -np.ones((N,), dtype=np.int64)\n",
        "    pairs = [(S[i, j], i, j) for i in range(N) for j in range(N)]\n",
        "    pairs.sort(reverse=True, key=lambda x: x[0])\n",
        "    for s, i, j in pairs:\n",
        "        if i in used_r or j in used_c:\n",
        "            continue\n",
        "        perm[i] = j\n",
        "        used_r.add(i); used_c.add(j)\n",
        "        if len(used_r) == N:\n",
        "            break\n",
        "    for i in range(N):\n",
        "        if perm[i] < 0:\n",
        "            for j in range(N):\n",
        "                if j not in used_c:\n",
        "                    perm[i] = j\n",
        "                    used_c.add(j)\n",
        "                    break\n",
        "    return perm\n",
        "\n",
        "def match_rows_by_cosine(A: np.ndarray, B: np.ndarray) -> tuple[np.ndarray, float, float]:\n",
        "    S = _cosine_sim_matrix(A, B)\n",
        "    N = S.shape[0]\n",
        "    perm = _exact_assignment_maxsim(S) if N <= 8 else _greedy_assignment_maxsim(S)\n",
        "    sims = np.asarray([S[i, perm[i]] for i in range(N)], dtype=np.float64)\n",
        "    return perm, float(sims.mean()), float(sims.min())\n",
        "\n",
        "# ----------------------------\n",
        "# Routing inertia curve (per-head, CPU-friendly)\n",
        "# ----------------------------\n",
        "def routing_inertia_curve_cpu(p_h_t_k: np.ndarray, max_lag: int = 64, eps: float = 1e-8) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    p_h_t_k: [H,T,K] (already probs)\n",
        "    Returns [H,L] where L=min(max_lag,T-1): mean cosine over t for each head.\n",
        "    \"\"\"\n",
        "    H, T, K = p_h_t_k.shape\n",
        "    if T < 2:\n",
        "        return np.zeros((H, 0), dtype=np.float32)\n",
        "    L = int(min(max_lag, T - 1))\n",
        "\n",
        "    pn = p_h_t_k / (np.linalg.norm(p_h_t_k, axis=-1, keepdims=True) + eps)  # [H,T,K]\n",
        "    out = np.zeros((H, L), dtype=np.float32)\n",
        "    for lag in range(1, L + 1):\n",
        "        a = pn[:, :-lag, :]\n",
        "        b = pn[:,  lag:, :]\n",
        "        out[:, lag - 1] = (a * b).sum(axis=-1).mean(axis=-1)\n",
        "    return out\n",
        "\n",
        "# ----------------------------\n",
        "# Head role features (SCHEMA-LOCKED, fixed D)\n",
        "# ----------------------------\n",
        "FEATURE_KEYS = [\n",
        "    \"inertia_d1\", \"inertia_d2\", \"inertia_d4\", \"inertia_d8\", \"inertia_d16\", \"inertia_d32\", \"inertia_d64\",\n",
        "    \"inertia_mean\", \"inertia_slope\",\n",
        "    \"ent_mean\", \"eff_slots\", \"maxp_mean\", \"top2_mass\", \"top4_mass\",\n",
        "    \"usage_ent\", \"argmax_fliprate\",\n",
        "    \"gini\",\n",
        "]\n",
        "\n",
        "def _gini(x, eps=1e-9):\n",
        "    # x: [...,K] nonneg\n",
        "    xs = np.sort(x, axis=-1)\n",
        "    K = xs.shape[-1]\n",
        "    idx = np.arange(1, K+1, dtype=np.float64)\n",
        "    num = np.sum((2*idx - K - 1.0) * xs, axis=-1)\n",
        "    den = (K * (np.sum(xs, axis=-1) + eps))\n",
        "    return (num / den).astype(np.float32)\n",
        "\n",
        "def build_head_role_features_locked(rw_bhtk: torch.Tensor) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    rw: [B,H,T,K] (often CPU already)\n",
        "    Returns: [H,D] numpy, L2-normalized, D fixed by FEATURE_KEYS\n",
        "    \"\"\"\n",
        "    eps = 1e-9\n",
        "    rw = rw_bhtk.detach().float().cpu().numpy()  # [B,H,T,K]\n",
        "    B, H, T, K = rw.shape\n",
        "\n",
        "    p = rw / (rw.sum(axis=-1, keepdims=True) + eps)        # [B,H,T,K]\n",
        "    p_htk = p.mean(axis=0)                                 # [H,T,K]\n",
        "\n",
        "    # entropy / concentration\n",
        "    ent = -(p_htk * np.log(np.clip(p_htk, eps, None))).sum(axis=-1)  # [H,T]\n",
        "    ent_mean = ent.mean(axis=-1)                                     # [H]\n",
        "    eff_slots = np.exp(ent_mean).astype(np.float32)                  # [H]\n",
        "    maxp_mean = p_htk.max(axis=-1).mean(axis=-1).astype(np.float32)  # [H]\n",
        "\n",
        "    sorted_p = np.sort(p_htk, axis=-1)\n",
        "    top2_mass = sorted_p[..., -min(2, K):].sum(axis=-1).mean(axis=-1).astype(np.float32)\n",
        "    top4_mass = sorted_p[..., -min(4, K):].sum(axis=-1).mean(axis=-1).astype(np.float32)\n",
        "\n",
        "    usage = p_htk.mean(axis=1)                           # [H,K]\n",
        "    usage = usage / (usage.sum(axis=-1, keepdims=True) + eps)\n",
        "    usage_ent = (-(usage * np.log(np.clip(usage, eps, None))).sum(axis=-1)).astype(np.float32)  # [H]\n",
        "\n",
        "    # argmax fliprate\n",
        "    arg = p_htk.argmax(axis=-1)          # [H,T]\n",
        "    flip = (arg[:, 1:] != arg[:, :-1]).mean(axis=-1).astype(np.float32) if T > 1 else np.zeros((H,), dtype=np.float32)\n",
        "\n",
        "    # inertia\n",
        "    curve = routing_inertia_curve_cpu(p_htk.astype(np.float32), max_lag=MAX_LAG, eps=eps)  # [H,L]\n",
        "    L = curve.shape[1]\n",
        "\n",
        "    def lag_or_zero(d):\n",
        "        if d <= 0 or d > L: return np.zeros((H,), dtype=np.float32)\n",
        "        return curve[:, d-1].astype(np.float32)\n",
        "\n",
        "    inertia_d1  = lag_or_zero(1)\n",
        "    inertia_d2  = lag_or_zero(2)\n",
        "    inertia_d4  = lag_or_zero(4)\n",
        "    inertia_d8  = lag_or_zero(8)\n",
        "    inertia_d16 = lag_or_zero(16)\n",
        "    inertia_d32 = lag_or_zero(32)\n",
        "    inertia_d64 = lag_or_zero(64)\n",
        "    inertia_mean = curve.mean(axis=-1).astype(np.float32) if L > 0 else np.zeros((H,), dtype=np.float32)\n",
        "\n",
        "    if L > 1:\n",
        "        xs = np.arange(1, L+1, dtype=np.float32)\n",
        "        xs = (xs - xs.mean()) / (xs.std() + eps)\n",
        "        ys = curve.astype(np.float32)\n",
        "        ys = (ys - ys.mean(axis=-1, keepdims=True)) / (ys.std(axis=-1, keepdims=True) + eps)\n",
        "        inertia_slope = (ys * xs[None, :]).mean(axis=-1).astype(np.float32)\n",
        "    else:\n",
        "        inertia_slope = np.zeros((H,), dtype=np.float32)\n",
        "\n",
        "    # gini (mean over time)\n",
        "    g = _gini(p_htk, eps=eps).mean(axis=-1).astype(np.float32)  # [H]\n",
        "\n",
        "    key_to_vec = dict(\n",
        "        inertia_d1=inertia_d1, inertia_d2=inertia_d2, inertia_d4=inertia_d4, inertia_d8=inertia_d8,\n",
        "        inertia_d16=inertia_d16, inertia_d32=inertia_d32, inertia_d64=inertia_d64,\n",
        "        inertia_mean=inertia_mean, inertia_slope=inertia_slope,\n",
        "        ent_mean=ent_mean.astype(np.float32),\n",
        "        eff_slots=eff_slots,\n",
        "        maxp_mean=maxp_mean,\n",
        "        top2_mass=top2_mass,\n",
        "        top4_mass=top4_mass,\n",
        "        usage_ent=usage_ent,\n",
        "        argmax_fliprate=flip,\n",
        "        gini=g,\n",
        "    )\n",
        "\n",
        "    feats = np.stack([key_to_vec[k] for k in FEATURE_KEYS], axis=-1)  # [H,D]\n",
        "    feats = feats / (np.linalg.norm(feats, axis=-1, keepdims=True) + 1e-9)\n",
        "    return feats.astype(np.float32)\n",
        "\n",
        "# ----------------------------\n",
        "# Canonicalization: heads across depth (adjacent compose)\n",
        "# ----------------------------\n",
        "def canonicalize_heads_across_layers(head_feat_by_layer: dict[int, np.ndarray]):\n",
        "    layers = sorted(head_feat_by_layer.keys())\n",
        "    if len(layers) == 0:\n",
        "        return {}, {}, []\n",
        "    H = head_feat_by_layer[layers[0]].shape[0]\n",
        "    canon_perm_to_layer = {layers[0]: np.arange(H, dtype=np.int64)}\n",
        "    align_q = {}\n",
        "    for i in range(len(layers) - 1):\n",
        "        la, lb = layers[i], layers[i + 1]\n",
        "        perm_ab, q, qmin = match_rows_by_cosine(head_feat_by_layer[la], head_feat_by_layer[lb])\n",
        "        align_q[(la, lb)] = (q, qmin)\n",
        "        canon_perm_to_layer[lb] = perm_ab[canon_perm_to_layer[la]]\n",
        "    return canon_perm_to_layer, align_q, layers\n",
        "\n",
        "# ----------------------------\n",
        "# Slot descriptors + canonicalization within a head (more informative)\n",
        "# ----------------------------\n",
        "def build_slot_descriptors(rw_bhtk: torch.Tensor, *, max_lag=16) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    rw: [B,H,T,K] probs\n",
        "    Returns desc: [H,K,D] where each slot gets a descriptor:\n",
        "      [usage_k, mean_p_k, inertia_k_lag1, inertia_k_lag4, inertia_k_lag8, ... , usage_vector(K)]\n",
        "    Designed to break slot symmetry under cyclic remaps.\n",
        "    \"\"\"\n",
        "    eps = 1e-9\n",
        "    rw = rw_bhtk.detach().float().cpu().numpy()   # [B,H,T,K]\n",
        "    B, H, T, K = rw.shape\n",
        "    p = rw / (rw.sum(axis=-1, keepdims=True) + eps)          # [B,H,T,K]\n",
        "    p_htk = p.mean(axis=0)                                   # [H,T,K]\n",
        "\n",
        "    usage = p_htk.mean(axis=1)                               # [H,K]\n",
        "    usage = usage / (usage.sum(axis=-1, keepdims=True) + eps)\n",
        "\n",
        "    # slot self-inertia: treat scalar series p_k(t) and correlate with lagged itself\n",
        "    L = int(min(max_lag, T-1)) if T > 1 else 0\n",
        "    use_lags = [d for d in (1, 4, 8, 16) if d <= L]\n",
        "    if len(use_lags) == 0:\n",
        "        use_lags = []\n",
        "\n",
        "    inertia = np.zeros((H, K, len(use_lags)), dtype=np.float32)\n",
        "    if len(use_lags) > 0:\n",
        "        for j, lag in enumerate(use_lags):\n",
        "            a = p_htk[:-0, :-lag, :] if False else p_htk[:, :-lag, :]  # [H,T-lag,K]\n",
        "            b = p_htk[:,  lag:, :]                                     # [H,T-lag,K]\n",
        "            # cosine on scalar series reduces to normalized dot:\n",
        "            # normalize per (H,K) across time\n",
        "            a2 = a - a.mean(axis=1, keepdims=True)\n",
        "            b2 = b - b.mean(axis=1, keepdims=True)\n",
        "            num = (a2 * b2).mean(axis=1)\n",
        "            den = (a2.std(axis=1) * b2.std(axis=1) + eps)\n",
        "            inertia[:, :, j] = (num / den).astype(np.float32)          # [H,K]\n",
        "\n",
        "    # Assemble descriptor: [usage_k, mean_over_time(p_k), inertia_lags..., usage_vector(K)]\n",
        "    mean_pk = p_htk.mean(axis=1).astype(np.float32)                    # [H,K]\n",
        "    parts = [usage[..., None].astype(np.float32), mean_pk[..., None]]\n",
        "    if len(use_lags) > 0:\n",
        "        parts.append(inertia)\n",
        "    # broadcast usage vector into each slot descriptor (context)\n",
        "    uv = np.repeat(usage[:, None, :], repeats=K, axis=1).astype(np.float32)  # [H,K,K]\n",
        "    parts.append(uv)\n",
        "\n",
        "    desc = np.concatenate(parts, axis=-1)  # [H,K,D]\n",
        "    desc = desc / (np.linalg.norm(desc, axis=-1, keepdims=True) + 1e-9)\n",
        "    return desc\n",
        "\n",
        "def canonicalize_slots_between_conditions(descA_hkd: np.ndarray, descB_hkd: np.ndarray):\n",
        "    H, K, D = descA_hkd.shape\n",
        "    perms = np.zeros((H, K), dtype=np.int64)\n",
        "    qs = []\n",
        "    qmins = []\n",
        "    for h in range(H):\n",
        "        perm, q, qmin = match_rows_by_cosine(descA_hkd[h], descB_hkd[h])\n",
        "        perms[h] = perm\n",
        "        qs.append(q); qmins.append(qmin)\n",
        "    return perms, float(np.mean(qs)), float(np.min(qmins))\n",
        "\n",
        "# ----------------------------\n",
        "# Extract read_weights per layer\n",
        "# ----------------------------\n",
        "def extract_rw_by_layer(infos):\n",
        "    out = {}\n",
        "    for li, info in enumerate(infos):\n",
        "        if info is None:\n",
        "            continue\n",
        "        r = info.get(\"read_weights\", None)\n",
        "        if r is None or (not hasattr(r, \"dim\")) or r.dim() != 4:\n",
        "            continue\n",
        "        out[li] = r  # usually CPU due to detach_to_cpu=True\n",
        "    return out\n",
        "\n",
        "# ----------------------------\n",
        "# Run a condition end-to-end\n",
        "# ----------------------------\n",
        "@torch.no_grad()\n",
        "def summarize_condition(model, xb, yb, *, label, rope_reset=False):\n",
        "    if rope_reset:\n",
        "        with RopePosRemap(model, mode=\"reset\", train_T=TRAIN_T):\n",
        "            logits, infos = run_model(model, xb, info_level=INFO_LEVEL, info_cfg=INFO_CFG)\n",
        "    else:\n",
        "        logits, infos = run_model(model, xb, info_level=INFO_LEVEL, info_cfg=INFO_CFG)\n",
        "\n",
        "    nll = streamed_nll_from_logits(logits, yb, chunk_t=LOSS_CHUNK_T, chunk_b=LOSS_CHUNK_B, compute_on_cpu=LOSS_ON_CPU)\n",
        "    ppl = float(np.exp(min(20.0, nll)))\n",
        "\n",
        "    # Proactively free logits now (helps fragmentation)\n",
        "    del logits\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    rw_by_layer = extract_rw_by_layer(infos)\n",
        "    head_feat_by_layer = {}\n",
        "    for l, rw in rw_by_layer.items():\n",
        "        head_feat_by_layer[l] = build_head_role_features_locked(rw)\n",
        "\n",
        "    canon_heads, align_adj, layers = canonicalize_heads_across_layers(head_feat_by_layer)\n",
        "\n",
        "    return dict(\n",
        "        label=label,\n",
        "        mean_nll=float(nll),\n",
        "        ppl=float(ppl),\n",
        "        rw_by_layer=rw_by_layer,\n",
        "        head_feat_by_layer=head_feat_by_layer,\n",
        "        canon_heads=canon_heads,\n",
        "        align_adj=align_adj,\n",
        "        layers=layers,\n",
        "    )\n",
        "\n",
        "# ----------------------------\n",
        "# Main: run 3 conditions on SAME window\n",
        "# ----------------------------\n",
        "xb_long, yb_long = get_same_window(cfg, T=T_LONG, B=BATCH, split=\"val\", device=DEVICE)\n",
        "xb_train = xb_long[:, :TRAIN_T].contiguous()\n",
        "yb_train = yb_long[:, :TRAIN_T].contiguous()\n",
        "\n",
        "print(f\"TRAIN_T={TRAIN_T} | T_LONG={T_LONG} | B={BATCH} | time_stride={INFO_CFG['time_stride']} | LOSS_CHUNK_T={LOSS_CHUNK_T}\")\n",
        "\n",
        "c_train = summarize_condition(model, xb_train, yb_train, label=f\"T={TRAIN_T} (train-length)\", rope_reset=False)\n",
        "c_base  = summarize_condition(model, xb_long,  yb_long,  label=f\"T={T_LONG} baseline\",       rope_reset=False)\n",
        "c_cyc   = summarize_condition(model, xb_long,  yb_long,  label=f\"T={T_LONG} cyclic RoPE (t%{TRAIN_T})\", rope_reset=True)\n",
        "\n",
        "# ----------------------------\n",
        "# Pick reference layer (middle of available layers)\n",
        "# ----------------------------\n",
        "def pick_ref_layer(cond):\n",
        "    Ls = cond[\"layers\"]\n",
        "    return Ls[len(Ls)//2] if Ls else None\n",
        "\n",
        "ref_layer = pick_ref_layer(c_train)\n",
        "if ref_layer is None:\n",
        "    raise RuntimeError(\"No layers with read_weights captured; check ASA plumbing / INFO_CFG.\")\n",
        "\n",
        "# ----------------------------\n",
        "# Cross-condition head match at ref layer (schema-locked => no dim mismatch)\n",
        "# ----------------------------\n",
        "def head_alignment_at_layer(condA, condB, layer):\n",
        "    A = condA[\"head_feat_by_layer\"][layer]\n",
        "    B = condB[\"head_feat_by_layer\"][layer]\n",
        "    perm, q, qmin = match_rows_by_cosine(A, B)\n",
        "    return perm, q, qmin\n",
        "\n",
        "perm_tb, q_tb, qmin_tb = head_alignment_at_layer(c_train, c_base, ref_layer)\n",
        "perm_tc, q_tc, qmin_tc = head_alignment_at_layer(c_train, c_cyc,  ref_layer)\n",
        "\n",
        "# ----------------------------\n",
        "# Slot canonicalization at ref layer (head-aligned first)\n",
        "# ----------------------------\n",
        "def slot_alignment_at_layer(condA, condB, layer, head_perm_A_to_B):\n",
        "    rwA = condA[\"rw_by_layer\"][layer]  # [B,H,T,K]\n",
        "    rwB = condB[\"rw_by_layer\"][layer]  # [B,H,T,K]\n",
        "    _, H, _, K = rwA.shape\n",
        "\n",
        "    descA = build_slot_descriptors(rwA, max_lag=16)  # [H,K,D]\n",
        "    descB = build_slot_descriptors(rwB, max_lag=16)  # [H,K,D]\n",
        "\n",
        "    # reorder B heads to match A heads\n",
        "    descB_aligned = descB[head_perm_A_to_B]\n",
        "\n",
        "    perm_slots_hk, mean_q, min_q = canonicalize_slots_between_conditions(descA, descB_aligned)\n",
        "    return perm_slots_hk, mean_q, min_q, dict(H=H, K=K, D=descA.shape[-1])\n",
        "\n",
        "perm_slots_tb, qslot_tb, qslotmin_tb, hkinfo = slot_alignment_at_layer(c_train, c_base, ref_layer, perm_tb)\n",
        "perm_slots_tc, qslot_tc, qslotmin_tc, _      = slot_alignment_at_layer(c_train, c_cyc,  ref_layer, perm_tc)\n",
        "\n",
        "# ----------------------------\n",
        "# Depth-wise head alignment quality (adjacent transitions)\n",
        "# ----------------------------\n",
        "def depth_alignment_df(cond):\n",
        "    rows = []\n",
        "    for (la, lb), (q, qmin) in cond[\"align_adj\"].items():\n",
        "        rows.append(dict(condition=cond[\"label\"], a=int(la), b=int(lb), transition=f\"{la}->{lb}\", mean=q, min=qmin))\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "df_depth = pd.concat([depth_alignment_df(c_train), depth_alignment_df(c_base), depth_alignment_df(c_cyc)], ignore_index=True)\n",
        "\n",
        "# ----------------------------\n",
        "# Summary tables\n",
        "# ----------------------------\n",
        "df_scores = pd.DataFrame([\n",
        "    dict(condition=c_train[\"label\"], mean_nll=c_train[\"mean_nll\"], ppl=c_train[\"ppl\"]),\n",
        "    dict(condition=c_base[\"label\"],  mean_nll=c_base[\"mean_nll\"],  ppl=c_base[\"ppl\"]),\n",
        "    dict(condition=c_cyc[\"label\"],   mean_nll=c_cyc[\"mean_nll\"],   ppl=c_cyc[\"ppl\"]),\n",
        "]).sort_values(\"ppl\")\n",
        "\n",
        "df_align = pd.DataFrame([\n",
        "    dict(compare=f\"train  baseline @ L{ref_layer}\", head_match_mean=q_tb, head_match_min=qmin_tb, slot_match_mean=qslot_tb, slot_match_min=qslotmin_tb),\n",
        "    dict(compare=f\"train  cyclic   @ L{ref_layer}\", head_match_mean=q_tc, head_match_min=qmin_tc, slot_match_mean=qslot_tc, slot_match_min=qslotmin_tc),\n",
        "])\n",
        "\n",
        "display(df_scores)\n",
        "display(df_align)\n",
        "\n",
        "if not df_depth.empty:\n",
        "    display(df_depth.groupby(\"condition\")[[\"mean\",\"min\"]].mean().reset_index())\n",
        "\n",
        "# ----------------------------\n",
        "# Optional plots\n",
        "# ----------------------------\n",
        "if not df_depth.empty:\n",
        "    plt.figure(figsize=(8.8, 4.2))\n",
        "    for name, g in df_depth.groupby(\"condition\"):\n",
        "        g = g.sort_values(\"a\")\n",
        "        plt.plot(g[\"a\"].values, g[\"mean\"].values, marker=\"o\", linewidth=2, label=name)\n",
        "    plt.xlabel(\"Layer (transition start)\")\n",
        "    plt.ylabel(\"Adjacent-layer head match quality (mean cosine)\")\n",
        "    plt.title(\"Depth-wise canonical head stability (adjacent matching)\")\n",
        "    plt.grid(True, alpha=0.25)\n",
        "    plt.legend(frameon=True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plt.figure(figsize=(7.8, 3.6))\n",
        "plt.bar([0, 1], [qslot_tb, qslot_tc])\n",
        "plt.xticks([0, 1], [f\"trainbaseline @L{ref_layer}\", f\"traincyclic @L{ref_layer}\"], rotation=10)\n",
        "plt.ylim(0, 1.02)\n",
        "plt.ylabel(\"Slot match quality (mean cosine)\")\n",
        "plt.title(\"Canonical slot alignment quality (after head alignment)\")\n",
        "plt.grid(True, axis=\"y\", alpha=0.25)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Ref layer: L{ref_layer} | H={hkinfo['H']} K={hkinfo['K']} | slot_desc_dim={hkinfo['D']} | head_feat_dim={len(FEATURE_KEYS)}\")\n",
        "print(\"Done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wXF_RINGfaNg"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Analysis 2B (DROP-IN): Chunkwise slot retention curves + slot-slot confusion heatmaps (baseline vs cyclic), OOM-safe\n",
        "# Assumes you already ran the prior \"Analysis 2\" cell and have:\n",
        "#   - c_train, c_base, c_cyc (dicts with rw_by_layer, head_feat_by_layer, etc.)\n",
        "#   - ref_layer (int)\n",
        "#   - match_rows_by_cosine (row matcher)\n",
        "#\n",
        "# Produces:\n",
        "#   - df_chunk_slot: chunkwise slot match quality (trainbaseline, traincyclic)\n",
        "#   - heatmaps: slot-slot similarity matrices (baseline vs cyclic) after head alignment\n",
        "#\n",
        "# Notes:\n",
        "#   - Uses the SAME slot descriptor as Analysis 2 (usage+time-bin context), but computed per-chunk.\n",
        "#   - Head alignment uses your existing head features at ref_layer (robust + canonical).\n",
        "#   - Slot alignment is recomputed per chunk to show drift (baseline) vs stability (cyclic).\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ----------------------------\n",
        "# knobs\n",
        "# ----------------------------\n",
        "CHUNK_TOKENS = 1024      # analyze drift per ~train length\n",
        "USE_REF_LAYER = ref_layer\n",
        "\n",
        "# If your rw was time-strided, this cell automatically maps chunk boundaries in the strided index space.\n",
        "# (so chunk count will be approx T_LONG / CHUNK_TOKENS, but based on rw.shape[2] effective T_eff)\n",
        "\n",
        "# ----------------------------\n",
        "# utilities\n",
        "# ----------------------------\n",
        "def _np_l2norm(x, axis=-1, eps=1e-9):\n",
        "    return x / (np.linalg.norm(x, axis=axis, keepdims=True) + eps)\n",
        "\n",
        "def _cosine_sim_matrix(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n",
        "    A = _np_l2norm(A.astype(np.float64), axis=1)\n",
        "    B = _np_l2norm(B.astype(np.float64), axis=1)\n",
        "    return A @ B.T\n",
        "\n",
        "def slot_desc_from_rw_chunk(rw_bhtk: torch.Tensor) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Same spirit as Analysis 2:\n",
        "      descriptor(slot k) = [usage_k, usage_vector (K dims), timebin_usage_vector (K dims)]\n",
        "    where timebin_usage_vector gives a weak 'temporal context' for the chunk.\n",
        "    Returns: [H,K,D]\n",
        "    \"\"\"\n",
        "    eps = 1e-9\n",
        "    rw = rw_bhtk.float()\n",
        "    B, H, T, K = rw.shape\n",
        "\n",
        "    # probs\n",
        "    p = rw / rw.sum(dim=-1, keepdim=True).clamp_min(eps)    # [B,H,T,K]\n",
        "\n",
        "    # usage within chunk\n",
        "    u = p.mean(dim=(0,2))                                   # [H,K]\n",
        "    u = u / u.sum(dim=-1, keepdim=True).clamp_min(eps)\n",
        "\n",
        "    # a tiny temporal \"shape\": split the chunk into 4 bins (in strided time)\n",
        "    nb = 4\n",
        "    edges = torch.linspace(0, T, nb+1, device=rw.device).long()\n",
        "    tb = []\n",
        "    for i in range(nb):\n",
        "        a = int(edges[i].item()); b = int(edges[i+1].item())\n",
        "        if b <= a:\n",
        "            tb.append(u)  # fallback\n",
        "        else:\n",
        "            uu = p[:, :, a:b, :].mean(dim=(0,2))            # [H,K]\n",
        "            uu = uu / uu.sum(dim=-1, keepdim=True).clamp_min(eps)\n",
        "            tb.append(uu)\n",
        "    tb = torch.stack(tb, dim=0).mean(dim=0)                 # [H,K] (avg of per-bin usages)\n",
        "\n",
        "    u_np  = u.detach().cpu().numpy()\n",
        "    tb_np = tb.detach().cpu().numpy()\n",
        "\n",
        "    # build slot descriptors\n",
        "    D = 1 + K + K\n",
        "    desc = np.zeros((H, K, D), dtype=np.float32)\n",
        "    for h in range(H):\n",
        "        for k in range(K):\n",
        "            desc[h,k,0] = u_np[h,k]\n",
        "            desc[h,k,1:1+K] = u_np[h]\n",
        "            desc[h,k,1+K:]  = tb_np[h]\n",
        "    desc = _np_l2norm(desc, axis=-1)\n",
        "    return desc\n",
        "\n",
        "def align_slots_per_head(descA_hkd: np.ndarray, descB_hkd: np.ndarray):\n",
        "    \"\"\"\n",
        "    descA, descB: [H,K,D], align B slots to A slots per head.\n",
        "    Returns:\n",
        "      perms_hk: [H,K], q_mean, q_min\n",
        "    \"\"\"\n",
        "    H, K, D = descA_hkd.shape\n",
        "    perms = np.zeros((H, K), dtype=np.int64)\n",
        "    sims_all = []\n",
        "    for h in range(H):\n",
        "        S = _cosine_sim_matrix(descA_hkd[h], descB_hkd[h])  # [K,K]\n",
        "        # reuse your assignment routine via match_rows_by_cosine if available\n",
        "        perm, q, qmin = match_rows_by_cosine(descA_hkd[h], descB_hkd[h])\n",
        "        perms[h] = perm\n",
        "        sims_all.extend([S[i, perm[i]] for i in range(K)])\n",
        "    sims_all = np.asarray(sims_all, dtype=np.float64)\n",
        "    return perms, float(sims_all.mean()), float(sims_all.min())\n",
        "\n",
        "def apply_head_perm_desc(desc_hkd: np.ndarray, head_perm_A_to_B: np.ndarray) -> np.ndarray:\n",
        "    # reorder B heads to line up with A heads\n",
        "    return desc_hkd[head_perm_A_to_B]\n",
        "\n",
        "def head_perm_at_layer(condA, condB, layer: int) -> np.ndarray:\n",
        "    A = condA[\"head_feat_by_layer\"][layer]\n",
        "    B = condB[\"head_feat_by_layer\"][layer]\n",
        "    perm, q, qmin = match_rows_by_cosine(A, B)\n",
        "    return perm\n",
        "\n",
        "# ----------------------------\n",
        "# gather tensors\n",
        "# ----------------------------\n",
        "rw_train = c_train[\"rw_by_layer\"][USE_REF_LAYER]  # [B,H,Ttrain_eff,K]\n",
        "rw_base  = c_base[\"rw_by_layer\"][USE_REF_LAYER]   # [B,H,Tlong_eff,K]\n",
        "rw_cyc   = c_cyc[\"rw_by_layer\"][USE_REF_LAYER]    # [B,H,Tlong_eff,K]\n",
        "\n",
        "# sanity\n",
        "B0, H, Tt_eff, K = rw_train.shape\n",
        "B1, H1, Tb_eff, K1 = rw_base.shape\n",
        "B2, H2, Tc_eff, K2 = rw_cyc.shape\n",
        "assert H == H1 == H2 and K == K1 == K2, \"H/K mismatch across conditions at ref layer.\"\n",
        "\n",
        "print(f\"[ref layer L{USE_REF_LAYER}] rw shapes:\")\n",
        "print(f\"  train: {tuple(rw_train.shape)}  baseline: {tuple(rw_base.shape)}  cyclic: {tuple(rw_cyc.shape)}\")\n",
        "\n",
        "# ----------------------------\n",
        "# head alignment (train -> long)\n",
        "# ----------------------------\n",
        "perm_tb = head_perm_at_layer(c_train, c_base, USE_REF_LAYER)\n",
        "perm_tc = head_perm_at_layer(c_train, c_cyc,  USE_REF_LAYER)\n",
        "\n",
        "# ----------------------------\n",
        "# chunk mapping in effective time coordinates\n",
        "# ----------------------------\n",
        "# Each rw time index corresponds to original tokens spaced by INFO_CFG['time_stride'].\n",
        "time_stride = int(INFO_CFG.get(\"time_stride\", 1))\n",
        "chunk_eff = max(1, CHUNK_TOKENS // time_stride)  # chunk size in rw time axis\n",
        "n_chunks = int(np.ceil(Tb_eff / chunk_eff))\n",
        "\n",
        "print(f\"time_stride={time_stride} => chunk_eff={chunk_eff} steps (~{CHUNK_TOKENS} tokens). n_chunks={n_chunks}\")\n",
        "\n",
        "# ----------------------------\n",
        "# per-chunk slot retention curves\n",
        "# ----------------------------\n",
        "rows = []\n",
        "for ci in range(n_chunks):\n",
        "    t0 = ci * chunk_eff\n",
        "    t1 = min(Tb_eff, (ci + 1) * chunk_eff)\n",
        "    if t1 - t0 < 2:\n",
        "        continue\n",
        "\n",
        "    # train descriptor: always computed on full train window (reference basis)\n",
        "    # (this is intentional: measure how well long-window chunk maps back to canonical train slots)\n",
        "    desc_train = slot_desc_from_rw_chunk(rw_train)  # [H,K,D] small\n",
        "\n",
        "    # baseline chunk\n",
        "    desc_base = slot_desc_from_rw_chunk(rw_base[:, :, t0:t1, :])\n",
        "    desc_base = apply_head_perm_desc(desc_base, perm_tb)\n",
        "    _, q_b, qmin_b = align_slots_per_head(desc_train, desc_base)\n",
        "\n",
        "    # cyclic chunk\n",
        "    desc_cyc = slot_desc_from_rw_chunk(rw_cyc[:, :, t0:t1, :])\n",
        "    desc_cyc = apply_head_perm_desc(desc_cyc, perm_tc)\n",
        "    _, q_c, qmin_c = align_slots_per_head(desc_train, desc_cyc)\n",
        "\n",
        "    center_tok = int((0.5 * (t0 + t1)) * time_stride)\n",
        "\n",
        "    rows.append(dict(\n",
        "        chunk=ci,\n",
        "        t0_eff=t0, t1_eff=t1,\n",
        "        center_token=center_tok,\n",
        "        slot_match_mean_train_base=q_b,\n",
        "        slot_match_min_train_base=qmin_b,\n",
        "        slot_match_mean_train_cyc=q_c,\n",
        "        slot_match_min_train_cyc=qmin_c,\n",
        "    ))\n",
        "\n",
        "df_chunk_slot = pd.DataFrame(rows)\n",
        "display(df_chunk_slot)\n",
        "\n",
        "# Plot: mean slot retention vs position\n",
        "if not df_chunk_slot.empty:\n",
        "    plt.figure(figsize=(9.0, 4.4))\n",
        "    plt.plot(df_chunk_slot[\"center_token\"].values, df_chunk_slot[\"slot_match_mean_train_base\"].values,\n",
        "             marker=\"o\", linewidth=2, label=\"trainbaseline (per-chunk)\")\n",
        "    plt.plot(df_chunk_slot[\"center_token\"].values, df_chunk_slot[\"slot_match_mean_train_cyc\"].values,\n",
        "             marker=\"o\", linewidth=2, label=\"traincyclic (per-chunk)\")\n",
        "    plt.ylim(-0.2, 1.02)\n",
        "    plt.xlabel(\"Absolute position (chunk center, token index)\")\n",
        "    plt.ylabel(\"Slot match quality (mean cosine)\")\n",
        "    plt.title(f\"Chunkwise slot identity retention @ L{USE_REF_LAYER} (after head alignment)\")\n",
        "    plt.grid(True, alpha=0.25)\n",
        "    plt.legend(frameon=True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot: min slot retention vs position (highlights worst-case flips)\n",
        "if not df_chunk_slot.empty:\n",
        "    plt.figure(figsize=(9.0, 4.4))\n",
        "    plt.plot(df_chunk_slot[\"center_token\"].values, df_chunk_slot[\"slot_match_min_train_base\"].values,\n",
        "             marker=\"o\", linewidth=2, label=\"trainbaseline min\")\n",
        "    plt.plot(df_chunk_slot[\"center_token\"].values, df_chunk_slot[\"slot_match_min_train_cyc\"].values,\n",
        "             marker=\"o\", linewidth=2, label=\"traincyclic min\")\n",
        "    plt.ylim(-1.02, 1.02)\n",
        "    plt.xlabel(\"Absolute position (chunk center, token index)\")\n",
        "    plt.ylabel(\"Slot match quality (min cosine)\")\n",
        "    plt.title(f\"Chunkwise WORST-CASE slot match @ L{USE_REF_LAYER} (after head alignment)\")\n",
        "    plt.grid(True, alpha=0.25)\n",
        "    plt.legend(frameon=True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ----------------------------\n",
        "# slot-slot confusion heatmaps (global, at ref layer)\n",
        "# ----------------------------\n",
        "# Build global descriptors (whole sequences)\n",
        "desc_train_full = slot_desc_from_rw_chunk(rw_train)  # [H,K,D]\n",
        "\n",
        "desc_base_full = slot_desc_from_rw_chunk(rw_base)\n",
        "desc_base_full = apply_head_perm_desc(desc_base_full, perm_tb)\n",
        "\n",
        "desc_cyc_full = slot_desc_from_rw_chunk(rw_cyc)\n",
        "desc_cyc_full = apply_head_perm_desc(desc_cyc_full, perm_tc)\n",
        "\n",
        "# Compute averaged slot-slot similarity across heads:\n",
        "def mean_slot_confusion(descA_hkd, descB_hkd):\n",
        "    H, K, D = descA_hkd.shape\n",
        "    mats = []\n",
        "    for h in range(H):\n",
        "        mats.append(_cosine_sim_matrix(descA_hkd[h], descB_hkd[h]))  # [K,K]\n",
        "    return np.mean(np.stack(mats, axis=0), axis=0)  # [K,K]\n",
        "\n",
        "S_base = mean_slot_confusion(desc_train_full, desc_base_full)\n",
        "S_cyc  = mean_slot_confusion(desc_train_full, desc_cyc_full)\n",
        "\n",
        "def plot_confusion(S, title):\n",
        "    plt.figure(figsize=(6.2, 5.3))\n",
        "    plt.imshow(S, aspect=\"auto\", interpolation=\"nearest\", vmin=-1.0, vmax=1.0)\n",
        "    plt.colorbar(label=\"Cosine similarity\")\n",
        "    plt.xticks(range(K), [f\"slot{j}\" for j in range(K)], rotation=45, ha=\"right\")\n",
        "    plt.yticks(range(K), [f\"slot{i}\" for i in range(K)])\n",
        "    plt.xlabel(\"Long-window slot (after head align)\")\n",
        "    plt.ylabel(\"Train slot\")\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_confusion(S_base, f\"Slot-slot confusion @ L{USE_REF_LAYER}: train vs baseline (mean over heads)\")\n",
        "plot_confusion(S_cyc,  f\"Slot-slot confusion @ L{USE_REF_LAYER}: train vs cyclic RoPE (mean over heads)\")\n",
        "\n",
        "# Quick scalar summaries (diagonal dominance)\n",
        "def diag_stats(S):\n",
        "    diag = np.diag(S)\n",
        "    off = S.copy()\n",
        "    np.fill_diagonal(off, np.nan)\n",
        "    return dict(\n",
        "        diag_mean=float(np.nanmean(diag)),\n",
        "        diag_min=float(np.nanmin(diag)),\n",
        "        off_mean=float(np.nanmean(off)),\n",
        "        off_max=float(np.nanmax(off)),\n",
        "        margin=float(np.nanmean(diag) - np.nanmean(off)),\n",
        "    )\n",
        "\n",
        "stats = pd.DataFrame([\n",
        "    dict(condition=\"train vs baseline\", **diag_stats(S_base)),\n",
        "    dict(condition=\"train vs cyclic\",   **diag_stats(S_cyc)),\n",
        "])\n",
        "display(stats)\n",
        "\n",
        "print(\"Done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7GfqT04w44bq"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#@title ASA-aware Sampling Lab v2 (DROP-IN): stable jerk metric + ASA margin gating + bidirectional controller\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ----------------------------\n",
        "# User knobs\n",
        "# ----------------------------\n",
        "USE_XB_PROMPT = True\n",
        "\n",
        "PROMPT_B = 1\n",
        "PROMPT_LEN = 256\n",
        "MAX_NEW_TOKENS = 256\n",
        "CTX_WINDOW = 2048\n",
        "\n",
        "ASA_SAMPLER_INFO_CFG = dict(\n",
        "    store_read_weights=True,\n",
        "    store_read_logits=False,\n",
        "    store_write_logits=False,\n",
        "    store_slot_state_norm=False,\n",
        "    detach_to_cpu=False,\n",
        "    time_stride=1,\n",
        "    batch_stride=1,\n",
        ")\n",
        "INFO_LEVEL = \"basic\"\n",
        "\n",
        "LAYER_PICK = \"mid\"   # \"mid\" | \"first\" | \"last\" | int\n",
        "\n",
        "# ----------------------------\n",
        "# Baseline sampling params\n",
        "# ----------------------------\n",
        "BASE_TEMP = 0.9\n",
        "BASE_TOP_P = 0.9\n",
        "BASE_TOP_K = 0\n",
        "\n",
        "# ----------------------------\n",
        "# ASA-adaptive bounds\n",
        "# ----------------------------\n",
        "TEMP_MIN = 0.35\n",
        "TEMP_MAX = 1.25\n",
        "TOPP_MIN = 0.60\n",
        "TOPP_MAX = 0.97\n",
        "\n",
        "# ----------------------------\n",
        "# Controller weights\n",
        "# ----------------------------\n",
        "# Uncertainty signals (cool down)\n",
        "W_ENT   = 0.9     # routing entropy\n",
        "W_FLIP  = 1.0     # argmax flip rate\n",
        "W_EFF   = 0.6     # effective slots proxy\n",
        "W_JERK  = 0.35    # jerk metric (bounded JSD)\n",
        "\n",
        "# Confidence signals (warm up)\n",
        "W_INERT = 0.9     # inertia (cosine)\n",
        "W_MARG  = 1.1     # routing margin (max - second max)\n",
        "\n",
        "ADAPT_STRENGTH = 1.15\n",
        "\n",
        "# EMA smoothing\n",
        "EMA_BETA = 0.92\n",
        "\n",
        "# Jerk metric config (bounded + cheap)\n",
        "JERK_TOPM = 256     # compute JSD over top-M tokens only\n",
        "JERK_CAP = 0.50     # cap jerk contribution (JSD is in [0, ln2] ~ [0,0.693])\n",
        "\n",
        "# ----------------------------\n",
        "# Utilities: nucleus/top-k\n",
        "# ----------------------------\n",
        "def top_k_logits(logits, k: int):\n",
        "    if k <= 0:\n",
        "        return logits\n",
        "    v, _ = torch.topk(logits, k)\n",
        "    thresh = v[..., -1, None]\n",
        "    return logits.masked_fill(logits < thresh, float(\"-inf\"))\n",
        "\n",
        "def top_p_logits(logits, p: float):\n",
        "    if p >= 1.0:\n",
        "        return logits\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "    sorted_probs, sorted_idx = torch.sort(probs, descending=True, dim=-1)\n",
        "    cdf = torch.cumsum(sorted_probs, dim=-1)\n",
        "    mask = cdf > p\n",
        "    mask[..., 0] = False\n",
        "    sorted_logits = logits.gather(-1, sorted_idx)\n",
        "    sorted_logits = sorted_logits.masked_fill(mask, float(\"-inf\"))\n",
        "    unsorted = torch.empty_like(sorted_logits).scatter_(-1, sorted_idx, sorted_logits)\n",
        "    return unsorted\n",
        "\n",
        "# ----------------------------\n",
        "# ASA layer picker + metrics\n",
        "# ----------------------------\n",
        "def pick_layer_index(infos, pick=\"mid\"):\n",
        "    valid = [i for i, info in enumerate(infos)\n",
        "             if (info is not None and isinstance(info, dict) and info.get(\"read_weights\", None) is not None)]\n",
        "    if not valid:\n",
        "        return None\n",
        "    if isinstance(pick, int):\n",
        "        return pick if pick in valid else valid[len(valid)//2]\n",
        "    if pick == \"first\":\n",
        "        return valid[0]\n",
        "    if pick == \"last\":\n",
        "        return valid[-1]\n",
        "    return valid[len(valid)//2]\n",
        "\n",
        "@torch.no_grad()\n",
        "def asa_lasttoken_stats(rw_bhtk, prev_p_bhk=None, prev_arg_bh=None, eps=1e-9):\n",
        "    \"\"\"\n",
        "    rw: [B,H,T,K]\n",
        "    returns dict of scalars + updated prevs\n",
        "    \"\"\"\n",
        "    p = rw_bhtk[:, :, -1, :].float()                    # [B,H,K]\n",
        "    p = p / p.sum(dim=-1, keepdim=True).clamp_min(eps)\n",
        "    B, H, K = p.shape\n",
        "\n",
        "    # entropy (normalized)\n",
        "    ent = -(p * torch.log(p.clamp_min(eps))).sum(dim=-1)  # [B,H]\n",
        "    ent_norm = (ent / math.log(K)).mean().item()\n",
        "\n",
        "    # effective slots proxy: exp(E[H(p)]) (mean over BH)\n",
        "    eff = torch.exp(ent).mean().item()\n",
        "\n",
        "    # margin: max - 2nd max (mean over BH)\n",
        "    top2 = torch.topk(p, k=min(2, K), dim=-1).values  # [B,H,2]\n",
        "    margin = (top2[..., 0] - top2[..., 1]).mean().item() if K >= 2 else 1.0\n",
        "\n",
        "    # inertia: cosine(p_t, p_{t-1})\n",
        "    if prev_p_bhk is None:\n",
        "        inertia = 1.0\n",
        "        prev_p_bhk = p\n",
        "    else:\n",
        "        a = p / p.norm(dim=-1, keepdim=True).clamp_min(eps)\n",
        "        b = prev_p_bhk / prev_p_bhk.norm(dim=-1, keepdim=True).clamp_min(eps)\n",
        "        inertia = (a * b).sum(dim=-1).mean().item()\n",
        "        prev_p_bhk = p\n",
        "\n",
        "    # flip rate: argmax changes across heads\n",
        "    arg = p.argmax(dim=-1)  # [B,H]\n",
        "    if prev_arg_bh is None:\n",
        "        flip = 0.0\n",
        "        prev_arg_bh = arg\n",
        "    else:\n",
        "        flip = (arg != prev_arg_bh).float().mean().item()\n",
        "        prev_arg_bh = arg\n",
        "\n",
        "    return dict(ent_norm=ent_norm, eff_slots=eff, margin=margin, inertia=inertia, flip=flip,\n",
        "                H=H, K=K), prev_p_bhk, prev_arg_bh\n",
        "\n",
        "# ----------------------------\n",
        "# Bounded jerk: JSD over top-M support\n",
        "# ----------------------------\n",
        "@torch.no_grad()\n",
        "def jsd_topm(logits_new_bv, logits_old_bv, topm=256, eps=1e-9):\n",
        "    \"\"\"\n",
        "    Jensen-Shannon divergence between distributions induced by logits, computed on union top-M support.\n",
        "    Returns scalar in [0, ln2] ~ [0,0.693]\n",
        "    \"\"\"\n",
        "    B, V = logits_new_bv.shape\n",
        "    m = min(int(topm), V)\n",
        "\n",
        "    # union support indices per batch: take topm of each, then union\n",
        "    idx_new = torch.topk(logits_new_bv, k=m, dim=-1).indices\n",
        "    idx_old = torch.topk(logits_old_bv, k=m, dim=-1).indices\n",
        "    idx = torch.cat([idx_new, idx_old], dim=-1)  # [B,2m]\n",
        "    # unique per row (small), but torch.unique on each row is annoying; we'll just keep 2m and accept duplicates via gather+scatter\n",
        "    # We'll build dense probs on this set by softmax then renorm on gathered entries.\n",
        "    ln_new = logits_new_bv.gather(-1, idx)\n",
        "    ln_old = logits_old_bv.gather(-1, idx)\n",
        "\n",
        "    p = torch.softmax(ln_new, dim=-1)\n",
        "    q = torch.softmax(ln_old, dim=-1)\n",
        "\n",
        "    m_mix = 0.5 * (p + q)\n",
        "    # KL(p||m) + KL(q||m)\n",
        "    kl_pm = (p * (torch.log(p.clamp_min(eps)) - torch.log(m_mix.clamp_min(eps)))).sum(dim=-1)\n",
        "    kl_qm = (q * (torch.log(q.clamp_min(eps)) - torch.log(m_mix.clamp_min(eps)))).sum(dim=-1)\n",
        "    jsd = 0.5 * (kl_pm + kl_qm)\n",
        "    return jsd.mean().item()\n",
        "\n",
        "# ----------------------------\n",
        "# Forward helper\n",
        "# ----------------------------\n",
        "@torch.no_grad()\n",
        "def forward_full(model, x_bt):\n",
        "    model.eval()\n",
        "    try:\n",
        "        logits, infos = model(\n",
        "            x_bt,\n",
        "            attention_mask=None,\n",
        "            return_info=True,\n",
        "            info_level=INFO_LEVEL,\n",
        "            info_cfg=ASA_SAMPLER_INFO_CFG,\n",
        "        )\n",
        "    except TypeError:\n",
        "        logits, infos = model(x_bt, attention_mask=None, return_info=True)\n",
        "    return logits, infos\n",
        "\n",
        "# ----------------------------\n",
        "# Controller\n",
        "# ----------------------------\n",
        "def ema(old, new, beta=0.92):\n",
        "    return float(new) if old is None else float(beta * old + (1.0 - beta) * new)\n",
        "\n",
        "def clamp01(x):\n",
        "    return float(max(0.0, min(1.0, x)))\n",
        "\n",
        "def adapt_params_v2(base_temp, base_top_p, s):\n",
        "    \"\"\"\n",
        "    s is dict of EMA'd signals:\n",
        "      ent_norm_ema in [0,1] (higher => uncertainty)\n",
        "      inertia_ema in [-1,1] (higher => stable)\n",
        "      flip_ema in [0,1] (higher => unstable)\n",
        "      margin_ema in [0,1-ish] (higher => confident)\n",
        "      eff_slots_ema in [1,K] (higher => uncertain / spread)\n",
        "      jerk_ema in [0, ln2]\n",
        "    \"\"\"\n",
        "    ent = clamp01(s[\"ent_norm_ema\"])\n",
        "    # inertia [-1,1] -> [0,1]\n",
        "    inert = clamp01(0.5 * (s[\"inertia_ema\"] + 1.0))\n",
        "    flip = clamp01(s[\"flip_ema\"])\n",
        "    # margin is usually small-ish; map through a saturating function\n",
        "    margin = float(s[\"margin_ema\"])\n",
        "    marg01 = clamp01(margin / 0.35)  # 0.35 is a decent \"pretty confident\" heuristic; adjust later\n",
        "\n",
        "    eff = float(s[\"eff_slots_ema\"])\n",
        "    K = max(2.0, float(s.get(\"K\", 16)))\n",
        "    eff01 = clamp01((eff - 1.0) / (K - 1.0))  # 1 => deterministic, K => uniform\n",
        "\n",
        "    jerk = float(s.get(\"jerk_ema\", 0.0))\n",
        "    jerk01 = clamp01(min(jerk, JERK_CAP) / JERK_CAP)\n",
        "\n",
        "    warm = (W_INERT * inert + W_MARG * marg01)\n",
        "    cool = (W_ENT * ent + W_FLIP * flip + W_EFF * eff01 + W_JERK * jerk01)\n",
        "    score = ADAPT_STRENGTH * (warm - cool)\n",
        "\n",
        "    # Turn score into a smooth multiplier.\n",
        "    # Positive => warm up; Negative => cool down.\n",
        "    m = math.tanh(score)  # [-1,1]\n",
        "\n",
        "    temp = base_temp * (1.0 + 0.55 * m)\n",
        "    topp = base_top_p * (1.0 + 0.22 * m)\n",
        "\n",
        "    temp = float(np.clip(temp, TEMP_MIN, TEMP_MAX))\n",
        "    topp = float(np.clip(topp, TOPP_MIN, TOPP_MAX))\n",
        "    return temp, topp, dict(ent=ent, inert=inert, flip=flip, marg01=marg01, eff01=eff01, jerk01=jerk01, score=score, m=m)\n",
        "\n",
        "# ----------------------------\n",
        "# Sampler\n",
        "# ----------------------------\n",
        "@torch.no_grad()\n",
        "def generate(model, prompt_bt, *, max_new, ctx_window, mode=\"baseline\",\n",
        "             base_temp=0.9, base_top_p=0.9, base_top_k=0):\n",
        "    x = prompt_bt.to(DEVICE)\n",
        "    prev_p_bhk = None\n",
        "    prev_arg_bh = None\n",
        "    prev_logits = None\n",
        "\n",
        "    st = dict(ent=None, inertia=None, flip=None, margin=None, eff=None, jerk=None)\n",
        "    logs = []\n",
        "\n",
        "    for step in range(int(max_new)):\n",
        "        x_ctx = x[:, -ctx_window:].contiguous()\n",
        "        logits_btv, infos = forward_full(model, x_ctx)\n",
        "        next_logits = logits_btv[:, -1, :]  # [B,V]\n",
        "\n",
        "        layer = pick_layer_index(infos, LAYER_PICK)\n",
        "        if layer is None:\n",
        "            # no ASA info; fall back to baseline\n",
        "            temp = base_temp\n",
        "            topp = base_top_p\n",
        "            ctrl = {}\n",
        "            ent = inert = flip = margin = eff = np.nan\n",
        "            H = K = -1\n",
        "        else:\n",
        "            rw = infos[layer][\"read_weights\"]\n",
        "            stats, prev_p_bhk, prev_arg_bh = asa_lasttoken_stats(rw, prev_p_bhk, prev_arg_bh)\n",
        "            ent, inert, flip = stats[\"ent_norm\"], stats[\"inertia\"], stats[\"flip\"]\n",
        "            margin, eff = stats[\"margin\"], stats[\"eff_slots\"]\n",
        "            H, K = stats[\"H\"], stats[\"K\"]\n",
        "\n",
        "            # jerk via bounded JSD on top-M support of logits\n",
        "            if prev_logits is None:\n",
        "                jerk = 0.0\n",
        "            else:\n",
        "                jerk = jsd_topm(next_logits, prev_logits, topm=JERK_TOPM)\n",
        "            prev_logits = next_logits.detach()\n",
        "\n",
        "            # EMA\n",
        "            st[\"ent\"] = ema(st[\"ent\"], ent, EMA_BETA)\n",
        "            st[\"inertia\"] = ema(st[\"inertia\"], inert, EMA_BETA)\n",
        "            st[\"flip\"] = ema(st[\"flip\"], flip, EMA_BETA)\n",
        "            st[\"margin\"] = ema(st[\"margin\"], margin, EMA_BETA)\n",
        "            st[\"eff\"] = ema(st[\"eff\"], eff, EMA_BETA)\n",
        "            st[\"jerk\"] = ema(st[\"jerk\"], jerk, EMA_BETA)\n",
        "\n",
        "            if mode == \"baseline\":\n",
        "                temp = base_temp\n",
        "                topp = base_top_p\n",
        "                ctrl = {}\n",
        "            else:\n",
        "                s = dict(\n",
        "                    ent_norm_ema=st[\"ent\"],\n",
        "                    inertia_ema=st[\"inertia\"],\n",
        "                    flip_ema=st[\"flip\"],\n",
        "                    margin_ema=st[\"margin\"],\n",
        "                    eff_slots_ema=st[\"eff\"],\n",
        "                    jerk_ema=st[\"jerk\"],\n",
        "                    K=K,\n",
        "                )\n",
        "                temp, topp, ctrl = adapt_params_v2(base_temp, base_top_p, s)\n",
        "\n",
        "        # sample\n",
        "        logits = next_logits / max(1e-6, float(temp))\n",
        "        logits = top_k_logits(logits, int(base_top_k))\n",
        "        logits = top_p_logits(logits, float(topp))\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        nxt = torch.multinomial(probs, num_samples=1)\n",
        "        x = torch.cat([x, nxt], dim=1)\n",
        "\n",
        "        logs.append(dict(\n",
        "            step=step, layer=int(layer if layer is not None else -1),\n",
        "            H=int(H), K=int(K),\n",
        "            ent_norm=float(ent) if not np.isnan(ent) else np.nan,\n",
        "            inertia=float(inert) if not np.isnan(inert) else np.nan,\n",
        "            flip=float(flip) if not np.isnan(flip) else np.nan,\n",
        "            margin=float(margin) if not np.isnan(margin) else np.nan,\n",
        "            eff_slots=float(eff) if not np.isnan(eff) else np.nan,\n",
        "            jerk=float(st[\"jerk\"]) if st[\"jerk\"] is not None else np.nan,\n",
        "            ent_ema=float(st[\"ent\"]) if st[\"ent\"] is not None else np.nan,\n",
        "            inertia_ema=float(st[\"inertia\"]) if st[\"inertia\"] is not None else np.nan,\n",
        "            flip_ema=float(st[\"flip\"]) if st[\"flip\"] is not None else np.nan,\n",
        "            margin_ema=float(st[\"margin\"]) if st[\"margin\"] is not None else np.nan,\n",
        "            eff_ema=float(st[\"eff\"]) if st[\"eff\"] is not None else np.nan,\n",
        "            temp=float(temp), top_p=float(topp),\n",
        "            ctrl_score=float(ctrl.get(\"score\", np.nan)),\n",
        "            ctrl_m=float(ctrl.get(\"m\", np.nan)),\n",
        "            ctrl_marg01=float(ctrl.get(\"marg01\", np.nan)),\n",
        "            ctrl_ent=float(ctrl.get(\"ent\", np.nan)),\n",
        "            ctrl_inert=float(ctrl.get(\"inert\", np.nan)),\n",
        "            ctrl_flip=float(ctrl.get(\"flip\", np.nan)),\n",
        "            ctrl_eff01=float(ctrl.get(\"eff01\", np.nan)),\n",
        "            ctrl_jerk01=float(ctrl.get(\"jerk01\", np.nan)),\n",
        "        ))\n",
        "\n",
        "    return x, pd.DataFrame(logs)\n",
        "\n",
        "# ----------------------------\n",
        "# Prompt\n",
        "# ----------------------------\n",
        "if USE_XB_PROMPT:\n",
        "    try:\n",
        "        xb\n",
        "    except NameError:\n",
        "        raise NameError(\"USE_XB_PROMPT=True but xb is not defined. Define xb or set USE_XB_PROMPT=False and pass prompt_ids.\")\n",
        "    prompt = xb[:PROMPT_B, :PROMPT_LEN].contiguous().long()\n",
        "else:\n",
        "    raise ValueError(\"Set USE_XB_PROMPT=True or define prompt_ids.\")\n",
        "\n",
        "print(f\"Prompt: B={prompt.shape[0]} T0={prompt.shape[1]} | CTX_WINDOW={CTX_WINDOW} | MAX_NEW_TOKENS={MAX_NEW_TOKENS}\")\n",
        "\n",
        "tok_base, df_base = generate(model, prompt, max_new=MAX_NEW_TOKENS, ctx_window=CTX_WINDOW,\n",
        "                             mode=\"baseline\", base_temp=BASE_TEMP, base_top_p=BASE_TOP_P, base_top_k=BASE_TOP_K)\n",
        "tok_adap, df_adap = generate(model, prompt, max_new=MAX_NEW_TOKENS, ctx_window=CTX_WINDOW,\n",
        "                             mode=\"asa_adaptive\", base_temp=BASE_TEMP, base_top_p=BASE_TOP_P, base_top_k=BASE_TOP_K)\n",
        "\n",
        "print(\"Done sampling.\")\n",
        "display(df_base.head(5))\n",
        "display(df_adap.head(5))\n",
        "\n",
        "# ----------------------------\n",
        "# Plots\n",
        "# ----------------------------\n",
        "def plot_series(dfA, dfB, col, title):\n",
        "    plt.figure(figsize=(9.5, 3.4))\n",
        "    plt.plot(dfA[\"step\"], dfA[col], linewidth=2, label=f\"baseline\")\n",
        "    plt.plot(dfB[\"step\"], dfB[col], linewidth=2, label=f\"ASA-adaptive\")\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"step\")\n",
        "    plt.grid(True, alpha=0.25)\n",
        "    plt.legend(frameon=True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_series(df_base, df_adap, \"temp\", \"Temperature schedule\")\n",
        "plot_series(df_base, df_adap, \"top_p\", \"Top-p schedule\")\n",
        "plot_series(df_base, df_adap, \"ent_ema\", \"Routing entropy (EMA, normalized)\")\n",
        "plot_series(df_base, df_adap, \"inertia_ema\", \"Routing inertia (EMA)\")\n",
        "plot_series(df_base, df_adap, \"flip_ema\", \"Routing argmax flip rate (EMA)\")\n",
        "plot_series(df_base, df_adap, \"margin_ema\", \"Routing margin (EMA)\")\n",
        "plot_series(df_base, df_adap, \"eff_ema\", \"Effective slots proxy (EMA)\")\n",
        "plot_series(df_base, df_adap, \"jerk\", \"Bounded jerk (EMA JSD over top-M logits)\")\n",
        "\n",
        "# Controller internals\n",
        "plot_series(df_base, df_adap, \"ctrl_score\", \"Controller score (warm - cool)\")\n",
        "plot_series(df_base, df_adap, \"ctrl_m\", \"Controller tanh(score)\")\n",
        "\n",
        "# Summary stats\n",
        "def summarize(df, name):\n",
        "    return dict(\n",
        "        name=name,\n",
        "        temp_mean=float(np.nanmean(df[\"temp\"])),\n",
        "        temp_min=float(np.nanmin(df[\"temp\"])),\n",
        "        temp_max=float(np.nanmax(df[\"temp\"])),\n",
        "        topp_mean=float(np.nanmean(df[\"top_p\"])),\n",
        "        ent_mean=float(np.nanmean(df[\"ent_norm\"])),\n",
        "        inert_mean=float(np.nanmean(df[\"inertia\"])),\n",
        "        flip_mean=float(np.nanmean(df[\"flip\"])),\n",
        "        margin_mean=float(np.nanmean(df[\"margin\"])),\n",
        "        eff_mean=float(np.nanmean(df[\"eff_slots\"])),\n",
        "        jerk_mean=float(np.nanmean(df[\"jerk\"])),\n",
        "    )\n",
        "\n",
        "df_sum = pd.DataFrame([summarize(df_base, \"baseline\"), summarize(df_adap, \"asa_adaptive\")])\n",
        "display(df_sum)\n",
        "\n",
        "# If you have a tokenizer:\n",
        "# print(tokenizer.decode(tok_adap[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZYKEplr845FQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#@title Qualitative sampling check (baseline vs RoPE-reset)  side-by-side + light ASA stats\n",
        "\n",
        "import math, numpy as np, pandas as pd, torch\n",
        "import torch.nn.functional as F\n",
        "from contextlib import nullcontext\n",
        "from dataclasses import dataclass\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ----------------------------\n",
        "# Assumptions:\n",
        "#   - model, cfg exist\n",
        "#   - tokenizer available as `tokenizer` OR provide your own decode_fn\n",
        "#   - You have ASA info plumbing (optional) for read_weights stats. If not, this still works.\n",
        "# ----------------------------\n",
        "\n",
        "# ----------------------------\n",
        "# Config\n",
        "# ----------------------------\n",
        "TRAIN_T = int(getattr(cfg, \"max_seq_len\", getattr(model, \"training_max_seq_len\", getattr(model, \"max_seq_len\", 256))))\n",
        "print(\"TRAIN_T =\", TRAIN_T)\n",
        "\n",
        "N_SAMPLES       = 4\n",
        "PROMPT_TOKENS   = 4096\n",
        "CTX_WINDOW      = 2048            # sliding window\n",
        "MAX_NEW_TOKENS  = 256\n",
        "TOP_P_BASE      = 0.90\n",
        "TEMP_BASE       = 0.90\n",
        "SEED            = 1337\n",
        "\n",
        "# For the \"rope reset\" condition:\n",
        "ROPE_MODE = \"reset\"               # \"reset\" or \"rand_block\"\n",
        "DISABLE_ALIBI = False             # try True as a 3rd condition if you want\n",
        "\n",
        "# Internals logging (optional)\n",
        "LOG_EVERY = 16                    # steps between metric logs\n",
        "CAPTURE_INFO = True               # set False if your wrapper doesn't support return_info\n",
        "INFO_LEVEL = \"basic\"\n",
        "INFO_CFG = dict(\n",
        "    store_read_weights=True,\n",
        "    store_read_logits=False,\n",
        "    store_write_logits=False,\n",
        "    store_slot_state_norm=False,\n",
        "    detach_to_cpu=True,\n",
        "    time_stride=16,\n",
        "    batch_stride=1,\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# Token decode\n",
        "# ----------------------------\n",
        "def default_decode(ids):\n",
        "    if \"tokenizer\" in globals() and tokenizer is not None:\n",
        "        return tokenizer.decode(ids, skip_special_tokens=True)\n",
        "    # fallback: show token ids\n",
        "    return \" \".join(map(str, ids))\n",
        "\n",
        "decode_fn = default_decode\n",
        "\n",
        "# ----------------------------\n",
        "# RoPE + ALiBi patchers (duck-typed)\n",
        "# ----------------------------\n",
        "def _is_rope_module(m) -> bool:\n",
        "    return hasattr(m, \"inv_freq\") and hasattr(m, \"get_cos_sin\") and callable(getattr(m, \"get_cos_sin\"))\n",
        "\n",
        "def _make_pos_map(T: int, period: int, mode: str, seed: int, device):\n",
        "    t = torch.arange(T, device=device, dtype=torch.long)\n",
        "    if mode == \"baseline\":\n",
        "        return t\n",
        "    if mode == \"reset\":\n",
        "        return t % period\n",
        "    if mode == \"rand_block\":\n",
        "        rng = np.random.default_rng(seed)\n",
        "        out = torch.empty((T,), device=device, dtype=torch.long)\n",
        "        n_blocks = (T + period - 1) // period\n",
        "        for b in range(n_blocks):\n",
        "            off = int(rng.integers(low=0, high=period))\n",
        "            s = b * period\n",
        "            e = min(T, (b + 1) * period)\n",
        "            out[s:e] = (torch.arange(e - s, device=device, dtype=torch.long) + off) % period\n",
        "        return out\n",
        "    raise ValueError(f\"Unknown mode={mode}\")\n",
        "\n",
        "class RopeRemapPatch:\n",
        "    def __init__(self, model, *, mode: str, period: int, seed: int):\n",
        "        self.model = model\n",
        "        self.mode = mode\n",
        "        self.period = int(period)\n",
        "        self.seed = int(seed)\n",
        "        self._saved = []\n",
        "\n",
        "    def __enter__(self):\n",
        "        rope_modules = [m for m in self.model.modules() if _is_rope_module(m)]\n",
        "        if len(rope_modules) == 0:\n",
        "            print(\" No RoPE-like modules found to patch.\")\n",
        "            return self\n",
        "\n",
        "        for rm in rope_modules:\n",
        "            old = rm.get_cos_sin\n",
        "\n",
        "            def make_wrapped(rm_ref, old_fn):\n",
        "                def wrapped(T: int, device, dtype):\n",
        "                    t_map = _make_pos_map(T, self.period, self.mode, self.seed, device=device)\n",
        "                    inv = rm_ref.inv_freq.to(device=device)\n",
        "                    freqs = torch.einsum(\"t,f->tf\", t_map.to(inv.dtype), inv)  # [T, d/2]\n",
        "                    emb = torch.cat([freqs, freqs], dim=-1)                    # [T, d]\n",
        "                    cos = emb.cos()[None, None, :, :].to(dtype=dtype)\n",
        "                    sin = emb.sin()[None, None, :, :].to(dtype=dtype)\n",
        "                    return cos, sin\n",
        "                return wrapped\n",
        "\n",
        "            rm.get_cos_sin = make_wrapped(rm, old)\n",
        "            self._saved.append((rm, old))\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc, tb):\n",
        "        for rm, old in self._saved:\n",
        "            rm.get_cos_sin = old\n",
        "        self._saved.clear()\n",
        "        return False\n",
        "\n",
        "class TempSetAlibiZero:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.saved = []\n",
        "\n",
        "    def __enter__(self):\n",
        "        for m in self.model.modules():\n",
        "            if hasattr(m, \"_alibi_strength_param\") and isinstance(getattr(m, \"_alibi_strength_param\"), torch.nn.Parameter):\n",
        "                p = m._alibi_strength_param\n",
        "                self.saved.append((\"param\", p, p.data.clone()))\n",
        "                p.data.fill_(-30.0)\n",
        "            elif hasattr(m, \"alibi_strength\") and isinstance(getattr(m, \"alibi_strength\"), (float, int)):\n",
        "                self.saved.append((\"attr\", m, float(m.alibi_strength)))\n",
        "                m.alibi_strength = 0.0\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc, tb):\n",
        "        for kind, obj, old in self.saved:\n",
        "            if kind == \"param\":\n",
        "                obj.data.copy_(old)\n",
        "            else:\n",
        "                obj.alibi_strength = old\n",
        "        self.saved.clear()\n",
        "        return False\n",
        "\n",
        "# ----------------------------\n",
        "# Helpers: top-p sampling\n",
        "# ----------------------------\n",
        "def sample_top_p(logits_1v, top_p: float, temperature: float):\n",
        "    logits = logits_1v / max(1e-6, float(temperature))\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "    # sort\n",
        "    p_sorted, idx = torch.sort(probs, descending=True)\n",
        "    cdf = torch.cumsum(p_sorted, dim=-1)\n",
        "    keep = cdf <= top_p\n",
        "    keep[..., 0] = True\n",
        "\n",
        "    p_keep = p_sorted * keep\n",
        "    p_keep = p_keep / p_keep.sum(dim=-1, keepdim=True).clamp_min(1e-9)\n",
        "\n",
        "    # sample in sorted space then map back\n",
        "    j = torch.multinomial(p_keep, num_samples=1)\n",
        "    tok = idx.gather(-1, j)\n",
        "    return tok.squeeze(-1)\n",
        "\n",
        "# ----------------------------\n",
        "# Optional: compute light ASA stats from infos\n",
        "# ----------------------------\n",
        "def light_asa_stats(infos):\n",
        "    \"\"\"\n",
        "    Returns dict with:\n",
        "      ent_norm, inertia, flip, margin, eff_slots\n",
        "    Computed on the *last* layer that has read_weights.\n",
        "    Uses the time-strided read_weights (already downsampled/offloaded).\n",
        "    \"\"\"\n",
        "    if infos is None:\n",
        "        return {}\n",
        "    last = None\n",
        "    for info in reversed(infos):\n",
        "        if info is None:\n",
        "            continue\n",
        "        rw = info.get(\"read_weights\", None)\n",
        "        if rw is not None and rw.dim() == 4:\n",
        "            last = rw.float()\n",
        "            break\n",
        "    if last is None:\n",
        "        return {}\n",
        "\n",
        "    # last: [B,H,T,K]\n",
        "    eps = 1e-9\n",
        "    p = last / last.sum(dim=-1, keepdim=True).clamp_min(eps)\n",
        "    B,H,T,K = p.shape\n",
        "\n",
        "    # entropy normalized by log K\n",
        "    ent = -(p * torch.log(p.clamp_min(eps))).sum(dim=-1)              # [B,H,T]\n",
        "    ent_norm = float((ent / math.log(K)).mean().cpu())\n",
        "\n",
        "    # inertia: mean cosine between successive time points\n",
        "    if T >= 2:\n",
        "        a = p[:, :, :-1, :]\n",
        "        b = p[:, :, 1:, :]\n",
        "        a = a / a.norm(dim=-1, keepdim=True).clamp_min(eps)\n",
        "        b = b / b.norm(dim=-1, keepdim=True).clamp_min(eps)\n",
        "        inertia = float((a*b).sum(dim=-1).mean().cpu())\n",
        "    else:\n",
        "        inertia = float(\"nan\")\n",
        "\n",
        "    # flip: argmax slot changes\n",
        "    arg = p.argmax(dim=-1)  # [B,H,T]\n",
        "    if T >= 2:\n",
        "        flip = float((arg[:, :, 1:] != arg[:, :, :-1]).float().mean().cpu())\n",
        "    else:\n",
        "        flip = float(\"nan\")\n",
        "\n",
        "    # margin: mean (top1 - top2)\n",
        "    top2 = torch.topk(p, k=min(2, K), dim=-1).values  # [B,H,T,2]\n",
        "    if top2.shape[-1] == 2:\n",
        "        margin = float((top2[..., 0] - top2[..., 1]).mean().cpu())\n",
        "    else:\n",
        "        margin = float(\"nan\")\n",
        "\n",
        "    # eff slots: exp(entropy)\n",
        "    eff_slots = float(torch.exp(ent.mean()).cpu())\n",
        "\n",
        "    return dict(ent_norm=ent_norm, inertia=inertia, flip=flip, margin=margin, eff_slots=eff_slots)\n",
        "\n",
        "# ----------------------------\n",
        "# Model forward wrappers\n",
        "# ----------------------------\n",
        "@torch.no_grad()\n",
        "def forward_logits_and_infos(x_ids):\n",
        "    \"\"\"\n",
        "    x_ids: [1,T] token ids\n",
        "    returns logits [1,T,V] and infos (or None)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    try:\n",
        "        out = model(x_ids, attention_mask=None, return_info=CAPTURE_INFO, info_level=INFO_LEVEL, info_cfg=INFO_CFG)\n",
        "        if isinstance(out, (tuple, list)) and len(out) == 2:\n",
        "            logits, infos = out\n",
        "        else:\n",
        "            logits, infos = out, None\n",
        "        return logits, infos\n",
        "    except TypeError:\n",
        "        out = model(x_ids, attention_mask=None, return_info=CAPTURE_INFO)\n",
        "        if isinstance(out, (tuple, list)) and len(out) == 2:\n",
        "            logits, infos = out\n",
        "        else:\n",
        "            logits, infos = out, None\n",
        "        return logits, infos\n",
        "\n",
        "# ----------------------------\n",
        "# Sampling routine\n",
        "# ----------------------------\n",
        "@dataclass\n",
        "class SampleResult:\n",
        "    name: str\n",
        "    prompt_ids: list\n",
        "    gen_ids: list\n",
        "    logs: pd.DataFrame\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_one(name: str, prompt_ids: torch.Tensor, *, rope_patch=False, rope_mode=\"reset\", alibi_zero=False):\n",
        "    # running sequence\n",
        "    x = prompt_ids.clone().to(DEVICE)  # [1,T0]\n",
        "    logs = []\n",
        "    remap_hits = 0\n",
        "\n",
        "    rope_ctx = RopeRemapPatch(model, mode=rope_mode, period=TRAIN_T, seed=SEED) if rope_patch else nullcontext()\n",
        "    alibi_ctx = TempSetAlibiZero(model) if alibi_zero else nullcontext()\n",
        "\n",
        "    with rope_ctx, alibi_ctx:\n",
        "        for step in range(MAX_NEW_TOKENS):\n",
        "            x_ctx = x[:, -CTX_WINDOW:]\n",
        "            T_ctx = int(x_ctx.shape[1])\n",
        "\n",
        "            # only count \"remap needed\" if we *would* exceed TRAIN_T in this call\n",
        "            did_remap = bool(rope_patch and (T_ctx > TRAIN_T))\n",
        "            remap_hits += int(did_remap)\n",
        "\n",
        "            logits, infos = forward_logits_and_infos(x_ctx)\n",
        "            next_logits = logits[:, -1, :]  # [1,V]\n",
        "            tok = sample_top_p(next_logits, top_p=TOP_P_BASE, temperature=TEMP_BASE)\n",
        "            x = torch.cat([x, tok.view(1,1)], dim=1)\n",
        "\n",
        "            if (step % LOG_EVERY) == 0:\n",
        "                stats = light_asa_stats(infos) if CAPTURE_INFO else {}\n",
        "                logs.append(dict(\n",
        "                    step=step,\n",
        "                    t_ctx=T_ctx,\n",
        "                    did_rope_remap=did_remap,\n",
        "                    ent_norm=stats.get(\"ent_norm\", np.nan),\n",
        "                    inertia=stats.get(\"inertia\", np.nan),\n",
        "                    flip=stats.get(\"flip\", np.nan),\n",
        "                    margin=stats.get(\"margin\", np.nan),\n",
        "                    eff_slots=stats.get(\"eff_slots\", np.nan),\n",
        "                    temp=TEMP_BASE,\n",
        "                    top_p=TOP_P_BASE,\n",
        "                ))\n",
        "\n",
        "    df = pd.DataFrame(logs)\n",
        "    return SampleResult(\n",
        "        name=name,\n",
        "        prompt_ids=prompt_ids.squeeze(0).tolist(),\n",
        "        gen_ids=x.squeeze(0).tolist(),\n",
        "        logs=df,\n",
        "    )\n",
        "\n",
        "# ----------------------------\n",
        "# Build deterministic prompts from same-window data\n",
        "# ----------------------------\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_prompts_from_data(cfg, *, T: int, B: int, split=\"val\"):\n",
        "    gen = make_batch_generator(cfg, split=split, device=DEVICE, seq_len=int(T), batches_per_epoch=1, infinite=False, num_workers=0)\n",
        "    xb, yb = next(gen)\n",
        "    xb = xb[:B].contiguous()\n",
        "    return xb\n",
        "\n",
        "prompts = get_prompts_from_data(cfg, T=PROMPT_TOKENS, B=N_SAMPLES, split=\"val\")\n",
        "\n",
        "# ----------------------------\n",
        "# Run both conditions\n",
        "# ----------------------------\n",
        "results = []\n",
        "all_logs = []\n",
        "\n",
        "for i in range(N_SAMPLES):\n",
        "    prompt = prompts[i:i+1]  # [1,T0]\n",
        "    r0 = generate_one(f\"sample{i}_baseline\", prompt, rope_patch=False, alibi_zero=False)\n",
        "    r1 = generate_one(f\"sample{i}_rope_reset\", prompt, rope_patch=True, rope_mode=ROPE_MODE, alibi_zero=DISABLE_ALIBI)\n",
        "    results.append((r0, r1))\n",
        "    all_logs.append(r0.logs.assign(sample=i, condition=\"baseline\"))\n",
        "    all_logs.append(r1.logs.assign(sample=i, condition=f\"rope_{ROPE_MODE}\"))\n",
        "\n",
        "df_logs = pd.concat(all_logs, ignore_index=True)\n",
        "display(df_logs.head(12))\n",
        "\n",
        "# ----------------------------\n",
        "# Print side-by-side text\n",
        "# ----------------------------\n",
        "def show_pair(r_base: SampleResult, r_rope: SampleResult, n_chars=1200):\n",
        "    base_txt = decode_fn(r_base.gen_ids)\n",
        "    rope_txt = decode_fn(r_rope.gen_ids)\n",
        "    print(\"=\"*110)\n",
        "    print(f\"[{r_base.name}]\")\n",
        "    print(base_txt[:n_chars])\n",
        "    print(\"-\"*110)\n",
        "    print(f\"[{r_rope.name}]\")\n",
        "    print(rope_txt[:n_chars])\n",
        "    print(\"=\"*110)\n",
        "\n",
        "for r0, r1 in results:\n",
        "    show_pair(r0, r1, n_chars=1200)\n",
        "\n",
        "# ----------------------------\n",
        "# Quick aggregate view of remap actually triggering\n",
        "# ----------------------------\n",
        "if not df_logs.empty:\n",
        "    agg = df_logs.groupby([\"condition\"]).agg(\n",
        "        ent_mean=(\"ent_norm\", \"mean\"),\n",
        "        inert_mean=(\"inertia\", \"mean\"),\n",
        "        flip_mean=(\"flip\", \"mean\"),\n",
        "        margin_mean=(\"margin\", \"mean\"),\n",
        "        eff_mean=(\"eff_slots\", \"mean\"),\n",
        "        tctx_max=(\"t_ctx\", \"max\"),\n",
        "        remap_frac=(\"did_rope_remap\", \"mean\"),\n",
        "    ).reset_index()\n",
        "    display(agg)\n",
        "\n",
        "print(\"Done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfznGmDqk-oM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQM-2FuOk-2s"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dgxo47zVmwRA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vNllaAoCmwbH"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Semantic Token Group Definitions & Classification\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Set\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# ============================\n",
        "# Comprehensive Semantic Groups\n",
        "# ============================\n",
        "\n",
        "SEMANTIC_GROUPS = {\n",
        "    'colors': {\n",
        "        'red', 'blue', 'green', 'yellow', 'orange', 'purple', 'pink', 'brown',\n",
        "        'black', 'white', 'gray', 'grey', 'gold', 'silver', 'crimson', 'azure',\n",
        "        'violet', 'indigo', 'scarlet', 'emerald', 'turquoise', 'cyan', 'magenta'\n",
        "    },\n",
        "\n",
        "    'numbers': {\n",
        "        'zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight',\n",
        "        'nine', 'ten', 'eleven', 'twelve', 'thirteen', 'fourteen', 'fifteen',\n",
        "        'twenty', 'thirty', 'forty', 'fifty', 'hundred', 'thousand', 'million',\n",
        "        'first', 'second', 'third', 'fourth', 'fifth', 'once', 'twice'\n",
        "    },\n",
        "\n",
        "    'animals': {\n",
        "        'dog', 'cat', 'bird', 'fish', 'lion', 'tiger', 'bear', 'wolf', 'fox',\n",
        "        'elephant', 'giraffe', 'zebra', 'horse', 'cow', 'pig', 'sheep', 'goat',\n",
        "        'chicken', 'duck', 'rabbit', 'mouse', 'rat', 'snake', 'frog', 'monkey',\n",
        "        'whale', 'dolphin', 'shark', 'eagle', 'owl', 'parrot', 'penguin'\n",
        "    },\n",
        "\n",
        "    'action_verbs': {\n",
        "        'run', 'running', 'ran', 'walk', 'walking', 'walked', 'jump', 'jumping',\n",
        "        'eat', 'eating', 'ate', 'drink', 'drinking', 'sleep', 'sleeping', 'slept',\n",
        "        'write', 'writing', 'wrote', 'read', 'reading', 'think', 'thinking',\n",
        "        'speak', 'speaking', 'spoke', 'listen', 'listening', 'watched', 'watching'\n",
        "    },\n",
        "\n",
        "    'ownership': {\n",
        "        'my', 'mine', 'your', 'yours', 'his', 'her', 'hers', 'their', 'theirs',\n",
        "        'our', 'ours', 'its', 'own', 'owns', 'owned', 'have', 'has', 'had',\n",
        "        'possess', 'possesses', 'possessed', 'belong', 'belongs', 'belonged'\n",
        "    },\n",
        "\n",
        "    'places': {\n",
        "        'city', 'town', 'village', 'country', 'state', 'nation', 'home', 'house',\n",
        "        'building', 'school', 'hospital', 'park', 'street', 'road', 'mountain',\n",
        "        'river', 'ocean', 'sea', 'lake', 'forest', 'desert', 'island', 'beach',\n",
        "        'restaurant', 'store', 'shop', 'office', 'church', 'temple', 'museum'\n",
        "    },\n",
        "\n",
        "    'dates_time': {\n",
        "        'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday',\n",
        "        'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august',\n",
        "        'september', 'october', 'november', 'december', 'morning', 'afternoon',\n",
        "        'evening', 'night', 'today', 'tomorrow', 'yesterday', 'week', 'month', 'year'\n",
        "    },\n",
        "\n",
        "    'emotions': {\n",
        "        'happy', 'sad', 'angry', 'fear', 'fearful', 'joy', 'joyful', 'love',\n",
        "        'hate', 'excited', 'calm', 'anxious', 'proud', 'ashamed', 'surprised',\n",
        "        'disgusted', 'confused', 'confident', 'nervous', 'jealous', 'grateful'\n",
        "    },\n",
        "\n",
        "    'quantifiers': {\n",
        "        'all', 'some', 'many', 'few', 'several', 'most', 'each', 'every',\n",
        "        'any', 'no', 'none', 'both', 'either', 'neither', 'much', 'little',\n",
        "        'more', 'less', 'fewer', 'enough', 'plenty'\n",
        "    },\n",
        "}\n",
        "\n",
        "def classify_token_semantic_group(token_id: int, tokenizer) -> str:\n",
        "    \"\"\"\n",
        "    Classify a token into semantic groups.\n",
        "    Returns group name or 'other'.\n",
        "    \"\"\"\n",
        "    token_str = tokenizer.decode([token_id]).strip().lower()\n",
        "\n",
        "    # Remove leading/trailing spaces and special characters\n",
        "    token_str = ''.join(c for c in token_str if c.isalnum())\n",
        "\n",
        "    # Check numeric tokens\n",
        "    if token_str.isdigit():\n",
        "        return 'numbers'\n",
        "\n",
        "    # Check each semantic group\n",
        "    for group_name, group_set in SEMANTIC_GROUPS.items():\n",
        "        if token_str in group_set:\n",
        "            return group_name\n",
        "\n",
        "    return 'other'\n",
        "\n",
        "def build_token_to_group_map(tokenizer, vocab_size: int = 50257) -> Dict[int, str]:\n",
        "    \"\"\"\n",
        "    Pre-compute token ID -> semantic group mapping.\n",
        "    \"\"\"\n",
        "    token_map = {}\n",
        "    for token_id in range(vocab_size):\n",
        "        group = classify_token_semantic_group(token_id, tokenizer)\n",
        "        token_map[token_id] = group\n",
        "\n",
        "    return token_map\n",
        "\n",
        "# Build mapping\n",
        "print(\"Building semantic token map...\")\n",
        "token_to_group = build_token_to_group_map(tokenizer)\n",
        "\n",
        "# Count tokens per group\n",
        "group_counts = defaultdict(int)\n",
        "for group in token_to_group.values():\n",
        "    group_counts[group] += 1\n",
        "\n",
        "print(\"\\nSemantic Group Statistics:\")\n",
        "print(\"=\"*60)\n",
        "for group, count in sorted(group_counts.items(), key=lambda x: -x[1]):\n",
        "    if group != 'other':\n",
        "        print(f\"  {group:>15s}: {count:>4d} tokens\")\n",
        "print(f\"  {'other':>15s}: {group_counts['other']:>4d} tokens\")\n",
        "\n",
        "print(\"\\n Token classification ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Xcl9xvewmwjb"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Experiment 1: Semantic Token Routing Trajectories Across Layers\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import FancyBboxPatch\n",
        "import seaborn as sns\n",
        "\n",
        "# ============================\n",
        "# Collect Routing for Semantic Groups\n",
        "# ============================\n",
        "\n",
        "def analyze_semantic_routing_trajectories(\n",
        "    model, cfg, tokenizer, token_to_group, device, num_batches=30\n",
        "):\n",
        "    \"\"\"\n",
        "    Track routing patterns for different semantic groups across layers.\n",
        "    Returns canonicalized routing distributions per group.\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Establish canonical roles\n",
        "    gen = make_batch_generator(cfg, split='val', device=device, batches_per_epoch=1, batch_size=32, infinite=False)\n",
        "    xb_canon, _ = next(gen)\n",
        "    canon_perm, _, valid_layers = compute_canonical_permutations(model, xb_canon, device)\n",
        "    #canon_perm = xb_canon\n",
        "    #valid_layers = list(range(cfg.num_heads))\n",
        "    print(f\"Analyzing semantic routing across {len(valid_layers)} layers...\")\n",
        "\n",
        "    H = cfg.num_heads\n",
        "    K = cfg.num_slots\n",
        "\n",
        "    # Track routing: group_routing[group][layer][canonical_head] = [K] distribution\n",
        "    group_routing = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: np.zeros(K))))\n",
        "    group_counts = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
        "\n",
        "    gen = make_batch_generator(cfg, split='val', device=device,\n",
        "                               batches_per_epoch=num_batches, infinite=False)\n",
        "\n",
        "    for batch_idx, (xb, yb) in enumerate(gen):\n",
        "        B, T = xb.shape\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _, infos = model(xb, return_info=True, info_level=\"basic\",\n",
        "                           info_cfg={'store_read_weights': True})\n",
        "\n",
        "        # For each valid layer\n",
        "        for layer_idx in valid_layers:\n",
        "            info = infos[layer_idx]\n",
        "            if info is None:\n",
        "                continue\n",
        "\n",
        "            rw = info.get('read_weights', None)  # [B,H,T,K]\n",
        "            if rw is None:\n",
        "                continue\n",
        "\n",
        "            perm = canon_perm[layer_idx]\n",
        "\n",
        "            # For each token\n",
        "            for b in range(B):\n",
        "                for t in range(T):\n",
        "                    token_id = xb[b, t].item()\n",
        "                    group = token_to_group.get(token_id, 'other')\n",
        "\n",
        "                    if group == 'other':\n",
        "                        continue\n",
        "\n",
        "                    # For each canonical head\n",
        "                    for canon_h in range(H):\n",
        "                        phys_h = perm[canon_h]\n",
        "                        routing = rw[b, phys_h, t, :].cpu().numpy()  # [K]\n",
        "\n",
        "                        group_routing[group][layer_idx][canon_h] += routing\n",
        "                        group_counts[group][layer_idx][canon_h] += 1\n",
        "\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            print(f\"  Processed {batch_idx + 1}/{num_batches} batches\")\n",
        "\n",
        "    # Normalize\n",
        "    for group in group_routing:\n",
        "        for layer_idx in group_routing[group]:\n",
        "            for canon_h in group_routing[group][layer_idx]:\n",
        "                count = group_counts[group][layer_idx][canon_h]\n",
        "                if count > 0:\n",
        "                    group_routing[group][layer_idx][canon_h] /= count\n",
        "\n",
        "    return group_routing, group_counts, valid_layers, canon_perm\n",
        "\n",
        "# Run analysis\n",
        "semantic_routing, semantic_counts, valid_layers, canon_perm = \\\n",
        "    analyze_semantic_routing_trajectories(model, cfg, tokenizer, token_to_group, DEVICE, num_batches=30)\n",
        "\n",
        "# ============================\n",
        "# Visualization 1: Slot Preference Heatmaps\n",
        "# ============================\n",
        "\n",
        "def plot_semantic_slot_preferences(semantic_routing, valid_layers, groups_to_plot=None):\n",
        "    \"\"\"\n",
        "    For each semantic group, show preferred slots across layers.\n",
        "    \"\"\"\n",
        "    if groups_to_plot is None:\n",
        "        groups_to_plot = [g for g in semantic_routing.keys() if g != 'other']\n",
        "\n",
        "    n_groups = len(groups_to_plot)\n",
        "    n_layers = len(valid_layers)\n",
        "    K = list(list(semantic_routing.values())[0].values())[0][0].shape[0]\n",
        "\n",
        "    fig, axes = plt.subplots(3, 3, figsize=(16, 12))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for idx, group in enumerate(groups_to_plot[:9]):\n",
        "        ax = axes[idx]\n",
        "\n",
        "        # Build matrix: [layers, slots] = average routing weight across all heads\n",
        "        matrix = np.zeros((n_layers, K))\n",
        "\n",
        "        for li, layer_idx in enumerate(valid_layers):\n",
        "            if layer_idx in semantic_routing[group]:\n",
        "                # Average across canonical heads\n",
        "                for canon_h in semantic_routing[group][layer_idx]:\n",
        "                    matrix[li] += semantic_routing[group][layer_idx][canon_h]\n",
        "                matrix[li] /= len(semantic_routing[group][layer_idx])\n",
        "\n",
        "        im = ax.imshow(matrix.T, aspect='auto', cmap='YlOrRd', interpolation='nearest')\n",
        "        ax.set_xlabel('Layer', fontsize=10)\n",
        "        ax.set_ylabel('Slot', fontsize=10)\n",
        "        ax.set_title(f'{group.upper()}', fontweight='bold', fontsize=11)\n",
        "        ax.set_xticks(range(n_layers))\n",
        "        ax.set_xticklabels([f'L{l}' for l in valid_layers], rotation=45)\n",
        "        plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "\n",
        "    # Hide unused subplots\n",
        "    for idx in range(len(groups_to_plot), 9):\n",
        "        axes[idx].axis('off')\n",
        "\n",
        "    fig.suptitle('Semantic Group Slot Preferences Across Depth',\n",
        "                 fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_semantic_slot_preferences(semantic_routing, valid_layers)\n",
        "\n",
        "# ============================\n",
        "# Visualization 2: Trajectory Flow Diagrams\n",
        "# ============================\n",
        "\n",
        "def plot_semantic_trajectory_sankey(semantic_routing, valid_layers, group='colors', head_idx=0):\n",
        "    \"\"\"\n",
        "    Sankey-style diagram showing how a semantic group flows through slots across layers.\n",
        "    \"\"\"\n",
        "    n_layers = min(4, len(valid_layers))  # Show first 4 layers\n",
        "    K = list(list(semantic_routing[group].values())[0].values())[0].shape[0]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "    # Get routing weights for this group, this canonical head\n",
        "    weights = []\n",
        "    for li in range(n_layers):\n",
        "        layer_idx = valid_layers[li]\n",
        "        if layer_idx in semantic_routing[group] and head_idx in semantic_routing[group][layer_idx]:\n",
        "            w = semantic_routing[group][layer_idx][head_idx]\n",
        "            weights.append(w)\n",
        "        else:\n",
        "            weights.append(np.ones(K) / K)\n",
        "\n",
        "    # Draw flow\n",
        "    layer_spacing = 3.0\n",
        "    slot_spacing = 0.15\n",
        "\n",
        "    colors_cmap = plt.cm.YlOrRd(np.linspace(0.3, 0.9, K))\n",
        "\n",
        "    for li in range(n_layers - 1):\n",
        "        x_from = li * layer_spacing\n",
        "        x_to = (li + 1) * layer_spacing\n",
        "\n",
        "        w_from = weights[li]\n",
        "        w_to = weights[li + 1]\n",
        "\n",
        "        # Draw flows from each slot to next layer\n",
        "        for slot_from in range(K):\n",
        "            y_from = slot_from * slot_spacing\n",
        "\n",
        "            # This slot's weight distributes to next layer\n",
        "            for slot_to in range(K):\n",
        "                y_to = slot_to * slot_spacing\n",
        "\n",
        "                # Flow width proportional to: w_from[slot_from] * w_to[slot_to]\n",
        "                flow_weight = w_from[slot_from] * w_to[slot_to] * 10\n",
        "\n",
        "                if flow_weight > 0.01:\n",
        "                    ax.plot([x_from, x_to], [y_from, y_to],\n",
        "                           alpha=flow_weight, linewidth=flow_weight*5,\n",
        "                           color=colors_cmap[slot_from])\n",
        "\n",
        "    # Draw slot nodes\n",
        "    for li in range(n_layers):\n",
        "        x = li * layer_spacing\n",
        "        for slot in range(K):\n",
        "            y = slot * slot_spacing\n",
        "            size = weights[li][slot] * 300\n",
        "            ax.scatter([x], [y], s=size, color=colors_cmap[slot],\n",
        "                      edgecolors='black', linewidths=1, zorder=10)\n",
        "\n",
        "    ax.set_xlim(-0.5, (n_layers-1) * layer_spacing + 0.5)\n",
        "    ax.set_ylim(-0.5, K * slot_spacing)\n",
        "    ax.set_xticks([i * layer_spacing for i in range(n_layers)])\n",
        "    ax.set_xticklabels([f'Layer {valid_layers[i]}' for i in range(n_layers)])\n",
        "    ax.set_ylabel('Slot ID', fontsize=11)\n",
        "    ax.set_title(f'Routing Trajectory: {group.upper()} tokens (Canonical Head {head_idx})',\n",
        "                 fontsize=13, fontweight='bold')\n",
        "    ax.grid(True, alpha=0.2, axis='y')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "for group in ['colors', 'numbers', 'animals', 'action_verbs']:\n",
        "    if group in semantic_routing:\n",
        "        plot_semantic_trajectory_sankey(semantic_routing, valid_layers, group=group, head_idx=0)\n",
        "\n",
        "print(\"\\n Trajectory analysis complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2nNwdwT8mw3n"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Experiment 2: Second-Order Attention  Does token T affect routing of token T+1?\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "\n",
        "# ============================\n",
        "# Analyze Context-Dependent Routing\n",
        "# ============================\n",
        "\n",
        "def analyze_second_order_effects(\n",
        "    model, cfg, tokenizer, token_to_group, device, num_batches=30\n",
        "):\n",
        "    \"\"\"\n",
        "    For each token at position t, measure:\n",
        "    1. Its own routing\n",
        "    2. The routing of token at t+1\n",
        "    3. Does the semantic group at t influence routing at t+1?\n",
        "    \"\"\"\n",
        "\n",
        "    # Establish canonicalization\n",
        "    gen = make_batch_generator(cfg, split='val', device=device, batches_per_epoch=1, infinite=False)\n",
        "    xb_canon, _ = next(gen)\n",
        "    canon_perm, _, valid_layers = compute_canonical_permutations(model, xb_canon, device)\n",
        "\n",
        "    print(\"Analyzing second-order routing effects...\")\n",
        "\n",
        "    H = cfg.num_heads\n",
        "    K = cfg.num_slots\n",
        "\n",
        "    # Track: given group at t, what's the routing distribution at t+1?\n",
        "    # second_order[group_t][group_t1][layer][canonical_head] = routing distribution at t+1\n",
        "    second_order = defaultdict(\n",
        "        lambda: defaultdict(\n",
        "            lambda: defaultdict(\n",
        "                lambda: defaultdict(lambda: np.zeros(K))\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "    second_order_counts = defaultdict(\n",
        "        lambda: defaultdict(\n",
        "            lambda: defaultdict(\n",
        "                lambda: defaultdict(int)\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "\n",
        "    gen = make_batch_generator(cfg, split='val', device=device,\n",
        "                               batches_per_epoch=num_batches, infinite=False)\n",
        "\n",
        "    for batch_idx, (xb, yb) in enumerate(gen):\n",
        "        B, T = xb.shape\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _, infos = model(xb, return_info=True, info_level=\"basic\",\n",
        "                           info_cfg={'store_read_weights': True})\n",
        "\n",
        "        # Focus on middle layers (where patterns are most interesting)\n",
        "        target_layer = valid_layers[len(valid_layers) // 2]\n",
        "\n",
        "        info = infos[target_layer]\n",
        "        if info is None:\n",
        "            continue\n",
        "\n",
        "        rw = info.get('read_weights', None)  # [B,H,T,K]\n",
        "        if rw is None:\n",
        "            continue\n",
        "\n",
        "        perm = canon_perm[target_layer]\n",
        "\n",
        "        # For each sequence\n",
        "        for b in range(B):\n",
        "            # For each position (except last)\n",
        "            for t in range(T - 1):\n",
        "                token_t = xb[b, t].item()\n",
        "                token_t1 = xb[b, t + 1].item()\n",
        "\n",
        "                group_t = token_to_group.get(token_t, 'other')\n",
        "                group_t1 = token_to_group.get(token_t1, 'other')\n",
        "\n",
        "                if group_t == 'other' or group_t1 == 'other':\n",
        "                    continue\n",
        "\n",
        "                # For each canonical head, get routing at t+1\n",
        "                for canon_h in range(H):\n",
        "                    phys_h = perm[canon_h]\n",
        "                    routing_t1 = rw[b, phys_h, t + 1, :].cpu().numpy()\n",
        "\n",
        "                    second_order[group_t][group_t1][target_layer][canon_h] += routing_t1\n",
        "                    second_order_counts[group_t][group_t1][target_layer][canon_h] += 1\n",
        "\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            print(f\"  Processed {batch_idx + 1}/{num_batches} batches\")\n",
        "\n",
        "    # Normalize\n",
        "    for group_t in second_order:\n",
        "        for group_t1 in second_order[group_t]:\n",
        "            for layer_idx in second_order[group_t][group_t1]:\n",
        "                for canon_h in second_order[group_t][group_t1][layer_idx]:\n",
        "                    count = second_order_counts[group_t][group_t1][layer_idx][canon_h]\n",
        "                    if count > 0:\n",
        "                        second_order[group_t][group_t1][layer_idx][canon_h] /= count\n",
        "\n",
        "    return second_order, second_order_counts\n",
        "\n",
        "second_order_routing, second_order_counts = \\\n",
        "    analyze_second_order_effects(model, cfg, tokenizer, token_to_group, DEVICE, num_batches=30)\n",
        "\n",
        "# ============================\n",
        "# Visualization: Context Effect Matrix\n",
        "# ============================\n",
        "\n",
        "def plot_second_order_matrix(second_order_routing, second_order_counts, layer_idx):\n",
        "    \"\"\"\n",
        "    Matrix showing: given group at t (rows), routing change at t+1 for group at t+1 (cols).\n",
        "    \"\"\"\n",
        "    # Get all groups that appear\n",
        "    groups_t = list(second_order_routing.keys())\n",
        "    groups_t1 = set()\n",
        "    for g in groups_t:\n",
        "        groups_t1.update(second_order_routing[g].keys())\n",
        "    groups_t1 = sorted(list(groups_t1))\n",
        "\n",
        "    if len(groups_t) == 0 or len(groups_t1) == 0:\n",
        "        print(\"No second-order data available\")\n",
        "        return\n",
        "\n",
        "    K = list(list(list(second_order_routing.values())[0].values())[0].values())[0][0].shape[0]\n",
        "\n",
        "    # Compute \"influence score\" = KL divergence from baseline\n",
        "    # Baseline = average routing for group_t1 regardless of context\n",
        "\n",
        "    # First, get baseline routing for each group_t1\n",
        "    baseline = {}\n",
        "    for group_t1 in groups_t1:\n",
        "        all_routing = []\n",
        "        total_count = 0\n",
        "        for group_t in groups_t:\n",
        "            if group_t1 in second_order_routing[group_t]:\n",
        "                if layer_idx in second_order_routing[group_t][group_t1]:\n",
        "                    for canon_h in second_order_routing[group_t][group_t1][layer_idx]:\n",
        "                        routing = second_order_routing[group_t][group_t1][layer_idx][canon_h]\n",
        "                        count = second_order_counts[group_t][group_t1][layer_idx][canon_h]\n",
        "                        all_routing.append(routing * count)\n",
        "                        total_count += count\n",
        "\n",
        "        if total_count > 0:\n",
        "            baseline[group_t1] = np.sum(all_routing, axis=0) / total_count\n",
        "        else:\n",
        "            baseline[group_t1] = np.ones(K) / K\n",
        "\n",
        "    # Build influence matrix\n",
        "    influence_matrix = np.zeros((len(groups_t), len(groups_t1)))\n",
        "\n",
        "    for i, group_t in enumerate(groups_t):\n",
        "        for j, group_t1 in enumerate(groups_t1):\n",
        "            if group_t1 in second_order_routing[group_t]:\n",
        "                if layer_idx in second_order_routing[group_t][group_t1]:\n",
        "                    # Average across heads\n",
        "                    kls = []\n",
        "                    for canon_h in second_order_routing[group_t][group_t1][layer_idx]:\n",
        "                        p = second_order_routing[group_t][group_t1][layer_idx][canon_h]\n",
        "                        q = baseline[group_t1]\n",
        "\n",
        "                        # KL(p||q)\n",
        "                        kl = np.sum(p * np.log((p + 1e-10) / (q + 1e-10)))\n",
        "                        kls.append(kl)\n",
        "\n",
        "                    if kls:\n",
        "                        influence_matrix[i, j] = np.mean(kls)\n",
        "\n",
        "    # Plot\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "    im = ax.imshow(influence_matrix, cmap='RdBu_r', aspect='auto',\n",
        "                  vmin=-influence_matrix.max(), vmax=influence_matrix.max())\n",
        "\n",
        "    ax.set_xticks(range(len(groups_t1)))\n",
        "    ax.set_xticklabels(groups_t1, rotation=45, ha='right')\n",
        "    ax.set_yticks(range(len(groups_t)))\n",
        "    ax.set_yticklabels(groups_t)\n",
        "\n",
        "    ax.set_xlabel('Token Group at t+1', fontsize=11, fontweight='bold')\n",
        "    ax.set_ylabel('Token Group at t', fontsize=11, fontweight='bold')\n",
        "    ax.set_title(f'Second-Order Context Effect (Layer {layer_idx})\\nKL divergence from baseline',\n",
        "                 fontsize=13, fontweight='bold')\n",
        "\n",
        "    plt.colorbar(im, ax=ax, label='Influence (KL divergence)')\n",
        "\n",
        "    # Add text annotations\n",
        "    for i in range(len(groups_t)):\n",
        "        for j in range(len(groups_t1)):\n",
        "            val = influence_matrix[i, j]\n",
        "            if abs(val) > 0.01:\n",
        "                ax.text(j, i, f'{val:.2f}', ha='center', va='center',\n",
        "                       fontsize=8, color='white' if abs(val) > influence_matrix.max()/2 else 'black')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print strongest effects\n",
        "    print(f\"\\nStrongest Second-Order Effects (Layer {layer_idx}):\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    effects = []\n",
        "    for i, group_t in enumerate(groups_t):\n",
        "        for j, group_t1 in enumerate(groups_t1):\n",
        "            effects.append((influence_matrix[i, j], group_t, group_t1))\n",
        "\n",
        "    effects.sort(key=lambda x: abs(x[0]), reverse=True)\n",
        "\n",
        "    for influence, group_t, group_t1 in effects[:10]:\n",
        "        direction = \"increases\" if influence > 0 else \"decreases\"\n",
        "        print(f\"  {group_t:>15s}  {group_t1:>15s}: {direction} divergence by {abs(influence):.3f}\")\n",
        "\n",
        "target_layer = valid_layers[len(valid_layers) // 2]\n",
        "plot_second_order_matrix(second_order_routing, second_order_counts, target_layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ME_jVLsBmxCD"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Experiment 3: Semantic Priming  Does \"red\" prime routing for \"apple\"?\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "\n",
        "# ============================\n",
        "# Semantic Pair Analysis\n",
        "# ============================\n",
        "\n",
        "SEMANTIC_PAIRS = {\n",
        "    'color_object': [\n",
        "        (['red', 'crimson', 'scarlet'], ['apple', 'rose', 'blood', 'fire']),\n",
        "        (['blue', 'azure'], ['sky', 'ocean', 'water', 'sea']),\n",
        "        (['green'], ['grass', 'tree', 'leaf', 'forest']),\n",
        "        (['white'], ['snow', 'cloud', 'milk', 'paper']),\n",
        "        (['yellow', 'gold'], ['sun', 'lemon', 'banana']),\n",
        "    ],\n",
        "\n",
        "    'number_object': [\n",
        "        (['two', 'three', 'four'], ['people', 'person', 'men', 'women', 'children']),\n",
        "        (['first', 'second', 'third'], ['place', 'position', 'time']),\n",
        "    ],\n",
        "\n",
        "    'action_object': [\n",
        "        (['eat', 'eating'], ['food', 'meal', 'dinner', 'lunch', 'breakfast']),\n",
        "        (['drink', 'drinking'], ['water', 'coffee', 'tea', 'beer', 'wine']),\n",
        "        (['read', 'reading'], ['book', 'books', 'novel', 'newspaper', 'article']),\n",
        "        (['write', 'writing'], ['letter', 'essay', 'story', 'document']),\n",
        "    ],\n",
        "}\n",
        "\n",
        "def find_semantic_pairs_in_batch(xb, tokenizer, pair_type='color_object'):\n",
        "    \"\"\"\n",
        "    Find instances where a prime token is followed (within 5 tokens) by a target.\n",
        "    Returns: list of (batch_idx, prime_pos, target_pos, prime_word, target_word)\n",
        "    \"\"\"\n",
        "    pairs = SEMANTIC_PAIRS[pair_type]\n",
        "    found_pairs = []\n",
        "\n",
        "    B, T = xb.shape\n",
        "\n",
        "    for b in range(B):\n",
        "        tokens = xb[b].cpu().numpy()\n",
        "        token_strs = [tokenizer.decode([t]).strip().lower() for t in tokens]\n",
        "\n",
        "        for primes, targets in pairs:\n",
        "            for t in range(T - 1):\n",
        "                token_t = token_strs[t]\n",
        "\n",
        "                # Check if this is a prime\n",
        "                if any(prime in token_t for prime in primes):\n",
        "                    # Look ahead up to 5 positions\n",
        "                    for offset in range(1, min(6, T - t)):\n",
        "                        token_ahead = token_strs[t + offset]\n",
        "\n",
        "                        # Check if this is a target\n",
        "                        if any(target in token_ahead for target in targets):\n",
        "                            found_pairs.append((b, t, t + offset, token_t, token_ahead))\n",
        "\n",
        "    return found_pairs\n",
        "\n",
        "def analyze_priming_effect(model, cfg, tokenizer, device, pair_type='color_object', num_batches=50):\n",
        "    \"\"\"\n",
        "    Compare routing when:\n",
        "    1. Target appears after prime (primed condition)\n",
        "    2. Target appears without prime (unprimed condition)\n",
        "    \"\"\"\n",
        "\n",
        "    # Establish canonicalization\n",
        "    gen = make_batch_generator(cfg, split='val', device=device, batches_per_epoch=1, infinite=False)\n",
        "    xb_canon, _ = next(gen)\n",
        "    canon_perm, _, valid_layers = compute_canonical_permutations(model, xb_canon, device)\n",
        "\n",
        "    H = cfg.num_heads\n",
        "    K = cfg.num_slots\n",
        "    target_layer = valid_layers[len(valid_layers) // 2]\n",
        "\n",
        "    # Storage\n",
        "    primed_routing = defaultdict(lambda: np.zeros(K))\n",
        "    unprimed_routing = defaultdict(lambda: np.zeros(K))\n",
        "    primed_counts = defaultdict(int)\n",
        "    unprimed_counts = defaultdict(int)\n",
        "\n",
        "    print(f\"Analyzing {pair_type} priming effects...\")\n",
        "\n",
        "    gen = make_batch_generator(cfg, split='val', device=device,\n",
        "                               batches_per_epoch=num_batches, infinite=False)\n",
        "\n",
        "    for batch_idx, (xb, yb) in enumerate(gen):\n",
        "        # Find semantic pairs in this batch\n",
        "        pairs = find_semantic_pairs_in_batch(xb, tokenizer, pair_type=pair_type)\n",
        "\n",
        "        if not pairs:\n",
        "            continue\n",
        "\n",
        "        # Run model\n",
        "        with torch.no_grad():\n",
        "            _, infos = model(xb, return_info=True, info_level=\"basic\",\n",
        "                           info_cfg={'store_read_weights': True})\n",
        "\n",
        "        info = infos[target_layer]\n",
        "        if info is None:\n",
        "            continue\n",
        "\n",
        "        rw = info.get('read_weights', None)\n",
        "        if rw is None:\n",
        "            continue\n",
        "\n",
        "        perm = canon_perm[target_layer]\n",
        "\n",
        "        # For each found pair\n",
        "        for b, prime_pos, target_pos, prime_word, target_word in pairs:\n",
        "            # Get routing at target position (primed)\n",
        "            for canon_h in range(H):\n",
        "                phys_h = perm[canon_h]\n",
        "                routing = rw[b, phys_h, target_pos, :].cpu().numpy()\n",
        "\n",
        "                primed_routing[canon_h] += routing\n",
        "                primed_counts[canon_h] += 1\n",
        "\n",
        "        # Also collect unprimed targets (targets without recent primes)\n",
        "        B, T = xb.shape\n",
        "        tokens = xb.cpu().numpy()\n",
        "        token_strs = [[tokenizer.decode([t]).strip().lower() for t in tokens[b]] for b in range(B)]\n",
        "\n",
        "        pairs_dict = SEMANTIC_PAIRS[pair_type]\n",
        "        all_targets = set()\n",
        "        for _, targets in pairs_dict:\n",
        "            all_targets.update(targets)\n",
        "\n",
        "        for b in range(B):\n",
        "            for t in range(T):\n",
        "                token_t = token_strs[b][t]\n",
        "\n",
        "                # Is this a target?\n",
        "                is_target = any(target in token_t for target in all_targets)\n",
        "\n",
        "                if is_target:\n",
        "                    # Check if there's a prime in the last 5 positions\n",
        "                    has_prime = False\n",
        "                    for lookback in range(1, min(6, t + 1)):\n",
        "                        token_prev = token_strs[b][t - lookback]\n",
        "                        for primes, _ in pairs_dict:\n",
        "                            if any(prime in token_prev for prime in primes):\n",
        "                                has_prime = True\n",
        "                                break\n",
        "                        if has_prime:\n",
        "                            break\n",
        "\n",
        "                    if not has_prime:\n",
        "                        # Unprimed target\n",
        "                        for canon_h in range(H):\n",
        "                            phys_h = perm[canon_h]\n",
        "                            routing = rw[b, phys_h, t, :].cpu().numpy()\n",
        "\n",
        "                            unprimed_routing[canon_h] += routing\n",
        "                            unprimed_counts[canon_h] += 1\n",
        "\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            print(f\"  Batch {batch_idx + 1}/{num_batches}: Found {len(pairs)} primed pairs\")\n",
        "\n",
        "    # Normalize\n",
        "    for canon_h in range(H):\n",
        "        if primed_counts[canon_h] > 0:\n",
        "            primed_routing[canon_h] /= primed_counts[canon_h]\n",
        "        if unprimed_counts[canon_h] > 0:\n",
        "            unprimed_routing[canon_h] /= unprimed_counts[canon_h]\n",
        "\n",
        "    return primed_routing, unprimed_routing, primed_counts, unprimed_counts\n",
        "\n",
        "# Run analysis\n",
        "for pair_type in ['color_object', 'action_object']:\n",
        "    primed, unprimed, primed_cnt, unprimed_cnt = \\\n",
        "        analyze_priming_effect(model, cfg, tokenizer, DEVICE, pair_type=pair_type, num_batches=50)\n",
        "\n",
        "    print(f\"\\n{pair_type.upper()} Priming Results:\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Primed instances: {sum(primed_cnt.values())}\")\n",
        "    print(f\"Unprimed instances: {sum(unprimed_cnt.values())}\")\n",
        "\n",
        "    # Plot difference\n",
        "    H = cfg.num_heads\n",
        "    K = cfg.num_slots\n",
        "\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for canon_h in range(H):\n",
        "        ax = axes[canon_h]\n",
        "\n",
        "        if primed_cnt[canon_h] > 0 and unprimed_cnt[canon_h] > 0:\n",
        "            diff = primed[canon_h] - unprimed[canon_h]\n",
        "\n",
        "            ax.bar(range(K), diff, color=['red' if d > 0 else 'blue' for d in diff])\n",
        "            ax.axhline(0, color='black', linewidth=0.8)\n",
        "            ax.set_xlabel('Slot', fontsize=9)\n",
        "            ax.set_ylabel(' Routing (primed - unprimed)', fontsize=9)\n",
        "            ax.set_title(f'Canonical Head {canon_h}', fontweight='bold')\n",
        "            ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    fig.suptitle(f'Priming Effect: {pair_type.replace(\"_\", \" \").title()}',\n",
        "                 fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hYF4PxgJmxVD"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Experiment 4: Semantic Clustering in Canonical Slot State Space (t-SNE)\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "\n",
        "# ============================\n",
        "# Collect Slot States by Semantic Group\n",
        "# ============================\n",
        "\n",
        "def collect_canonical_slot_states(\n",
        "    model, cfg, tokenizer, token_to_group, device,\n",
        "    num_batches=20, layer_idx=6, max_samples_per_group=500\n",
        "):\n",
        "    \"\"\"\n",
        "    Collect actual slot state vectors for tokens, grouped by semantics.\n",
        "    Uses canonicalization so we track SEMANTIC ROLES, not physical heads.\n",
        "\n",
        "    Returns:\n",
        "        states_by_group: dict[group][canonical_head] = [N_samples, d] numpy array\n",
        "        metadata: dict with token info\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Establish canonicalization\n",
        "    gen = make_batch_generator(cfg, split='val', device=device, batches_per_epoch=1, infinite=False)\n",
        "    xb_canon, _ = next(gen)\n",
        "    canon_perm, _, valid_layers = compute_canonical_permutations(model, xb_canon, device)\n",
        "\n",
        "    if layer_idx not in valid_layers:\n",
        "        layer_idx = valid_layers[len(valid_layers) // 2]\n",
        "        print(f\"Using layer {layer_idx} instead\")\n",
        "\n",
        "    perm = canon_perm[layer_idx]\n",
        "    H = cfg.num_heads\n",
        "    d = cfg.embed_dim // cfg.num_heads\n",
        "    K = cfg.num_slots\n",
        "\n",
        "    print(f\"Collecting canonical slot states from Layer {layer_idx}...\")\n",
        "    print(f\"Head dim: {d}, Num slots: {K}\")\n",
        "\n",
        "    # We need to hook into the model to extract actual slot states\n",
        "    # For now, we'll use a simpler approach: collect read weights weighted slot combinations\n",
        "    # This approximates the \"active slot state\" for each token\n",
        "\n",
        "    states_by_group = defaultdict(lambda: defaultdict(list))\n",
        "    token_info = defaultdict(lambda: defaultdict(list))\n",
        "    group_sample_counts = defaultdict(int)\n",
        "\n",
        "    gen = make_batch_generator(cfg, split='val', device=device,\n",
        "                               batches_per_epoch=num_batches, infinite=False)\n",
        "\n",
        "    for batch_idx, (xb, yb) in enumerate(gen):\n",
        "        B, T = xb.shape\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # We need to extract actual slot states during forward pass\n",
        "            # For this, we'll modify the forward to store intermediate states\n",
        "            # Simplified: use read weights as proxy for now\n",
        "\n",
        "            _, infos = model(xb, return_info=True, info_level=\"basic\",\n",
        "                           info_cfg={'store_read_weights': True})\n",
        "\n",
        "        info = infos[layer_idx]\n",
        "        if info is None:\n",
        "            continue\n",
        "\n",
        "        rw = info.get('read_weights', None)  # [B,H,T,K]\n",
        "        if rw is None:\n",
        "            continue\n",
        "\n",
        "        # For each token\n",
        "        for b in range(B):\n",
        "            for t in range(T):\n",
        "                token_id = xb[b, t].item()\n",
        "                group = token_to_group.get(token_id, 'other')\n",
        "\n",
        "                if group == 'other':\n",
        "                    continue\n",
        "\n",
        "                # Check if we have enough samples\n",
        "                if group_sample_counts[group] >= max_samples_per_group:\n",
        "                    continue\n",
        "\n",
        "                # For each CANONICAL head\n",
        "                for canon_h in range(H):\n",
        "                    phys_h = perm[canon_h]\n",
        "\n",
        "                    # Get routing weights for this token at this head\n",
        "                    routing = rw[b, phys_h, t, :].cpu().numpy()  # [K]\n",
        "\n",
        "                    # Store as \"slot state approximation\"\n",
        "                    # In reality, we'd want the actual V @ slot_state vectors\n",
        "                    # For now, use routing weights as a K-dimensional representation\n",
        "                    states_by_group[group][canon_h].append(routing)\n",
        "                    token_info[group][canon_h].append({\n",
        "                        'token_id': token_id,\n",
        "                        'token_str': tokenizer.decode([token_id]),\n",
        "                        'position': t,\n",
        "                        'batch': batch_idx\n",
        "                    })\n",
        "\n",
        "                group_sample_counts[group] += 1\n",
        "\n",
        "        if (batch_idx + 1) % 5 == 0:\n",
        "            print(f\"  Batch {batch_idx + 1}/{num_batches} | Samples: {dict(group_sample_counts)}\")\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    for group in states_by_group:\n",
        "        for canon_h in states_by_group[group]:\n",
        "            states_by_group[group][canon_h] = np.array(states_by_group[group][canon_h])\n",
        "\n",
        "    print(\"\\nCollection complete:\")\n",
        "    for group in sorted(states_by_group.keys()):\n",
        "        total = sum(len(states_by_group[group][h]) for h in states_by_group[group])\n",
        "        print(f\"  {group:>15s}: {total:>5d} samples across {len(states_by_group[group])} heads\")\n",
        "\n",
        "    return states_by_group, token_info, layer_idx, perm\n",
        "\n",
        "# Collect states\n",
        "states_by_group, token_info, layer_idx, canon_perm = collect_canonical_slot_states(\n",
        "    model, cfg, tokenizer, token_to_group, DEVICE,\n",
        "    num_batches=30, layer_idx=6, max_samples_per_group=400\n",
        ")\n",
        "\n",
        "# ============================\n",
        "# t-SNE Analysis by Canonical Head\n",
        "# ============================\n",
        "\n",
        "def tsne_semantic_clustering_per_head(states_by_group, layer_idx, canon_head_idx):\n",
        "    \"\"\"\n",
        "    For a specific CANONICAL head, perform t-SNE on all semantic groups.\n",
        "    Color by semantic group to see if they cluster.\n",
        "    \"\"\"\n",
        "\n",
        "    # Collect all states for this canonical head\n",
        "    all_states = []\n",
        "    all_groups = []\n",
        "    all_tokens = []\n",
        "\n",
        "    semantic_groups = [g for g in states_by_group.keys() if g != 'other']\n",
        "\n",
        "    for group in semantic_groups:\n",
        "        if canon_head_idx in states_by_group[group]:\n",
        "            states = states_by_group[group][canon_head_idx]\n",
        "            all_states.append(states)\n",
        "            all_groups.extend([group] * len(states))\n",
        "\n",
        "    if len(all_states) == 0:\n",
        "        print(f\"No data for canonical head {canon_head_idx}\")\n",
        "        return None, None, None\n",
        "\n",
        "    all_states = np.vstack(all_states)\n",
        "\n",
        "    print(f\"\\nCanonical Head {canon_head_idx}:\")\n",
        "    print(f\"  Total samples: {len(all_states)}\")\n",
        "    print(f\"  Feature dim: {all_states.shape[1]}\")\n",
        "\n",
        "    # PCA first (for speed and denoising)\n",
        "    print(\"  Running PCA...\")\n",
        "    n_components_pca = min(20, all_states.shape[1] - 1)\n",
        "    pca = PCA(n_components=n_components_pca)\n",
        "    states_pca = pca.fit_transform(all_states)\n",
        "    print(f\"  PCA variance explained: {pca.explained_variance_ratio_.sum():.2%}\")\n",
        "\n",
        "    # t-SNE\n",
        "    print(\"  Running t-SNE...\")\n",
        "    n_samples = len(states_pca)\n",
        "    perplexity = min(30, n_samples // 4)\n",
        "\n",
        "    tsne = TSNE(\n",
        "        n_components=2,\n",
        "        perplexity=perplexity,\n",
        "        n_iter=1000,\n",
        "        random_state=42,\n",
        "        verbose=0\n",
        "    )\n",
        "    coords = tsne.fit_transform(states_pca)\n",
        "\n",
        "    return coords, all_groups, states_pca\n",
        "\n",
        "def plot_tsne_semantic_clusters(coords, groups, layer_idx, canon_head_idx):\n",
        "    \"\"\"\n",
        "    Plot t-SNE with points colored by semantic group.\n",
        "    \"\"\"\n",
        "\n",
        "    # Color map\n",
        "    unique_groups = sorted(set(groups))\n",
        "    color_palette = sns.color_palette(\"husl\", len(unique_groups))\n",
        "    group_to_color = {g: color_palette[i] for i, g in enumerate(unique_groups)}\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 10))\n",
        "\n",
        "    # Plot each group\n",
        "    for group in unique_groups:\n",
        "        mask = np.array([g == group for g in groups])\n",
        "        ax.scatter(\n",
        "            coords[mask, 0],\n",
        "            coords[mask, 1],\n",
        "            c=[group_to_color[group]],\n",
        "            label=group,\n",
        "            alpha=0.6,\n",
        "            s=30,\n",
        "            edgecolors='none'\n",
        "        )\n",
        "\n",
        "    ax.set_xlabel('t-SNE 1', fontsize=12)\n",
        "    ax.set_ylabel('t-SNE 2', fontsize=12)\n",
        "    ax.set_title(\n",
        "        f'Semantic Clustering in Slot State Space\\n'\n",
        "        f'Layer {layer_idx}, Canonical Head {canon_head_idx}',\n",
        "        fontsize=14, fontweight='bold'\n",
        "    )\n",
        "    ax.legend(title='Semantic Group', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    ax.grid(True, alpha=0.2)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Run t-SNE for multiple canonical heads\n",
        "for canon_h in range(min(4, cfg.num_heads)):\n",
        "    coords, groups, states_pca = tsne_semantic_clustering_per_head(\n",
        "        states_by_group, layer_idx, canon_h\n",
        "    )\n",
        "\n",
        "    if coords is not None:\n",
        "        plot_tsne_semantic_clusters(coords, groups, layer_idx, canon_h)\n",
        "\n",
        "# ============================\n",
        "# Quantitative Clustering Analysis\n",
        "# ============================\n",
        "\n",
        "def measure_semantic_clustering_quality(coords, groups):\n",
        "    \"\"\"\n",
        "    Measure how well semantic groups cluster using silhouette score.\n",
        "    \"\"\"\n",
        "    from sklearn.metrics import silhouette_score\n",
        "    from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "    # Encode groups as integers\n",
        "    unique_groups = sorted(set(groups))\n",
        "    group_to_int = {g: i for i, g in enumerate(unique_groups)}\n",
        "    labels = np.array([group_to_int[g] for g in groups])\n",
        "\n",
        "    # Silhouette score (-1 to 1, higher is better)\n",
        "    if len(unique_groups) > 1:\n",
        "        sil_score = silhouette_score(coords, labels)\n",
        "    else:\n",
        "        sil_score = 0.0\n",
        "\n",
        "    # Compute within-group vs between-group distances\n",
        "    distances = squareform(pdist(coords))\n",
        "\n",
        "    within_group_dists = []\n",
        "    between_group_dists = []\n",
        "\n",
        "    for i in range(len(groups)):\n",
        "        for j in range(i + 1, len(groups)):\n",
        "            dist = distances[i, j]\n",
        "\n",
        "            if groups[i] == groups[j]:\n",
        "                within_group_dists.append(dist)\n",
        "            else:\n",
        "                between_group_dists.append(dist)\n",
        "\n",
        "    within_mean = np.mean(within_group_dists) if within_group_dists else 0\n",
        "    between_mean = np.mean(between_group_dists) if between_group_dists else 0\n",
        "\n",
        "    separation_ratio = between_mean / (within_mean + 1e-9)\n",
        "\n",
        "    return sil_score, separation_ratio, within_mean, between_mean\n",
        "\n",
        "print(\"\\nClustering Quality by Canonical Head:\")\n",
        "print(\"=\"*70)\n",
        "print(f\"{'Head':<8} {'Silhouette':<12} {'Sep Ratio':<12} {'Within':<10} {'Between':<10}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "for canon_h in range(min(cfg.num_heads, 8)):\n",
        "    coords, groups, _ = tsne_semantic_clustering_per_head(states_by_group, layer_idx, canon_h)\n",
        "\n",
        "    if coords is not None:\n",
        "        sil, sep_ratio, within, between = measure_semantic_clustering_quality(coords, groups)\n",
        "        print(f\"{canon_h:<8} {sil:>11.3f} {sep_ratio:>11.2f} {within:>9.3f} {between:>9.3f}\")\n",
        "\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"   Silhouette > 0.3: Good clustering by semantic group\")\n",
        "print(\"   Sep Ratio > 1.5: Between-group distances exceed within-group\")\n",
        "print(\"   Higher values = stronger semantic organization in slot space\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sf-gUX-tmxc_"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Experiment 5: Long-Range Context Influence on Canonical Routing\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# ============================\n",
        "# Context Window Analysis\n",
        "# ============================\n",
        "\n",
        "def analyze_long_range_context_influence(\n",
        "    model, cfg, tokenizer, token_to_group, device,\n",
        "    num_batches=30, max_lookback=32\n",
        "):\n",
        "    \"\"\"\n",
        "    For each token at position t, measure how its routing is influenced by\n",
        "    tokens at positions t-k for k=1,2,...,max_lookback.\n",
        "\n",
        "    Uses canonical roles to ensure meaningful cross-layer comparison.\n",
        "    \"\"\"\n",
        "\n",
        "    # Establish canonicalization\n",
        "    gen = make_batch_generator(cfg, split='val', device=device, batches_per_epoch=1, infinite=False)\n",
        "    xb_canon, _ = next(gen)\n",
        "    canon_perm, _, valid_layers = compute_canonical_permutations(model, xb_canon, device)\n",
        "\n",
        "    target_layer = valid_layers[len(valid_layers) // 2]\n",
        "    print(f\"Analyzing long-range dependencies at Layer {target_layer}...\")\n",
        "\n",
        "    H = cfg.num_heads\n",
        "    K = cfg.num_slots\n",
        "    perm = canon_perm[target_layer]\n",
        "\n",
        "    # For each semantic group pair and distance\n",
        "    # influence[group_context][group_target][distance][canon_h] = correlation strength\n",
        "    influence = defaultdict(\n",
        "        lambda: defaultdict(\n",
        "            lambda: defaultdict(\n",
        "                lambda: defaultdict(list)\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "\n",
        "    gen = make_batch_generator(cfg, split='val', device=device,\n",
        "                               batches_per_epoch=num_batches, infinite=False)\n",
        "\n",
        "    print(f\"Processing {num_batches} batches...\")\n",
        "\n",
        "    for batch_idx, (xb, yb) in enumerate(gen):\n",
        "        B, T = xb.shape\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _, infos = model(xb, return_info=True, info_level=\"basic\",\n",
        "                           info_cfg={'store_read_weights': True})\n",
        "\n",
        "        info = infos[target_layer]\n",
        "        if info is None:\n",
        "            continue\n",
        "\n",
        "        rw = info.get('read_weights', None)  # [B,H,T,K]\n",
        "        if rw is None:\n",
        "            continue\n",
        "\n",
        "        # For each sequence\n",
        "        for b in range(B):\n",
        "            # For each target position\n",
        "            for t in range(max_lookback, T):\n",
        "                token_t = xb[b, t].item()\n",
        "                group_t = token_to_group.get(token_t, 'other')\n",
        "\n",
        "                if group_t == 'other':\n",
        "                    continue\n",
        "\n",
        "                # Get routing at position t\n",
        "                routing_t = {}\n",
        "                for canon_h in range(H):\n",
        "                    phys_h = perm[canon_h]\n",
        "                    routing_t[canon_h] = rw[b, phys_h, t, :].cpu().numpy()\n",
        "\n",
        "                # Look back at different distances\n",
        "                for distance in range(1, min(max_lookback + 1, t + 1)):\n",
        "                    pos_context = t - distance\n",
        "                    token_context = xb[b, pos_context].item()\n",
        "                    group_context = token_to_group.get(token_context, 'other')\n",
        "\n",
        "                    if group_context == 'other':\n",
        "                        continue\n",
        "\n",
        "                    # Get routing at context position\n",
        "                    routing_context = {}\n",
        "                    for canon_h in range(H):\n",
        "                        phys_h = perm[canon_h]\n",
        "                        routing_context[canon_h] = rw[b, phys_h, pos_context, :].cpu().numpy()\n",
        "\n",
        "                    # Compute correlation between context and target routing\n",
        "                    for canon_h in range(H):\n",
        "                        # Pearson correlation of slot distributions\n",
        "                        if routing_t[canon_h].std() > 1e-6 and routing_context[canon_h].std() > 1e-6:\n",
        "                            corr, _ = pearsonr(routing_t[canon_h], routing_context[canon_h])\n",
        "                            influence[group_context][group_t][distance][canon_h].append(corr)\n",
        "\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            print(f\"  Batch {batch_idx + 1}/{num_batches}\")\n",
        "\n",
        "    # Average correlations\n",
        "    avg_influence = defaultdict(\n",
        "        lambda: defaultdict(\n",
        "            lambda: defaultdict(\n",
        "                lambda: defaultdict(float)\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "\n",
        "    for group_ctx in influence:\n",
        "        for group_tgt in influence[group_ctx]:\n",
        "            for distance in influence[group_ctx][group_tgt]:\n",
        "                for canon_h in influence[group_ctx][group_tgt][distance]:\n",
        "                    corrs = influence[group_ctx][group_tgt][distance][canon_h]\n",
        "                    if corrs:\n",
        "                        avg_influence[group_ctx][group_tgt][distance][canon_h] = np.mean(corrs)\n",
        "\n",
        "    return avg_influence, target_layer\n",
        "\n",
        "influence_data, target_layer = analyze_long_range_context_influence(\n",
        "    model, cfg, tokenizer, token_to_group, DEVICE,\n",
        "    num_batches=30, max_lookback=32\n",
        ")\n",
        "\n",
        "# ============================\n",
        "# Visualization 1: Influence Decay Curves\n",
        "# ============================\n",
        "\n",
        "def plot_influence_decay_curves(influence_data, target_layer, group_pairs=None):\n",
        "    \"\"\"\n",
        "    Plot how context influence decays with distance for different semantic pairs.\n",
        "    \"\"\"\n",
        "\n",
        "    if group_pairs is None:\n",
        "        # Select interesting pairs\n",
        "        group_pairs = [\n",
        "            ('ownership', 'ownership'),  # Same group\n",
        "            ('colors', 'places'),         # Different groups\n",
        "            ('action_verbs', 'places'),\n",
        "            ('numbers', 'quantifiers'),\n",
        "        ]\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for idx, (group_ctx, group_tgt) in enumerate(group_pairs):\n",
        "        if idx >= 4:\n",
        "            break\n",
        "\n",
        "        ax = axes[idx]\n",
        "\n",
        "        if group_ctx not in influence_data or group_tgt not in influence_data[group_ctx]:\n",
        "            ax.text(0.5, 0.5, 'No data', ha='center', va='center', transform=ax.transAxes)\n",
        "            ax.set_title(f'{group_ctx}  {group_tgt}', fontweight='bold')\n",
        "            continue\n",
        "\n",
        "        # Get data for this pair\n",
        "        distances = sorted(influence_data[group_ctx][group_tgt].keys())\n",
        "\n",
        "        # Plot curve for each canonical head (average across heads for clarity)\n",
        "        H = cfg.num_heads\n",
        "\n",
        "        # Average across all heads\n",
        "        avg_by_distance = []\n",
        "        for dist in distances:\n",
        "            head_values = []\n",
        "            for canon_h in range(H):\n",
        "                val = influence_data[group_ctx][group_tgt][dist].get(canon_h, 0)\n",
        "                if val != 0:\n",
        "                    head_values.append(val)\n",
        "            avg_by_distance.append(np.mean(head_values) if head_values else 0)\n",
        "\n",
        "        ax.plot(distances, avg_by_distance, marker='o', linewidth=2, markersize=6, label='Mean')\n",
        "\n",
        "        # Also plot top 2 heads separately\n",
        "        for canon_h in [0, 1]:  # Just show first two canonical heads\n",
        "            values = [influence_data[group_ctx][group_tgt][dist].get(canon_h, 0) for dist in distances]\n",
        "            ax.plot(distances, values, marker='s', linewidth=1, markersize=4,\n",
        "                   alpha=0.5, label=f'Canon H{canon_h}')\n",
        "\n",
        "        ax.axhline(0, color='black', linewidth=0.8, linestyle='--', alpha=0.3)\n",
        "        ax.set_xlabel('Distance (tokens)', fontsize=11)\n",
        "        ax.set_ylabel('Routing Correlation', fontsize=11)\n",
        "        ax.set_title(f'{group_ctx}  {group_tgt}', fontweight='bold')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        ax.legend(fontsize=8)\n",
        "\n",
        "        # Fit exponential decay\n",
        "        from scipy.optimize import curve_fit\n",
        "\n",
        "        def exp_decay(x, a, b):\n",
        "            return a * np.exp(-b * x)\n",
        "\n",
        "        try:\n",
        "            x_data = np.array(distances)\n",
        "            y_data = np.array(avg_by_distance)\n",
        "\n",
        "            # Only fit on positive values\n",
        "            mask = y_data > 0\n",
        "            if mask.sum() > 3:\n",
        "                popt, _ = curve_fit(exp_decay, x_data[mask], y_data[mask], p0=[0.5, 0.1])\n",
        "\n",
        "                x_fit = np.linspace(min(distances), max(distances), 100)\n",
        "                y_fit = exp_decay(x_fit, *popt)\n",
        "                ax.plot(x_fit, y_fit, '--', color='red', alpha=0.5, linewidth=1.5,\n",
        "                       label=f'Exp fit: ={popt[1]:.3f}')\n",
        "                ax.legend(fontsize=8)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    fig.suptitle(f'Context Influence Decay (Layer {target_layer})',\n",
        "                 fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_influence_decay_curves(influence_data, target_layer)\n",
        "\n",
        "# ============================\n",
        "# Visualization 2: Context Memory Heatmap\n",
        "# ============================\n",
        "\n",
        "def plot_context_memory_heatmap(influence_data, target_layer, max_distance=16):\n",
        "    \"\"\"\n",
        "    Heatmap showing which semantic group pairs have longest-range dependencies.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get all semantic groups\n",
        "    groups_ctx = sorted(influence_data.keys())\n",
        "    groups_tgt_all = set()\n",
        "    for gc in groups_ctx:\n",
        "        groups_tgt_all.update(influence_data[gc].keys())\n",
        "    groups_tgt = sorted(groups_tgt_all)\n",
        "\n",
        "    # Compute \"memory strength\" = average correlation at distance 8-16\n",
        "    memory_matrix = np.zeros((len(groups_ctx), len(groups_tgt)))\n",
        "\n",
        "    for i, group_ctx in enumerate(groups_ctx):\n",
        "        for j, group_tgt in enumerate(groups_tgt):\n",
        "            if group_tgt in influence_data[group_ctx]:\n",
        "                # Average over long distances\n",
        "                long_range_corrs = []\n",
        "                for dist in range(8, min(max_distance + 1, 33)):\n",
        "                    if dist in influence_data[group_ctx][group_tgt]:\n",
        "                        for canon_h in influence_data[group_ctx][group_tgt][dist]:\n",
        "                            val = influence_data[group_ctx][group_tgt][dist][canon_h]\n",
        "                            if val != 0:\n",
        "                                long_range_corrs.append(val)\n",
        "\n",
        "                if long_range_corrs:\n",
        "                    memory_matrix[i, j] = np.mean(long_range_corrs)\n",
        "\n",
        "    # Plot\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "    im = ax.imshow(memory_matrix, cmap='RdYlGn', aspect='auto', vmin=-0.2, vmax=0.5)\n",
        "\n",
        "    ax.set_xticks(range(len(groups_tgt)))\n",
        "    ax.set_xticklabels(groups_tgt, rotation=45, ha='right')\n",
        "    ax.set_yticks(range(len(groups_ctx)))\n",
        "    ax.set_yticklabels(groups_ctx)\n",
        "\n",
        "    ax.set_xlabel('Target Token Group (position t)', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('Context Token Group (position t-k)', fontsize=12, fontweight='bold')\n",
        "    ax.set_title(\n",
        "        f'Long-Range Context Memory Strength (Layer {target_layer})\\n'\n",
        "        f'Average routing correlation at distances 8-16 tokens',\n",
        "        fontsize=13, fontweight='bold'\n",
        "    )\n",
        "\n",
        "    plt.colorbar(im, ax=ax, label='Correlation')\n",
        "\n",
        "    # Annotate strong effects\n",
        "    for i in range(len(groups_ctx)):\n",
        "        for j in range(len(groups_tgt)):\n",
        "            val = memory_matrix[i, j]\n",
        "            if abs(val) > 0.2:\n",
        "                ax.text(j, i, f'{val:.2f}', ha='center', va='center',\n",
        "                       fontsize=8, color='white' if abs(val) > 0.35 else 'black')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print strongest long-range effects\n",
        "    print(f\"\\nStrongest Long-Range Dependencies (distance 8-16):\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    effects = []\n",
        "    for i, group_ctx in enumerate(groups_ctx):\n",
        "        for j, group_tgt in enumerate(groups_tgt):\n",
        "            effects.append((memory_matrix[i, j], group_ctx, group_tgt))\n",
        "\n",
        "    effects.sort(key=lambda x: abs(x[0]), reverse=True)\n",
        "\n",
        "    for corr, group_ctx, group_tgt in effects[:10]:\n",
        "        if abs(corr) > 0.1:\n",
        "            print(f\"  {group_ctx:>15s}  {group_tgt:>15s}: r = {corr:+.3f}\")\n",
        "\n",
        "plot_context_memory_heatmap(influence_data, target_layer, max_distance=16)\n",
        "\n",
        "# ============================\n",
        "# Visualization 3: Per-Head Context Sensitivity\n",
        "# ============================\n",
        "\n",
        "def plot_head_context_sensitivity(influence_data, target_layer):\n",
        "    \"\"\"\n",
        "    Which canonical heads are most sensitive to long-range context?\n",
        "    \"\"\"\n",
        "    H = cfg.num_heads\n",
        "\n",
        "    # For each canonical head, compute average long-range correlation\n",
        "    head_sensitivity = np.zeros(H)\n",
        "    head_counts = np.zeros(H)\n",
        "\n",
        "    for group_ctx in influence_data:\n",
        "        for group_tgt in influence_data[group_ctx]:\n",
        "            for dist in influence_data[group_ctx][group_tgt]:\n",
        "                if dist >= 8:  # Long-range\n",
        "                    for canon_h in influence_data[group_ctx][group_tgt][dist]:\n",
        "                        val = influence_data[group_ctx][group_tgt][dist][canon_h]\n",
        "                        if val != 0:\n",
        "                            head_sensitivity[canon_h] += abs(val)\n",
        "                            head_counts[canon_h] += 1\n",
        "\n",
        "    # Average\n",
        "    for h in range(H):\n",
        "        if head_counts[h] > 0:\n",
        "            head_sensitivity[h] /= head_counts[h]\n",
        "\n",
        "    # Plot\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    colors = ['red' if s > head_sensitivity.mean() else 'blue' for s in head_sensitivity]\n",
        "    bars = ax.bar(range(H), head_sensitivity, color=colors, alpha=0.7)\n",
        "\n",
        "    ax.axhline(head_sensitivity.mean(), color='black', linestyle='--',\n",
        "              linewidth=1.5, label=f'Mean: {head_sensitivity.mean():.3f}')\n",
        "\n",
        "    ax.set_xlabel('Canonical Head', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('Average Long-Range Correlation (|r|)', fontsize=12, fontweight='bold')\n",
        "    ax.set_title(\n",
        "        f'Canonical Head Sensitivity to Long-Range Context (Layer {target_layer})',\n",
        "        fontsize=13, fontweight='bold'\n",
        "    )\n",
        "    ax.set_xticks(range(H))\n",
        "    ax.set_xticklabels([f'H{i}' for i in range(H)])\n",
        "    ax.grid(True, alpha=0.3, axis='y')\n",
        "    ax.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\nCanonical Head Context Sensitivity Rankings:\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    ranked = sorted(enumerate(head_sensitivity), key=lambda x: -x[1])\n",
        "    for rank, (h, sens) in enumerate(ranked, 1):\n",
        "        print(f\"  {rank:>2d}. Canonical Head {h}: {sens:.4f}\")\n",
        "\n",
        "plot_head_context_sensitivity(influence_data, target_layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ccMhdS4Zk--G"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Experiment 6: How Long Do Semantic Groups \"Persist\" in Slot Memory?\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "\n",
        "# ============================\n",
        "# Persistence Measurement\n",
        "# ============================\n",
        "\n",
        "def measure_semantic_persistence(\n",
        "    model, cfg, tokenizer, token_to_group, device, num_batches=30\n",
        "):\n",
        "    \"\"\"\n",
        "    For each semantic group, measure how long its \"signature\" persists\n",
        "    in the routing patterns after the token has passed.\n",
        "\n",
        "    Method: At position t where group G appears, measure how long\n",
        "    subsequent positions show routing similar to G's characteristic pattern.\n",
        "    \"\"\"\n",
        "\n",
        "    # Establish canonicalization\n",
        "    gen = make_batch_generator(cfg, split='val', device=device, batches_per_epoch=1, infinite=False)\n",
        "    xb_canon, _ = next(gen)\n",
        "    canon_perm, _, valid_layers = compute_canonical_permutations(model, xb_canon, device)\n",
        "\n",
        "    target_layer = valid_layers[len(valid_layers) // 2]\n",
        "    print(f\"Measuring semantic persistence at Layer {target_layer}...\")\n",
        "\n",
        "    H = cfg.num_heads\n",
        "    K = cfg.num_slots\n",
        "    perm = canon_perm[target_layer]\n",
        "\n",
        "    # First pass: build \"signature\" routing pattern for each group\n",
        "    group_signatures = defaultdict(lambda: defaultdict(lambda: np.zeros(K)))\n",
        "    group_sig_counts = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    gen = make_batch_generator(cfg, split='val', device=device,\n",
        "                               batches_per_epoch=15, infinite=False)\n",
        "\n",
        "    print(\"Building semantic signatures...\")\n",
        "    for batch_idx, (xb, yb) in enumerate(gen):\n",
        "        B, T = xb.shape\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _, infos = model(xb, return_info=True, info_level=\"basic\",\n",
        "                           info_cfg={'store_read_weights': True})\n",
        "\n",
        "        info = infos[target_layer]\n",
        "        if info is None:\n",
        "            continue\n",
        "\n",
        "        rw = info.get('read_weights', None)\n",
        "        if rw is None:\n",
        "            continue\n",
        "\n",
        "        for b in range(B):\n",
        "            for t in range(T):\n",
        "                token_id = xb[b, t].item()\n",
        "                group = token_to_group.get(token_id, 'other')\n",
        "\n",
        "                if group == 'other':\n",
        "                    continue\n",
        "\n",
        "                # Record routing for this group\n",
        "                for canon_h in range(H):\n",
        "                    phys_h = perm[canon_h]\n",
        "                    routing = rw[b, phys_h, t, :].cpu().numpy()\n",
        "\n",
        "                    group_signatures[group][canon_h] += routing\n",
        "                    group_sig_counts[group][canon_h] += 1\n",
        "\n",
        "    # Normalize signatures\n",
        "    for group in group_signatures:\n",
        "        for canon_h in group_signatures[group]:\n",
        "            count = group_sig_counts[group][canon_h]\n",
        "            if count > 0:\n",
        "                group_signatures[group][canon_h] /= count\n",
        "\n",
        "    print(f\"Signatures built for {len(group_signatures)} groups\")\n",
        "\n",
        "    # Second pass: measure persistence\n",
        "    # persistence[group][lookahead] = list of similarity scores\n",
        "    persistence = defaultdict(lambda: defaultdict(list))\n",
        "    max_lookahead = 20\n",
        "\n",
        "    gen = make_batch_generator(cfg, split='val', device=device,\n",
        "                               batches_per_epoch=num_batches, infinite=False)\n",
        "\n",
        "    print(\"Measuring persistence...\")\n",
        "    for batch_idx, (xb, yb) in enumerate(gen):\n",
        "        B, T = xb.shape\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _, infos = model(xb, return_info=True, info_level=\"basic\",\n",
        "                           info_cfg={'store_read_weights': True})\n",
        "\n",
        "        info = infos[target_layer]\n",
        "        if info is None:\n",
        "            continue\n",
        "\n",
        "        rw = info.get('read_weights', None)\n",
        "        if rw is None:\n",
        "            continue\n",
        "\n",
        "        for b in range(B):\n",
        "            for t in range(T - max_lookahead):\n",
        "                token_id = xb[b, t].item()\n",
        "                group = token_to_group.get(token_id, 'other')\n",
        "\n",
        "                if group == 'other' or group not in group_signatures:\n",
        "                    continue\n",
        "\n",
        "                # Measure similarity at future positions\n",
        "                for lookahead in range(1, max_lookahead + 1):\n",
        "                    future_pos = t + lookahead\n",
        "                    if future_pos >= T:\n",
        "                        break\n",
        "\n",
        "                    # Compute cosine similarity between future routing and group signature\n",
        "                    similarities = []\n",
        "                    for canon_h in range(H):\n",
        "                        phys_h = perm[canon_h]\n",
        "                        future_routing = rw[b, phys_h, future_pos, :].cpu().numpy()\n",
        "                        signature = group_signatures[group][canon_h]\n",
        "\n",
        "                        # Cosine similarity\n",
        "                        dot = np.dot(future_routing, signature)\n",
        "                        norm_future = np.linalg.norm(future_routing)\n",
        "                        norm_sig = np.linalg.norm(signature)\n",
        "\n",
        "                        if norm_future > 1e-9 and norm_sig > 1e-9:\n",
        "                            sim = dot / (norm_future * norm_sig)\n",
        "                            similarities.append(sim)\n",
        "\n",
        "                    if similarities:\n",
        "                        persistence[group][lookahead].append(np.mean(similarities))\n",
        "\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            print(f\"  Batch {batch_idx + 1}/{num_batches}\")\n",
        "\n",
        "    # Average persistence curves\n",
        "    persistence_curves = {}\n",
        "    for group in persistence:\n",
        "        curve = []\n",
        "        for lookahead in range(1, max_lookahead + 1):\n",
        "            if lookahead in persistence[group]:\n",
        "                curve.append(np.mean(persistence[group][lookahead]))\n",
        "            else:\n",
        "                curve.append(0)\n",
        "        persistence_curves[group] = curve\n",
        "\n",
        "    return persistence_curves, max_lookahead, target_layer\n",
        "\n",
        "persistence_curves, max_lookahead, target_layer = measure_semantic_persistence(\n",
        "    model, cfg, tokenizer, token_to_group, DEVICE, num_batches=30\n",
        ")\n",
        "\n",
        "# ============================\n",
        "# Visualization\n",
        "# ============================\n",
        "\n",
        "def plot_persistence_curves(persistence_curves, max_lookahead, target_layer):\n",
        "    \"\"\"\n",
        "    Plot persistence decay curves for each semantic group.\n",
        "    \"\"\"\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 7))\n",
        "\n",
        "    lookaheads = range(1, max_lookahead + 1)\n",
        "\n",
        "    # Color palette\n",
        "    groups = sorted(persistence_curves.keys())\n",
        "    colors = sns.color_palette(\"husl\", len(groups))\n",
        "\n",
        "    for group, color in zip(groups, colors):\n",
        "        curve = persistence_curves[group]\n",
        "        ax.plot(lookaheads, curve, marker='o', label=group,\n",
        "               linewidth=2, markersize=4, color=color)\n",
        "\n",
        "    ax.axhline(0, color='black', linewidth=0.8, linestyle='--', alpha=0.3)\n",
        "    ax.set_xlabel('Lookahead (tokens)', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('Similarity to Signature', fontsize=12, fontweight='bold')\n",
        "    ax.set_title(\n",
        "        f'Semantic Persistence in Routing Patterns (Layer {target_layer})\\n'\n",
        "        f'How long does a semantic group\\'s \"signature\" persist?',\n",
        "        fontsize=13, fontweight='bold'\n",
        "    )\n",
        "    ax.legend(title='Semantic Group', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Compute half-life for each group\n",
        "    print(f\"\\nSemantic Persistence Half-Lives:\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    for group in sorted(persistence_curves.keys()):\n",
        "        curve = np.array(persistence_curves[group])\n",
        "\n",
        "        # Find half-life (position where similarity drops to 50% of initial)\n",
        "        if curve[0] > 0:\n",
        "            target = curve[0] * 0.5\n",
        "            half_life_idx = np.where(curve < target)[0]\n",
        "\n",
        "            if len(half_life_idx) > 0:\n",
        "                half_life = half_life_idx[0] + 1\n",
        "            else:\n",
        "                half_life = max_lookahead\n",
        "        else:\n",
        "            half_life = 0\n",
        "\n",
        "        print(f\"  {group:>15s}: {half_life:>3d} tokens\")\n",
        "\n",
        "    print(\"\\nInterpretation:\")\n",
        "    print(\"   Longer half-life = semantic group maintains influence longer\")\n",
        "    print(\"   Ownership/quantifiers expected to have long persistence\")\n",
        "    print(\"   Colors/numbers might have shorter, local influence\")\n",
        "\n",
        "plot_persistence_curves(persistence_curves, max_lookahead, target_layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RUWq_ZBspUKt"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Refined Experiment: What DOES Cluster in Slot Space?\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_score\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "\n",
        "# ============================\n",
        "# Alternative Grouping Schemes\n",
        "# ============================\n",
        "\n",
        "def classify_token_by_frequency(token_id, tokenizer, freq_map):\n",
        "    \"\"\"\n",
        "    Group tokens by frequency (common vs rare).\n",
        "    \"\"\"\n",
        "    # This is a placeholder - in practice you'd use actual corpus frequencies\n",
        "    # For now, use token_id as proxy (lower IDs are often more frequent in BPE)\n",
        "    if token_id < 1000:\n",
        "        return 'very_common'\n",
        "    elif token_id < 5000:\n",
        "        return 'common'\n",
        "    elif token_id < 20000:\n",
        "        return 'uncommon'\n",
        "    else:\n",
        "        return 'rare'\n",
        "\n",
        "def classify_token_by_length(token_id, tokenizer):\n",
        "    \"\"\"\n",
        "    Group by subword length.\n",
        "    \"\"\"\n",
        "    token_str = tokenizer.decode([token_id])\n",
        "    length = len(token_str.strip())\n",
        "\n",
        "    if length == 1:\n",
        "        return 'single_char'\n",
        "    elif length <= 3:\n",
        "        return 'short'\n",
        "    elif length <= 6:\n",
        "        return 'medium'\n",
        "    else:\n",
        "        return 'long'\n",
        "\n",
        "def classify_token_by_position(position, total_length):\n",
        "    \"\"\"\n",
        "    Group by position in sequence.\n",
        "    \"\"\"\n",
        "    relative_pos = position / total_length\n",
        "\n",
        "    if relative_pos < 0.2:\n",
        "        return 'early'\n",
        "    elif relative_pos < 0.4:\n",
        "        return 'early_mid'\n",
        "    elif relative_pos < 0.6:\n",
        "        return 'middle'\n",
        "    elif relative_pos < 0.8:\n",
        "        return 'late_mid'\n",
        "    else:\n",
        "        return 'late'\n",
        "\n",
        "def classify_token_syntactic(token_id, tokenizer):\n",
        "    \"\"\"\n",
        "    Rough syntactic classification.\n",
        "    \"\"\"\n",
        "    token_str = tokenizer.decode([token_id]).strip().lower()\n",
        "\n",
        "    # Function words\n",
        "    function = {'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been',\n",
        "                'and', 'or', 'but', 'if', 'when', 'where', 'who', 'what',\n",
        "                'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}\n",
        "\n",
        "    # Pronouns\n",
        "    pronouns = {'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her',\n",
        "                'us', 'them', 'my', 'your', 'his', 'its', 'our', 'their'}\n",
        "\n",
        "    # Modals\n",
        "    modals = {'can', 'could', 'may', 'might', 'must', 'shall', 'should', 'will', 'would'}\n",
        "\n",
        "    if token_str in function:\n",
        "        return 'function'\n",
        "    elif token_str in pronouns:\n",
        "        return 'pronoun'\n",
        "    elif token_str in modals:\n",
        "        return 'modal'\n",
        "    elif token_str.endswith(('ing', 'ed', 'es', 's')):\n",
        "        return 'inflected'\n",
        "    else:\n",
        "        return 'content'\n",
        "\n",
        "# ============================\n",
        "# Re-collect with Multiple Grouping Schemes\n",
        "# ============================\n",
        "\n",
        "def collect_states_multiple_schemes(\n",
        "    model, cfg, tokenizer, device, num_batches=20, layer_idx=6, max_per_group=300\n",
        "):\n",
        "    \"\"\"\n",
        "    Collect routing states grouped by MULTIPLE classification schemes.\n",
        "    \"\"\"\n",
        "\n",
        "    # Establish canonicalization\n",
        "    gen = make_batch_generator(cfg, split='val', device=device, batches_per_epoch=1, infinite=False)\n",
        "    xb_canon, _ = next(gen)\n",
        "    canon_perm, _, valid_layers = compute_canonical_permutations(model, xb_canon, device)\n",
        "\n",
        "    if layer_idx not in valid_layers:\n",
        "        layer_idx = valid_layers[len(valid_layers) // 2]\n",
        "\n",
        "    perm = canon_perm[layer_idx]\n",
        "    H = cfg.num_heads\n",
        "    K = cfg.num_slots\n",
        "\n",
        "    print(f\"Collecting with multiple classification schemes (Layer {layer_idx})...\")\n",
        "\n",
        "    # Storage for different schemes\n",
        "    schemes = {\n",
        "        'frequency': defaultdict(lambda: defaultdict(list)),\n",
        "        'length': defaultdict(lambda: defaultdict(list)),\n",
        "        'position': defaultdict(lambda: defaultdict(list)),\n",
        "        'syntactic': defaultdict(lambda: defaultdict(list)),\n",
        "    }\n",
        "\n",
        "    counts = {\n",
        "        'frequency': defaultdict(int),\n",
        "        'length': defaultdict(int),\n",
        "        'position': defaultdict(int),\n",
        "        'syntactic': defaultdict(int),\n",
        "    }\n",
        "\n",
        "    gen = make_batch_generator(cfg, split='val', device=device,\n",
        "                               batches_per_epoch=num_batches, infinite=False)\n",
        "\n",
        "    for batch_idx, (xb, yb) in enumerate(gen):\n",
        "        B, T = xb.shape\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _, infos = model(xb, return_info=True, info_level=\"basic\",\n",
        "                           info_cfg={'store_read_weights': True})\n",
        "\n",
        "        info = infos[layer_idx]\n",
        "        if info is None:\n",
        "            continue\n",
        "\n",
        "        rw = info.get('read_weights', None)\n",
        "        if rw is None:\n",
        "            continue\n",
        "\n",
        "        for b in range(B):\n",
        "            for t in range(T):\n",
        "                token_id = xb[b, t].item()\n",
        "\n",
        "                # Classify by multiple schemes\n",
        "                freq_group = classify_token_by_frequency(token_id, tokenizer, None)\n",
        "                len_group = classify_token_by_length(token_id, tokenizer)\n",
        "                pos_group = classify_token_by_position(t, T)\n",
        "                syn_group = classify_token_syntactic(token_id, tokenizer)\n",
        "\n",
        "                # Check limits\n",
        "                if (counts['frequency'][freq_group] >= max_per_group and\n",
        "                    counts['length'][len_group] >= max_per_group and\n",
        "                    counts['position'][pos_group] >= max_per_group and\n",
        "                    counts['syntactic'][syn_group] >= max_per_group):\n",
        "                    continue\n",
        "\n",
        "                # Collect routing for canonical head 0 (can do others later)\n",
        "                canon_h = 0\n",
        "                phys_h = perm[canon_h]\n",
        "                routing = rw[b, phys_h, t, :].cpu().numpy()\n",
        "\n",
        "                # Store in each scheme\n",
        "                if counts['frequency'][freq_group] < max_per_group:\n",
        "                    schemes['frequency'][freq_group][canon_h].append(routing)\n",
        "                    counts['frequency'][freq_group] += 1\n",
        "\n",
        "                if counts['length'][len_group] < max_per_group:\n",
        "                    schemes['length'][len_group][canon_h].append(routing)\n",
        "                    counts['length'][len_group] += 1\n",
        "\n",
        "                if counts['position'][pos_group] < max_per_group:\n",
        "                    schemes['position'][pos_group][canon_h].append(routing)\n",
        "                    counts['position'][pos_group] += 1\n",
        "\n",
        "                if counts['syntactic'][syn_group] < max_per_group:\n",
        "                    schemes['syntactic'][syn_group][canon_h].append(routing)\n",
        "                    counts['syntactic'][syn_group] += 1\n",
        "\n",
        "        if (batch_idx + 1) % 5 == 0:\n",
        "            print(f\"  Batch {batch_idx + 1}/{num_batches}\")\n",
        "            print(f\"    Frequency: {dict(counts['frequency'])}\")\n",
        "            print(f\"    Syntactic: {dict(counts['syntactic'])}\")\n",
        "\n",
        "    # Convert to numpy\n",
        "    for scheme in schemes:\n",
        "        for group in schemes[scheme]:\n",
        "            for canon_h in schemes[scheme][group]:\n",
        "                schemes[scheme][group][canon_h] = np.array(schemes[scheme][group][canon_h])\n",
        "\n",
        "    return schemes, layer_idx\n",
        "\n",
        "schemes_data, layer_idx = collect_states_multiple_schemes(\n",
        "    model, cfg, tokenizer, DEVICE, num_batches=25, layer_idx=6, max_per_group=300\n",
        ")\n",
        "\n",
        "# ============================\n",
        "# Compare Clustering Quality Across Schemes\n",
        "# ============================\n",
        "\n",
        "def test_clustering_quality(scheme_data, scheme_name, canon_h=0):\n",
        "    \"\"\"\n",
        "    Run t-SNE and compute clustering metrics for a grouping scheme.\n",
        "    \"\"\"\n",
        "\n",
        "    # Collect all states\n",
        "    all_states = []\n",
        "    all_groups = []\n",
        "\n",
        "    for group in sorted(scheme_data.keys()):\n",
        "        if canon_h in scheme_data[group]:\n",
        "            states = scheme_data[group][canon_h]\n",
        "            all_states.append(states)\n",
        "            all_groups.extend([group] * len(states))\n",
        "\n",
        "    if len(all_states) == 0:\n",
        "        return None\n",
        "\n",
        "    all_states = np.vstack(all_states)\n",
        "\n",
        "    print(f\"\\n{scheme_name.upper()} Scheme:\")\n",
        "    print(f\"  Samples: {len(all_states)}, Groups: {len(set(all_groups))}\")\n",
        "\n",
        "    # PCA\n",
        "    n_comp = min(20, all_states.shape[1] - 1)\n",
        "    pca = PCA(n_components=n_comp)\n",
        "    states_pca = pca.fit_transform(all_states)\n",
        "\n",
        "    # t-SNE\n",
        "    perplexity = min(30, len(states_pca) // 4)\n",
        "    tsne = TSNE(n_components=2, perplexity=perplexity, max_iter=1000, random_state=42)\n",
        "    coords = tsne.fit_transform(states_pca)\n",
        "\n",
        "    # Clustering metrics\n",
        "    unique_groups = sorted(set(all_groups))\n",
        "    group_to_int = {g: i for i, g in enumerate(unique_groups)}\n",
        "    labels = np.array([group_to_int[g] for g in all_groups])\n",
        "\n",
        "    sil_score = silhouette_score(coords, labels) if len(unique_groups) > 1 else 0\n",
        "\n",
        "    # Separation ratio\n",
        "    from scipy.spatial.distance import pdist, squareform\n",
        "    distances = squareform(pdist(coords))\n",
        "\n",
        "    within = []\n",
        "    between = []\n",
        "    for i in range(len(all_groups)):\n",
        "        for j in range(i + 1, len(all_groups)):\n",
        "            if all_groups[i] == all_groups[j]:\n",
        "                within.append(distances[i, j])\n",
        "            else:\n",
        "                between.append(distances[i, j])\n",
        "\n",
        "    within_mean = np.mean(within) if within else 0\n",
        "    between_mean = np.mean(between) if between else 0\n",
        "    sep_ratio = between_mean / (within_mean + 1e-9)\n",
        "\n",
        "    print(f\"  Silhouette: {sil_score:.3f}\")\n",
        "    print(f\"  Separation ratio: {sep_ratio:.2f}\")\n",
        "\n",
        "    # Plot\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "    color_palette = sns.color_palette(\"husl\", len(unique_groups))\n",
        "    group_to_color = {g: color_palette[i] for i, g in enumerate(unique_groups)}\n",
        "\n",
        "    for group in unique_groups:\n",
        "        mask = np.array([g == group for g in all_groups])\n",
        "        ax.scatter(coords[mask, 0], coords[mask, 1],\n",
        "                  c=[group_to_color[group]], label=group,\n",
        "                  alpha=0.6, s=25, edgecolors='none')\n",
        "\n",
        "    ax.set_xlabel('t-SNE 1', fontsize=11)\n",
        "    ax.set_ylabel('t-SNE 2', fontsize=11)\n",
        "    ax.set_title(f'{scheme_name.title()} Clustering (Layer {layer_idx})\\n'\n",
        "                f'Silhouette: {sil_score:.3f}, Sep Ratio: {sep_ratio:.2f}',\n",
        "                fontsize=13, fontweight='bold')\n",
        "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
        "    ax.grid(True, alpha=0.2)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return sil_score, sep_ratio\n",
        "\n",
        "# Test each scheme\n",
        "results = {}\n",
        "for scheme_name, scheme_data in schemes_data.items():\n",
        "    result = test_clustering_quality(scheme_data, scheme_name, canon_h=0)\n",
        "    if result:\n",
        "        results[scheme_name] = result\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CLUSTERING QUALITY COMPARISON\")\n",
        "print(\"=\"*70)\n",
        "print(f\"{'Scheme':<15} {'Silhouette':<15} {'Separation Ratio':<20}\")\n",
        "print(\"-\"*70)\n",
        "for scheme_name in ['frequency', 'length', 'position', 'syntactic']:\n",
        "    if scheme_name in results:\n",
        "        sil, sep = results[scheme_name]\n",
        "        print(f\"{scheme_name:<15} {sil:>14.3f} {sep:>19.2f}\")\n",
        "\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"  Best clustering scheme reveals the PRIMARY organizing principle\")\n",
        "print(\"  Positive silhouette + high separation = strong organization\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6ZVdn9XRpUTP"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title What Information Do Slots Actually Encode?\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from scipy.stats import entropy\n",
        "\n",
        "# ============================\n",
        "# Mutual Information Analysis\n",
        "# ============================\n",
        "\n",
        "def compute_slot_mutual_information(\n",
        "    model, cfg, tokenizer, token_to_group, device, num_batches=30, layer_idx=6\n",
        "):\n",
        "    \"\"\"\n",
        "    Compute mutual information between slot selection and various token properties.\n",
        "    Higher MI = slot selection is more predictive of that property.\n",
        "    \"\"\"\n",
        "\n",
        "    # Establish canonicalization\n",
        "    gen = make_batch_generator(cfg, split='val', device=device, batches_per_epoch=1, infinite=False)\n",
        "    xb_canon, _ = next(gen)\n",
        "    canon_perm, _, valid_layers = compute_canonical_permutations(model, xb_canon, device)\n",
        "\n",
        "    if layer_idx not in valid_layers:\n",
        "        layer_idx = valid_layers[len(valid_layers) // 2]\n",
        "\n",
        "    perm = canon_perm[layer_idx]\n",
        "    H = cfg.num_heads\n",
        "    K = cfg.num_slots\n",
        "\n",
        "    print(f\"Computing mutual information (Layer {layer_idx})...\")\n",
        "\n",
        "    # Joint distributions: P(slot, property)\n",
        "    # We'll track for canonical head 0\n",
        "    canon_h = 0\n",
        "\n",
        "    joint_semantic = np.zeros((K, len(SEMANTIC_GROUPS)))\n",
        "    joint_frequency = np.zeros((K, 4))  # 4 frequency bins\n",
        "    joint_syntactic = np.zeros((K, 5))  # 5 syntactic categories\n",
        "    joint_position = np.zeros((K, 5))   # 5 position bins\n",
        "\n",
        "    semantic_groups_list = sorted([g for g in SEMANTIC_GROUPS.keys()])\n",
        "\n",
        "    gen = make_batch_generator(cfg, split='val', device=device,\n",
        "                               batches_per_epoch=num_batches, infinite=False)\n",
        "\n",
        "    for batch_idx, (xb, yb) in enumerate(gen):\n",
        "        B, T = xb.shape\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _, infos = model(xb, return_info=True, info_level=\"basic\",\n",
        "                           info_cfg={'store_read_weights': True})\n",
        "\n",
        "        info = infos[layer_idx]\n",
        "        if info is None:\n",
        "            continue\n",
        "\n",
        "        rw = info.get('read_weights', None)\n",
        "        if rw is None:\n",
        "            continue\n",
        "\n",
        "        phys_h = perm[canon_h]\n",
        "\n",
        "        for b in range(B):\n",
        "            for t in range(T):\n",
        "                token_id = xb[b, t].item()\n",
        "\n",
        "                # Get selected slot (argmax)\n",
        "                slot = rw[b, phys_h, t, :].argmax().item()\n",
        "\n",
        "                # Semantic group\n",
        "                sem_group = token_to_group.get(token_id, 'other')\n",
        "                if sem_group in semantic_groups_list:\n",
        "                    sem_idx = semantic_groups_list.index(sem_group)\n",
        "                    joint_semantic[slot, sem_idx] += 1\n",
        "\n",
        "                # Frequency\n",
        "                freq_group = classify_token_by_frequency(token_id, tokenizer, None)\n",
        "                freq_map = {'very_common': 0, 'common': 1, 'uncommon': 2, 'rare': 3}\n",
        "                if freq_group in freq_map:\n",
        "                    joint_frequency[slot, freq_map[freq_group]] += 1\n",
        "\n",
        "                # Syntactic\n",
        "                syn_group = classify_token_syntactic(token_id, tokenizer)\n",
        "                syn_map = {'function': 0, 'pronoun': 1, 'modal': 2, 'inflected': 3, 'content': 4}\n",
        "                if syn_group in syn_map:\n",
        "                    joint_syntactic[slot, syn_map[syn_group]] += 1\n",
        "\n",
        "                # Position\n",
        "                pos_group = classify_token_by_position(t, T)\n",
        "                pos_map = {'early': 0, 'early_mid': 1, 'middle': 2, 'late_mid': 3, 'late': 4}\n",
        "                if pos_group in pos_map:\n",
        "                    joint_position[slot, pos_map[pos_group]] += 1\n",
        "\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            print(f\"  Batch {batch_idx + 1}/{num_batches}\")\n",
        "\n",
        "    # Compute mutual information: MI(X;Y) = H(X) + H(Y) - H(X,Y)\n",
        "    def mutual_info(joint):\n",
        "        \"\"\"Compute MI from joint distribution.\"\"\"\n",
        "        joint = joint / joint.sum()  # Normalize\n",
        "\n",
        "        marginal_x = joint.sum(axis=1)  # P(slot)\n",
        "        marginal_y = joint.sum(axis=0)  # P(property)\n",
        "\n",
        "        H_X = entropy(marginal_x)\n",
        "        H_Y = entropy(marginal_y)\n",
        "        H_XY = entropy(joint.flatten())\n",
        "\n",
        "        MI = H_X + H_Y - H_XY\n",
        "\n",
        "        # Normalized MI\n",
        "        MI_norm = MI / min(H_X, H_Y) if min(H_X, H_Y) > 0 else 0\n",
        "\n",
        "        return MI, MI_norm, H_X, H_Y\n",
        "\n",
        "    mi_semantic, mi_semantic_norm, H_slot_sem, H_sem = mutual_info(joint_semantic)\n",
        "    mi_frequency, mi_frequency_norm, H_slot_freq, H_freq = mutual_info(joint_frequency)\n",
        "    mi_syntactic, mi_syntactic_norm, H_slot_syn, H_syn = mutual_info(joint_syntactic)\n",
        "    mi_position, mi_position_norm, H_slot_pos, H_pos = mutual_info(joint_position)\n",
        "\n",
        "    return {\n",
        "        'semantic': (mi_semantic, mi_semantic_norm),\n",
        "        'frequency': (mi_frequency, mi_frequency_norm),\n",
        "        'syntactic': (mi_syntactic, mi_syntactic_norm),\n",
        "        'position': (mi_position, mi_position_norm),\n",
        "    }\n",
        "\n",
        "mi_results = compute_slot_mutual_information(\n",
        "    model, cfg, tokenizer, token_to_group, DEVICE, num_batches=30, layer_idx=6\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"MUTUAL INFORMATION: Slot Selection vs Token Properties\")\n",
        "print(\"=\"*70)\n",
        "print(f\"{'Property':<15} {'MI (bits)':<15} {'Normalized MI':<15}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "for prop_name in ['semantic', 'frequency', 'syntactic', 'position']:\n",
        "    mi, mi_norm = mi_results[prop_name]\n",
        "    print(f\"{prop_name:<15} {mi:>14.4f} {mi_norm:>14.4f}\")\n",
        "\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"  Higher MI = slot selection is more informative about this property\")\n",
        "print(\"  Normalized MI  [0,1]: 0 = independent, 1 = deterministic\")\n",
        "print(\"  This reveals what slots ACTUALLY encode!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YvbfosKhpUZH"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Experiment: Identify and Characterize the Discrete Routing Modes\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_score\n",
        "import seaborn as sns\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "# ============================\n",
        "# Step 1: Re-collect t-SNE embeddings with metadata\n",
        "# ============================\n",
        "\n",
        "def collect_routing_with_full_metadata(\n",
        "    model, cfg, tokenizer, token_to_group, device,\n",
        "    num_batches=30, layer_idx=6, max_samples_per_group=400\n",
        "):\n",
        "    \"\"\"\n",
        "    Collect routing states WITH full token metadata for mode analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    # Establish canonicalization\n",
        "    gen = make_batch_generator(cfg, split='val', device=device, batches_per_epoch=1, infinite=False)\n",
        "    xb_canon, _ = next(gen)\n",
        "    canon_perm, _, valid_layers = compute_canonical_permutations(model, xb_canon, device)\n",
        "\n",
        "    if layer_idx not in valid_layers:\n",
        "        layer_idx = valid_layers[len(valid_layers) // 2]\n",
        "\n",
        "    perm = canon_perm[layer_idx]\n",
        "    H = cfg.num_heads\n",
        "    K = cfg.num_slots\n",
        "\n",
        "    print(f\"Collecting routing with metadata (Layer {layer_idx})...\")\n",
        "\n",
        "    # Storage: canonical_head -> list of (routing, metadata)\n",
        "    data_by_head = defaultdict(list)\n",
        "    group_counts = defaultdict(int)\n",
        "\n",
        "    gen = make_batch_generator(cfg, split='val', device=device,\n",
        "                               batches_per_epoch=num_batches, infinite=False)\n",
        "\n",
        "    for batch_idx, (xb, yb) in enumerate(gen):\n",
        "        B, T = xb.shape\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _, infos = model(xb, return_info=True, info_level=\"basic\",\n",
        "                           info_cfg={'store_read_weights': True})\n",
        "\n",
        "        info = infos[layer_idx]\n",
        "        if info is None:\n",
        "            continue\n",
        "\n",
        "        rw = info.get('read_weights', None)\n",
        "        if rw is None:\n",
        "            continue\n",
        "\n",
        "        for b in range(B):\n",
        "            for t in range(T):\n",
        "                token_id = xb[b, t].item()\n",
        "                group = token_to_group.get(token_id, 'other')\n",
        "\n",
        "                # Limit samples per group for balance\n",
        "                if group != 'other' and group_counts[group] >= max_samples_per_group:\n",
        "                    continue\n",
        "\n",
        "                token_str = tokenizer.decode([token_id]).strip()\n",
        "\n",
        "                # Classify additional properties\n",
        "                syntactic = classify_token_syntactic(token_id, tokenizer)\n",
        "                freq_class = classify_token_by_frequency(token_id, tokenizer, None)\n",
        "                pos_class = classify_token_by_position(t, T)\n",
        "\n",
        "                # For each canonical head\n",
        "                for canon_h in range(H):\n",
        "                    phys_h = perm[canon_h]\n",
        "                    routing = rw[b, phys_h, t, :].cpu().numpy()\n",
        "\n",
        "                    # Store routing with metadata\n",
        "                    metadata = {\n",
        "                        'token_id': token_id,\n",
        "                        'token_str': token_str,\n",
        "                        'semantic_group': group,\n",
        "                        'syntactic': syntactic,\n",
        "                        'frequency': freq_class,\n",
        "                        'position': pos_class,\n",
        "                        'seq_position': t,\n",
        "                        'seq_length': T,\n",
        "                        'relative_pos': t / T,\n",
        "                        'batch_idx': batch_idx,\n",
        "                    }\n",
        "\n",
        "                    data_by_head[canon_h].append((routing, metadata))\n",
        "\n",
        "                if group != 'other':\n",
        "                    group_counts[group] += 1\n",
        "\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            print(f\"  Batch {batch_idx + 1}/{num_batches}\")\n",
        "\n",
        "    print(f\"\\nCollected {sum(len(data_by_head[h]) for h in data_by_head)} samples\")\n",
        "    return data_by_head, layer_idx\n",
        "\n",
        "# Collect data\n",
        "print(\"Collecting routing data with full metadata...\")\n",
        "data_by_head, layer_idx = collect_routing_with_full_metadata(\n",
        "    model, cfg, tokenizer, token_to_group, DEVICE,\n",
        "    num_batches=30, layer_idx=6, max_samples_per_group=400\n",
        ")\n",
        "\n",
        "# ============================\n",
        "# Step 2: Compute t-SNE and Cluster\n",
        "# ============================\n",
        "\n",
        "def cluster_routing_modes(data_by_head, canon_h, n_clusters_range=range(4, 12)):\n",
        "    \"\"\"\n",
        "    Perform t-SNE and cluster to identify routing modes.\n",
        "    Tests multiple cluster counts to find optimal.\n",
        "    \"\"\"\n",
        "\n",
        "    if canon_h not in data_by_head or len(data_by_head[canon_h]) == 0:\n",
        "        return None\n",
        "\n",
        "    # Extract routing vectors and metadata\n",
        "    routings = []\n",
        "    metadata_list = []\n",
        "\n",
        "    for routing, metadata in data_by_head[canon_h]:\n",
        "        routings.append(routing)\n",
        "        metadata_list.append(metadata)\n",
        "\n",
        "    routings = np.array(routings)\n",
        "\n",
        "    print(f\"\\nCanonical Head {canon_h}:\")\n",
        "    print(f\"  Samples: {len(routings)}\")\n",
        "\n",
        "    # PCA first\n",
        "    n_pca = min(20, routings.shape[1] - 1)\n",
        "    pca = PCA(n_components=n_pca)\n",
        "    routings_pca = pca.fit_transform(routings)\n",
        "    print(f\"  PCA variance: {pca.explained_variance_ratio_.sum():.2%}\")\n",
        "\n",
        "    # t-SNE\n",
        "    print(\"  Running t-SNE...\")\n",
        "    perplexity = min(30, len(routings) // 4)\n",
        "    tsne = TSNE(n_components=2, perplexity=perplexity, max_iter=1000, random_state=42)\n",
        "    coords = tsne.fit_transform(routings_pca)\n",
        "\n",
        "    # Try different cluster counts\n",
        "    print(\"  Testing cluster counts...\")\n",
        "    best_n = None\n",
        "    best_score = -1\n",
        "    scores = {}\n",
        "\n",
        "    for n in n_clusters_range:\n",
        "        kmeans = KMeans(n_clusters=n, random_state=42, n_init=10)\n",
        "        labels = kmeans.fit_predict(coords)\n",
        "        score = silhouette_score(coords, labels)\n",
        "        scores[n] = score\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_n = n\n",
        "\n",
        "    print(f\"  Best cluster count: {best_n} (silhouette: {best_score:.3f})\")\n",
        "\n",
        "    # Cluster with best n\n",
        "    kmeans = KMeans(n_clusters=best_n, random_state=42, n_init=10)\n",
        "    mode_labels = kmeans.fit_predict(coords)\n",
        "\n",
        "    # Also try agglomerative for comparison\n",
        "    agg = AgglomerativeClustering(n_clusters=best_n)\n",
        "    mode_labels_agg = agg.fit_predict(coords)\n",
        "\n",
        "    return {\n",
        "        'coords': coords,\n",
        "        'metadata': metadata_list,\n",
        "        'routings': routings,\n",
        "        'routings_pca': routings_pca,\n",
        "        'mode_labels': mode_labels,\n",
        "        'mode_labels_agg': mode_labels_agg,\n",
        "        'n_modes': best_n,\n",
        "        'silhouette': best_score,\n",
        "        'cluster_scores': scores,\n",
        "        'kmeans': kmeans,\n",
        "    }\n",
        "\n",
        "# Cluster all heads\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CLUSTERING ROUTING MODES\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "clustered_heads = {}\n",
        "for canon_h in range(cfg.num_heads):\n",
        "    result = cluster_routing_modes(data_by_head, canon_h, n_clusters_range=range(5, 10))\n",
        "    if result:\n",
        "        clustered_heads[canon_h] = result\n",
        "\n",
        "# ============================\n",
        "# Step 3: Visualize Modes\n",
        "# ============================\n",
        "\n",
        "def plot_routing_modes(result, canon_h, layer_idx):\n",
        "    \"\"\"\n",
        "    Plot t-SNE colored by discovered modes.\n",
        "    \"\"\"\n",
        "\n",
        "    coords = result['coords']\n",
        "    mode_labels = result['mode_labels']\n",
        "    n_modes = result['n_modes']\n",
        "    metadata = result['metadata']\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
        "\n",
        "    # Left: Modes\n",
        "    ax = axes[0]\n",
        "    colors = sns.color_palette(\"husl\", n_modes)\n",
        "\n",
        "    for mode_id in range(n_modes):\n",
        "        mask = mode_labels == mode_id\n",
        "        ax.scatter(coords[mask, 0], coords[mask, 1],\n",
        "                  c=[colors[mode_id]], label=f'Mode {mode_id}',\n",
        "                  alpha=0.6, s=25, edgecolors='none')\n",
        "\n",
        "    # Plot cluster centers\n",
        "    centers = result['kmeans'].cluster_centers_\n",
        "    ax.scatter(centers[:, 0], centers[:, 1],\n",
        "              c='black', marker='X', s=200, edgecolors='white', linewidths=2,\n",
        "              label='Centers', zorder=10)\n",
        "\n",
        "    ax.set_xlabel('t-SNE 1', fontsize=12)\n",
        "    ax.set_ylabel('t-SNE 2', fontsize=12)\n",
        "    ax.set_title(f'Routing Modes (Layer {layer_idx}, Canon Head {canon_h})\\n'\n",
        "                f'{n_modes} modes, silhouette={result[\"silhouette\"]:.3f}',\n",
        "                fontsize=13, fontweight='bold')\n",
        "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
        "    ax.grid(True, alpha=0.2)\n",
        "\n",
        "    # Right: Semantic groups overlay\n",
        "    ax = axes[1]\n",
        "    semantic_groups = sorted(set(m['semantic_group'] for m in metadata if m['semantic_group'] != 'other'))\n",
        "    group_colors = sns.color_palette(\"Set2\", len(semantic_groups))\n",
        "    group_to_color = {g: group_colors[i] for i, g in enumerate(semantic_groups)}\n",
        "\n",
        "    for group in semantic_groups:\n",
        "        mask = np.array([m['semantic_group'] == group for m in metadata])\n",
        "        ax.scatter(coords[mask, 0], coords[mask, 1],\n",
        "                  c=[group_to_color[group]], label=group,\n",
        "                  alpha=0.5, s=20, edgecolors='none')\n",
        "\n",
        "    ax.set_xlabel('t-SNE 1', fontsize=12)\n",
        "    ax.set_ylabel('t-SNE 2', fontsize=12)\n",
        "    ax.set_title(f'Same Space: Semantic Groups Overlay',\n",
        "                fontsize=13, fontweight='bold')\n",
        "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
        "    ax.grid(True, alpha=0.2)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot for key heads\n",
        "for canon_h in [0, 1, 3, 7]:  # Different heads including star pattern head 3\n",
        "    if canon_h in clustered_heads:\n",
        "        plot_routing_modes(clustered_heads[canon_h], canon_h, layer_idx)\n",
        "\n",
        "# ============================\n",
        "# Step 4: Characterize Each Mode\n",
        "# ============================\n",
        "\n",
        "def characterize_routing_modes(result, canon_h):\n",
        "    \"\"\"\n",
        "    For each mode, analyze what tokens/properties it contains.\n",
        "    \"\"\"\n",
        "\n",
        "    mode_labels = result['mode_labels']\n",
        "    metadata = result['metadata']\n",
        "    n_modes = result['n_modes']\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"MODE CHARACTERIZATION: Canonical Head {canon_h}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    mode_stats = {}\n",
        "\n",
        "    for mode_id in range(n_modes):\n",
        "        mask = mode_labels == mode_id\n",
        "        mode_metadata = [m for m, include in zip(metadata, mask) if include]\n",
        "\n",
        "        n_samples = len(mode_metadata)\n",
        "\n",
        "        # Gather statistics\n",
        "        semantic_dist = Counter(m['semantic_group'] for m in mode_metadata)\n",
        "        syntactic_dist = Counter(m['syntactic'] for m in mode_metadata)\n",
        "        position_dist = Counter(m['position'] for m in mode_metadata)\n",
        "        freq_dist = Counter(m['frequency'] for m in mode_metadata)\n",
        "\n",
        "        # Relative position stats\n",
        "        rel_positions = [m['relative_pos'] for m in mode_metadata]\n",
        "        mean_rel_pos = np.mean(rel_positions)\n",
        "        std_rel_pos = np.std(rel_positions)\n",
        "\n",
        "        # Sample tokens\n",
        "        sample_tokens = [m['token_str'] for m in mode_metadata[:30]]\n",
        "\n",
        "        mode_stats[mode_id] = {\n",
        "            'n_samples': n_samples,\n",
        "            'semantic': semantic_dist,\n",
        "            'syntactic': syntactic_dist,\n",
        "            'position': position_dist,\n",
        "            'frequency': freq_dist,\n",
        "            'mean_rel_pos': mean_rel_pos,\n",
        "            'std_rel_pos': std_rel_pos,\n",
        "            'sample_tokens': sample_tokens,\n",
        "        }\n",
        "\n",
        "        # Print summary\n",
        "        print(f\"\\n{''*70}\")\n",
        "        print(f\"MODE {mode_id}: {n_samples} samples ({n_samples/len(metadata)*100:.1f}%)\")\n",
        "        print(f\"{''*70}\")\n",
        "\n",
        "        # Top semantic groups\n",
        "        print(f\"  Semantic (top 5):\")\n",
        "        for group, count in semantic_dist.most_common(5):\n",
        "            pct = count / n_samples * 100\n",
        "            print(f\"    {group:>15s}: {count:>4d} ({pct:>5.1f}%)\")\n",
        "\n",
        "        # Top syntactic categories\n",
        "        print(f\"  Syntactic (top 3):\")\n",
        "        for cat, count in syntactic_dist.most_common(3):\n",
        "            pct = count / n_samples * 100\n",
        "            print(f\"    {cat:>15s}: {count:>4d} ({pct:>5.1f}%)\")\n",
        "\n",
        "        # Position bias\n",
        "        print(f\"  Position:\")\n",
        "        for pos, count in position_dist.most_common():\n",
        "            pct = count / n_samples * 100\n",
        "            print(f\"    {pos:>15s}: {count:>4d} ({pct:>5.1f}%)\")\n",
        "\n",
        "        print(f\"  Avg relative position: {mean_rel_pos:.2f}  {std_rel_pos:.2f}\")\n",
        "\n",
        "        # Sample tokens\n",
        "        print(f\"  Sample tokens: {', '.join(sample_tokens[:15])}...\")\n",
        "\n",
        "    return mode_stats\n",
        "\n",
        "# Characterize modes for interesting heads\n",
        "mode_characterizations = {}\n",
        "for canon_h in [0, 1, 3, 7]:\n",
        "    if canon_h in clustered_heads:\n",
        "        mode_characterizations[canon_h] = characterize_routing_modes(\n",
        "            clustered_heads[canon_h], canon_h\n",
        "        )\n",
        "\n",
        "# ============================\n",
        "# Step 5: Mode Comparison Matrix\n",
        "# ============================\n",
        "\n",
        "def compare_modes_across_heads(clustered_heads, mode_characterizations):\n",
        "    \"\"\"\n",
        "    Compare what each mode represents across different heads.\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"CROSS-HEAD MODE COMPARISON\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # For each head, find the most distinctive characteristic of each mode\n",
        "    head_mode_signatures = {}\n",
        "\n",
        "    for canon_h in sorted(clustered_heads.keys()):\n",
        "        stats = mode_characterizations.get(canon_h, {})\n",
        "        n_modes = clustered_heads[canon_h]['n_modes']\n",
        "\n",
        "        signatures = []\n",
        "\n",
        "        for mode_id in range(n_modes):\n",
        "            if mode_id not in stats:\n",
        "                continue\n",
        "\n",
        "            # Find most distinctive feature\n",
        "            semantic_top = stats[mode_id]['semantic'].most_common(1)[0] if stats[mode_id]['semantic'] else ('none', 0)\n",
        "            syntactic_top = stats[mode_id]['syntactic'].most_common(1)[0] if stats[mode_id]['syntactic'] else ('none', 0)\n",
        "            position_top = stats[mode_id]['position'].most_common(1)[0] if stats[mode_id]['position'] else ('none', 0)\n",
        "\n",
        "            signature = {\n",
        "                'semantic': semantic_top[0],\n",
        "                'semantic_pct': semantic_top[1] / stats[mode_id]['n_samples'] * 100,\n",
        "                'syntactic': syntactic_top[0],\n",
        "                'syntactic_pct': syntactic_top[1] / stats[mode_id]['n_samples'] * 100,\n",
        "                'position': position_top[0],\n",
        "                'position_pct': position_top[1] / stats[mode_id]['n_samples'] * 100,\n",
        "                'mean_pos': stats[mode_id]['mean_rel_pos'],\n",
        "            }\n",
        "\n",
        "            signatures.append(signature)\n",
        "\n",
        "        head_mode_signatures[canon_h] = signatures\n",
        "\n",
        "    # Print comparison table\n",
        "    print(f\"\\n{'Head':<6} {'Mode':<6} {'Primary Characteristic':<30} {'Strength':<10}\")\n",
        "    print(\"\" * 70)\n",
        "\n",
        "    for canon_h in sorted(head_mode_signatures.keys()):\n",
        "        for mode_id, sig in enumerate(head_mode_signatures[canon_h]):\n",
        "            # Determine primary characteristic\n",
        "            if sig['syntactic_pct'] > 40:\n",
        "                primary = f\"Syntactic: {sig['syntactic']}\"\n",
        "                strength = f\"{sig['syntactic_pct']:.1f}%\"\n",
        "            elif sig['semantic_pct'] > 35:\n",
        "                primary = f\"Semantic: {sig['semantic']}\"\n",
        "                strength = f\"{sig['semantic_pct']:.1f}%\"\n",
        "            elif sig['position_pct'] > 40:\n",
        "                primary = f\"Position: {sig['position']}\"\n",
        "                strength = f\"{sig['position_pct']:.1f}%\"\n",
        "            else:\n",
        "                primary = f\"Mixed (pos={sig['mean_pos']:.2f})\"\n",
        "                strength = \"\"\n",
        "\n",
        "            print(f\"H{canon_h:<5} M{mode_id:<5} {primary:<30} {strength:<10}\")\n",
        "\n",
        "compare_modes_across_heads(clustered_heads, mode_characterizations)\n",
        "\n",
        "# ============================\n",
        "# Step 6: Mode Routing Patterns\n",
        "# ============================\n",
        "\n",
        "def analyze_mode_routing_patterns(result, canon_h, mode_characterizations):\n",
        "    \"\"\"\n",
        "    For each mode, what does the ROUTING look like?\n",
        "    Which slots are preferred?\n",
        "    \"\"\"\n",
        "\n",
        "    mode_labels = result['mode_labels']\n",
        "    routings = result['routings']\n",
        "    n_modes = result['n_modes']\n",
        "    K = routings.shape[1]\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"MODE ROUTING PATTERNS: Canonical Head {canon_h}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # Average routing per mode\n",
        "    mode_avg_routing = np.zeros((n_modes, K))\n",
        "\n",
        "    for mode_id in range(n_modes):\n",
        "        mask = mode_labels == mode_id\n",
        "        mode_avg_routing[mode_id] = routings[mask].mean(axis=0)\n",
        "\n",
        "    # Plot heatmap\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "    im = ax.imshow(mode_avg_routing, aspect='auto', cmap='YlOrRd', interpolation='nearest')\n",
        "\n",
        "    ax.set_xlabel('Slot', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('Mode', fontsize=12, fontweight='bold')\n",
        "    ax.set_title(f'Average Routing Pattern per Mode (Canon Head {canon_h})',\n",
        "                fontsize=13, fontweight='bold')\n",
        "    ax.set_yticks(range(n_modes))\n",
        "    ax.set_yticklabels([f'Mode {i}' for i in range(n_modes)])\n",
        "\n",
        "    plt.colorbar(im, ax=ax, label='Routing Weight')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print top slots per mode\n",
        "    print(f\"\\nTop Slots per Mode:\")\n",
        "    for mode_id in range(n_modes):\n",
        "        top_slots = np.argsort(-mode_avg_routing[mode_id])[:5]\n",
        "        top_weights = mode_avg_routing[mode_id][top_slots]\n",
        "\n",
        "        stats = mode_characterizations[mode_id]\n",
        "        primary_feature = stats['syntactic'].most_common(1)[0][0] if stats['syntactic'] else 'unknown'\n",
        "\n",
        "        print(f\"  Mode {mode_id} ({primary_feature}):\")\n",
        "        for slot, weight in zip(top_slots, top_weights):\n",
        "            print(f\"    Slot {slot:>2d}: {weight:.3f}\")\n",
        "\n",
        "# Analyze routing patterns\n",
        "for canon_h in [0, 3, 7]:\n",
        "    if canon_h in clustered_heads and canon_h in mode_characterizations:\n",
        "        analyze_mode_routing_patterns(\n",
        "            clustered_heads[canon_h],\n",
        "            canon_h,\n",
        "            mode_characterizations[canon_h]\n",
        "        )\n",
        "\n",
        "print(\"\\n Mode analysis complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTS4TIxKyHi7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KoMwpaaayHtQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Canonical Head Alignment (FOUNDATION / OFFICIAL): heads across layers via read-weights features + Hungarian\n",
        "# This is the \"boringly stable\" base cell for canonicalizing HEADS across LAYERS.\n",
        "# Canonical definition (OFFICIAL):\n",
        "#   - Choose an anchor layer L0 (default: first valid layer that has read_weights)\n",
        "#   - For each layer l, store a permutation:\n",
        "#         canon_perm_to_layer[l][i] = head_index_in_layer_l for canonical_head_i\n",
        "#   - Any per-head tensor/vector x_h from layer l can be canonicalized by:\n",
        "#         x_canon[i] = x_layer[ canon_perm_to_layer[l][i] ]\n",
        "#\n",
        "# Minimal assumptions:\n",
        "#   - You already have: model, xb (token batch)\n",
        "#   - model(xb, return_info=True, info_level=\"basic\", info_cfg=...) returns infos with read_weights per layer\n",
        "#\n",
        "# Dependencies:\n",
        "#   - SciPy optional (Hungarian). If missing, deterministic greedy fallback is used.\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ----------------------------\n",
        "# Capture policy (read_weights only; OOM-friendly)\n",
        "# ----------------------------\n",
        "ASA_CANON_HEAD_INFO_CFG = dict(\n",
        "    store_read_weights=True,\n",
        "    store_read_logits=False,\n",
        "    store_write_logits=False,\n",
        "    store_slot_state_norm=False,\n",
        "    detach_to_cpu=False,    # keep on GPU for reduction; we move reduced tensors to CPU\n",
        "    time_stride=1,\n",
        "    batch_stride=1,\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# Small math helpers\n",
        "# ----------------------------\n",
        "def _safe_log(x: torch.Tensor, eps: float = 1e-9) -> torch.Tensor:\n",
        "    return torch.log(x.clamp_min(eps))\n",
        "\n",
        "def cosine_sim(a: torch.Tensor, b: torch.Tensor, eps: float = 1e-9) -> torch.Tensor:\n",
        "    # cosine over last dim\n",
        "    an = a.norm(dim=-1).clamp_min(eps)\n",
        "    bn = b.norm(dim=-1).clamp_min(eps)\n",
        "    return (a * b).sum(dim=-1) / (an * bn)\n",
        "\n",
        "# ----------------------------\n",
        "# Hungarian aligner (max cosine) with deterministic greedy fallback\n",
        "# ----------------------------\n",
        "def hungarian_align(A_hd: np.ndarray, B_hd: np.ndarray, eps: float = 1e-9):\n",
        "    \"\"\"\n",
        "    Align rows of B to rows of A by maximizing cosine similarity.\n",
        "    A,B: [H,D] numpy arrays\n",
        "    Returns:\n",
        "      perm: array length H where perm[i] is B-row matched to A-row i   (A-head -> B-head)\n",
        "      q   : mean matched cosine similarity\n",
        "      diag: mean diagonal cosine similarity (identity match quality)\n",
        "    \"\"\"\n",
        "    A = A_hd.astype(np.float64, copy=False)\n",
        "    B = B_hd.astype(np.float64, copy=False)\n",
        "\n",
        "    A = A / (np.linalg.norm(A, axis=1, keepdims=True) + eps)\n",
        "    B = B / (np.linalg.norm(B, axis=1, keepdims=True) + eps)\n",
        "\n",
        "    S = A @ B.T  # [H,H]\n",
        "    H = S.shape[0]\n",
        "    diag = float(np.diag(S).mean())\n",
        "\n",
        "    # Hungarian if available\n",
        "    try:\n",
        "        from scipy.optimize import linear_sum_assignment\n",
        "        row, col = linear_sum_assignment(-S)  # maximize\n",
        "        perm = np.empty((H,), dtype=np.int64)\n",
        "        perm[row] = col\n",
        "        q = float(S[row, col].mean())\n",
        "        return perm, q, diag\n",
        "    except Exception:\n",
        "        # deterministic greedy fallback\n",
        "        used = np.zeros((H,), dtype=bool)\n",
        "        perm = np.full((H,), -1, dtype=np.int64)\n",
        "        vals = []\n",
        "        for i in range(H):\n",
        "            j = int(np.argmax(np.where(used, -1e18, S[i])))\n",
        "            used[j] = True\n",
        "            perm[i] = j\n",
        "            vals.append(S[i, j])\n",
        "        q = float(np.mean(vals)) if vals else float(\"nan\")\n",
        "        return perm, q, diag\n",
        "\n",
        "# ----------------------------\n",
        "# Canonical permutation application helper (canonical -> layer)\n",
        "# ----------------------------\n",
        "def apply_perm_canon_to_layer(x_h: torch.Tensor, perm_canon_to_layer: np.ndarray) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    x_h: torch tensor with head axis at dim=0: [H, ...]\n",
        "    perm_canon_to_layer maps canonical head i -> layer head perm[i]\n",
        "    output[i] = x_h[perm[i]]\n",
        "    \"\"\"\n",
        "    idx = torch.as_tensor(perm_canon_to_layer, device=x_h.device, dtype=torch.long)\n",
        "    return x_h.index_select(0, idx)\n",
        "\n",
        "# ----------------------------\n",
        "# Head features from read_weights (OFFICIAL CANON FEATURES)\n",
        "# ----------------------------\n",
        "@torch.no_grad()\n",
        "def build_head_features_from_rw(rw_bhtk: torch.Tensor, *, max_lag: int = 16) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    rw_bhtk: [B,H,T,K] probabilities (or unnormalized nonneg weights over slots).\n",
        "    Returns [H,D] numpy features capturing routing + inertia shape:\n",
        "      - inertia_lag1 : mean cosine(p_t, p_{t+1})\n",
        "      - inertia_mean : mean cosine over lags 1..L\n",
        "      - inertia_slope: standardized slope of cosine vs lag\n",
        "      - entropy_mean : mean entropy of p_t over slots\n",
        "      - eff_slots    : exp(entropy_mean)\n",
        "      - top4_mass    : mean sum of top4 slot probs\n",
        "    Notes:\n",
        "      - Uses p = mean_B rw, then normalizes over K.\n",
        "      - Normalizes feature vectors to unit norm for cosine matching stability.\n",
        "    \"\"\"\n",
        "    p = rw_bhtk.float().mean(dim=0)  # [H,T,K]\n",
        "    p = p / p.sum(dim=-1, keepdim=True).clamp_min(1e-9)\n",
        "\n",
        "    ent = -(p * _safe_log(p)).sum(dim=-1)       # [H,T]\n",
        "    ent_mean = ent.mean(dim=-1)                 # [H]\n",
        "    eff_slots = torch.exp(ent_mean)             # [H]\n",
        "\n",
        "    K = p.shape[-1]\n",
        "    topk = min(4, K)\n",
        "    top4 = torch.topk(p, k=topk, dim=-1).values.sum(dim=-1)  # [H,T]\n",
        "    top4_mean = top4.mean(dim=-1)                            # [H]\n",
        "\n",
        "    T = p.shape[1]\n",
        "    L = min(int(max_lag), max(1, T - 1))\n",
        "    cos_lags = []\n",
        "    for lag in range(1, L + 1):\n",
        "        a = p[:, :-lag, :]   # [H, T-lag, K]\n",
        "        b = p[:, lag:, :]    # [H, T-lag, K]\n",
        "        c = cosine_sim(a, b).mean(dim=-1)  # [H]\n",
        "        cos_lags.append(c)\n",
        "    cos_lags = torch.stack(cos_lags, dim=-1)   # [H,L]\n",
        "\n",
        "    inertia_lag1 = cos_lags[:, 0]\n",
        "    inertia_mean = cos_lags.mean(dim=-1)\n",
        "\n",
        "    # standardized slope of cosine vs lag (robust to scale)\n",
        "    xs = torch.arange(1, L + 1, device=p.device, dtype=torch.float32)\n",
        "    xs = (xs - xs.mean()) / xs.std().clamp_min(1e-9)\n",
        "    ys = (cos_lags - cos_lags.mean(dim=-1, keepdim=True)) / cos_lags.std(dim=-1, keepdim=True).clamp_min(1e-9)\n",
        "    inertia_slope = (ys * xs).mean(dim=-1)\n",
        "\n",
        "    feats = torch.stack([inertia_lag1, inertia_mean, inertia_slope, ent_mean, eff_slots, top4_mean], dim=-1)  # [H,6]\n",
        "    feats = feats / feats.norm(dim=-1, keepdim=True).clamp_min(1e-9)\n",
        "    return feats.detach().cpu().numpy().astype(np.float64)\n",
        "\n",
        "# ----------------------------\n",
        "# Get read_weights infos from model\n",
        "# ----------------------------\n",
        "@torch.no_grad()\n",
        "def get_infos_for_head_canon(model, xb: torch.Tensor, *, batch_size: int = 8, info_cfg: dict = None):\n",
        "    \"\"\"\n",
        "    Runs model forward and returns infos capturing read_weights.\n",
        "    \"\"\"\n",
        "    if info_cfg is None:\n",
        "        info_cfg = ASA_CANON_HEAD_INFO_CFG\n",
        "    x = xb[:batch_size]\n",
        "    logits, infos = model(\n",
        "        x,\n",
        "        return_info=True,\n",
        "        info_level=\"basic\",       # read_weights available in \"basic\" per your ASA\n",
        "        info_cfg=info_cfg,\n",
        "        attention_mask=None,\n",
        "    )\n",
        "    if not infos:\n",
        "        raise RuntimeError(\"Model returned empty infos; check ASA plumbing / forward passthrough.\")\n",
        "    return infos\n",
        "\n",
        "# ----------------------------\n",
        "# Main: canonicalize heads across layers (compose Hungarian permutations)\n",
        "# ----------------------------\n",
        "def canonicalize_heads_across_layers(\n",
        "    *,\n",
        "    model=None,\n",
        "    xb: torch.Tensor=None,\n",
        "    infos=None,\n",
        "    batch_size: int = 8,\n",
        "    max_lag: int = 16,\n",
        "    anchor_layer=None,\n",
        "    info_cfg: dict = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Provide either:\n",
        "      - infos (precomputed list of per-layer info dicts), OR\n",
        "      - (model, xb) to compute infos internally\n",
        "\n",
        "    Returns canon dict:\n",
        "      canon = {\n",
        "        \"valid_layers\": [...],\n",
        "        \"anchor_layer\": L0,\n",
        "        \"H\": num_heads,\n",
        "        \"K\": num_slots,\n",
        "        \"T\": seq_len,\n",
        "        \"head_features_by_layer\": {l: [H,D]},\n",
        "        \"canon_perm_to_layer\":    {l: array[H] mapping canonical -> layer head},\n",
        "        \"align_df\":               pandas-like list of dicts with diag/best/improvement,\n",
        "        \"align_quality\":          {(la,lb): best_cos},\n",
        "        \"feature_dim\":            D,\n",
        "      }\n",
        "    \"\"\"\n",
        "    if infos is None:\n",
        "        if model is None or xb is None:\n",
        "            raise ValueError(\"Provide either infos=... or (model=..., xb=...)\")\n",
        "        infos = get_infos_for_head_canon(model, xb, batch_size=batch_size, info_cfg=info_cfg)\n",
        "\n",
        "    # collect valid layers with read_weights\n",
        "    rw_by_layer = {}\n",
        "    valid_layers = []\n",
        "    for l, info in enumerate(infos):\n",
        "        rw = None if (info is None) else info.get(\"read_weights\", None)\n",
        "        if rw is None or (not torch.is_tensor(rw)) or rw.dim() != 4:\n",
        "            continue\n",
        "        rw_by_layer[l] = rw\n",
        "        valid_layers.append(l)\n",
        "\n",
        "    if len(valid_layers) < 2:\n",
        "        raise RuntimeError(\"Need at least 2 layers with read_weights captured for canonicalization.\")\n",
        "\n",
        "    L0 = valid_layers[0] if anchor_layer is None else int(anchor_layer)\n",
        "    if L0 not in valid_layers:\n",
        "        raise ValueError(f\"anchor_layer={L0} is not in valid_layers={valid_layers}\")\n",
        "\n",
        "    B, H, T, K = rw_by_layer[L0].shape\n",
        "\n",
        "    # head features per valid layer\n",
        "    head_features_by_layer = {}\n",
        "    for l in valid_layers:\n",
        "        head_features_by_layer[l] = build_head_features_from_rw(rw_by_layer[l], max_lag=max_lag)  # [H,D]\n",
        "\n",
        "    D = head_features_by_layer[L0].shape[1]\n",
        "\n",
        "    # canonical permutation: canonical -> layer head index\n",
        "    canon_perm_to_layer = {L0: np.arange(H, dtype=np.int64)}\n",
        "    align_quality = {}\n",
        "    align_rows = []\n",
        "\n",
        "    # build a forward chain anchored at L0 by walking through valid_layers order\n",
        "    # If L0 is not the first, we still canonicalize by chaining from L0 outward both directions.\n",
        "    idx0 = valid_layers.index(L0)\n",
        "\n",
        "    # forward direction (increasing within valid_layers)\n",
        "    for i in range(idx0, len(valid_layers) - 1):\n",
        "        la = valid_layers[i]\n",
        "        lb = valid_layers[i + 1]\n",
        "        A = head_features_by_layer[la]\n",
        "        Bf = head_features_by_layer[lb]\n",
        "\n",
        "        perm_ab, best, diag = hungarian_align(A, Bf)  # la-head -> lb-head\n",
        "        align_quality[(la, lb)] = best\n",
        "        align_rows.append(dict(layer=la, transition=f\"{la}{lb}\", diag=float(diag), best=float(best), improvement=float(best - diag)))\n",
        "\n",
        "        canon_to_la = canon_perm_to_layer[la]\n",
        "        canon_to_lb = perm_ab[canon_to_la]            # compose: canon->la then la->lb gives canon->lb\n",
        "        canon_perm_to_layer[lb] = canon_to_lb\n",
        "\n",
        "    # backward direction (decreasing within valid_layers)\n",
        "    # Need perm_ba to map lb-head -> la-head; invert perm_ab.\n",
        "    for i in range(idx0, 0, -1):\n",
        "        lb = valid_layers[i]\n",
        "        la = valid_layers[i - 1]\n",
        "\n",
        "        # compute alignment la->lb (same orientation as above), then invert it\n",
        "        A = head_features_by_layer[la]\n",
        "        Bf = head_features_by_layer[lb]\n",
        "        perm_ab, best, diag = hungarian_align(A, Bf)  # la-head -> lb-head\n",
        "        align_quality[(la, lb)] = best\n",
        "        align_rows.append(dict(layer=la, transition=f\"{la}{lb}\", diag=float(diag), best=float(best), improvement=float(best - diag)))\n",
        "\n",
        "        # invert perm_ab to map lb -> la\n",
        "        perm_ba = np.empty_like(perm_ab)\n",
        "        perm_ba[perm_ab] = np.arange(H, dtype=np.int64)  # lb-head -> la-head\n",
        "\n",
        "        canon_to_lb = canon_perm_to_layer[lb]\n",
        "        canon_to_la = perm_ba[canon_to_lb]               # canon->lb then lb->la gives canon->la\n",
        "        canon_perm_to_layer[la] = canon_to_la\n",
        "\n",
        "    # sanity checks\n",
        "    for l in valid_layers:\n",
        "        p = canon_perm_to_layer[l]\n",
        "        if p.shape != (H,):\n",
        "            raise AssertionError(f\"perm shape wrong at layer {l}: {p.shape}\")\n",
        "        if sorted(p.tolist()) != list(range(H)):\n",
        "            raise AssertionError(f\"perm not a permutation at layer {l}: {p}\")\n",
        "\n",
        "    canon = dict(\n",
        "        valid_layers=valid_layers,\n",
        "        anchor_layer=L0,\n",
        "        H=int(H),\n",
        "        K=int(K),\n",
        "        T=int(T),\n",
        "        head_features_by_layer=head_features_by_layer,\n",
        "        canon_perm_to_layer=canon_perm_to_layer,\n",
        "        align_quality=align_quality,\n",
        "        align_rows=align_rows,\n",
        "        feature_dim=int(D),\n",
        "    )\n",
        "    return canon\n",
        "\n",
        "# ----------------------------\n",
        "# Example usage (safe to keep; comment out if you want)\n",
        "# ----------------------------\n",
        "canon = canonicalize_heads_across_layers(model=model, xb=xb, batch_size=8, max_lag=16, anchor_layer=None)\n",
        "print(f\"Loaded per-head stats: L={len(canon['valid_layers'])} layers, H={canon['H']} heads\")\n",
        "print(f\"Canonical head alignment anchor: layer {canon['anchor_layer']}\")\n",
        "try:\n",
        "    import pandas as pd\n",
        "    df = pd.DataFrame(canon[\"align_rows\"]).sort_values([\"layer\"])\n",
        "    display(df)\n",
        "    print(f\"Average head-alignment quality over adjacent transitions: best={df['best'].mean():.4f} improvement={df['improvement'].mean():.4f}\")\n",
        "except Exception:\n",
        "    # no pandas/display available\n",
        "    rows = canon[\"align_rows\"]\n",
        "    bests = [r[\"best\"] for r in rows] if rows else []\n",
        "    improvs = [r[\"improvement\"] for r in rows] if rows else []\n",
        "    if bests:\n",
        "        print(f\"Average head-alignment quality: best={float(np.mean(bests)):.4f} improvement={float(np.mean(improvs)):.4f}\")\n",
        "print(\" Canonical heads ready: canon['canon_perm_to_layer'][layer] maps canonical -> layer head index.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfDvu_dgyH6M"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "_nuLwxAMsrIZ"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}