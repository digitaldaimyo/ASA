{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Addressed State Attention: Mechanistic Probing (Reorganized)\n\nThis notebook reorganizes the original mechanistic probing analysis into a narrative flow that matches the ASA substrate story while preserving all computations, plots, and exports.\n\n**Runtime:** ~5\u201310 minutes on CPU (depending on dataset cache)\n\n**Outputs:** artifacts/ (plots, CSV/JSON exports), canonical tables (df_hl, df_layer_regimes)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Model Loading\nInitialize paths, dependencies, and the ASA checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YzCxjE1Zxe__",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 36794,
          "status": "ok",
          "timestamp": 1769532437747,
          "user": {
            "displayName": "Secondscapes",
            "userId": "04150343613140054698"
          },
          "user_tz": 300
        },
        "id": "YzCxjE1Zxe__",
        "outputId": "6322a117-5b32-4470-89e5-c9a1a12ca113"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Prepare ASA Imports and Dirs\n",
        "\n",
        "import os, sys, subprocess, platform, json, time, random\n",
        "from pathlib import Path\n",
        "\n",
        "repo_dir = 'ASA'\n",
        "\n",
        "# Clone the repository if it doesn't exist\n",
        "if not Path(repo_dir).exists():\n",
        "    subprocess.run(['git','clone','https://github.com/digitaldaimyo/ASA.git'], check=True)\n",
        "\n",
        "# Get the absolute path of the repository\n",
        "repo_path_abs = Path(os.getcwd()) / repo_dir\n",
        "\n",
        "# Add the repository root to sys.path if not already present\n",
        "# This helps discover top-level modules or packages directly under the repo root.\n",
        "if str(repo_path_abs) not in sys.path:\n",
        "    sys.path.insert(0, str(repo_path_abs))\n",
        "\n",
        "# Check for a common 'src' subdirectory and add it to sys.path if it exists.\n",
        "# This handles projects where the actual Python package is nested under 'src/'.\n",
        "src_path_abs = repo_path_abs / 'src'\n",
        "if src_path_abs.is_dir() and str(src_path_abs) not in sys.path:\n",
        "    sys.path.insert(0, str(src_path_abs))\n",
        "\n",
        "# Change the current working directory into the repository\n",
        "os.chdir(repo_dir)\n",
        "\n",
        "# Install the package in editable mode and other dependencies\n",
        "subprocess.run([sys.executable,'-m','pip','install','-e','.'], check=True)\n",
        "subprocess.run([sys.executable,'-m','pip','install','-q','huggingface_hub','safetensors','transformers','datasets', 'torchinfo'], check=True)\n",
        "import torch\n",
        "from huggingface_hub import hf_hub_download\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "device = torch.device('cpu')\n",
        "print('Python:', platform.python_version())\n",
        "print('Torch:', torch.__version__)\n",
        "print('Device:', device)\n",
        "try:\n",
        "    commit = subprocess.check_output(['git','rev-parse','HEAD']).decode().strip()\n",
        "    print('Repo commit:', commit)\n",
        "except Exception:\n",
        "    print('Repo commit: unavailable')\n",
        "\n",
        "seed = 1337\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "artifacts_dir = Path('artifacts')\n",
        "artifacts_dir.mkdir(exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wlVXx9SexfAB",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 26466,
          "status": "ok",
          "timestamp": 1769532464216,
          "user": {
            "displayName": "Secondscapes",
            "userId": "04150343613140054698"
          },
          "user_tz": 300
        },
        "id": "wlVXx9SexfAB",
        "outputId": "00583de0-3fff-447d-d2e3-b8473f202785",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#@title Load and Summarize Model rom HF\n",
        "from torchinfo import summary as model_summary\n",
        "from asa.load_pretrained import load_pretrained\n",
        "\n",
        "HF_REPO = 'DigitalShogun/ASA-ASM-wikitext103-raw'\n",
        "DEFAULT_CKPT = 'ASA_ASM_wt103-rawv1_gpt2_T1024_L21_D384_H8_K16_M32_ropek1_alibi1_gamma1_step75000_best.pt'\n",
        "\n",
        "def count_params(model):\n",
        "    n = sum(p.numel() for p in model.parameters())\n",
        "    n_train = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return n, n_train\n",
        "\n",
        "def cfg_summary(cfg):\n",
        "    return {\n",
        "        \"layers\": cfg.num_layers,\n",
        "        \"d_model\": cfg.embed_dim,\n",
        "        \"heads\": cfg.num_heads,\n",
        "        \"slots\": cfg.num_slots,\n",
        "        \"T\": cfg.max_seq_len,\n",
        "        \"slotspace\": cfg.use_slotspace_refine,\n",
        "        \"slotspace_dim\": cfg.slotspace_dim,\n",
        "        \"content_read\": cfg.use_content_read,\n",
        "        \"alibi_write\": cfg.use_alibi_write,\n",
        "        \"rope_keys\": cfg.use_rope_keys,\n",
        "    }\n",
        "\n",
        "def print_cfg_summary(cfg, model):\n",
        "    n, n_train = count_params(model)\n",
        "    summ = cfg_summary(cfg)\n",
        "    print(\"Config:\", \", \".join([f\"{k}={v}\" for k,v in summ.items()]))\n",
        "    print(f\"Params: {n/1e6:.2f}M\")\n",
        "\n",
        "\n",
        "model, report, cfg_obj = load_pretrained(HF_REPO, DEFAULT_CKPT, variant='baseline', device='cpu')\n",
        "print('Loaded model with vocab_size:', cfg_obj.vocab_size)\n",
        "print('Checkpoint source:', report['state_dict_source'])\n",
        "print('Allowlisted gaps:', {\n",
        "    'missing': len(report['allowed_missing']),\n",
        "    'unexpected': len(report['allowed_unexpected']),\n",
        "    'mismatched': len(report['allowed_mismatched']),\n",
        "})\n",
        "\n",
        "input_ids = torch.randint(0, cfg_obj.vocab_size, (1, 32))\n",
        "with torch.no_grad():\n",
        "    logits, _ = model(input_ids)\n",
        "print('Logits shape:', tuple(logits.shape))\n",
        "assert logits.shape == (1, 32, cfg_obj.vocab_size)\n",
        "assert torch.isfinite(logits).all()\n",
        "\n",
        "run_metadata = {\n",
        "    'repo': HF_REPO,\n",
        "    'checkpoint': DEFAULT_CKPT,\n",
        "    'state_dict_source': report['state_dict_source'],\n",
        "    'seed': seed,\n",
        "    'timestamp': time.time(),\n",
        "    'config': cfg_obj.__dict__,\n",
        "}\n",
        "(artifacts_dir / 'run_metadata.json').write_text(json.dumps(run_metadata, indent=2))\n",
        "\n",
        "print(cfg_obj)\n",
        "print_cfg_summary(cfg_obj, model)\n",
        "model_summary(model, input_size=(1, 1024), dtypes=[torch.long])\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"Name: {name} | Shape: {param.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Generation (Single Forward Pass + Cache)\nBuild or load cached token streams and collect model infos in one pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dAhhPhib5oZB",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 8845,
          "status": "ok",
          "timestamp": 1769532473028,
          "user": {
            "displayName": "Secondscapes",
            "userId": "04150343613140054698"
          },
          "user_tz": 300
        },
        "id": "dAhhPhib5oZB",
        "outputId": "586e6f15-311d-44ed-ab02-d65baf75591a"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "#@title Batch Data Generation\n",
        "\n",
        "import os, random, pickle\n",
        "from typing import List, Tuple, Optional, Iterator, Dict\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, IterableDataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Minimal dataset classes (same as training)\n",
        "# -------------------------\n",
        "class StableValidationDataset(Dataset):\n",
        "    def __init__(self, samples: List[Tuple[torch.Tensor, torch.Tensor]]):\n",
        "        self.samples = samples\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx): return self.samples[idx]\n",
        "\n",
        "def _ensure_dir(path: str):\n",
        "    d = os.path.dirname(path)\n",
        "    if d:\n",
        "        os.makedirs(d, exist_ok=True)\n",
        "\n",
        "def build_or_load_token_stream(\n",
        "    *,\n",
        "    cache_path: str,\n",
        "    dataset_name: str,\n",
        "    dataset_config: str,\n",
        "    split: str,\n",
        "    tokenizer_name: str,\n",
        "    min_chars: int = 1,\n",
        "    add_eos_between_rows: bool = True,\n",
        "    max_rows: Optional[int] = None,\n",
        ") -> List[int]:\n",
        "    if os.path.exists(cache_path):\n",
        "        with open(cache_path, \"rb\") as f:\n",
        "            stream = pickle.load(f)\n",
        "        return stream\n",
        "\n",
        "    _ensure_dir(cache_path)\n",
        "    tok = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=True)\n",
        "    eos = tok.eos_token_id\n",
        "    assert eos is not None, \"Tokenizer must have eos_token_id\"\n",
        "\n",
        "    ds = load_dataset(dataset_name, dataset_config, split=split)\n",
        "\n",
        "    stream: List[int] = []\n",
        "    used = 0\n",
        "    for row in ds:\n",
        "        if max_rows is not None and used >= max_rows:\n",
        "            break\n",
        "        text = (row.get(\"text\") or \"\").strip()\n",
        "        if len(text) < min_chars:\n",
        "            continue\n",
        "        ids = tok.encode(text, add_special_tokens=False)\n",
        "        if not ids:\n",
        "            continue\n",
        "        stream.extend(ids)\n",
        "        if add_eos_between_rows:\n",
        "            stream.append(eos)\n",
        "        used += 1\n",
        "\n",
        "    with open(cache_path, \"wb\") as f:\n",
        "        pickle.dump(stream, f)\n",
        "    return stream\n",
        "\n",
        "def build_or_load_validation_windows(\n",
        "    *,\n",
        "    cache_path: str,\n",
        "    token_stream: List[int],\n",
        "    max_seq_len: int,\n",
        "    stride_frac: float,\n",
        "    val_samples_target: int,\n",
        ") -> StableValidationDataset:\n",
        "    if os.path.exists(cache_path):\n",
        "        with open(cache_path, \"rb\") as f:\n",
        "            samples = pickle.load(f)\n",
        "        return StableValidationDataset(samples)\n",
        "\n",
        "    _ensure_dir(cache_path)\n",
        "\n",
        "    T = int(max_seq_len)\n",
        "    stride = max(1, int(T * float(stride_frac)))\n",
        "    need = int(val_samples_target)\n",
        "\n",
        "    max_start = len(token_stream) - (T + 1)\n",
        "    if max_start <= 0:\n",
        "        raise ValueError(\"Validation token stream too small for max_seq_len+1\")\n",
        "\n",
        "    samples: List[Tuple[torch.Tensor, torch.Tensor]] = []\n",
        "    for start in range(0, max_start + 1, stride):\n",
        "        chunk = token_stream[start:start + T + 1]\n",
        "        if len(chunk) < T + 1:\n",
        "            break\n",
        "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
        "        samples.append((x, y))\n",
        "        if len(samples) >= need:\n",
        "            break\n",
        "\n",
        "    with open(cache_path, \"wb\") as f:\n",
        "        pickle.dump(samples, f)\n",
        "    return StableValidationDataset(samples)\n",
        "\n",
        "class WikiTextRandomWindowStream(IterableDataset):\n",
        "    def __init__(self, token_stream: List[int], max_seq_len: int, train_samples_target: int, seed: int):\n",
        "        super().__init__()\n",
        "        self.stream = token_stream\n",
        "        self.T = int(max_seq_len)\n",
        "        self.target = int(train_samples_target)\n",
        "        self.seed = int(seed)\n",
        "        self.max_start = len(self.stream) - (self.T + 1)\n",
        "        if self.max_start <= 0:\n",
        "            raise ValueError(\"Train token stream too small for max_seq_len+1\")\n",
        "\n",
        "    def __iter__(self):\n",
        "        info = torch.utils.data.get_worker_info()\n",
        "        wid = info.id if info is not None else 0\n",
        "        rng = random.Random(self.seed + 17 * wid)\n",
        "\n",
        "        yielded = 0\n",
        "        while yielded < self.target:\n",
        "            start = rng.randint(0, self.max_start)\n",
        "            chunk = self.stream[start:start + self.T + 1]\n",
        "            x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
        "            y = torch.tensor(chunk[1:], dtype=torch.long)\n",
        "            yield x, y\n",
        "            yielded += 1\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Analysis batch generator\n",
        "# -------------------------\n",
        "def make_batch_generator(\n",
        "    cfg,\n",
        "    *,\n",
        "    split: str = \"val\",                 # \"val\" or \"train\"\n",
        "    device: Optional[torch.device] = None,\n",
        "    seq_len: int = 0,\n",
        "    num_workers: int = 0,               # 0 is safest for Colab analysis\n",
        "    pin_memory: bool = True,\n",
        "    batches_per_epoch: Optional[int] = None,  # if set, generator stops after N batches\n",
        "    infinite: bool = True,              # if True, cycles (val) or keeps sampling (train)\n",
        "    seed: Optional[int] = None,         # override cfg.seed for sampling\n",
        ") -> Iterator[Tuple[torch.Tensor, torch.Tensor]]:\n",
        "    \"\"\"\n",
        "    Yields xb,yb batches shaped [B,T] int64.\n",
        "\n",
        "    - split=\"val\": deterministic stable windows (cached) then cycles if infinite=True\n",
        "    - split=\"train\": random windows from cached token stream; infinite is naturally True\n",
        "    \"\"\"\n",
        "    assert split in (\"val\", \"train\")\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Resolve cfg fields with fallback (works for dict-like or dataclass)\n",
        "    def get(name, default=None):\n",
        "        return getattr(cfg, name, cfg.get(name, default) if isinstance(cfg, dict) else default)\n",
        "\n",
        "    cache_dir = get(\"cache_dir\")\n",
        "    #dataset_name = get(\"dataset_name\")\n",
        "    dataset_name = \"Salesforce/wikitext\"\n",
        "    dataset_config = get(\"dataset_config\")\n",
        "    tokenizer_name = get(\"tokenizer_name\")\n",
        "    max_seq_len = seq_len if seq_len > 0 else int(get(\"max_seq_len\"))\n",
        "    #batch_size = int(get(\"batch_size\"))\n",
        "    batch_size = 4\n",
        "    stride_frac_val = float(get(\"stride_frac_val\", 0.5))\n",
        "    val_samples_target = int(get(\"val_samples_target\", 25_000))\n",
        "    val_windows_cache = get(\"val_windows_cache\")\n",
        "    train_samples_target = int(get(\"train_samples_target\", 100_000_000))\n",
        "    used_seed = int(seed if seed is not None else get(\"seed\", 1337))\n",
        "\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    train_stream_cache = os.path.join(cache_dir, f\"{dataset_config}_train_stream.pkl\")\n",
        "    val_stream_cache   = os.path.join(cache_dir, f\"{dataset_config}_val_stream.pkl\")\n",
        "\n",
        "    if split == \"train\":\n",
        "        train_stream = build_or_load_token_stream(\n",
        "            cache_path=train_stream_cache,\n",
        "            dataset_name=dataset_name,\n",
        "            dataset_config=dataset_config,\n",
        "            split=\"train\",\n",
        "            tokenizer_name=tokenizer_name,\n",
        "            min_chars=1,\n",
        "            add_eos_between_rows=True,\n",
        "        )\n",
        "        ds = WikiTextRandomWindowStream(\n",
        "            token_stream=train_stream,\n",
        "            max_seq_len=max_seq_len,\n",
        "            train_samples_target=train_samples_target,\n",
        "            seed=used_seed,\n",
        "        )\n",
        "        loader = DataLoader(\n",
        "            ds,\n",
        "            batch_size=batch_size,\n",
        "            num_workers=num_workers,\n",
        "            pin_memory=pin_memory and torch.cuda.is_available(),\n",
        "        )\n",
        "\n",
        "        yielded = 0\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device, non_blocking=True)\n",
        "            yb = yb.to(device, non_blocking=True)\n",
        "            yield xb, yb\n",
        "            yielded += 1\n",
        "            if batches_per_epoch is not None and yielded >= batches_per_epoch:\n",
        "                if infinite:\n",
        "                    yielded = 0\n",
        "                    continue\n",
        "                break\n",
        "\n",
        "    else:\n",
        "        # val\n",
        "        val_stream = build_or_load_token_stream(\n",
        "            cache_path=val_stream_cache,\n",
        "            dataset_name=dataset_name,\n",
        "            dataset_config=dataset_config,\n",
        "            split=\"validation\",\n",
        "            tokenizer_name=tokenizer_name,\n",
        "            min_chars=1,\n",
        "            add_eos_between_rows=True,\n",
        "        )\n",
        "        val_ds = build_or_load_validation_windows(\n",
        "            cache_path=val_windows_cache,\n",
        "            token_stream=val_stream,\n",
        "            max_seq_len=max_seq_len,\n",
        "            stride_frac=stride_frac_val,\n",
        "            val_samples_target=val_samples_target,\n",
        "        )\n",
        "        loader = DataLoader(\n",
        "            val_ds,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=num_workers,\n",
        "            pin_memory=pin_memory and torch.cuda.is_available(),\n",
        "        )\n",
        "\n",
        "        while True:\n",
        "            yielded = 0\n",
        "            for xb, yb in loader:\n",
        "                xb = xb.to(device, non_blocking=True)\n",
        "                yb = yb.to(device, non_blocking=True)\n",
        "                yield xb, yb\n",
        "                yielded += 1\n",
        "                if batches_per_epoch is not None and yielded >= batches_per_epoch:\n",
        "                    break\n",
        "            if not infinite:\n",
        "                break\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Quick smoke test\n",
        "# -------------------------\n",
        "\n",
        "analysis_data_gen = make_batch_generator(cfg_obj, batches_per_epoch=50, split=\"val\", device=device)\n",
        "xb, yb = next(analysis_data_gen)\n",
        "print(xb.shape, yb.shape, xb.device, xb.dtype)\n",
        "\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HUE9rDG761o_",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 46308,
          "status": "ok",
          "timestamp": 1769532519351,
          "user": {
            "displayName": "Secondscapes",
            "userId": "04150343613140054698"
          },
          "user_tz": 300
        },
        "id": "HUE9rDG761o_",
        "outputId": "b84d59d6-cb10-4124-93c4-f4a2e5ecdeba"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#@title Forward Pass, Collect Out Infos\n",
        "\n",
        "@torch.no_grad()\n",
        "def run_with_infos(model, xb: torch.Tensor, attention_mask=None):\n",
        "    out = model(xb, return_info=True, attention_mask=attention_mask)\n",
        "    if isinstance(out, tuple) and len(out) == 2:\n",
        "        logits, infos = out\n",
        "    else:\n",
        "        raise ValueError(\"Model did not return (logits, infos) under return_info=True\")\n",
        "    return logits, infos\n",
        "\n",
        "logits, infos = run_with_infos(model, xb[:4])\n",
        "print(\"logits:\", logits.shape, logits.dtype)\n",
        "print(\"num layers infos:\", len(infos))\n",
        "print(\"info keys example:\", list(infos[0].keys()))\n",
        "print(\"read_weights shape:\", infos[0][\"read_weights\"].shape)\n",
        "\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Core Metrics (M, P, C, L)\nCompute routing decomposition, memory metrics, and head-level summaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WhghOKMHcHIQ",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "id": "WhghOKMHcHIQ",
        "outputId": "18f021ac-8b4b-4c0f-b98b-a71f585270cc",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769532579879,
          "user_tz": 300,
          "elapsed": 60524,
          "user": {
            "displayName": "Secondscapes",
            "userId": "04150343613140054698"
          }
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#@title ASA Analysis 2 (Corrected): Content vs Key routing decomposition + Visuals\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "#   (your original functions \u2014 unchanged)\n",
        "# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "\n",
        "@torch.no_grad()\n",
        "def _ensure_bhtk(x, name):\n",
        "    if x is None:\n",
        "        return None\n",
        "    if x.dim() != 4:\n",
        "        raise ValueError(f\"{name} expected 4D [B,H,T,K], got {tuple(x.shape)}\")\n",
        "    return x\n",
        "\n",
        "@torch.no_grad()\n",
        "def _maybe_fix_shape(info, key_name, want_T=None):\n",
        "    x = info.get(key_name, None)\n",
        "    if x is None:\n",
        "        return None, False\n",
        "    x = x.to(torch.float32)\n",
        "    partial = False\n",
        "    if want_T is not None and x.shape[-2] != want_T:\n",
        "        partial = True\n",
        "    return x, partial\n",
        "\n",
        "@torch.no_grad()\n",
        "def content_key_breakdown_correct(model, xb, yb, attention_mask=None, eps=1e-8):\n",
        "    model.eval()\n",
        "    logits, infos = model(xb, attention_mask=attention_mask, return_info=True)\n",
        "    base_loss = F.cross_entropy(logits.view(-1, logits.size(-1)), yb.view(-1)).item()\n",
        "\n",
        "    B, T = xb.shape[0], xb.shape[1]\n",
        "\n",
        "    rows = []\n",
        "    any_partial = False\n",
        "\n",
        "    for li, info in enumerate(infos):\n",
        "        key, partial_k = _maybe_fix_shape(info, \"read_logits_key\", want_T=T)\n",
        "        tot, partial_t = _maybe_fix_shape(info, \"read_logits\", want_T=T)\n",
        "        content, partial_c = _maybe_fix_shape(info, \"read_logits_content\", want_T=T)\n",
        "\n",
        "        partial = partial_k or partial_t or partial_c\n",
        "        any_partial = any_partial or partial\n",
        "\n",
        "        if key is None or tot is None:\n",
        "            continue  # skip layers without data\n",
        "\n",
        "        B2, H, TT, K = tot.shape\n",
        "\n",
        "        def centerK(z):\n",
        "            return z - z.mean(dim=-1, keepdim=True)\n",
        "\n",
        "        key_c = centerK(key)\n",
        "        tot_c = centerK(tot)\n",
        "        cont_c = centerK(content) if content is not None else None\n",
        "\n",
        "        var_tot = tot_c.var(dim=-1, unbiased=False).mean().item()\n",
        "        var_key = key_c.var(dim=-1, unbiased=False).mean().item()\n",
        "        var_cont = cont_c.var(dim=-1, unbiased=False).mean().item() if cont_c is not None else float(\"nan\")\n",
        "\n",
        "        if cont_c is not None:\n",
        "            cov_kc = (key_c * cont_c).mean(dim=-1).mean().item()\n",
        "            corr_kc = cov_kc / (math.sqrt(var_key + eps) * math.sqrt(var_cont + eps))\n",
        "        else:\n",
        "            cov_kc = corr_kc = float(\"nan\")\n",
        "\n",
        "        def cos2(a, b):\n",
        "            an = a.norm(dim=-1).clamp_min(eps)\n",
        "            bn = b.norm(dim=-1).clamp_min(eps)\n",
        "            c = (a * b).sum(dim=-1) / (an * bn)\n",
        "            return (c * c)\n",
        "\n",
        "        r2_key = cos2(tot_c, key_c).mean().item()\n",
        "\n",
        "        if cont_c is not None:\n",
        "            r2_cont = cos2(tot_c, cont_c).mean().item()\n",
        "\n",
        "            kk = (key_c * key_c).sum(dim=-1)\n",
        "            cc = (cont_c * cont_c).sum(dim=-1)\n",
        "            kc = (key_c * cont_c).sum(dim=-1)\n",
        "            kt = (key_c * tot_c).sum(dim=-1)\n",
        "            ct = (cont_c * tot_c).sum(dim=-1)\n",
        "\n",
        "            det = (kk * cc - kc * kc).clamp_min(eps)\n",
        "            alpha = (cc * kt - kc * ct) / det\n",
        "            beta = (-kc * kt + kk * ct) / det\n",
        "\n",
        "            proj = alpha.unsqueeze(-1) * key_c + beta.unsqueeze(-1) * cont_c\n",
        "            r2_span = (proj.norm(dim=-1).pow(2) / tot_c.norm(dim=-1).clamp_min(eps).pow(2)).mean().item()\n",
        "        else:\n",
        "            r2_cont = r2_span = float(\"nan\")\n",
        "\n",
        "        pk = torch.softmax(key / 1.0, dim=-1)\n",
        "        pt = torch.softmax(tot / 1.0, dim=-1)\n",
        "\n",
        "        m = 0.5 * (pk + pt)\n",
        "        js = 0.5 * (F.kl_div(m.log(), pk, reduction=\"none\").sum(dim=-1) +\n",
        "                    F.kl_div(m.log(), pt, reduction=\"none\").sum(dim=-1))\n",
        "        js_mean = js.mean().item()\n",
        "\n",
        "        agree = (key.argmax(dim=-1) == tot.argmax(dim=-1)).float().mean().item()\n",
        "        maxp_delta = (pt.max(dim=-1).values - pk.max(dim=-1).values).abs().mean().item()\n",
        "\n",
        "        rows.append({\n",
        "            \"layer\": li,\n",
        "            \"base_loss\": base_loss,\n",
        "            \"partial_T\": partial,\n",
        "            \"T_seen\": TT,\n",
        "            \"var_total_overK\": var_tot,\n",
        "            \"var_key_overK\": var_key,\n",
        "            \"var_content_overK\": var_cont,\n",
        "            \"corr_key_content_overK\": corr_kc,\n",
        "            \"r2_total_explained_by_key\": r2_key,\n",
        "            \"r2_total_explained_by_content\": r2_cont,\n",
        "            \"r2_total_explained_by_span_key_content\": r2_span,\n",
        "            \"argmax_agreement_key_vs_total\": agree,\n",
        "            \"js_divergence_key_total\": js_mean,\n",
        "            \"mean_abs_delta_maxprob\": maxp_delta,\n",
        "        })\n",
        "\n",
        "    if any_partial:\n",
        "        print(\"\u26a0 Warning: Some layers have partial T (likely chunk logging). \"\n",
        "              \"Results may underestimate full-sequence effects.\\n\")\n",
        "\n",
        "    return rows\n",
        "\n",
        "\n",
        "# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "#                   Visualization Functions\n",
        "# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "\n",
        "def plot_content_key_decomposition(rows, figsize=(15, 10)):\n",
        "    df = pd.DataFrame(rows)\n",
        "    layers = df[\"layer\"]\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=figsize, sharex=True)\n",
        "    axes = axes.flat\n",
        "\n",
        "    # 1. Explained variance (R\u00b2)\n",
        "    axes[0].plot(layers, df[\"r2_total_explained_by_key\"], 'o-', color='C0', label='Key only', lw=2.4)\n",
        "    axes[0].plot(layers, df[\"r2_total_explained_by_content\"], 's-', color='C1', label='Content only', lw=2.4)\n",
        "    axes[0].plot(layers, df[\"r2_total_explained_by_span_key_content\"], 'd--', color='C2', label='Key + Content span',\n",
        "                 lw=2.8, alpha=0.9)\n",
        "    axes[0].set_title(\"Explained Variance of Total Read Logits\")\n",
        "    axes[0].set_ylabel(\"R\u00b2 (cos\u00b2 angle after centering)\")\n",
        "    axes[0].set_ylim(0, 1.05)\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    axes[0].legend()\n",
        "\n",
        "    # 2. Routing outcome similarity\n",
        "    axes[1].plot(layers, df[\"argmax_agreement_key_vs_total\"], 'o-', color='C4', label='Argmax agreement', lw=2.5)\n",
        "    axes[1].plot(layers, 1 - df[\"js_divergence_key_total\"], 's-', color='C3', label='1 \u2212 JS divergence', lw=2.5)\n",
        "    axes[1].set_title(\"How similar is key-routing to final routing?\")\n",
        "    axes[1].set_ylabel(\"Similarity (\u2191 = more similar)\")\n",
        "    axes[1].set_ylim(0, 1.05)\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    axes[1].legend()\n",
        "\n",
        "    # 3. Variance breakdown\n",
        "    axes[2].plot(layers, df[\"var_total_overK\"], 'o-', color='C0', label='Total', lw=2.2)\n",
        "    axes[2].plot(layers, df[\"var_key_overK\"], 's-', color='C1', label='Key component', lw=2.2)\n",
        "    axes[2].plot(layers, df[\"var_content_overK\"], 'd-', color='C2', label='Content component', lw=2.2)\n",
        "    axes[2].set_title(\"Variance over slots (after centering)\")\n",
        "    axes[2].set_ylabel(\"Mean variance over K\")\n",
        "    axes[2].set_yscale('log')\n",
        "    axes[2].grid(True, alpha=0.3)\n",
        "    axes[2].legend()\n",
        "\n",
        "    # 4. Correlation & max-prob shift\n",
        "    axes[3].plot(layers, df[\"corr_key_content_overK\"], 'o-', color='purple', label='Key\u2013Content correlation', lw=2.5)\n",
        "    axes[3].plot(layers, df[\"mean_abs_delta_maxprob\"], 's-', color='darkorange', label='Mean |\u0394 max-prob|', lw=2.5)\n",
        "    axes[3].axhline(0, color='gray', lw=1, alpha=0.6)\n",
        "    axes[3].set_title(\"Key\u2013Content interaction & confidence shift\")\n",
        "    axes[3].set_ylabel(\"Correlation / \u0394 max-prob\")\n",
        "    axes[3].grid(True, alpha=0.3)\n",
        "    axes[3].legend()\n",
        "\n",
        "    for ax in axes:\n",
        "        ax.set_xlabel(\"Layer\")\n",
        "\n",
        "    fig.suptitle(\"Content vs Key Routing Decomposition\", fontsize=16, y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "#                          Run & Visualize\n",
        "# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "\n",
        "print(\"Computing content vs key routing decomposition...\\n\")\n",
        "\n",
        "rows = content_key_breakdown_correct(model, xb, yb, attention_mask=None)\n",
        "\n",
        "print(\"Per-layer results:\")\n",
        "for r in rows:\n",
        "    print(f\"Layer {r['layer']:2d}: \"\n",
        "          f\"R\u00b2(key)={r['r2_total_explained_by_key']:.3f}  \"\n",
        "          f\"R\u00b2(content)={r['r2_total_explained_by_content']:.3f}  \"\n",
        "          f\"R\u00b2(span)={r['r2_total_explained_by_span_key_content']:.3f}  |  \"\n",
        "          f\"argmax agree={r['argmax_agreement_key_vs_total']:.3f}  \"\n",
        "          f\"JS={r['js_divergence_key_total']:.4f}\")\n",
        "\n",
        "print(\"\\nPlotting summary...\\n\")\n",
        "\n",
        "plot_content_key_decomposition(rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PLfNBSlzWsJ8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "executionInfo": {
          "elapsed": 47591,
          "status": "ok",
          "timestamp": 1769532627474,
          "user": {
            "displayName": "Secondscapes",
            "userId": "04150343613140054698"
          },
          "user_tz": 300
        },
        "id": "PLfNBSlzWsJ8",
        "outputId": "3a736c79-74b6-4bf4-ebe5-084da19ea247",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Slot Half-Life (ASA-meaningful) \u2014 curated + rigorous summary (REPLACEMENT CELL)\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "# Core metrics (unchanged semantics)\n",
        "# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "\n",
        "@torch.no_grad()\n",
        "def _tail_mass_half_life_from_logp(logp: torch.Tensor, threshold: float = 0.5) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Smallest n such that tail mass (from most-recent backwards) >= threshold.\n",
        "    \"\"\"\n",
        "    thr = float(threshold)\n",
        "    p = logp.exp()\n",
        "    tail = torch.flip(p, dims=[-1]).cumsum(dim=-1)\n",
        "    hl = (tail < thr).sum(dim=-1).to(torch.int32) + 1\n",
        "    return hl\n",
        "\n",
        "@torch.no_grad()\n",
        "def _ess_from_logp(logp: torch.Tensor, eps: float = 1e-12) -> torch.Tensor:\n",
        "    p = logp.exp()\n",
        "    denom = (p * p).sum(dim=-1).clamp_min(eps)\n",
        "    return 1.0 / denom\n",
        "\n",
        "@torch.no_grad()\n",
        "def slot_memory_timescale_from_write_logits(\n",
        "    write_logits: torch.Tensor,\n",
        "    t_points: torch.Tensor,\n",
        "    threshold: float = 0.5,\n",
        "):\n",
        "    \"\"\"\n",
        "    write_logits: [B,H,K,T]\n",
        "    Returns:\n",
        "      tail_hl: [B,H,K,P] int32\n",
        "      ess:     [B,H,K,P] float32\n",
        "    \"\"\"\n",
        "    B, H, K, T = write_logits.shape\n",
        "    t_points = t_points.to(write_logits.device)\n",
        "    P = int(t_points.numel())\n",
        "\n",
        "    tail_hl_out = torch.empty((B, H, K, P), device=write_logits.device, dtype=torch.int32)\n",
        "    ess_out     = torch.empty((B, H, K, P), device=write_logits.device, dtype=torch.float32)\n",
        "\n",
        "    for pi, t in enumerate(t_points.tolist()):\n",
        "        t = int(t)\n",
        "        L = t + 1\n",
        "        logits = write_logits[..., :L]\n",
        "        logp = F.log_softmax(logits, dim=-1)\n",
        "        tail_hl_out[..., pi] = _tail_mass_half_life_from_logp(logp, threshold=threshold)\n",
        "        ess_out[..., pi]     = _ess_from_logp(logp)\n",
        "\n",
        "    return tail_hl_out, ess_out\n",
        "\n",
        "@torch.no_grad()\n",
        "def slot_half_life_analysis(\n",
        "    model,\n",
        "    xb: torch.Tensor,\n",
        "    attention_mask=None,\n",
        "    threshold: float = 0.5,\n",
        "    n_timepoints: int = 32,\n",
        "    min_t: int = 32,\n",
        "    max_t: int = None,\n",
        "    per_head: bool = True,\n",
        "):\n",
        "    \"\"\"\n",
        "    Returns dict with:\n",
        "      - per_layer: list of layer records with mean/median + optional per-head means\n",
        "      - raw_t_points: sampled t_points\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    device = xb.device\n",
        "    _, infos = model(xb, attention_mask=attention_mask, return_info=True)\n",
        "\n",
        "    T = xb.shape[1]\n",
        "    if max_t is None:\n",
        "        max_t = T - 1\n",
        "    max_t = min(int(max_t), T - 1)\n",
        "    min_t = min(int(min_t), max_t)\n",
        "\n",
        "    t_points = torch.linspace(min_t, max_t, steps=int(n_timepoints), device=device).round().to(torch.long)\n",
        "    t_points = torch.unique(t_points)\n",
        "\n",
        "    out = {\n",
        "        \"threshold\": float(threshold),\n",
        "        \"t_points\": t_points.detach().cpu().tolist(),\n",
        "        \"per_layer\": [],\n",
        "    }\n",
        "\n",
        "    for layer, info in enumerate(infos):\n",
        "        if info is None or info.get(\"write_logits\", None) is None:\n",
        "            continue\n",
        "\n",
        "        wl = info[\"write_logits\"].to(device=device, dtype=torch.float32)  # [B,H,K,T]\n",
        "        B, H, K, T2 = wl.shape\n",
        "        if T2 != T:\n",
        "            print(f\"Warning: Layer {layer} write_logits T={T2} != xb T={T} (skipping layer).\")\n",
        "            continue\n",
        "\n",
        "        tail_hl, ess = slot_memory_timescale_from_write_logits(wl, t_points, threshold=threshold)\n",
        "        hl_f = tail_hl.to(torch.float32)\n",
        "\n",
        "        layer_rec = {\n",
        "            \"layer\": int(layer),\n",
        "            \"mean_tail_half_life_tokens\": float(hl_f.mean().cpu().item()),\n",
        "            \"p25_tail_half_life_tokens\":  float(torch.quantile(hl_f.flatten(), 0.25).cpu().item()),\n",
        "            \"median_tail_half_life_tokens\": float(hl_f.median().cpu().item()),\n",
        "            \"p75_tail_half_life_tokens\":  float(torch.quantile(hl_f.flatten(), 0.75).cpu().item()),\n",
        "            \"mean_ess\": float(ess.mean().cpu().item()),\n",
        "            \"median_ess\": float(ess.median().cpu().item()),\n",
        "            \"B\": int(B), \"H\": int(H), \"K\": int(K), \"T\": int(T),\n",
        "        }\n",
        "\n",
        "        if per_head:\n",
        "            # mean over (B,K,P) -> per head [H]\n",
        "            layer_rec[\"per_head_mean_tail_half_life\"] = hl_f.mean(dim=(0,2,3)).detach().cpu().numpy()\n",
        "            layer_rec[\"per_head_mean_ess\"]            = ess.mean(dim=(0,2,3)).detach().cpu().numpy()\n",
        "\n",
        "        out[\"per_layer\"].append(layer_rec)\n",
        "\n",
        "    return out\n",
        "\n",
        "# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "# Curated plots & tables\n",
        "#   - 1 summary figure: HL (median with IQR) + ESS mean (secondary axis)\n",
        "#   - 1 heatmap: per-head half-life by layer\n",
        "#   - 1 table: top/bottom heads by half-life (aggregated across layers)\n",
        "# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "\n",
        "def _build_layer_df(res):\n",
        "    df = pd.DataFrame(res[\"per_layer\"]).sort_values(\"layer\").reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "def _stack_per_head(res, key):\n",
        "    # returns mat [n_layers, H] aligned to df order\n",
        "    layers = [r[\"layer\"] for r in res[\"per_layer\"]]\n",
        "    layers_sorted_idx = np.argsort(layers)\n",
        "    recs = [res[\"per_layer\"][i] for i in layers_sorted_idx]\n",
        "    mat = np.stack([r[key] for r in recs], axis=0)\n",
        "    layers_sorted = [r[\"layer\"] for r in recs]\n",
        "    return layers_sorted, mat\n",
        "\n",
        "def plot_half_life_summary_curated(df, threshold):\n",
        "    layers = df[\"layer\"].to_numpy()\n",
        "    med = df[\"median_tail_half_life_tokens\"].to_numpy()\n",
        "    p25 = df[\"p25_tail_half_life_tokens\"].to_numpy()\n",
        "    p75 = df[\"p75_tail_half_life_tokens\"].to_numpy()\n",
        "    ess = df[\"mean_ess\"].to_numpy()\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(11,4))\n",
        "\n",
        "    ax.plot(layers, med, \"o-\", lw=2.2, label=\"median tail half-life\")\n",
        "    ax.fill_between(layers, p25, p75, alpha=0.2, label=\"IQR (25\u201375%)\")\n",
        "    ax.set_xlabel(\"Layer\")\n",
        "    ax.set_ylabel(\"Tail half-life (tokens)\")\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    ax2 = ax.twinx()\n",
        "    ax2.plot(layers, ess, \"s--\", alpha=0.85, lw=2.0, label=\"mean ESS\")\n",
        "    ax2.set_ylabel(\"Mean ESS\")\n",
        "\n",
        "    # one combined legend\n",
        "    h1, l1 = ax.get_legend_handles_labels()\n",
        "    h2, l2 = ax2.get_legend_handles_labels()\n",
        "    ax.legend(h1+h2, l1+l2, loc=\"best\", frameon=True)\n",
        "\n",
        "    ax.set_title(f\"Slot memory timescales across depth (threshold={threshold})\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_per_head_hl_heatmap(layers, hl_mat):\n",
        "    plt.figure(figsize=(11, 0.45*len(layers)+2))\n",
        "    plt.imshow(hl_mat, aspect=\"auto\")\n",
        "    plt.yticks(range(len(layers)), [f\"L{l}\" for l in layers])\n",
        "    plt.xticks(range(hl_mat.shape[1]), [f\"H{h}\" for h in range(hl_mat.shape[1])])\n",
        "    plt.colorbar(label=\"mean tail half-life (tokens)\")\n",
        "    plt.title(\"Per-head mean tail half-life by layer\")\n",
        "    plt.xlabel(\"Head index\")\n",
        "    plt.ylabel(\"Layer\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def summarize_heads_across_layers(layers, hl_mat, ess_mat, topk=5):\n",
        "    \"\"\"\n",
        "    hl_mat, ess_mat: [n_layers,H]\n",
        "    Returns a compact table ranking heads by across-layer mean HL.\n",
        "    \"\"\"\n",
        "    hl_mean = hl_mat.mean(axis=0)\n",
        "    hl_std  = hl_mat.std(axis=0)\n",
        "    ess_mean = ess_mat.mean(axis=0)\n",
        "\n",
        "    dfh = pd.DataFrame({\n",
        "        \"head\": np.arange(hl_mat.shape[1], dtype=np.int32),\n",
        "        \"HL_mean\": hl_mean,\n",
        "        \"HL_std\": hl_std,\n",
        "        \"ESS_mean\": ess_mean,\n",
        "    }).sort_values(\"HL_mean\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "    return dfh.head(topk), dfh.tail(topk), dfh\n",
        "\n",
        "# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "# Run + curated outputs\n",
        "# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "\n",
        "print(\"Computing slot memory timescales...\\n\")\n",
        "\n",
        "res_hl = slot_half_life_analysis(\n",
        "    model,\n",
        "    xb,\n",
        "    attention_mask=None,\n",
        "    threshold=0.5,\n",
        "    n_timepoints=48,   # enough density for stable estimates\n",
        "    min_t=32,\n",
        "    max_t=None,\n",
        "    per_head=True,\n",
        ")\n",
        "\n",
        "df = _build_layer_df(res_hl)\n",
        "\n",
        "print(\"=== Slot memory timescale summary (per layer) ===\")\n",
        "display(df[[\n",
        "    \"layer\",\n",
        "    \"mean_tail_half_life_tokens\",\n",
        "    \"p25_tail_half_life_tokens\",\n",
        "    \"median_tail_half_life_tokens\",\n",
        "    \"p75_tail_half_life_tokens\",\n",
        "    \"mean_ess\",\n",
        "    \"median_ess\",\n",
        "]])\n",
        "\n",
        "# 1) Summary figure (the one you actually want in a report)\n",
        "plot_half_life_summary_curated(df, threshold=res_hl[\"threshold\"])\n",
        "\n",
        "# 2) Per-head heatmap (compact and comparative)\n",
        "layers_sorted, hl_mat = _stack_per_head(res_hl, \"per_head_mean_tail_half_life\")\n",
        "_, ess_mat = _stack_per_head(res_hl, \"per_head_mean_ess\")\n",
        "\n",
        "plot_per_head_hl_heatmap(layers_sorted, hl_mat)\n",
        "\n",
        "# 3) Small \u201cwho are the memory heads?\u201d table\n",
        "top, bottom, dfh = summarize_heads_across_layers(layers_sorted, hl_mat, ess_mat, topk=5)\n",
        "print(\"\\n=== Heads with largest mean tail half-life (across layers) ===\")\n",
        "display(top)\n",
        "print(\"\\n=== Heads with smallest mean tail half-life (across layers) ===\")\n",
        "display(bottom)\n",
        "\n",
        "# Optional: if you want a single scalar summary for write behavior per layer:\n",
        "print(\"\\nScalar checks:\")\n",
        "print(f\"Layers analyzed: {len(df)}  |  heads: {int(df['H'].iloc[0])}  |  slots(K): {int(df['K'].iloc[0])}\")\n",
        "print(f\"Overall mean tail half-life: {df['mean_tail_half_life_tokens'].mean():.2f} tokens\")\n",
        "print(f\"Overall mean ESS:           {df['mean_ess'].mean():.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_3KUs1s6gGlR",
      "metadata": {
        "cellView": "form",
        "id": "_3KUs1s6gGlR",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769532692024,
          "user_tz": 300,
          "elapsed": 39,
          "user": {
            "displayName": "Secondscapes",
            "userId": "04150343613140054698"
          }
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#@title Head Clustering + Head Alignment (Hungarian) \u2014 utilities\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# --------------------------\n",
        "# Small helpers\n",
        "# --------------------------\n",
        "\n",
        "def _cos_sim_matrix(A: np.ndarray, B: np.ndarray, eps: float = 1e-8) -> np.ndarray:\n",
        "    # A: [H,D], B: [H,D] (or [H2,D])\n",
        "    An = A / (np.linalg.norm(A, axis=-1, keepdims=True) + eps)\n",
        "    Bn = B / (np.linalg.norm(B, axis=-1, keepdims=True) + eps)\n",
        "    return An @ Bn.T  # [H,H2]\n",
        "\n",
        "def _safe_softmax(x, axis=-1):\n",
        "    x = x - np.max(x, axis=axis, keepdims=True)\n",
        "    ex = np.exp(x)\n",
        "    return ex / (np.sum(ex, axis=axis, keepdims=True) + 1e-12)\n",
        "\n",
        "def _entropy(p, axis=-1, eps=1e-12):\n",
        "    p = np.clip(p, eps, 1.0)\n",
        "    return -np.sum(p * np.log(p), axis=axis)\n",
        "\n",
        "def _effective_support(p, axis=-1, eps=1e-12):\n",
        "    # exp(entropy) is a nice \"effective count\"\n",
        "    return np.exp(_entropy(p, axis=axis, eps=eps))\n",
        "\n",
        "def _topk_mass(p, k=4, axis=-1):\n",
        "    part = np.partition(p, -k, axis=axis)\n",
        "    topk = part.take(indices=range(p.shape[axis]-k, p.shape[axis]), axis=axis)\n",
        "    return np.sum(topk, axis=axis)\n",
        "\n",
        "# --------------------------\n",
        "# Routing inertia per head\n",
        "# --------------------------\n",
        "\n",
        "@torch.no_grad()\n",
        "def routing_inertia_per_head(t2a: torch.Tensor, max_lag: int = 64, eps: float = 1e-8) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    t2a: [B,H,T,K] probabilities\n",
        "    returns: [H,L] where L=min(max_lag,T-1) mean over (B,T')\n",
        "    \"\"\"\n",
        "    B,H,T,K = t2a.shape\n",
        "    L = int(min(max_lag, T-1))\n",
        "    p = t2a.float()\n",
        "    # normalize over K for cosine\n",
        "    p = p / (p.norm(dim=-1, keepdim=True).clamp_min(eps))\n",
        "    # windows: [B,H,T-L,K,L+1] -> [B,H,T-L,L+1,K]\n",
        "    w = p.unfold(dimension=2, size=L+1, step=1).permute(0,1,2,4,3).contiguous()\n",
        "    a = w[:,:,:,:1,:]          # [B,H,T-L,1,K]\n",
        "    b = w[:,:,:,1:,:]          # [B,H,T-L,L,K]\n",
        "    sim = (a*b).sum(dim=-1)    # [B,H,T-L,L]\n",
        "    return sim.mean(dim=(0,2)) # [H,L]\n",
        "\n",
        "def summarize_inertia_curve(curve_hl: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    curve_hl: [H,L] (cos similarity vs lag)\n",
        "    Returns per-head summary features: [H, 4]\n",
        "      - lag1\n",
        "      - mean over lags\n",
        "      - slope (linear fit)\n",
        "      - \"half-decay lag\" where curve falls below (lag1+min)/2\n",
        "    \"\"\"\n",
        "    H,L = curve_hl.shape\n",
        "    x = np.arange(1, L+1, dtype=np.float32)\n",
        "\n",
        "    out = np.zeros((H,4), dtype=np.float32)\n",
        "    for h in range(H):\n",
        "        y = curve_hl[h].astype(np.float32)\n",
        "\n",
        "        lag1 = y[0]\n",
        "        mean = float(y.mean())\n",
        "\n",
        "        # slope via least squares on (x,y)\n",
        "        xc = x - x.mean()\n",
        "        yc = y - y.mean()\n",
        "        slope = float((xc*yc).sum() / (xc*xc).sum() + 1e-12)\n",
        "\n",
        "        # half-decay lag heuristic\n",
        "        y_min = float(y.min())\n",
        "        thr = 0.5*(lag1 + y_min)\n",
        "        idx = np.argmax(y < thr)  # 0 if already below\n",
        "        half_lag = float(x[idx]) if (y < thr).any() else float(L)\n",
        "\n",
        "        out[h] = [lag1, mean, slope, half_lag]\n",
        "    return out\n",
        "\n",
        "# --------------------------\n",
        "# Per-head memory half-life + ESS from write logits\n",
        "# --------------------------\n",
        "\n",
        "@torch.no_grad()\n",
        "def per_head_half_life_ess(write_logits: torch.Tensor, threshold: float = 0.5) -> tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    write_logits: [B,H,K,T]\n",
        "    Computes tail half-life and ESS for each (B,H,K,Tpoints) then averages over (B,K,Tpoints).\n",
        "    Returns:\n",
        "      hl_h:  [H] float\n",
        "      ess_h: [H] float\n",
        "    \"\"\"\n",
        "    import math\n",
        "    B,H,K,T = write_logits.shape\n",
        "    # choose a set of time points (like your analysis); keep it light\n",
        "    t_points = torch.linspace(32, T-1, steps=48, device=write_logits.device).round().long().unique()\n",
        "    hl_acc = []\n",
        "    ess_acc = []\n",
        "    for t in t_points.tolist():\n",
        "        L = int(t)+1\n",
        "        logits = write_logits[..., :L]                  # [B,H,K,L]\n",
        "        logp = F.log_softmax(logits, dim=-1)            # over time\n",
        "        p = logp.exp()\n",
        "\n",
        "        tail = torch.flip(p, dims=[-1]).cumsum(dim=-1)  # [B,H,K,L]\n",
        "        hl = (tail < threshold).sum(dim=-1).float() + 1 # [B,H,K]\n",
        "        ess = 1.0 / (p*p).sum(dim=-1).clamp_min(1e-12)  # [B,H,K]\n",
        "\n",
        "        hl_acc.append(hl.mean(dim=(0,2)))               # [H]\n",
        "        ess_acc.append(ess.mean(dim=(0,2)))             # [H]\n",
        "\n",
        "    hl_h  = torch.stack(hl_acc).mean(dim=0).detach().cpu().numpy()\n",
        "    ess_h = torch.stack(ess_acc).mean(dim=0).detach().cpu().numpy()\n",
        "    return hl_h, ess_h\n",
        "\n",
        "# --------------------------\n",
        "# Head-level routing distribution stats (from read_weights)\n",
        "# --------------------------\n",
        "\n",
        "@torch.no_grad()\n",
        "def per_head_routing_stats(read_weights: torch.Tensor, topk: int = 4) -> dict:\n",
        "    \"\"\"\n",
        "    read_weights: [B,H,T,K] probabilities\n",
        "    Returns: dict of per-head arrays [H]\n",
        "      - tok_ent: entropy over K averaged over (B,T)\n",
        "      - eff_slots: exp(entropy) averaged over (B,T)\n",
        "      - topk_mass: average top-k mass over (B,T)\n",
        "    \"\"\"\n",
        "    p = read_weights.detach().float().cpu().numpy()  # [B,H,T,K]\n",
        "    # average over B,T -> keep head structure\n",
        "    # ent per token:\n",
        "    ent = _entropy(p, axis=-1)        # [B,H,T]\n",
        "    tok_ent_h = ent.mean(axis=(0,2))  # [H]\n",
        "    eff_h = np.exp(tok_ent_h)         # [H]\n",
        "    topk_h = _topk_mass(p, k=topk, axis=-1).mean(axis=(0,2))  # [H]\n",
        "    return {\"tok_ent\": tok_ent_h.astype(np.float32),\n",
        "            \"eff_slots\": eff_h.astype(np.float32),\n",
        "            \"topk_mass\": topk_h.astype(np.float32)}\n",
        "\n",
        "# --------------------------\n",
        "# Build head feature vectors per layer\n",
        "# --------------------------\n",
        "\n",
        "def head_features_for_layer(info: dict,\n",
        "                            *,\n",
        "                            max_lag: int = 64,\n",
        "                            include_inertia: bool = True,\n",
        "                            include_memory: bool = True,\n",
        "                            include_routing_stats: bool = True,\n",
        "                            threshold_hl: float = 0.5) -> tuple[np.ndarray, list[str]]:\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      X: [H,D] numpy\n",
        "      names: list of feature names length D\n",
        "    \"\"\"\n",
        "    feats = []\n",
        "    names = []\n",
        "\n",
        "    # read_weights required for inertia + routing stats\n",
        "    rw = info.get(\"read_weights\", None)\n",
        "\n",
        "    if include_inertia and (rw is not None):\n",
        "        curve = routing_inertia_per_head(rw, max_lag=max_lag)   # [H,L]\n",
        "        curve_np = curve.detach().cpu().numpy()\n",
        "        summ = summarize_inertia_curve(curve_np)                # [H,4]\n",
        "        feats.append(summ); names += [\"inertia_lag1\", \"inertia_mean\", \"inertia_slope\", \"inertia_half_lag\"]\n",
        "\n",
        "    if include_routing_stats and (rw is not None):\n",
        "        stats = per_head_routing_stats(rw, topk=4)\n",
        "        feats.append(stats[\"tok_ent\"][:,None]); names += [\"tok_ent\"]\n",
        "        feats.append(stats[\"eff_slots\"][:,None]); names += [\"eff_slots\"]\n",
        "        feats.append(stats[\"topk_mass\"][:,None]); names += [\"top4_mass\"]\n",
        "\n",
        "    if include_memory:\n",
        "        wl = info.get(\"write_logits\", None)\n",
        "        if wl is not None:\n",
        "            hl_h, ess_h = per_head_half_life_ess(wl, threshold=threshold_hl)  # [H],[H]\n",
        "            feats.append(hl_h[:,None]);  names += [\"write_tail_half_life\"]\n",
        "            feats.append(ess_h[:,None]); names += [\"write_ess\"]\n",
        "\n",
        "    if not feats:\n",
        "        raise ValueError(\"No features extracted (missing read_weights / write_logits?)\")\n",
        "\n",
        "    X = np.concatenate(feats, axis=1).astype(np.float32)  # [H,D]\n",
        "    return X, names\n",
        "\n",
        "def build_head_feature_bank(infos,\n",
        "                            *,\n",
        "                            max_lag: int = 64,\n",
        "                            include_inertia: bool = True,\n",
        "                            include_memory: bool = True,\n",
        "                            include_routing_stats: bool = True,\n",
        "                            threshold_hl: float = 0.5):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      feat_by_layer: list of [H,D]\n",
        "      feature_names: [D]\n",
        "    \"\"\"\n",
        "    feat_by_layer = []\n",
        "    feature_names = None\n",
        "    for li, info in enumerate(infos):\n",
        "        if info is None:\n",
        "            feat_by_layer.append(None); continue\n",
        "        if info.get(\"read_weights\", None) is None and info.get(\"write_logits\", None) is None:\n",
        "            feat_by_layer.append(None); continue\n",
        "\n",
        "        X, names = head_features_for_layer(\n",
        "            info,\n",
        "            max_lag=max_lag,\n",
        "            include_inertia=include_inertia,\n",
        "            include_memory=include_memory,\n",
        "            include_routing_stats=include_routing_stats,\n",
        "            threshold_hl=threshold_hl,\n",
        "        )\n",
        "        feat_by_layer.append(X)\n",
        "        if feature_names is None:\n",
        "            feature_names = names\n",
        "    return feat_by_layer, feature_names\n",
        "\n",
        "# --------------------------\n",
        "# Hungarian alignment over heads (H x H)\n",
        "# --------------------------\n",
        "\n",
        "def hungarian_best_cos_heads(A_hd: np.ndarray, B_hd: np.ndarray):\n",
        "    \"\"\"\n",
        "    A_hd, B_hd: [H,D]\n",
        "    returns:\n",
        "      diag_cos, best_cos, perm (perm[h] = matched head in B for head h in A)\n",
        "    \"\"\"\n",
        "    from scipy.optimize import linear_sum_assignment\n",
        "    S = _cos_sim_matrix(A_hd, B_hd)  # [H,H]\n",
        "    diag = float(np.diag(S).mean())\n",
        "    row, col = linear_sum_assignment(-S)\n",
        "    best = float(S[row, col].mean())\n",
        "    perm = np.empty((S.shape[0],), dtype=np.int64)\n",
        "    perm[row] = col\n",
        "    return diag, best, perm\n",
        "\n",
        "def compute_adjacent_head_alignment(feat_by_layer):\n",
        "    rows = []\n",
        "    for l in range(len(feat_by_layer)-1):\n",
        "        A = feat_by_layer[l]\n",
        "        B = feat_by_layer[l+1]\n",
        "        if A is None or B is None:\n",
        "            continue\n",
        "        d, b, perm = hungarian_best_cos_heads(A, B)\n",
        "        rows.append({\"l\": l, \"l_to\": l+1, \"diag_cos\": d, \"best_cos\": b, \"improvement\": b-d, \"perm\": perm})\n",
        "    return rows\n",
        "\n",
        "# --------------------------\n",
        "# Simple K-means (numpy) so no sklearn dependency\n",
        "# --------------------------\n",
        "\n",
        "def kmeans(X: np.ndarray, k: int, n_init: int = 10, iters: int = 100, seed: int = 0):\n",
        "    \"\"\"\n",
        "    X: [N,D]\n",
        "    Returns:\n",
        "      labels: [N]\n",
        "      centers: [k,D]\n",
        "      inertia: float\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    best = None\n",
        "\n",
        "    # normalize helps clustering when features have different scales\n",
        "    Xn = X.copy()\n",
        "    Xn = (Xn - Xn.mean(axis=0, keepdims=True)) / (Xn.std(axis=0, keepdims=True) + 1e-6)\n",
        "\n",
        "    for init in range(n_init):\n",
        "        # kmeans++ lite: pick one random, then farthest\n",
        "        centers = np.empty((k, Xn.shape[1]), dtype=np.float32)\n",
        "        idx0 = rng.integers(0, Xn.shape[0])\n",
        "        centers[0] = Xn[idx0]\n",
        "        dist = np.sum((Xn - centers[0])**2, axis=1)\n",
        "        for j in range(1, k):\n",
        "            idx = np.argmax(dist)\n",
        "            centers[j] = Xn[idx]\n",
        "            dist = np.minimum(dist, np.sum((Xn - centers[j])**2, axis=1))\n",
        "\n",
        "        for _ in range(iters):\n",
        "            d2 = np.sum((Xn[:,None,:] - centers[None,:,:])**2, axis=2)  # [N,k]\n",
        "            labels = np.argmin(d2, axis=1)\n",
        "            new_centers = np.stack([Xn[labels==j].mean(axis=0) if np.any(labels==j) else centers[j] for j in range(k)])\n",
        "            if np.max(np.abs(new_centers - centers)) < 1e-5:\n",
        "                centers = new_centers\n",
        "                break\n",
        "            centers = new_centers\n",
        "\n",
        "        inertia_val = float(np.sum((Xn - centers[labels])**2))\n",
        "        if best is None or inertia_val < best[0]:\n",
        "            best = (inertia_val, labels.copy(), centers.copy())\n",
        "\n",
        "    return best[1], best[2], best[0]\n",
        "\n",
        "# --------------------------\n",
        "# Plotting\n",
        "# --------------------------\n",
        "\n",
        "def plot_head_cluster_map(cluster_by_layer, title=\"Head clusters by layer\"):\n",
        "    layers = sorted([l for l in cluster_by_layer.keys()])\n",
        "    H = len(cluster_by_layer[layers[0]])\n",
        "    mat = np.stack([cluster_by_layer[l] for l in layers], axis=0)  # [L,H]\n",
        "\n",
        "    plt.figure(figsize=(10, 0.6*len(layers)+2))\n",
        "    plt.imshow(mat, aspect=\"auto\")\n",
        "    plt.yticks(range(len(layers)), [f\"L{l}\" for l in layers])\n",
        "    plt.xticks(range(H), [f\"H{h}\" for h in range(H)])\n",
        "    plt.colorbar(label=\"cluster id\")\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Head\")\n",
        "    plt.ylabel(\"Layer\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_alignment_quality(df: pd.DataFrame, title=\"Head alignment (adjacent layers)\"):\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.plot(df[\"l\"], df[\"diag_cos\"], \"o--\", label=\"diag\")\n",
        "    plt.plot(df[\"l\"], df[\"best_cos\"], \"o-\", label=\"hungarian best\")\n",
        "    plt.plot(df[\"l\"], df[\"improvement\"], \"s-\", label=\"improvement\")\n",
        "    plt.axhline(0, lw=1)\n",
        "    plt.xlabel(\"Layer transition l \u2192 l+1\")\n",
        "    plt.ylabel(\"cosine similarity\")\n",
        "    plt.title(title)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_feature_importance_like(centers: np.ndarray, feature_names: list[str], title=\"Cluster centers (z-scored space)\"):\n",
        "    # centers are in normalized space from kmeans; just show relative profiles\n",
        "    plt.figure(figsize=(max(10, len(feature_names)*0.7), 4))\n",
        "    for j in range(centers.shape[0]):\n",
        "        plt.plot(range(len(feature_names)), centers[j], marker=\"o\", label=f\"cluster {j}\")\n",
        "    plt.xticks(range(len(feature_names)), feature_names, rotation=35, ha=\"right\")\n",
        "    plt.title(title)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    #plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "I7YTE2Y4m3_4",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "I7YTE2Y4m3_4",
        "outputId": "d561688d-8791-4457-a7cf-5e07f45be6c3",
        "collapsed": true,
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769532707338,
          "user_tz": 300,
          "elapsed": 15310,
          "user": {
            "displayName": "Secondscapes",
            "userId": "04150343613140054698"
          }
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Head clustering + rigorous head alignment (adjacent + ANCHOR) + paper-grade metrics (FULL CELL)\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# =============================================================================\n",
        "# Helpers\n",
        "# =============================================================================\n",
        "\n",
        "def _cos_sim_matrix(A: np.ndarray, B: np.ndarray, eps: float = 1e-8) -> np.ndarray:\n",
        "    An = A / (np.linalg.norm(A, axis=-1, keepdims=True) + eps)\n",
        "    Bn = B / (np.linalg.norm(B, axis=-1, keepdims=True) + eps)\n",
        "    return An @ Bn.T  # [H,H2]\n",
        "\n",
        "def _entropy(p, axis=-1, eps=1e-12):\n",
        "    p = np.clip(p, eps, 1.0)\n",
        "    return -np.sum(p * np.log(p), axis=axis)\n",
        "\n",
        "def _topk_mass(p, k=4, axis=-1):\n",
        "    part = np.partition(p, -k, axis=axis)\n",
        "    topk = part.take(indices=range(p.shape[axis]-k, p.shape[axis]), axis=axis)\n",
        "    return np.sum(topk, axis=axis)\n",
        "\n",
        "# =============================================================================\n",
        "# Routing inertia per head\n",
        "# =============================================================================\n",
        "\n",
        "@torch.no_grad()\n",
        "def routing_inertia_per_head(t2a: torch.Tensor, max_lag: int = 64, eps: float = 1e-8) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    t2a: [B,H,T,K] probabilities\n",
        "    returns: [H,L] where L=min(max_lag,T-1) mean over (B,T')\n",
        "    \"\"\"\n",
        "    B,H,T,K = t2a.shape\n",
        "    L = int(min(max_lag, T-1))\n",
        "    p = t2a.float()\n",
        "    p = p / (p.norm(dim=-1, keepdim=True).clamp_min(eps))\n",
        "    w = p.unfold(dimension=2, size=L+1, step=1).permute(0,1,2,4,3).contiguous()\n",
        "    a = w[:,:,:,:1,:]          # [B,H,T-L,1,K]\n",
        "    b = w[:,:,:,1:,:]          # [B,H,T-L,L,K]\n",
        "    sim = (a*b).sum(dim=-1)    # [B,H,T-L,L]\n",
        "    return sim.mean(dim=(0,2)) # [H,L]\n",
        "\n",
        "def summarize_inertia_curve(curve_hl: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    curve_hl: [H,L] (cos similarity vs lag)\n",
        "    Returns per-head summary features: [H, 4]\n",
        "      - lag1\n",
        "      - mean over lags\n",
        "      - slope (linear fit)\n",
        "      - half-decay lag where curve < (lag1+min)/2\n",
        "    \"\"\"\n",
        "    H,L = curve_hl.shape\n",
        "    x = np.arange(1, L+1, dtype=np.float32)\n",
        "\n",
        "    out = np.zeros((H,4), dtype=np.float32)\n",
        "    for h in range(H):\n",
        "        y = curve_hl[h].astype(np.float32)\n",
        "\n",
        "        lag1 = float(y[0])\n",
        "        mean = float(y.mean())\n",
        "\n",
        "        xc = x - x.mean()\n",
        "        yc = y - y.mean()\n",
        "        slope = float((xc*yc).sum() / ((xc*xc).sum() + 1e-12))\n",
        "\n",
        "        y_min = float(y.min())\n",
        "        thr = 0.5*(lag1 + y_min)\n",
        "        idx = np.argmax(y < thr)\n",
        "        half_lag = float(x[idx]) if (y < thr).any() else float(L)\n",
        "\n",
        "        out[h] = [lag1, mean, slope, half_lag]\n",
        "    return out\n",
        "\n",
        "# =============================================================================\n",
        "# Per-head memory half-life + ESS from write logits\n",
        "# =============================================================================\n",
        "\n",
        "@torch.no_grad()\n",
        "def per_head_half_life_ess(write_logits: torch.Tensor, threshold: float = 0.5) -> tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    write_logits: [B,H,K,T]\n",
        "    Returns:\n",
        "      hl_h:  [H]\n",
        "      ess_h: [H]\n",
        "    \"\"\"\n",
        "    B,H,K,T = write_logits.shape\n",
        "    t_points = torch.linspace(32, T-1, steps=48, device=write_logits.device).round().long().unique()\n",
        "    hl_acc = []\n",
        "    ess_acc = []\n",
        "    for t in t_points.tolist():\n",
        "        L = int(t)+1\n",
        "        logits = write_logits[..., :L]                  # [B,H,K,L]\n",
        "        logp = F.log_softmax(logits, dim=-1)            # over time\n",
        "        p = logp.exp()\n",
        "\n",
        "        tail = torch.flip(p, dims=[-1]).cumsum(dim=-1)  # [B,H,K,L]\n",
        "        hl = (tail < threshold).sum(dim=-1).float() + 1 # [B,H,K]\n",
        "        ess = 1.0 / (p*p).sum(dim=-1).clamp_min(1e-12)  # [B,H,K]\n",
        "\n",
        "        hl_acc.append(hl.mean(dim=(0,2)))               # [H]\n",
        "        ess_acc.append(ess.mean(dim=(0,2)))             # [H]\n",
        "\n",
        "    hl_h  = torch.stack(hl_acc).mean(dim=0).detach().cpu().numpy()\n",
        "    ess_h = torch.stack(ess_acc).mean(dim=0).detach().cpu().numpy()\n",
        "    return hl_h, ess_h\n",
        "\n",
        "# =============================================================================\n",
        "# Head-level routing distribution stats (from read_weights)\n",
        "# =============================================================================\n",
        "\n",
        "@torch.no_grad()\n",
        "def per_head_routing_stats(read_weights: torch.Tensor, topk: int = 4) -> dict:\n",
        "    \"\"\"\n",
        "    read_weights: [B,H,T,K] probabilities\n",
        "    Returns: dict of per-head arrays [H]\n",
        "      - tok_ent: entropy over K averaged over (B,T)\n",
        "      - eff_slots: exp(entropy)\n",
        "      - topk_mass: average top-k mass\n",
        "    \"\"\"\n",
        "    p = read_weights.detach().float().cpu().numpy()  # [B,H,T,K]\n",
        "    ent = _entropy(p, axis=-1)                       # [B,H,T]\n",
        "    tok_ent_h = ent.mean(axis=(0,2))                 # [H]\n",
        "    eff_h = np.exp(tok_ent_h)                        # [H]\n",
        "    topk_h = _topk_mass(p, k=topk, axis=-1).mean(axis=(0,2))\n",
        "    return {\"tok_ent\": tok_ent_h.astype(np.float32),\n",
        "            \"eff_slots\": eff_h.astype(np.float32),\n",
        "            \"topk_mass\": topk_h.astype(np.float32)}\n",
        "\n",
        "# =============================================================================\n",
        "# Build head feature vectors per layer\n",
        "# =============================================================================\n",
        "\n",
        "def head_features_for_layer(\n",
        "    info: dict,\n",
        "    *,\n",
        "    max_lag: int = 64,\n",
        "    include_inertia: bool = True,\n",
        "    include_memory: bool = True,\n",
        "    include_routing_stats: bool = True,\n",
        "    threshold_hl: float = 0.5\n",
        ") -> tuple[np.ndarray, list[str]]:\n",
        "    feats = []\n",
        "    names = []\n",
        "\n",
        "    rw = info.get(\"read_weights\", None)\n",
        "\n",
        "    if include_inertia and (rw is not None):\n",
        "        curve = routing_inertia_per_head(rw, max_lag=max_lag)   # [H,L]\n",
        "        curve_np = curve.detach().cpu().numpy()\n",
        "        summ = summarize_inertia_curve(curve_np)                # [H,4]\n",
        "        feats.append(summ)\n",
        "        names += [\"inertia_lag1\", \"inertia_mean\", \"inertia_slope\", \"inertia_half_lag\"]\n",
        "\n",
        "    if include_routing_stats and (rw is not None):\n",
        "        stats = per_head_routing_stats(rw, topk=4)\n",
        "        feats.append(stats[\"tok_ent\"][:,None]);   names += [\"tok_ent\"]\n",
        "        feats.append(stats[\"eff_slots\"][:,None]); names += [\"eff_slots\"]\n",
        "        feats.append(stats[\"topk_mass\"][:,None]); names += [\"top4_mass\"]\n",
        "\n",
        "    if include_memory:\n",
        "        wl = info.get(\"write_logits\", None)\n",
        "        if wl is not None:\n",
        "            hl_h, ess_h = per_head_half_life_ess(wl, threshold=threshold_hl)\n",
        "            feats.append(hl_h[:,None]);  names += [\"write_tail_half_life\"]\n",
        "            feats.append(ess_h[:,None]); names += [\"write_ess\"]\n",
        "\n",
        "    if not feats:\n",
        "        raise ValueError(\"No features extracted (missing read_weights / write_logits?)\")\n",
        "\n",
        "    X = np.concatenate(feats, axis=1).astype(np.float32)\n",
        "    return X, names\n",
        "\n",
        "def build_head_feature_bank(\n",
        "    infos,\n",
        "    *,\n",
        "    max_lag: int = 64,\n",
        "    include_inertia: bool = True,\n",
        "    include_memory: bool = True,\n",
        "    include_routing_stats: bool = True,\n",
        "    threshold_hl: float = 0.5\n",
        "):\n",
        "    feat_by_layer = []\n",
        "    feature_names = None\n",
        "\n",
        "    for li, info in enumerate(infos):\n",
        "        if info is None:\n",
        "            feat_by_layer.append(None); continue\n",
        "        if info.get(\"read_weights\", None) is None and info.get(\"write_logits\", None) is None:\n",
        "            feat_by_layer.append(None); continue\n",
        "\n",
        "        X, names = head_features_for_layer(\n",
        "            info,\n",
        "            max_lag=max_lag,\n",
        "            include_inertia=include_inertia,\n",
        "            include_memory=include_memory,\n",
        "            include_routing_stats=include_routing_stats,\n",
        "            threshold_hl=threshold_hl,\n",
        "        )\n",
        "        feat_by_layer.append(X)\n",
        "        if feature_names is None:\n",
        "            feature_names = names\n",
        "\n",
        "    return feat_by_layer, feature_names\n",
        "\n",
        "# =============================================================================\n",
        "# Hungarian alignment (adjacent + anchor)\n",
        "# =============================================================================\n",
        "\n",
        "def hungarian_best_cos_heads(A_hd: np.ndarray, B_hd: np.ndarray):\n",
        "    \"\"\"\n",
        "    A_hd, B_hd: [H,D]\n",
        "    returns:\n",
        "      diag_cos, best_cos, perm\n",
        "    perm[h] = matched head index in B for head h in A\n",
        "    \"\"\"\n",
        "    from scipy.optimize import linear_sum_assignment\n",
        "    S = _cos_sim_matrix(A_hd, B_hd)  # [H,H]\n",
        "    diag = float(np.diag(S).mean())\n",
        "    row, col = linear_sum_assignment(-S)\n",
        "    best = float(S[row, col].mean())\n",
        "    perm = np.empty((S.shape[0],), dtype=np.int64)\n",
        "    perm[row] = col\n",
        "    return diag, best, perm\n",
        "\n",
        "def compute_adjacent_head_alignment(feat_by_layer):\n",
        "    rows = []\n",
        "    for l in range(len(feat_by_layer)-1):\n",
        "        A = feat_by_layer[l]\n",
        "        B = feat_by_layer[l+1]\n",
        "        if A is None or B is None:\n",
        "            continue\n",
        "        if A.shape[0] != B.shape[0]:\n",
        "            continue\n",
        "        d, b, perm = hungarian_best_cos_heads(A, B)\n",
        "        rows.append({\"l\": l, \"l_to\": l+1, \"diag_cos\": d, \"best_cos\": b, \"improvement\": b-d, \"perm\": perm})\n",
        "    return rows\n",
        "\n",
        "def compute_anchor_head_alignment(feat_by_layer, valid_layers, anchor_layer: int):\n",
        "    \"\"\"\n",
        "    Align each layer l to anchor_layer via one Hungarian solve:\n",
        "      perm_anchor_to_l[h_anchor] = h_l\n",
        "    Returns rows with perm from anchor -> l.\n",
        "    \"\"\"\n",
        "    A = feat_by_layer[anchor_layer]\n",
        "    if A is None:\n",
        "        raise ValueError(\"Anchor layer has no features.\")\n",
        "    rows = []\n",
        "    for l in valid_layers:\n",
        "        if l == anchor_layer:\n",
        "            continue\n",
        "        B = feat_by_layer[l]\n",
        "        if B is None:\n",
        "            continue\n",
        "        if A.shape[0] != B.shape[0]:\n",
        "            continue\n",
        "        d, b, perm = hungarian_best_cos_heads(A, B)\n",
        "        rows.append({\"anchor\": anchor_layer, \"l\": l, \"diag_cos\": d, \"best_cos\": b, \"improvement\": b-d, \"perm\": perm})\n",
        "    return rows\n",
        "\n",
        "def make_aligned_feat_by_anchor(feat_by_layer, valid_layers, anchor_rows, anchor_layer: int):\n",
        "    \"\"\"\n",
        "    Creates feat_aligned where all layers are reordered into ANCHOR slot space:\n",
        "      aligned_feat[l][h_anchor] = feat_by_layer[l][ perm_anchor_to_l[h_anchor] ]\n",
        "    \"\"\"\n",
        "    perm_by_l = {int(r[\"l\"]): r[\"perm\"] for r in anchor_rows}\n",
        "    feat_aligned = list(feat_by_layer)\n",
        "    for l in valid_layers:\n",
        "        if l == anchor_layer:\n",
        "            continue\n",
        "        if l not in perm_by_l:\n",
        "            continue\n",
        "        feat_aligned[l] = feat_by_layer[l][perm_by_l[l]]\n",
        "    return feat_aligned, perm_by_l\n",
        "\n",
        "# =============================================================================\n",
        "# K-means (numpy)\n",
        "# =============================================================================\n",
        "\n",
        "def kmeans(X: np.ndarray, k: int, n_init: int = 10, iters: int = 100, seed: int = 0):\n",
        "    \"\"\"\n",
        "    X: [N,D]\n",
        "    Returns:\n",
        "      labels: [N]\n",
        "      centers: [k,D] (z-scored space)\n",
        "      inertia: float (z-scored space)\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    best = None\n",
        "\n",
        "    Xn = X.copy()\n",
        "    Xn = (Xn - Xn.mean(axis=0, keepdims=True)) / (Xn.std(axis=0, keepdims=True) + 1e-6)\n",
        "\n",
        "    for _ in range(n_init):\n",
        "        centers = np.empty((k, Xn.shape[1]), dtype=np.float32)\n",
        "        idx0 = rng.integers(0, Xn.shape[0])\n",
        "        centers[0] = Xn[idx0]\n",
        "        dist = np.sum((Xn - centers[0])**2, axis=1)\n",
        "        for j in range(1, k):\n",
        "            idx = int(np.argmax(dist))\n",
        "            centers[j] = Xn[idx]\n",
        "            dist = np.minimum(dist, np.sum((Xn - centers[j])**2, axis=1))\n",
        "\n",
        "        for _it in range(iters):\n",
        "            d2 = np.sum((Xn[:,None,:] - centers[None,:,:])**2, axis=2)  # [N,k]\n",
        "            labels = np.argmin(d2, axis=1)\n",
        "            new_centers = np.stack([\n",
        "                Xn[labels==j].mean(axis=0) if np.any(labels==j) else centers[j]\n",
        "                for j in range(k)\n",
        "            ])\n",
        "            if np.max(np.abs(new_centers - centers)) < 1e-5:\n",
        "                centers = new_centers\n",
        "                break\n",
        "            centers = new_centers\n",
        "\n",
        "        inertia_val = float(np.sum((Xn - centers[labels])**2))\n",
        "        if best is None or inertia_val < best[0]:\n",
        "            best = (inertia_val, labels.copy(), centers.copy())\n",
        "\n",
        "    return best[1], best[2], best[0]\n",
        "\n",
        "# =============================================================================\n",
        "# Plotting\n",
        "# =============================================================================\n",
        "\n",
        "def plot_head_cluster_map(cluster_by_layer, title=\"Head clusters by layer\"):\n",
        "    layers = sorted(cluster_by_layer.keys())\n",
        "    H = len(cluster_by_layer[layers[0]])\n",
        "    mat = np.stack([cluster_by_layer[l] for l in layers], axis=0)  # [L,H]\n",
        "\n",
        "    plt.figure(figsize=(10, 0.6*len(layers)+2))\n",
        "    plt.imshow(mat, aspect=\"auto\")\n",
        "    plt.yticks(range(len(layers)), [f\"L{l}\" for l in layers])\n",
        "    plt.xticks(range(H), [f\"H{h}\" for h in range(H)])\n",
        "    plt.colorbar(label=\"cluster id\")\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Head\")\n",
        "    plt.ylabel(\"Layer\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_alignment_quality(df: pd.DataFrame, xcol: str, title=\"Head alignment\"):\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.plot(df[xcol], df[\"diag_cos\"], \"o--\", label=\"diag\")\n",
        "    plt.plot(df[xcol], df[\"best_cos\"], \"o-\", label=\"hungarian best\")\n",
        "    plt.plot(df[xcol], df[\"improvement\"], \"s-\", label=\"improvement\")\n",
        "    plt.axhline(0, lw=1)\n",
        "    plt.xlabel(xcol)\n",
        "    plt.ylabel(\"cosine similarity\")\n",
        "    plt.title(title)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_feature_importance_like(centers: np.ndarray, feature_names: list[str], title=\"Cluster centers (z-scored space)\"):\n",
        "    plt.figure(figsize=(max(10, len(feature_names)*0.7), 4))\n",
        "    for j in range(centers.shape[0]):\n",
        "        plt.plot(range(len(feature_names)), centers[j], marker=\"o\", label=f\"cluster {j}\")\n",
        "    plt.xticks(range(len(feature_names)), feature_names, rotation=35, ha=\"right\")\n",
        "    plt.title(title)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_cluster_occupancy(occ, layers, k, title):\n",
        "    plt.figure(figsize=(10, 0.35*len(layers)+2))\n",
        "    plt.imshow(occ, aspect=\"auto\")\n",
        "    plt.yticks(range(len(layers)), [f\"L{l}\" for l in layers])\n",
        "    plt.xticks(range(k), [f\"C{c}\" for c in range(k)])\n",
        "    plt.colorbar(label=\"fraction of heads in cluster\")\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Cluster\")\n",
        "    plt.ylabel(\"Layer\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_slot_entropy(ent, title):\n",
        "    plt.figure(figsize=(10, 3.5))\n",
        "    plt.bar(np.arange(len(ent)), ent)\n",
        "    plt.grid(True, axis=\"y\", alpha=0.3)\n",
        "    plt.xlabel(\"Head slot (column index)\")\n",
        "    plt.ylabel(\"Entropy of cluster ID across depth\")\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_stability_curves(df_adj, title):\n",
        "    plt.figure(figsize=(10, 3.5))\n",
        "    plt.plot(df_adj[\"l\"], df_adj[\"keep_rate\"], \"o-\", label=\"keep-rate\")\n",
        "    plt.plot(df_adj[\"l\"], df_adj[\"ARI\"], \"s-\", label=\"ARI\")\n",
        "    plt.plot(df_adj[\"l\"], df_adj[\"NMI\"], \"^-\", label=\"NMI\")\n",
        "    plt.ylim(-0.05, 1.05)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.xlabel(\"Layer transition l \u2192 l+1\")\n",
        "    plt.ylabel(\"score\")\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# =============================================================================\n",
        "# Packing / mapping\n",
        "# =============================================================================\n",
        "\n",
        "def _pack_all_heads(feat_by_layer, valid_layers):\n",
        "    Xs, meta = [], []\n",
        "    for l in valid_layers:\n",
        "        X = feat_by_layer[l]\n",
        "        H = X.shape[0]\n",
        "        Xs.append(X)\n",
        "        for h in range(H):\n",
        "            meta.append((l, h))\n",
        "    X_all = np.concatenate(Xs, axis=0)\n",
        "    meta = np.array(meta, dtype=np.int32)\n",
        "    return X_all, meta\n",
        "\n",
        "def _clusters_to_map(meta, labels, feat_by_layer):\n",
        "    cluster_by_layer = {}\n",
        "    for (l,h), c in zip(meta, labels):\n",
        "        l = int(l); h = int(h); c = int(c)\n",
        "        cluster_by_layer.setdefault(l, np.full((feat_by_layer[l].shape[0],), -1, dtype=np.int32))\n",
        "        cluster_by_layer[l][h] = c\n",
        "    return cluster_by_layer\n",
        "\n",
        "# =============================================================================\n",
        "# Paper-grade metrics\n",
        "# =============================================================================\n",
        "\n",
        "def adjacent_partition_metrics(cluster_by_layer, perm_by_l):\n",
        "    \"\"\"\n",
        "    Per-transition stability metrics after providing head correspondence perm_by_l\n",
        "      perm_by_l[l] maps head indices in layer l -> head indices in layer l+1\n",
        "    \"\"\"\n",
        "    from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
        "\n",
        "    layers = sorted(cluster_by_layer.keys())\n",
        "    rows = []\n",
        "    for l in layers:\n",
        "        if (l+1) not in cluster_by_layer:\n",
        "            continue\n",
        "        if l not in perm_by_l:\n",
        "            continue\n",
        "\n",
        "        A = np.asarray(cluster_by_layer[l])\n",
        "        B = np.asarray(cluster_by_layer[l+1])\n",
        "        perm = perm_by_l[l]\n",
        "        if perm.shape[0] != B.shape[0]:\n",
        "            continue\n",
        "\n",
        "        B_aligned = B[perm]\n",
        "        keep = float((A == B_aligned).mean())\n",
        "        ari = float(adjusted_rand_score(A, B_aligned))\n",
        "        nmi = float(normalized_mutual_info_score(A, B_aligned))\n",
        "        rows.append({\"l\": l, \"keep_rate\": keep, \"ARI\": ari, \"NMI\": nmi})\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def slot_entropy_across_depth(cluster_by_layer):\n",
        "    layers = sorted(cluster_by_layer.keys())\n",
        "    mat = np.stack([cluster_by_layer[l] for l in layers], axis=0)  # [L,H]\n",
        "    L, H = mat.shape\n",
        "    ent = np.zeros((H,), dtype=np.float32)\n",
        "    for h in range(H):\n",
        "        vals = mat[:, h]\n",
        "        uniq, cnt = np.unique(vals, return_counts=True)\n",
        "        p = cnt.astype(np.float32) / cnt.sum()\n",
        "        ent[h] = float(_entropy(p, axis=0))\n",
        "    return ent\n",
        "\n",
        "def cluster_lifecycle(cluster_by_layer, k):\n",
        "    layers = sorted(cluster_by_layer.keys())\n",
        "    H = len(cluster_by_layer[layers[0]])\n",
        "    occ = np.zeros((len(layers), k), dtype=np.float32)\n",
        "    for i, l in enumerate(layers):\n",
        "        v = np.asarray(cluster_by_layer[l])\n",
        "        for c in range(k):\n",
        "            occ[i, c] = float((v == c).mean())\n",
        "\n",
        "    first, last, life = [], [], []\n",
        "    for c in range(k):\n",
        "        present = np.where(occ[:, c] > 0)[0]\n",
        "        if len(present) == 0:\n",
        "            first.append(np.nan); last.append(np.nan); life.append(np.nan)\n",
        "        else:\n",
        "            f = layers[int(present[0])]\n",
        "            t = layers[int(present[-1])]\n",
        "            first.append(f); last.append(t); life.append(t - f + 1)\n",
        "\n",
        "    df = pd.DataFrame({\"cluster\": np.arange(k, dtype=np.int32),\n",
        "                       \"first_layer\": first, \"last_layer\": last, \"lifespan_layers\": life})\n",
        "    return df, occ, layers\n",
        "\n",
        "# =============================================================================\n",
        "# Bootstrap CIs (rigor)\n",
        "# =============================================================================\n",
        "\n",
        "def bootstrap_ci(values, n_boot=2000, seed=0, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Nonparametric bootstrap CI for the mean.\n",
        "    values: 1D array-like\n",
        "    Returns (mean, lo, hi)\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    v = np.asarray(values, dtype=np.float64)\n",
        "    if v.size == 0:\n",
        "        return (np.nan, np.nan, np.nan)\n",
        "    means = []\n",
        "    for _ in range(n_boot):\n",
        "        samp = rng.choice(v, size=v.size, replace=True)\n",
        "        means.append(samp.mean())\n",
        "    means = np.asarray(means)\n",
        "    lo = float(np.quantile(means, alpha/2))\n",
        "    hi = float(np.quantile(means, 1-alpha/2))\n",
        "    return (float(v.mean()), lo, hi)\n",
        "\n",
        "def bootstrap_ci_rows(df, col, n_boot=2000, seed=0, alpha=0.05):\n",
        "    if df is None or len(df) == 0:\n",
        "        return (np.nan, np.nan, np.nan)\n",
        "    return bootstrap_ci(df[col].to_numpy(), n_boot=n_boot, seed=seed, alpha=alpha)\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN RUN\n",
        "# =============================================================================\n",
        "\n",
        "# ---- knobs ----\n",
        "MAX_LAG = 64\n",
        "INCLUDE_INERTIA = True\n",
        "INCLUDE_MEMORY = True\n",
        "INCLUDE_ROUTING_STATS = True\n",
        "HL_THRESHOLD = 0.5\n",
        "\n",
        "N_CLUSTERS = 7\n",
        "N_INIT = 15\n",
        "SEED = 0\n",
        "\n",
        "# Alignment strategy for \"aligned\" mode:\n",
        "#  - \"anchor\": align all layers to a single anchor layer (recommended)\n",
        "#  - \"adjacent\": keep your original adjacent composition approach (kept here mainly for comparison)\n",
        "ALIGN_MODE = \"anchor\"   # {\"anchor\",\"adjacent\"}\n",
        "ANCHOR_LAYER = None     # if None, uses middle valid layer\n",
        "\n",
        "# Bootstrap rigor knobs\n",
        "BOOTSTRAP = True\n",
        "N_BOOT = 2000\n",
        "ALPHA = 0.05\n",
        "\n",
        "# ---- build features ----\n",
        "feat_by_layer, feature_names = build_head_feature_bank(\n",
        "    infos,\n",
        "    max_lag=MAX_LAG,\n",
        "    include_inertia=INCLUDE_INERTIA,\n",
        "    include_memory=INCLUDE_MEMORY,\n",
        "    include_routing_stats=INCLUDE_ROUTING_STATS,\n",
        "    threshold_hl=HL_THRESHOLD,\n",
        ")\n",
        "\n",
        "valid_layers = [i for i, x in enumerate(feat_by_layer) if x is not None]\n",
        "print(\"Valid layers with head features:\", valid_layers)\n",
        "print(\"Feature dim:\", len(feature_names), feature_names)\n",
        "\n",
        "# ---- adjacent alignment diagnostics (always useful) ----\n",
        "align_rows_adj = compute_adjacent_head_alignment(feat_by_layer)\n",
        "df_align_adj = pd.DataFrame([{k: v for k, v in r.items() if k != \"perm\"} for r in align_rows_adj])\n",
        "print(\"\\nAdjacent-layer head alignment summary:\")\n",
        "display(df_align_adj)\n",
        "if len(df_align_adj) > 0:\n",
        "    plot_alignment_quality(df_align_adj, xcol=\"l\", title=\"Head feature alignment across adjacent layers\")\n",
        "\n",
        "perm_by_l_adj = {int(r[\"l\"]): r[\"perm\"] for r in align_rows_adj}\n",
        "\n",
        "# ---- choose anchor layer + compute anchor alignment ----\n",
        "if ANCHOR_LAYER is None:\n",
        "    ANCHOR_LAYER = valid_layers[len(valid_layers)//2]\n",
        "\n",
        "anchor_rows = compute_anchor_head_alignment(feat_by_layer, valid_layers, anchor_layer=ANCHOR_LAYER)\n",
        "df_align_anchor = pd.DataFrame([{k: v for k, v in r.items() if k != \"perm\"} for r in anchor_rows]).sort_values(\"l\")\n",
        "print(f\"\\nAnchor alignment summary (anchor=L{ANCHOR_LAYER}):\")\n",
        "display(df_align_anchor)\n",
        "if len(df_align_anchor) > 0:\n",
        "    plot_alignment_quality(df_align_anchor, xcol=\"l\", title=f\"Head feature alignment to anchor layer L{ANCHOR_LAYER}\")\n",
        "\n",
        "# ---- construct aligned feature bank using anchor perms ----\n",
        "feat_aligned_anchor, perm_anchor_to_l = make_aligned_feat_by_anchor(\n",
        "    feat_by_layer, valid_layers, anchor_rows, anchor_layer=ANCHOR_LAYER\n",
        ")\n",
        "\n",
        "# For anchor-aligned data, adjacent transition perm (l->l+1) is derivable from anchor perms:\n",
        "# B_aligned_to_A at transition l->l+1 is achieved by indexing both in anchor-slot space already,\n",
        "# so we can use identity correspondence when comparing slots. But for strict comparison with your old\n",
        "# formulation (A[h] corresponds to B[perm[h]]), we can just use perm_by_l = identity.\n",
        "def _identity_perm_by_l_from_cluster_map(cluster_by_layer):\n",
        "    perm_by = {}\n",
        "    layers = sorted(cluster_by_layer.keys())\n",
        "    for l in layers:\n",
        "        if (l+1) not in cluster_by_layer:\n",
        "            continue\n",
        "        H = len(cluster_by_layer[l])\n",
        "        perm_by[l] = np.arange(H, dtype=np.int64)\n",
        "    return perm_by\n",
        "\n",
        "# ---- clustering runner ----\n",
        "def run_mode(mode_name, feat_src):\n",
        "    X_all, meta = _pack_all_heads(feat_src, valid_layers)\n",
        "    labels, centers, inertia_val = kmeans(X_all, k=N_CLUSTERS, n_init=N_INIT, iters=200, seed=SEED)\n",
        "    cluster_by_layer = _clusters_to_map(meta, labels, feat_src)\n",
        "    return {\n",
        "        \"mode\": mode_name,\n",
        "        \"X_all\": X_all,\n",
        "        \"meta\": meta,\n",
        "        \"labels\": labels,\n",
        "        \"centers\": centers,\n",
        "        \"inertia\": float(inertia_val),\n",
        "        \"cluster_by_layer\": cluster_by_layer,\n",
        "    }\n",
        "\n",
        "results = {}\n",
        "results[\"no_align\"] = run_mode(\"no_align\", feat_by_layer)\n",
        "\n",
        "if ALIGN_MODE == \"anchor\":\n",
        "    results[\"aligned\"] = run_mode(\"aligned_anchor\", feat_aligned_anchor)\n",
        "elif ALIGN_MODE == \"adjacent\":\n",
        "    # Keep old behavior: composing adjacent perms to define slots (less rigorous than anchor)\n",
        "    # Here we simply reuse your original \"no reindexing\" and interpret stability using perm_by_l_adj.\n",
        "    results[\"aligned\"] = run_mode(\"aligned_adjacent_semantics\", feat_by_layer)\n",
        "else:\n",
        "    raise ValueError(\"ALIGN_MODE must be 'anchor' or 'adjacent'.\")\n",
        "\n",
        "# ---- paper-grade summary table (with optional bootstrap CIs) ----\n",
        "summary_rows = []\n",
        "for key, R in results.items():\n",
        "    cb = R[\"cluster_by_layer\"]\n",
        "\n",
        "    # pick correspondence for adjacent stability metrics\n",
        "    if key == \"aligned\" and ALIGN_MODE == \"anchor\":\n",
        "        perm_by_l_for_metrics = _identity_perm_by_l_from_cluster_map(cb)\n",
        "    else:\n",
        "        perm_by_l_for_metrics = perm_by_l_adj\n",
        "\n",
        "    df_adj = adjacent_partition_metrics(cb, perm_by_l_for_metrics)\n",
        "    ent = slot_entropy_across_depth(cb)\n",
        "\n",
        "    row = {\n",
        "        \"mode\": R[\"mode\"],\n",
        "        \"k\": N_CLUSTERS,\n",
        "        \"inertia(z)\": R[\"inertia\"],\n",
        "        \"keep_mean\": float(df_adj[\"keep_rate\"].mean()) if len(df_adj) else np.nan,\n",
        "        \"ARI_mean\": float(df_adj[\"ARI\"].mean()) if len(df_adj) else np.nan,\n",
        "        \"NMI_mean\": float(df_adj[\"NMI\"].mean()) if len(df_adj) else np.nan,\n",
        "        \"slot_entropy_mean\": float(ent.mean()),\n",
        "    }\n",
        "\n",
        "    if BOOTSTRAP and len(df_adj) > 0:\n",
        "        km, klo, khi = bootstrap_ci_rows(df_adj, \"keep_rate\", n_boot=N_BOOT, seed=SEED, alpha=ALPHA)\n",
        "        am, alo, ahi = bootstrap_ci_rows(df_adj, \"ARI\", n_boot=N_BOOT, seed=SEED+1, alpha=ALPHA)\n",
        "        nm, nlo, nhi = bootstrap_ci_rows(df_adj, \"NMI\", n_boot=N_BOOT, seed=SEED+2, alpha=ALPHA)\n",
        "        em, elo, ehi = bootstrap_ci(ent, n_boot=N_BOOT, seed=SEED+3, alpha=ALPHA)\n",
        "\n",
        "        row.update({\n",
        "            \"keep_CI\": f\"{km:.3f} [{klo:.3f},{khi:.3f}]\",\n",
        "            \"ARI_CI\":  f\"{am:.3f} [{alo:.3f},{ahi:.3f}]\",\n",
        "            \"NMI_CI\":  f\"{nm:.3f} [{nlo:.3f},{nhi:.3f}]\",\n",
        "            \"entropy_CI\": f\"{em:.3f} [{elo:.3f},{ehi:.3f}]\",\n",
        "        })\n",
        "\n",
        "    summary_rows.append(row)\n",
        "\n",
        "df_summary = pd.DataFrame(summary_rows)\n",
        "print(\"\\n=== PAPER TABLE: summary metrics (\u2191 keep/ARI/NMI better; \u2193 entropy better) ===\")\n",
        "display(df_summary)\n",
        "\n",
        "# =============================================================================\n",
        "# Detailed outputs per mode\n",
        "# =============================================================================\n",
        "\n",
        "for key, R in results.items():\n",
        "    cb = R[\"cluster_by_layer\"]\n",
        "\n",
        "    # correspondence for adjacent stability metrics\n",
        "    if key == \"aligned\" and ALIGN_MODE == \"anchor\":\n",
        "        perm_by_l_for_metrics = _identity_perm_by_l_from_cluster_map(cb)\n",
        "        corr_note = f\"adjacent stability uses IDENTITY (slots anchored to L{ANCHOR_LAYER})\"\n",
        "    else:\n",
        "        perm_by_l_for_metrics = perm_by_l_adj\n",
        "        corr_note = \"adjacent stability uses ADJACENT Hungarian perms\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"MODE: {R['mode']}\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"K-means inertia (z-scored space): {R['inertia']:.2f}\")\n",
        "    print(f\"Correspondence: {corr_note}\")\n",
        "\n",
        "    plot_head_cluster_map(cb, title=f\"[{R['mode']}] Head clusters by layer (k={N_CLUSTERS})\")\n",
        "    plot_feature_importance_like(R[\"centers\"], feature_names, title=f\"[{R['mode']}] Cluster centers (z-scored space)\")\n",
        "\n",
        "    df_adj = adjacent_partition_metrics(cb, perm_by_l_for_metrics)\n",
        "    print(f\"\\n[{R['mode']}] Per-transition stability metrics:\")\n",
        "    display(df_adj)\n",
        "    if len(df_adj) > 0:\n",
        "        plot_stability_curves(df_adj, title=f\"[{R['mode']}] Stability across depth ({corr_note})\")\n",
        "\n",
        "    ent = slot_entropy_across_depth(cb)\n",
        "    plot_slot_entropy(ent, title=f\"[{R['mode']}] Per-slot cluster entropy across depth (lower = more stable)\")\n",
        "\n",
        "    df_life, occ, layers = cluster_lifecycle(cb, k=N_CLUSTERS)\n",
        "    print(f\"\\n[{R['mode']}] Cluster lifecycle table:\")\n",
        "    display(df_life)\n",
        "    plot_cluster_occupancy(occ, layers, N_CLUSTERS, title=f\"[{R['mode']}] Cluster occupancy vs depth (lifecycle heatmap)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OXJu5KFdx1CO",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "executionInfo": {
          "elapsed": 1224,
          "status": "ok",
          "timestamp": 1769532708583,
          "user": {
            "displayName": "Secondscapes",
            "userId": "04150343613140054698"
          },
          "user_tz": 300
        },
        "id": "OXJu5KFdx1CO",
        "outputId": "86efee5c-58f7-404f-d3a1-5d1c6e6cfb7e",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#@title Head functional regimes across depth (FINAL, curated analysis cell)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ============================================================\n",
        "# ASSUMES ALREADY COMPUTED (from previous pipeline):\n",
        "#   - feat_by_layer\n",
        "#   - feat_aligned_anchor (anchor-aligned feature bank)\n",
        "#   - feature_names\n",
        "#   - results[\"aligned_anchor\"]\n",
        "#   - perm_anchor_to_l\n",
        "#   - valid_layers\n",
        "#   - ANCHOR_LAYER\n",
        "# ============================================================\n",
        "\n",
        "R = results[\"aligned\"] # Corrected: Access 'aligned' key\n",
        "cluster_by_layer = R[\"cluster_by_layer\"]\n",
        "k = N_CLUSTERS\n",
        "layers = sorted(cluster_by_layer.keys())\n",
        "L = len(layers)\n",
        "H = len(cluster_by_layer[layers[0]])\n",
        "\n",
        "# ============================================================\n",
        "# 1. ALIGNMENT QUALITY (METHOD VALIDATION)\n",
        "# ============================================================\n",
        "\n",
        "df_align_adj = pd.DataFrame(\n",
        "    [{k: v for k, v in r.items() if k != \"perm\"} for r in align_rows_adj]\n",
        ")\n",
        "\n",
        "df_align_anchor = pd.DataFrame(\n",
        "    [{k: v for k, v in r.items() if k != \"perm\"} for r in anchor_rows]\n",
        ").sort_values(\"l\")\n",
        "\n",
        "print(\"\\n=== Alignment diagnostics ===\")\n",
        "display(df_align_adj[[\"l\",\"diag_cos\",\"best_cos\",\"improvement\"]])\n",
        "display(df_align_anchor[[\"l\",\"diag_cos\",\"best_cos\",\"improvement\"]])\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(df_align_adj[\"l\"], df_align_adj[\"diag_cos\"], \"o--\", label=\"adjacent diag\")\n",
        "plt.plot(df_align_adj[\"l\"], df_align_adj[\"best_cos\"], \"o-\", label=\"adjacent best\")\n",
        "plt.plot(df_align_anchor[\"l\"], df_align_anchor[\"best_cos\"], \"s-\", label=\"anchor best\")\n",
        "plt.axhline(0.9, color=\"k\", alpha=0.2)\n",
        "plt.xlabel(\"Layer\")\n",
        "plt.ylabel(\"Cosine similarity\")\n",
        "plt.title(\"Head-feature alignment quality\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# 2. CLUSTER LIFECYCLE (CORE RESULT)\n",
        "# ============================================================\n",
        "\n",
        "def cluster_lifecycle(cluster_by_layer, k):\n",
        "    occ = np.zeros((L, k), dtype=np.float32)\n",
        "    for i, l in enumerate(layers):\n",
        "        v = np.asarray(cluster_by_layer[l])\n",
        "        for c in range(k):\n",
        "            occ[i, c] = (v == c).mean()\n",
        "\n",
        "    first, last, life = [], [], []\n",
        "    for c in range(k):\n",
        "        idx = np.where(occ[:,c] > 0)[0]\n",
        "        if len(idx) == 0:\n",
        "            first.append(np.nan); last.append(np.nan); life.append(np.nan)\n",
        "        else:\n",
        "            first.append(layers[idx[0]])\n",
        "            last.append(layers[idx[-1]])\n",
        "            life.append(layers[idx[-1]] - layers[idx[0]] + 1)\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        \"cluster\": np.arange(k),\n",
        "        \"first_layer\": first,\n",
        "        \"last_layer\": last,\n",
        "        \"lifespan_layers\": life,\n",
        "        \"occupancy_sum\": occ.sum(axis=0),\n",
        "    })\n",
        "    return df, occ\n",
        "\n",
        "df_life, occ = cluster_lifecycle(cluster_by_layer, k)\n",
        "\n",
        "print(\"\\n=== Cluster lifecycle table ===\")\n",
        "display(df_life.sort_values(\"lifespan_layers\", ascending=False))\n",
        "\n",
        "plt.figure(figsize=(10,0.35*L+2))\n",
        "plt.imshow(occ, aspect=\"auto\")\n",
        "plt.yticks(range(L), [f\"L{l}\" for l in layers])\n",
        "plt.xticks(range(k), [f\"C{c}\" for c in range(k)])\n",
        "plt.colorbar(label=\"fraction of heads\")\n",
        "plt.title(\"Cluster occupancy vs depth (functional regime lifetimes)\")\n",
        "plt.xlabel(\"Cluster\")\n",
        "plt.ylabel(\"Layer\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# 3. SLOT-LEVEL ROLE DYNAMICS (KILLER RESULT)\n",
        "# ============================================================\n",
        "\n",
        "# cluster assignments in anchor slot basis\n",
        "mat = np.stack([cluster_by_layer[l] for l in layers], axis=0)  # [L,H]\n",
        "\n",
        "# number of regime switches per slot\n",
        "switches = (mat[1:] != mat[:-1]).sum(axis=0)\n",
        "\n",
        "# entropy per slot\n",
        "def slot_entropy(mat):\n",
        "    ent = np.zeros((H,), dtype=np.float32)\n",
        "    for h in range(H):\n",
        "        u,c = np.unique(mat[:,h], return_counts=True)\n",
        "        p = c / c.sum()\n",
        "        ent[h] = -np.sum(p*np.log(p+1e-12))\n",
        "    return ent\n",
        "\n",
        "ent = slot_entropy(mat)\n",
        "\n",
        "# mean feature vector per slot\n",
        "Xs = np.stack([feat_aligned_anchor[l] for l in layers], axis=0)  # [L,H,D]\n",
        "feat_mean = Xs.mean(axis=0)\n",
        "\n",
        "df_slot = pd.DataFrame({\n",
        "    \"slot\": np.arange(H),\n",
        "    \"switches\": switches,\n",
        "    \"entropy\": ent,\n",
        "})\n",
        "\n",
        "for j, name in enumerate(feature_names):\n",
        "    df_slot[f\"mean_{name}\"] = feat_mean[:, j]\n",
        "\n",
        "print(\"\\n=== Per-slot role dynamics (anchor slot basis) ===\")\n",
        "display(df_slot.sort_values(\"switches\", ascending=False))\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.hist(switches, bins=np.arange(switches.max()+2)-0.5)\n",
        "plt.xlabel(\"Number of regime switches\")\n",
        "plt.ylabel(\"Number of head slots\")\n",
        "plt.title(\"Slot-level regime switching\")\n",
        "plt.grid(True, axis=\"y\", alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# 4. MECHANISTIC CORRELATES OF ADAPTIVITY\n",
        "# ============================================================\n",
        "\n",
        "corr_rows = []\n",
        "y = switches.astype(np.float64)\n",
        "for j, name in enumerate(feature_names):\n",
        "    x = feat_mean[:, j].astype(np.float64)\n",
        "    if x.std() < 1e-12 or y.std() < 1e-12:\n",
        "        r = np.nan\n",
        "    else:\n",
        "        r = float(np.corrcoef(x, y)[0,1])\n",
        "    corr_rows.append({\"feature\": name, \"corr_with_switches\": r})\n",
        "\n",
        "df_corr = pd.DataFrame(corr_rows).sort_values(\"corr_with_switches\", ascending=False)\n",
        "\n",
        "print(\"\\n=== Correlation of regime switching with head features ===\")\n",
        "display(df_corr)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.barh(df_corr[\"feature\"], df_corr[\"corr_with_switches\"])\n",
        "plt.axvline(0, color=\"k\", lw=1)\n",
        "plt.xlabel(\"Pearson r\")\n",
        "plt.title(\"Which features predict adaptive vs stable heads?\")\n",
        "plt.grid(True, axis=\"x\", alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Second-Order Control Fields\nExtract slotspace and content read fields that shape routing dynamics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uEY-e_JCZ0vE",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uEY-e_JCZ0vE",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769532691973,
          "user_tz": 300,
          "elapsed": 64491,
          "user": {
            "displayName": "Secondscapes",
            "userId": "04150343613140054698"
          }
        },
        "outputId": "2d7e67c2-4df8-48b6-f433-875664459fde"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Second-Order Slotspace Attention (FINAL, curated, story-completing)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ----------------------------\n",
        "# Helpers\n",
        "# ----------------------------\n",
        "\n",
        "def _softplus_scalar(x: float) -> float:\n",
        "    return float(F.softplus(torch.tensor(x)).item())\n",
        "\n",
        "def _bootstrap_corr_ci(x, y, n_boot=5000, seed=0, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Pearson r + bootstrap CI.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    x = np.asarray(x, dtype=np.float64)\n",
        "    y = np.asarray(y, dtype=np.float64)\n",
        "    if x.std() < 1e-12 or y.std() < 1e-12:\n",
        "        return np.nan, (np.nan, np.nan)\n",
        "\n",
        "    r0 = float(np.corrcoef(x, y)[0,1])\n",
        "    n = len(x)\n",
        "    boots = []\n",
        "    for _ in range(n_boot):\n",
        "        idx = rng.integers(0, n, size=n)\n",
        "        xb = x[idx]; yb = y[idx]\n",
        "        if xb.std() < 1e-12 or yb.std() < 1e-12:\n",
        "            continue\n",
        "        boots.append(float(np.corrcoef(xb, yb)[0,1]))\n",
        "    if len(boots) == 0:\n",
        "        return r0, (np.nan, np.nan)\n",
        "    lo = float(np.quantile(boots, alpha/2))\n",
        "    hi = float(np.quantile(boots, 1-alpha/2))\n",
        "    return r0, (lo, hi)\n",
        "\n",
        "# ----------------------------\n",
        "# 1) Extract params per layer\n",
        "# ----------------------------\n",
        "\n",
        "def extract_second_order_params(model):\n",
        "    \"\"\"\n",
        "    Returns DataFrame with columns:\n",
        "      layer, slotspace_gate, content_read_gamma\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    for layer_idx, block in enumerate(model.blocks):\n",
        "        attn = getattr(block, \"attn\", None)\n",
        "        if attn is None:\n",
        "            rows.append({\"layer\": layer_idx, \"slotspace_gate\": np.nan, \"content_read_gamma\": np.nan})\n",
        "            continue\n",
        "\n",
        "        # slotspace gate\n",
        "        gate = np.nan\n",
        "        if hasattr(attn, \"_slotspace_gate_raw\") and (attn._slotspace_gate_raw is not None):\n",
        "            raw = float(attn._slotspace_gate_raw.detach().cpu().item())\n",
        "            gate = _softplus_scalar(raw)\n",
        "\n",
        "        # content gamma (clamped if max provided)\n",
        "        gamma = np.nan\n",
        "        if hasattr(attn, \"_content_read_gamma_raw\") and (attn._content_read_gamma_raw is not None):\n",
        "            raw = float(attn._content_read_gamma_raw.detach().cpu().item())\n",
        "            gamma = _softplus_scalar(raw)\n",
        "            max_g = getattr(attn, \"content_read_max_gamma\", None)\n",
        "            if (max_g is not None) and (float(max_g) > 0):\n",
        "                gamma = min(gamma, float(max_g))\n",
        "\n",
        "        rows.append({\"layer\": layer_idx, \"slotspace_gate\": gate, \"content_read_gamma\": gamma})\n",
        "\n",
        "    df = pd.DataFrame(rows).dropna().reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "df_so = extract_second_order_params(model)\n",
        "\n",
        "print(\"=== Second-order parameters (per layer) ===\")\n",
        "display(df_so)\n",
        "\n",
        "# ----------------------------\n",
        "# 2) Main plot (one figure)\n",
        "# ----------------------------\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(11,4))\n",
        "ax.plot(df_so[\"layer\"], df_so[\"slotspace_gate\"], \"o-\", lw=2.2, label=\"slotspace_gate (softplus)\")\n",
        "ax.set_xlabel(\"Layer\")\n",
        "ax.set_ylabel(\"Gate value\")\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_title(\"Second-order strength across depth\")\n",
        "\n",
        "ax2 = ax.twinx()\n",
        "ax2.plot(df_so[\"layer\"], df_so[\"content_read_gamma\"], \"s--\", lw=2.0, alpha=0.85, label=\"content_read_gamma (softplus)\")\n",
        "ax2.set_ylabel(\"Gamma value\")\n",
        "\n",
        "h1, l1 = ax.get_legend_handles_labels()\n",
        "h2, l2 = ax2.get_legend_handles_labels()\n",
        "ax.legend(h1+h2, l1+l2, loc=\"best\", frameon=True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ----------------------------\n",
        "# 3) Optional: delta_norm from infos (if present)\n",
        "# ----------------------------\n",
        "\n",
        "delta_df = None\n",
        "try:\n",
        "    _, test_infos = run_with_infos(model, xb)\n",
        "    delta = []\n",
        "    if test_infos and (test_infos[0] is not None) and (\"slotspace_delta_norm\" in test_infos[0]):\n",
        "        for li, info in enumerate(test_infos):\n",
        "            if info is None or \"slotspace_delta_norm\" not in info:\n",
        "                continue\n",
        "            dn = info[\"slotspace_delta_norm\"]\n",
        "            dn = float(dn.detach().cpu().item()) if isinstance(dn, torch.Tensor) else float(dn)\n",
        "            delta.append((li, dn))\n",
        "        if len(delta) > 0:\n",
        "            delta_df = pd.DataFrame(delta, columns=[\"layer\", \"slotspace_delta_norm\"])\n",
        "            print(\"\\n=== Logged delta norms (per layer) ===\")\n",
        "            display(delta_df)\n",
        "\n",
        "            plt.figure(figsize=(11,3.2))\n",
        "            plt.bar(delta_df[\"layer\"], delta_df[\"slotspace_delta_norm\"], alpha=0.8)\n",
        "            plt.xlabel(\"Layer\")\n",
        "            plt.ylabel(\"slotspace_delta_norm\")\n",
        "            plt.title(\"Magnitude of second-order correction (logged)\")\n",
        "            plt.grid(True, axis=\"y\", alpha=0.3)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "except Exception as e:\n",
        "    print(f\"\\n(note) Could not read delta norms from infos: {type(e).__name__}: {e}\")\n",
        "\n",
        "# ----------------------------\n",
        "# 4) Merge with \u201cbehavioral\u201d summaries you already have\n",
        "#    - from res_hl (slot half-life cell)\n",
        "#    - from feat_by_layer (head feature bank)\n",
        "# ----------------------------\n",
        "\n",
        "# 4a) Half-life summary (if res_hl exists)\n",
        "df_hl = None\n",
        "if \"res_hl\" in globals() and isinstance(res_hl, dict) and (\"per_layer\" in res_hl):\n",
        "    df_hl = pd.DataFrame(res_hl[\"per_layer\"])[[\n",
        "        \"layer\",\n",
        "        \"mean_tail_half_life_tokens\",\n",
        "        \"median_tail_half_life_tokens\",\n",
        "        \"mean_ess\",\n",
        "        \"median_ess\",\n",
        "    ]].copy()\n",
        "\n",
        "# 4b) Routing/memory feature summaries from feat_by_layer (if exists)\n",
        "df_feat = None\n",
        "if \"feat_by_layer\" in globals() and \"feature_names\" in globals() and feat_by_layer is not None:\n",
        "    rows = []\n",
        "    for l, X in enumerate(feat_by_layer):\n",
        "        if X is None:\n",
        "            continue\n",
        "        X = np.asarray(X)\n",
        "        rec = {\"layer\": int(l)}\n",
        "        # layer-level mean of each feature across heads\n",
        "        for j, nm in enumerate(feature_names):\n",
        "            rec[f\"mean_{nm}\"] = float(X[:, j].mean())\n",
        "        rows.append(rec)\n",
        "    if len(rows) > 0:\n",
        "        df_feat = pd.DataFrame(rows)\n",
        "\n",
        "# Merge all available\n",
        "df_merge = df_so.copy()\n",
        "if delta_df is not None:\n",
        "    df_merge = df_merge.merge(delta_df, on=\"layer\", how=\"left\")\n",
        "if df_hl is not None:\n",
        "    df_merge = df_merge.merge(df_hl, on=\"layer\", how=\"left\")\n",
        "if df_feat is not None:\n",
        "    df_merge = df_merge.merge(df_feat, on=\"layer\", how=\"left\")\n",
        "\n",
        "print(\"\\n=== Merged analysis table (second-order + behavior) ===\")\n",
        "display(df_merge)\n",
        "\n",
        "# ----------------------------\n",
        "# 5) Paper table: correlations (with bootstrap CIs)\n",
        "# ----------------------------\n",
        "\n",
        "targets = []\n",
        "# Always include these\n",
        "targets += [\"slotspace_gate\", \"content_read_gamma\"]\n",
        "# Optional: if present\n",
        "if \"slotspace_delta_norm\" in df_merge.columns:\n",
        "    targets += [\"slotspace_delta_norm\"]\n",
        "\n",
        "# Candidate behavioral metrics (include only if present)\n",
        "candidates = []\n",
        "for col in [\n",
        "    # memory timescale summaries\n",
        "    \"mean_tail_half_life_tokens\", \"median_tail_half_life_tokens\", \"mean_ess\", \"median_ess\",\n",
        "    # feature bank summaries (layer means)\n",
        "    \"mean_inertia_lag1\", \"mean_inertia_mean\", \"mean_inertia_slope\", \"mean_inertia_half_lag\",\n",
        "    \"mean_tok_ent\", \"mean_eff_slots\", \"mean_top4_mass\",\n",
        "    \"mean_write_tail_half_life\", \"mean_write_ess\",\n",
        "]:\n",
        "    if col in df_merge.columns:\n",
        "        candidates.append(col)\n",
        "\n",
        "corr_rows = []\n",
        "for t in targets:\n",
        "    for c in candidates:\n",
        "        d = df_merge[[t, c]].dropna()\n",
        "        if len(d) < 4:\n",
        "            continue\n",
        "        r, (lo, hi) = _bootstrap_corr_ci(d[t].to_numpy(), d[c].to_numpy(), seed=SEED if \"SEED\" in globals() else 0)\n",
        "        corr_rows.append({\n",
        "            \"x(second_order)\": t,\n",
        "            \"y(behavior)\": c,\n",
        "            \"n_layers\": int(len(d)),\n",
        "            \"pearson_r\": r,\n",
        "            \"CI_95\": f\"[{lo:.2f}, {hi:.2f}]\",\n",
        "        })\n",
        "\n",
        "df_corr = pd.DataFrame(corr_rows)\n",
        "if len(df_corr) == 0:\n",
        "    print(\"\\n(no correlations computed \u2014 missing behavioral columns or not enough layers)\")\n",
        "else:\n",
        "    # rank by absolute effect size\n",
        "    df_corr[\"abs_r\"] = df_corr[\"pearson_r\"].abs()\n",
        "    df_corr = df_corr.sort_values([\"x(second_order)\", \"abs_r\"], ascending=[True, False]).drop(columns=[\"abs_r\"])\n",
        "    print(\"\\n=== PAPER TABLE: second-order params vs behavior (Pearson r with bootstrap CI) ===\")\n",
        "    display(df_corr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Drift & Phase Analysis\nAnalyze drift in substrate space and layer-level phase regimes."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title Substrate field + drift cell (FINAL, field intensity + head drift)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.collections import LineCollection\n",
        "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
        "\n",
        "# ============================================================\n",
        "# 0) CONFIG\n",
        "# ============================================================\n",
        "\n",
        "LOCK_THRESHOLD = 8.0  # inertia_half_lag >= this -> locked\n",
        "\n",
        "LAYER_RANGE = None  # e.g. (0, 20) or None for all\n",
        "ARROW_STRIDE = 1    # plot every k layers per head (2D drift)\n",
        "HEAD_SUBSAMPLE = None  # e.g. [0,3,6] or None for all heads\n",
        "\n",
        "ARROW_SCALE_2D = 1.0  # multiplies dM,dP\n",
        "ARROW_ALPHA = 0.55\n",
        "POINT_ALPHA = 0.85\n",
        "\n",
        "# 3D arrows can get cluttered; keep sparse by default:\n",
        "DO_3D_QUIVER = False\n",
        "QUIVER_STRIDE_LAYER = 3\n",
        "QUIVER_STRIDE_HEAD = 1\n",
        "QUIVER_SCALE_3D = 0.8\n",
        "\n",
        "# ============================================================\n",
        "# 1) helpers\n",
        "# ============================================================\n",
        "\n",
        "def z(x):\n",
        "    x = np.asarray(x, dtype=np.float64)\n",
        "    return (x - x.mean()) / (x.std() + 1e-8)\n",
        "\n",
        "def _require(name):\n",
        "    if name not in globals():\n",
        "        raise RuntimeError(f\"Missing required variable: {name}\")\n",
        "\n",
        "# ============================================================\n",
        "# 2) BUILD head\u00d7layer substrate table (true per-head, per-layer)\n",
        "#    Uses feat_by_layer + feature_names, and df_slot for per-head switches.\n",
        "# ============================================================\n",
        "\n",
        "_require(\"feat_by_layer\")\n",
        "_require(\"feature_names\")\n",
        "_require(\"df_so\")\n",
        "\n",
        "# switches per head (slot) from df_slot if present\n",
        "switch_map = None\n",
        "if \"df_slot\" in globals() and df_slot is not None and \"slot\" in df_slot.columns and \"switches\" in df_slot.columns:\n",
        "    switch_map = df_slot.set_index(\"slot\")[\"switches\"].to_dict()\n",
        "\n",
        "# Determine layers that exist in feat_by_layer\n",
        "layers = [i for i, X in enumerate(feat_by_layer) if X is not None]\n",
        "if len(layers) == 0:\n",
        "    raise RuntimeError(\"feat_by_layer has no valid layers (all None).\")\n",
        "\n",
        "if LAYER_RANGE is not None:\n",
        "    lo, hi = LAYER_RANGE\n",
        "    layers = [l for l in layers if lo <= l <= hi]\n",
        "\n",
        "# Identify required per-head features\n",
        "need = [\"write_ess\", \"inertia_mean\", \"top4_mass\", \"inertia_half_lag\"]\n",
        "missing = [nm for nm in need if nm not in feature_names]\n",
        "if missing:\n",
        "    raise RuntimeError(f\"feature_names missing required fields: {missing}\\nHave: {feature_names}\")\n",
        "\n",
        "idx = {nm: feature_names.index(nm) for nm in feature_names}\n",
        "\n",
        "rows = []\n",
        "for l in layers:\n",
        "    X = np.asarray(feat_by_layer[l])  # [H, F]\n",
        "    H = X.shape[0]\n",
        "    for h in range(H):\n",
        "        rec = {\n",
        "            \"layer\": int(l),\n",
        "            \"head\": int(h),\n",
        "            \"write_ess\": float(X[h, idx[\"write_ess\"]]),\n",
        "            \"inertia_mean\": float(X[h, idx[\"inertia_mean\"]]),\n",
        "            \"top4_mass\": float(X[h, idx[\"top4_mass\"]]),\n",
        "            \"inertia_half_lag\": float(X[h, idx[\"inertia_half_lag\"]]),\n",
        "        }\n",
        "        if switch_map is not None:\n",
        "            rec[\"switches\"] = float(switch_map.get(h, np.nan))\n",
        "        else:\n",
        "            rec[\"switches\"] = np.nan\n",
        "        rows.append(rec)\n",
        "\n",
        "df_hl = pd.DataFrame(rows)\n",
        "\n",
        "# optional head subsample\n",
        "if HEAD_SUBSAMPLE is not None:\n",
        "    df_hl = df_hl[df_hl[\"head\"].isin(list(HEAD_SUBSAMPLE))].copy()\n",
        "\n",
        "# substrate coordinates (z across all head\u00d7layer points)\n",
        "df_hl[\"M\"] = z(df_hl[\"write_ess\"])\n",
        "df_hl[\"P\"] = z(df_hl[\"inertia_mean\"])\n",
        "df_hl[\"C\"] = z(df_hl[\"top4_mass\"])\n",
        "\n",
        "df_hl[\"locked\"] = (df_hl[\"inertia_half_lag\"] >= LOCK_THRESHOLD).astype(int)\n",
        "\n",
        "# size proxy\n",
        "if df_hl[\"switches\"].notna().any():\n",
        "    df_hl[\"size\"] = 30 + 18 * (df_hl[\"switches\"] - df_hl[\"switches\"].min()) / (df_hl[\"switches\"].max() - df_hl[\"switches\"].min() + 1e-8)\n",
        "else:\n",
        "    df_hl[\"size\"] = 60.0\n",
        "\n",
        "print(\"df_hl head\u00d7layer table:\", df_hl.shape)\n",
        "display(df_hl.head())\n",
        "\n",
        "# ============================================================\n",
        "# 3) BUILD environment field table per layer\n",
        "#    E_slot = z(slotspace_gate * slotspace_delta_norm) if delta exists else z(slotspace_gate)\n",
        "#    E_gamma = z(content_read_gamma)\n",
        "# ============================================================\n",
        "\n",
        "df_env = df_so[[\"layer\", \"slotspace_gate\", \"content_read_gamma\"]].copy()\n",
        "\n",
        "# slotspace_delta_norm can appear as:\n",
        "# - delta_df[\"slotspace_delta_norm\"] from your Second-Order cell\n",
        "# - df_merge[\"slotspace_delta_norm\"] from merged analysis\n",
        "delta_col = None\n",
        "if \"df_merge\" in globals() and df_merge is not None and \"slotspace_delta_norm\" in df_merge.columns:\n",
        "    df_env = df_env.merge(df_merge[[\"layer\", \"slotspace_delta_norm\"]], on=\"layer\", how=\"left\")\n",
        "    delta_col = \"slotspace_delta_norm\"\n",
        "elif \"delta_df\" in globals() and delta_df is not None and \"slotspace_delta_norm\" in delta_df.columns:\n",
        "    df_env = df_env.merge(delta_df[[\"layer\", \"slotspace_delta_norm\"]], on=\"layer\", how=\"left\")\n",
        "    delta_col = \"slotspace_delta_norm\"\n",
        "\n",
        "if delta_col is not None and df_env[delta_col].notna().any():\n",
        "    df_env[\"E_slot_raw\"] = df_env[\"slotspace_gate\"] * df_env[delta_col]\n",
        "else:\n",
        "    df_env[\"E_slot_raw\"] = df_env[\"slotspace_gate\"]\n",
        "\n",
        "df_env[\"E_slot\"] = z(df_env[\"E_slot_raw\"].fillna(df_env[\"E_slot_raw\"].mean()))\n",
        "df_env[\"E_gamma\"] = z(df_env[\"content_read_gamma\"].fillna(df_env[\"content_read_gamma\"].mean()))\n",
        "\n",
        "# attach environment to head\u00d7layer table\n",
        "df_hl = df_hl.merge(df_env[[\"layer\", \"E_slot\", \"E_gamma\", \"slotspace_gate\", \"content_read_gamma\"]], on=\"layer\", how=\"left\")\n",
        "\n",
        "print(\"\\nEnvironment table:\", df_env.shape, \"| delta used:\", (delta_col is not None))\n",
        "display(df_env.head())\n",
        "\n",
        "# ============================================================\n",
        "# 4) COMPUTE drift per head across depth (\u0394 to next layer)\n",
        "# ============================================================\n",
        "\n",
        "df_hl = df_hl.sort_values([\"head\", \"layer\"]).reset_index(drop=True)\n",
        "\n",
        "# compute forward differences within each head trajectory\n",
        "df_hl[\"M_next\"] = df_hl.groupby(\"head\")[\"M\"].shift(-1)\n",
        "df_hl[\"P_next\"] = df_hl.groupby(\"head\")[\"P\"].shift(-1)\n",
        "df_hl[\"C_next\"] = df_hl.groupby(\"head\")[\"C\"].shift(-1)\n",
        "df_hl[\"layer_next\"] = df_hl.groupby(\"head\")[\"layer\"].shift(-1)\n",
        "\n",
        "# only keep diffs when next layer is exactly +1 (contiguous depth)\n",
        "contig = (df_hl[\"layer_next\"] == (df_hl[\"layer\"] + 1))\n",
        "df_hl[\"dM\"] = np.where(contig, df_hl[\"M_next\"] - df_hl[\"M\"], np.nan)\n",
        "df_hl[\"dP\"] = np.where(contig, df_hl[\"P_next\"] - df_hl[\"P\"], np.nan)\n",
        "df_hl[\"dC\"] = np.where(contig, df_hl[\"C_next\"] - df_hl[\"C\"], np.nan)\n",
        "df_hl[\"drift_norm\"] = np.sqrt(df_hl[\"dM\"]**2 + df_hl[\"dP\"]**2 + df_hl[\"dC\"]**2)\n",
        "\n",
        "# ============================================================\n",
        "# 5) PLOT A: 2D drift in (M,P), colored by E_slot (field intensity)\n",
        "# ============================================================\n",
        "\n",
        "def plot_drift_2d(df, field_col, title):\n",
        "    d = df.copy()\n",
        "    # stride by layer within each head\n",
        "    if ARROW_STRIDE > 1:\n",
        "        d = d[d[\"layer\"] % ARROW_STRIDE == 0].copy()\n",
        "    d = d.dropna(subset=[\"dM\", \"dP\", field_col])\n",
        "\n",
        "    # segments for LineCollection (each: (x,y)->(x+dx,y+dy))\n",
        "    x0 = d[\"M\"].to_numpy()\n",
        "    y0 = d[\"P\"].to_numpy()\n",
        "    x1 = x0 + ARROW_SCALE_2D * d[\"dM\"].to_numpy()\n",
        "    y1 = y0 + ARROW_SCALE_2D * d[\"dP\"].to_numpy()\n",
        "    segs = np.stack([np.stack([x0, y0], axis=1), np.stack([x1, y1], axis=1)], axis=1)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(9.5, 7.0))\n",
        "\n",
        "    # background points\n",
        "    ax.scatter(\n",
        "        df[\"M\"], df[\"P\"],\n",
        "        c=df[\"layer\"], cmap=\"viridis\",\n",
        "        s=df[\"size\"], alpha=POINT_ALPHA,\n",
        "        edgecolor=\"k\", linewidths=0.4\n",
        "    )\n",
        "\n",
        "    # locked ring overlay\n",
        "    locked = df[df[\"locked\"] == 1]\n",
        "    if len(locked) > 0:\n",
        "        ax.scatter(\n",
        "            locked[\"M\"], locked[\"P\"],\n",
        "            s=locked[\"size\"] + 40,\n",
        "            facecolors=\"none\", edgecolors=\"red\",\n",
        "            linewidths=1.3, alpha=0.85\n",
        "        )\n",
        "\n",
        "    lc = LineCollection(segs, array=d[field_col].to_numpy(), cmap=\"coolwarm\", linewidths=2.2, alpha=ARROW_ALPHA)\n",
        "    ax.add_collection(lc)\n",
        "    cbar = plt.colorbar(lc, ax=ax)\n",
        "    cbar.set_label(field_col)\n",
        "\n",
        "    ax.set_xlabel(\"M: z(write_ess)\")\n",
        "    ax.set_ylabel(\"P: z(inertia_mean)\")\n",
        "    ax.set_title(title)\n",
        "    ax.grid(True, alpha=0.25)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_drift_2d(\n",
        "    df_hl,\n",
        "    field_col=\"E_slot\",\n",
        "    title=\"Head drift in substrate (M,P) across depth\\narrows colored by E_slot = z(slotspace_gate * delta_norm or gate)\"\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# 6) PLOT B: same 2D drift, colored by E_gamma\n",
        "# ============================================================\n",
        "\n",
        "plot_drift_2d(\n",
        "    df_hl,\n",
        "    field_col=\"E_gamma\",\n",
        "    title=\"Head drift in substrate (M,P) across depth\\narrows colored by E_gamma = z(content_read_gamma)\"\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# 7) Optional: 3D scatter + sparse 3D quiver colored by E_slot\n",
        "# ============================================================\n",
        "\n",
        "if DO_3D_QUIVER:\n",
        "    d3 = df_hl.dropna(subset=[\"dM\", \"dP\", \"dC\", \"E_slot\"]).copy()\n",
        "    # sparse selection\n",
        "    d3 = d3[(d3[\"layer\"] % QUIVER_STRIDE_LAYER == 0) & (d3[\"head\"] % QUIVER_STRIDE_HEAD == 0)].copy()\n",
        "\n",
        "    fig = plt.figure(figsize=(10, 8))\n",
        "    ax = fig.add_subplot(111, projection=\"3d\")\n",
        "\n",
        "    # points\n",
        "    sc = ax.scatter(\n",
        "        df_hl[\"M\"], df_hl[\"P\"], df_hl[\"C\"],\n",
        "        c=df_hl[\"layer\"], cmap=\"viridis\",\n",
        "        s=df_hl[\"size\"], alpha=0.85\n",
        "    )\n",
        "\n",
        "    # locked marker overlay (triangles)\n",
        "    locked = df_hl[df_hl[\"locked\"] == 1]\n",
        "    ax.scatter(\n",
        "        locked[\"M\"], locked[\"P\"], locked[\"C\"],\n",
        "        marker=\"^\", s=locked[\"size\"] + 30,\n",
        "        c=locked[\"layer\"], cmap=\"viridis\",\n",
        "        alpha=0.9, edgecolor=\"k\", linewidths=0.3\n",
        "    )\n",
        "\n",
        "    # quiver colored by E_slot: draw in bins (matplotlib 3D quiver doesn't support per-arrow colormap well)\n",
        "    # We'll approximate by 5 quantile bins.\n",
        "    bins = np.quantile(d3[\"E_slot\"], [0, .2, .4, .6, .8, 1.0])\n",
        "    for b0, b1 in zip(bins[:-1], bins[1:]):\n",
        "        sub = d3[(d3[\"E_slot\"] >= b0) & (d3[\"E_slot\"] <= b1)]\n",
        "        if len(sub) == 0:\n",
        "            continue\n",
        "        ax.quiver(\n",
        "            sub[\"M\"], sub[\"P\"], sub[\"C\"],\n",
        "            QUIVER_SCALE_3D * sub[\"dM\"], QUIVER_SCALE_3D * sub[\"dP\"], QUIVER_SCALE_3D * sub[\"dC\"],\n",
        "            length=1.0, normalize=False, alpha=0.35\n",
        "        )\n",
        "\n",
        "    ax.set_xlabel(\"M\")\n",
        "    ax.set_ylabel(\"P\")\n",
        "    ax.set_zlabel(\"C\")\n",
        "    ax.set_title(\"3D head\u00d7layer substrate with sparse drift quivers\\n(points colored by layer; quivers ~ E_slot binned)\")\n",
        "    fig.colorbar(sc, ax=ax, shrink=0.6, label=\"layer\")\n",
        "    ax.view_init(elev=25, azim=135)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# 8) Quick diagnostics: does drift increase when field is high?\n",
        "# ============================================================\n",
        "\n",
        "dcheck = df_hl.dropna(subset=[\"drift_norm\", \"E_slot\", \"E_gamma\"]).copy()\n",
        "print(\"\\nDiagnostics (layer-conditioned):\")\n",
        "print(\"corr(drift_norm, E_slot) =\", float(np.corrcoef(dcheck[\"drift_norm\"], dcheck[\"E_slot\"])[0,1]))\n",
        "print(\"corr(drift_norm, E_gamma) =\", float(np.corrcoef(dcheck[\"drift_norm\"], dcheck[\"E_gamma\"])[0,1]))\n",
        "\n",
        "# optional: per-layer drift mean vs fields\n",
        "df_layer_drift = dcheck.groupby(\"layer\").agg(\n",
        "    drift_mean=(\"drift_norm\", \"mean\"),\n",
        "    E_slot=(\"E_slot\", \"mean\"),\n",
        "    E_gamma=(\"E_gamma\", \"mean\"),\n",
        "    gate=(\"slotspace_gate\", \"mean\"),\n",
        "    gamma=(\"content_read_gamma\", \"mean\"),\n",
        ").reset_index()\n",
        "\n",
        "display(df_layer_drift)\n",
        "\n",
        "plt.figure(figsize=(10, 3.6))\n",
        "plt.plot(df_layer_drift[\"layer\"], df_layer_drift[\"drift_mean\"], \"o-\", lw=2)\n",
        "plt.xlabel(\"Layer\")\n",
        "plt.ylabel(\"Mean ||\u0394(M,P,C)|| per head\")\n",
        "plt.title(\"Mean head drift magnitude across depth\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 3.6))\n",
        "plt.plot(df_layer_drift[\"layer\"], df_layer_drift[\"E_slot\"], \"o-\", lw=2, label=\"E_slot\")\n",
        "plt.plot(df_layer_drift[\"layer\"], df_layer_drift[\"E_gamma\"], \"s--\", lw=2, label=\"E_gamma\", alpha=0.85)\n",
        "plt.xlabel(\"Layer\")\n",
        "plt.ylabel(\"z-scored field\")\n",
        "plt.title(\"Environment fields across depth (z-scored)\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "cellView": "form",
        "id": "ALs6s0QdOsPZ",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769532784365,
          "user_tz": 300,
          "elapsed": 1215,
          "user": {
            "displayName": "Secondscapes",
            "userId": "04150343613140054698"
          }
        },
        "outputId": "6e9e0224-ddee-4cf8-e8b9-6528b3893f1d"
      },
      "id": "ALs6s0QdOsPZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title Canonicalize env + drift \u2192 df_layer (fix E_slot/E_gamma name clashes)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def z(x):\n",
        "    x = np.asarray(x, dtype=np.float64)\n",
        "    return (x - x.mean()) / (x.std() + 1e-8)\n",
        "\n",
        "def _require_var(name):\n",
        "    if name not in globals():\n",
        "        raise RuntimeError(f\"Missing required variable: {name}\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1) Ensure df_hl exists (head\u00d7layer). If not, you must run the field+drift cell.\n",
        "# ------------------------------------------------------------\n",
        "_require_var(\"df_hl\")\n",
        "df_hl = df_hl.copy()\n",
        "if \"layer\" not in df_hl.columns:\n",
        "    raise RuntimeError(\"df_hl must include a 'layer' column.\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2) Build/resolve df_env (layer fields) with E_slot/E_gamma\n",
        "# ------------------------------------------------------------\n",
        "df_env_local = None\n",
        "if \"df_env\" in globals() and df_env is not None and len(df_env) > 0:\n",
        "    df_env_local = df_env.copy()\n",
        "else:\n",
        "    # reconstruct from df_so (+ delta if available) like your field+drift cell\n",
        "    _require_var(\"df_so\")\n",
        "    df_env_local = df_so[[\"layer\", \"slotspace_gate\", \"content_read_gamma\"]].copy()\n",
        "\n",
        "    if \"df_merge\" in globals() and df_merge is not None and \"slotspace_delta_norm\" in df_merge.columns:\n",
        "        df_env_local = df_env_local.merge(df_merge[[\"layer\", \"slotspace_delta_norm\"]], on=\"layer\", how=\"left\")\n",
        "    elif \"delta_df\" in globals() and delta_df is not None and \"slotspace_delta_norm\" in delta_df.columns:\n",
        "        df_env_local = df_env_local.merge(delta_df[[\"layer\", \"slotspace_delta_norm\"]], on=\"layer\", how=\"left\")\n",
        "\n",
        "    if \"slotspace_delta_norm\" in df_env_local.columns and df_env_local[\"slotspace_delta_norm\"].notna().any():\n",
        "        df_env_local[\"E_slot_raw\"] = df_env_local[\"slotspace_gate\"] * df_env_local[\"slotspace_delta_norm\"]\n",
        "    else:\n",
        "        df_env_local[\"E_slot_raw\"] = df_env_local[\"slotspace_gate\"]\n",
        "\n",
        "    df_env_local[\"E_slot\"] = z(df_env_local[\"E_slot_raw\"].fillna(df_env_local[\"E_slot_raw\"].mean()))\n",
        "    df_env_local[\"E_gamma\"] = z(df_env_local[\"content_read_gamma\"].fillna(df_env_local[\"content_read_gamma\"].mean()))\n",
        "\n",
        "# Coerce/canonicalize naming even if df_env was overwritten elsewhere\n",
        "if \"E_slot\" not in df_env_local.columns:\n",
        "    if \"E_slot_raw\" in df_env_local.columns:\n",
        "        df_env_local[\"E_slot\"] = z(df_env_local[\"E_slot_raw\"].fillna(df_env_local[\"E_slot_raw\"].mean()))\n",
        "    elif \"slotspace_gate\" in df_env_local.columns:\n",
        "        df_env_local[\"E_slot\"] = z(df_env_local[\"slotspace_gate\"].fillna(df_env_local[\"slotspace_gate\"].mean()))\n",
        "    else:\n",
        "        raise RuntimeError(f\"df_env has no E_slot/E_slot_raw/slotspace_gate columns: {list(df_env_local.columns)}\")\n",
        "\n",
        "if \"E_gamma\" not in df_env_local.columns:\n",
        "    if \"content_read_gamma\" in df_env_local.columns:\n",
        "        df_env_local[\"E_gamma\"] = z(df_env_local[\"content_read_gamma\"].fillna(df_env_local[\"content_read_gamma\"].mean()))\n",
        "    else:\n",
        "        raise RuntimeError(f\"df_env has no E_gamma/content_read_gamma columns: {list(df_env_local.columns)}\")\n",
        "\n",
        "# Keep canonical columns (+ gate/gamma if present)\n",
        "keep_cols = [\"layer\", \"E_slot\", \"E_gamma\"]\n",
        "for extra in [\"slotspace_gate\", \"content_read_gamma\", \"slotspace_delta_norm\", \"E_slot_raw\"]:\n",
        "    if extra in df_env_local.columns:\n",
        "        keep_cols.append(extra)\n",
        "df_env = df_env_local[keep_cols].copy()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3) Build/resolve df_layer_drift (layer drift)\n",
        "# ------------------------------------------------------------\n",
        "df_layer_drift_local = None\n",
        "if \"df_layer_drift\" in globals() and df_layer_drift is not None and len(df_layer_drift) > 0:\n",
        "    # accept if it has drift_mean already; otherwise rebuild\n",
        "    if \"drift_mean\" in df_layer_drift.columns:\n",
        "        df_layer_drift_local = df_layer_drift.copy()\n",
        "    else:\n",
        "        df_layer_drift_local = None\n",
        "\n",
        "if df_layer_drift_local is None:\n",
        "    # compute from df_hl (requires drift_norm)\n",
        "    if \"drift_norm\" not in df_hl.columns:\n",
        "        raise RuntimeError(\"df_hl is missing drift_norm; run the field+drift cell (step 4) first.\")\n",
        "    dcheck = df_hl.dropna(subset=[\"drift_norm\"]).copy()\n",
        "    df_layer_drift_local = dcheck.groupby(\"layer\", as_index=False).agg(\n",
        "        drift_mean=(\"drift_norm\", \"mean\")\n",
        "    )\n",
        "\n",
        "df_layer_drift = df_layer_drift_local[[\"layer\", \"drift_mean\"]].copy()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4) Canonical df_layer for phase classification\n",
        "# ------------------------------------------------------------\n",
        "df_layer = (\n",
        "    df_env.merge(df_layer_drift, on=\"layer\", how=\"left\")\n",
        "          .sort_values(\"layer\")\n",
        "          .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "print(\"Resolved inputs:\")\n",
        "print(\"  head\u00d7layer: df_hl\", df_hl.shape)\n",
        "print(\"  env(layer): df_env\", df_env.shape, \"cols=\", list(df_env.columns))\n",
        "print(\"  drift(layer): df_layer_drift\", df_layer_drift.shape, \"cols=\", list(df_layer_drift.columns))\n",
        "print(\"  canonical df_layer:\", df_layer.shape, \"cols=\", list(df_layer.columns))\n",
        "\n",
        "display(df_layer.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "cellView": "form",
        "id": "gX13wNAQVC6q",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769532784642,
          "user_tz": 300,
          "elapsed": 88,
          "user": {
            "displayName": "Secondscapes",
            "userId": "04150343613140054698"
          }
        },
        "outputId": "10ef4698-b0b0-40aa-e00b-15f6f6a3de69"
      },
      "id": "gX13wNAQVC6q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Taxonomy Classification\nAssign substrate classes and run taxonomy sweep logic."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title Substrate taxonomy + field regimes (FINAL, name-clash safe + auto-resolve)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ============================================================\n",
        "# 0) Resolve inputs without clobbering existing names\n",
        "# ============================================================\n",
        "\n",
        "def _pick_first_existing(names):\n",
        "    for n in names:\n",
        "        if n in globals() and globals()[n] is not None:\n",
        "            return n, globals()[n]\n",
        "    return None, None\n",
        "\n",
        "# Head\u00d7layer table\n",
        "hl_name, df_hl = _pick_first_existing([\"df_hl\", \"df_hp\"])\n",
        "if df_hl is None:\n",
        "    raise RuntimeError(\"Need df_hl (or df_hp) head\u00d7layer table. Run your Substrate field + drift cell first.\")\n",
        "df_hl_in = df_hl.copy()\n",
        "\n",
        "# Environment per layer\n",
        "env_name, dfE = _pick_first_existing([\"df_env\", \"env_df\"])\n",
        "if dfE is None:\n",
        "    # fall back: build from df_so/df_merge if available\n",
        "    if \"df_so\" not in globals() or df_so is None:\n",
        "        raise RuntimeError(\"Need df_env OR df_so to build environment table.\")\n",
        "    dfE = df_so[[\"layer\", \"slotspace_gate\", \"content_read_gamma\"]].copy()\n",
        "    # try to add delta\n",
        "    if \"df_merge\" in globals() and df_merge is not None and \"slotspace_delta_norm\" in df_merge.columns:\n",
        "        dfE = dfE.merge(df_merge[[\"layer\", \"slotspace_delta_norm\"]], on=\"layer\", how=\"left\")\n",
        "    elif \"delta_df\" in globals() and delta_df is not None and \"slotspace_delta_norm\" in delta_df.columns:\n",
        "        dfE = dfE.merge(delta_df[[\"layer\", \"slotspace_delta_norm\"]], on=\"layer\", how=\"left\")\n",
        "\n",
        "    def _z(x):\n",
        "        x = np.asarray(x, dtype=np.float64)\n",
        "        return (x - x.mean()) / (x.std() + 1e-8)\n",
        "\n",
        "    if \"slotspace_delta_norm\" in dfE.columns and dfE[\"slotspace_delta_norm\"].notna().any():\n",
        "        dfE[\"E_slot_raw\"] = dfE[\"slotspace_gate\"] * dfE[\"slotspace_delta_norm\"]\n",
        "    else:\n",
        "        dfE[\"E_slot_raw\"] = dfE[\"slotspace_gate\"]\n",
        "    dfE[\"E_slot\"] = _z(dfE[\"E_slot_raw\"].fillna(dfE[\"E_slot_raw\"].mean()))\n",
        "    dfE[\"E_gamma\"] = _z(dfE[\"content_read_gamma\"].fillna(dfE[\"content_read_gamma\"].mean()))\n",
        "\n",
        "df_env_in = dfE.copy()\n",
        "\n",
        "# Drift per layer\n",
        "drift_name, dfD = _pick_first_existing([\"df_layer_drift\", \"drift_df\"])\n",
        "if dfD is None:\n",
        "    # compute from head\u00d7layer if drift_norm exists, else fail\n",
        "    if \"drift_norm\" not in df_hl_in.columns:\n",
        "        raise RuntimeError(\"Need df_layer_drift (or drift_df), or df_hl must contain drift_norm.\")\n",
        "    dfD = (\n",
        "        df_hl_in.dropna(subset=[\"drift_norm\"])\n",
        "               .groupby(\"layer\")\n",
        "               .agg(drift_mean=(\"drift_norm\", \"mean\"))\n",
        "               .reset_index()\n",
        "    )\n",
        "\n",
        "df_drift_in = dfD.copy()\n",
        "\n",
        "print(\"Resolved inputs:\")\n",
        "print(\"  head\u00d7layer:\", \"df_hl\" if hl_name is None else hl_name, df_hl_in.shape)\n",
        "print(\"  env(layer):\", \"df_env\" if env_name is None else env_name, df_env_in.shape)\n",
        "print(\"  drift(layer):\", \"df_layer_drift\" if drift_name is None else drift_name, df_drift_in.shape)\n",
        "\n",
        "# ============================================================\n",
        "# 1) Canonical config + helper bins\n",
        "# ============================================================\n",
        "\n",
        "taxonomy_cfg = {\n",
        "    \"LOCK_THRESHOLD\": 8.0,\n",
        "    \"AXIS_Q_LO\": 0.30,\n",
        "    \"AXIS_Q_HI\": 0.70,\n",
        "    \"FIELD_POS\": +0.5,\n",
        "    \"FIELD_NEG\": -0.5,\n",
        "    \"DRIFT_Q_LO\": 0.30,\n",
        "    \"DRIFT_Q_HI\": 0.70,\n",
        "}\n",
        "\n",
        "LOCK_THRESHOLD = taxonomy_cfg[\"LOCK_THRESHOLD\"]\n",
        "\n",
        "# Ensure locked exists and is defined the same way everywhere\n",
        "if \"locked\" not in df_hl_in.columns:\n",
        "    if \"inertia_half_lag\" not in df_hl_in.columns:\n",
        "        raise RuntimeError(\"df_hl missing 'locked' and missing 'inertia_half_lag' to compute it.\")\n",
        "    df_hl_in[\"locked\"] = (df_hl_in[\"inertia_half_lag\"] >= LOCK_THRESHOLD).astype(int)\n",
        "\n",
        "# Axes must exist\n",
        "for col in [\"M\", \"P\", \"C\"]:\n",
        "    if col not in df_hl_in.columns:\n",
        "        raise RuntimeError(f\"df_hl missing required substrate axis '{col}' (expected z-scored).\")\n",
        "\n",
        "# ============================================================\n",
        "# 2) Axis thresholds (global across all head\u00d7layer points)\n",
        "# ============================================================\n",
        "\n",
        "Q_LO = taxonomy_cfg[\"AXIS_Q_LO\"]\n",
        "Q_HI = taxonomy_cfg[\"AXIS_Q_HI\"]\n",
        "\n",
        "axis_thresholds = {}\n",
        "for ax in [\"M\", \"P\", \"C\"]:\n",
        "    axis_thresholds[ax] = {\n",
        "        \"lo\": float(df_hl_in[ax].quantile(Q_LO)),\n",
        "        \"hi\": float(df_hl_in[ax].quantile(Q_HI)),\n",
        "    }\n",
        "\n",
        "def bin_axis(x, lo, hi):\n",
        "    if x <= lo:\n",
        "        return \"LOW\"\n",
        "    elif x >= hi:\n",
        "        return \"HIGH\"\n",
        "    else:\n",
        "        return \"MID\"\n",
        "\n",
        "for ax in [\"M\", \"P\", \"C\"]:\n",
        "    lo = axis_thresholds[ax][\"lo\"]\n",
        "    hi = axis_thresholds[ax][\"hi\"]\n",
        "    df_hl_in[f\"{ax}_bin\"] = df_hl_in[ax].apply(lambda v: bin_axis(v, lo, hi))\n",
        "\n",
        "# ============================================================\n",
        "# 3) Field regimes (layer-level)\n",
        "# ============================================================\n",
        "\n",
        "FIELD_POS = taxonomy_cfg[\"FIELD_POS\"]\n",
        "FIELD_NEG = taxonomy_cfg[\"FIELD_NEG\"]\n",
        "\n",
        "def field_state(x):\n",
        "    if x >= FIELD_POS:\n",
        "        return \"HIGH\"\n",
        "    elif x <= FIELD_NEG:\n",
        "        return \"LOW\"\n",
        "    else:\n",
        "        return \"MID\"\n",
        "\n",
        "df_layer = df_env_in.copy()\n",
        "\n",
        "# Standardize column names for env\n",
        "if \"E_slot\" not in df_layer.columns or \"E_gamma\" not in df_layer.columns:\n",
        "    raise RuntimeError(\"Environment table must contain E_slot and E_gamma.\")\n",
        "\n",
        "df_layer[\"slot_state\"] = df_layer[\"E_slot\"].apply(field_state)\n",
        "df_layer[\"gamma_state\"] = df_layer[\"E_gamma\"].apply(field_state)\n",
        "\n",
        "def field_regime(row):\n",
        "    ss, gs = row[\"slot_state\"], row[\"gamma_state\"]\n",
        "    if ss == \"LOW\" and gs == \"LOW\":\n",
        "        return \"LOW_FIELD\"\n",
        "    if gs == \"HIGH\" and ss != \"HIGH\":\n",
        "        return \"GAMMA_DOM\"\n",
        "    if gs == \"HIGH\" and ss == \"HIGH\":\n",
        "        return \"HANDOFF\"\n",
        "    if ss == \"HIGH\" and gs != \"HIGH\":\n",
        "        return \"SLOT_DOM\"\n",
        "    return \"MIXED_MID\"\n",
        "\n",
        "df_layer[\"field_regime\"] = df_layer.apply(field_regime, axis=1)\n",
        "\n",
        "# ============================================================\n",
        "# 4) Drift regimes + phases (layer-level)\n",
        "# ============================================================\n",
        "\n",
        "if \"drift_mean\" not in df_drift_in.columns:\n",
        "    raise RuntimeError(\"Drift table must contain drift_mean.\")\n",
        "\n",
        "D_LO = float(df_drift_in[\"drift_mean\"].quantile(taxonomy_cfg[\"DRIFT_Q_LO\"]))\n",
        "D_HI = float(df_drift_in[\"drift_mean\"].quantile(taxonomy_cfg[\"DRIFT_Q_HI\"]))\n",
        "\n",
        "def drift_regime(x):\n",
        "    if x >= D_HI:\n",
        "        return \"HIGH\"\n",
        "    elif x <= D_LO:\n",
        "        return \"LOW\"\n",
        "    else:\n",
        "        return \"MID\"\n",
        "\n",
        "df_drift_in = df_drift_in[[\"layer\", \"drift_mean\"]].copy()\n",
        "df_drift_in[\"drift_regime\"] = df_drift_in[\"drift_mean\"].apply(drift_regime)\n",
        "\n",
        "df_layer_regimes = (\n",
        "    df_layer.merge(df_drift_in, on=\"layer\", how=\"left\")\n",
        "            .sort_values(\"layer\")\n",
        "            .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "def depth_phase(row):\n",
        "    fr = row[\"field_regime\"]\n",
        "    dr = row[\"drift_regime\"]\n",
        "\n",
        "    # These are the paper-style phases we discussed:\n",
        "    if fr == \"LOW_FIELD\" and dr == \"HIGH\":\n",
        "        return \"PHASE_FORMATION\"\n",
        "    if fr == \"GAMMA_DOM\" and dr != \"LOW\":\n",
        "        return \"PHASE_REMODELING\"\n",
        "    if fr == \"HANDOFF\":\n",
        "        return \"PHASE_HANDOFF\"\n",
        "    if fr == \"SLOT_DOM\" and dr == \"LOW\":\n",
        "        return \"PHASE_CONSOLIDATION\"\n",
        "    if fr == \"SLOT_DOM\":\n",
        "        return \"PHASE_TERMINAL_TUNING\"\n",
        "    return \"PHASE_MIXED\"\n",
        "\n",
        "df_layer_regimes[\"phase\"] = df_layer_regimes.apply(depth_phase, axis=1)\n",
        "\n",
        "# ============================================================\n",
        "# 5) Substrate classes (head\u00d7layer) as region + field regime\n",
        "# ============================================================\n",
        "\n",
        "df_taxonomy = df_hl_in.merge(\n",
        "    df_layer_regimes[[\"layer\", \"field_regime\", \"phase\"]],\n",
        "    on=\"layer\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "def substrate_class(row):\n",
        "    M, P, C = row[\"M_bin\"], row[\"P_bin\"], row[\"C_bin\"]\n",
        "    locked = int(row[\"locked\"])\n",
        "    fr = row[\"field_regime\"]\n",
        "\n",
        "    # Priority order: most specific archetypes first.\n",
        "    if locked and M == \"HIGH\" and P == \"HIGH\":\n",
        "        return \"FROZEN_HIGH_CAPACITY_ANCHOR\"\n",
        "\n",
        "    if locked and P == \"HIGH\" and C == \"HIGH\":\n",
        "        return \"POLICY_ANCHOR\"\n",
        "\n",
        "    if locked and C == \"HIGH\" and M == \"LOW\":\n",
        "        return \"BRITTLE_LOCKER\"\n",
        "\n",
        "    if fr == \"HANDOFF\":\n",
        "        return \"TRANSITIONER\"\n",
        "\n",
        "    if (not locked) and M == \"HIGH\":\n",
        "        return \"HIGH_CAPACITY_INTEGRATOR\"\n",
        "\n",
        "    if (not locked) and C == \"HIGH\" and P != \"HIGH\":\n",
        "        return \"SEMANTIC_SPECIALIST\"\n",
        "\n",
        "    if (not locked) and P == \"LOW\" and C == \"LOW\":\n",
        "        return \"EXPLORATORY_ROUTER\"\n",
        "\n",
        "    return \"OTHER\"\n",
        "\n",
        "df_taxonomy[\"substrate_class\"] = df_taxonomy.apply(substrate_class, axis=1)\n",
        "\n",
        "# ============================================================\n",
        "# 6) Outputs + compact summaries\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n=== taxonomy_cfg ===\")\n",
        "print(taxonomy_cfg)\n",
        "print(\"\\n=== axis_thresholds (global quantiles) ===\")\n",
        "print(axis_thresholds)\n",
        "print(\"\\n=== drift thresholds ===\")\n",
        "print({\"D_LO\": D_LO, \"D_HI\": D_HI})\n",
        "\n",
        "print(\"\\n=== Layer-level regimes (df_layer_regimes) ===\")\n",
        "display(df_layer_regimes[[\"layer\", \"E_slot\", \"E_gamma\", \"field_regime\", \"drift_mean\", \"drift_regime\", \"phase\"]])\n",
        "\n",
        "print(\"\\n=== Substrate class counts (head\u00d7layer points) ===\")\n",
        "display(df_taxonomy[\"substrate_class\"].value_counts().to_frame(\"count\"))\n",
        "\n",
        "print(\"\\n=== Substrate classes by phase ===\")\n",
        "display(pd.crosstab(df_taxonomy[\"phase\"], df_taxonomy[\"substrate_class\"]))\n",
        "\n",
        "print(\"\\n=== Locked vs adaptive by class ===\")\n",
        "display(pd.crosstab(df_taxonomy[\"substrate_class\"], df_taxonomy[\"locked\"]))\n",
        "\n",
        "# Keep these names stable going forward:\n",
        "# df_layer_regimes: layer -> field_regime, drift_regime, phase\n",
        "# df_taxonomy: head\u00d7layer points -> M/P/C bins + regime + class"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "cellView": "form",
        "id": "tVWr-x-1SPp4",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769532784553,
          "user_tz": 300,
          "elapsed": 179,
          "user": {
            "displayName": "Secondscapes",
            "userId": "04150343613140054698"
          }
        },
        "outputId": "4b6447e8-6ae3-4e06-8515-ac374ae06be4"
      },
      "id": "tVWr-x-1SPp4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title Taxonomy sweep (FIXED env resolution) \u2014 rebuild canonical df_layer with E_slot/E_gamma\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display  # <-- FIX: display may not be defined outside notebook\n",
        "\n",
        "# ----------------------------\n",
        "# CONFIG\n",
        "# ----------------------------\n",
        "taxonomy_cfg = dict(\n",
        "    LOCK_THRESHOLD=8.0,\n",
        "    AXIS_Q_LO=0.30,\n",
        "    AXIS_Q_HI=0.70,\n",
        "    FIELD_POS=0.50,\n",
        "    FIELD_NEG=-0.50,\n",
        "    DRIFT_Q_LO=0.30,\n",
        "    DRIFT_Q_HI=0.70,\n",
        ")\n",
        "\n",
        "DO_SWEEP = True\n",
        "FIELD_T_LIST = [0.25, 0.35, 0.50]\n",
        "AXIS_Q_LIST  = [(0.25,0.75), (0.30,0.70)]\n",
        "DRIFT_Q_LIST = [(0.25,0.75), (0.30,0.70)]\n",
        "\n",
        "# ----------------------------\n",
        "# Helpers\n",
        "# ----------------------------\n",
        "def _require(df, cols, name):\n",
        "    if df is None or not isinstance(df, pd.DataFrame) or len(df) == 0:\n",
        "        raise RuntimeError(f\"{name} is missing or empty.\")\n",
        "    missing = [c for c in cols if c not in df.columns]\n",
        "    if missing:\n",
        "        raise RuntimeError(f\"{name} missing columns: {missing}\\nHave: {list(df.columns)}\")\n",
        "\n",
        "def z(x):\n",
        "    x = np.asarray(x, dtype=np.float64)\n",
        "    return (x - x.mean()) / (x.std() + 1e-8)\n",
        "\n",
        "def _bin3(x, lo, hi):\n",
        "    return np.where(x <= lo, \"LO\", np.where(x >= hi, \"HI\", \"MID\"))\n",
        "\n",
        "def _field_state(x, t):\n",
        "    return np.where(x >= t, \"HIGH\", np.where(x <= -t, \"LOW\", \"MID\"))\n",
        "\n",
        "def _axis_thresholds(df_points, q_lo=0.3, q_hi=0.7):\n",
        "    out = {}\n",
        "    for ax in [\"M\",\"P\",\"C\"]:\n",
        "        out[ax] = {\n",
        "            \"lo\": float(np.quantile(df_points[ax].to_numpy(), q_lo)),\n",
        "            \"hi\": float(np.quantile(df_points[ax].to_numpy(), q_hi)),\n",
        "        }\n",
        "    return out\n",
        "\n",
        "def _drift_thresholds(df_layer, q_lo=0.3, q_hi=0.7):\n",
        "    v = df_layer[\"drift_mean\"].to_numpy()\n",
        "    v = v[np.isfinite(v)]\n",
        "    # FIX: avoid quantile() on empty\n",
        "    if v.size == 0:\n",
        "        return 0.0, 0.0\n",
        "    if np.all(v == v[0]):\n",
        "        return float(v[0]), float(v[0])\n",
        "    return float(np.quantile(v, q_lo)), float(np.quantile(v, q_hi))\n",
        "\n",
        "def _coalesce(df, candidates, new_name):\n",
        "    for c in candidates:\n",
        "        if c in df.columns:\n",
        "            df[new_name] = df[c]\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def _first_df_from_globals(names):\n",
        "    \"\"\"Return first non-empty DataFrame from globals by name.\"\"\"\n",
        "    for n in names:\n",
        "        if n in globals():\n",
        "            obj = globals()[n]\n",
        "            if isinstance(obj, pd.DataFrame) and len(obj) > 0:\n",
        "                return obj.copy()\n",
        "    return None\n",
        "\n",
        "# ----------------------------\n",
        "# 1) Resolve df_hl (head\u00d7layer)\n",
        "# ----------------------------\n",
        "def build_df_hl_from_feat(feat_by_layer, feature_names, lock_threshold=8.0):\n",
        "    need = [\"write_ess\", \"inertia_mean\", \"top4_mass\", \"inertia_half_lag\"]\n",
        "    missing = [nm for nm in need if nm not in feature_names]\n",
        "    if missing:\n",
        "        raise RuntimeError(f\"feature_names missing required fields: {missing}\\nHave: {feature_names}\")\n",
        "\n",
        "    name_to_j = {nm: j for j, nm in enumerate(feature_names)}\n",
        "\n",
        "    rows = []\n",
        "    for layer, X in enumerate(feat_by_layer):\n",
        "        if X is None:\n",
        "            continue\n",
        "        X = np.asarray(X)  # [H, F]\n",
        "        if X.ndim != 2 or X.shape[1] < len(feature_names):\n",
        "            raise RuntimeError(f\"feat_by_layer[{layer}] has unexpected shape {X.shape}; expected [H, F].\")\n",
        "        H = X.shape[0]\n",
        "        for h in range(H):\n",
        "            rows.append({\n",
        "                \"layer\": int(layer),\n",
        "                \"head\": int(h),\n",
        "                \"write_ess\": float(X[h, name_to_j[\"write_ess\"]]),\n",
        "                \"inertia_mean\": float(X[h, name_to_j[\"inertia_mean\"]]),\n",
        "                \"top4_mass\": float(X[h, name_to_j[\"top4_mass\"]]),\n",
        "                \"inertia_half_lag\": float(X[h, name_to_j[\"inertia_half_lag\"]]),\n",
        "            })\n",
        "    df = pd.DataFrame(rows)\n",
        "    if len(df) == 0:\n",
        "        raise RuntimeError(\"Rebuilt df_hl is empty; feat_by_layer contained no usable layers.\")\n",
        "\n",
        "    df[\"M\"] = z(df[\"write_ess\"])\n",
        "    df[\"P\"] = z(df[\"inertia_mean\"])\n",
        "    df[\"C\"] = z(df[\"top4_mass\"])\n",
        "    df[\"locked\"] = (df[\"inertia_half_lag\"] >= float(lock_threshold)).astype(int)\n",
        "\n",
        "    # optional size from df_slot switches if available\n",
        "    if \"df_slot\" in globals() and isinstance(globals()[\"df_slot\"], pd.DataFrame) and len(globals()[\"df_slot\"]) > 0:\n",
        "        _df_slot = globals()[\"df_slot\"]\n",
        "        if \"slot\" in _df_slot.columns and \"switches\" in _df_slot.columns:\n",
        "            smap = _df_slot.set_index(\"slot\")[\"switches\"].to_dict()\n",
        "            df[\"switches\"] = df[\"head\"].map(lambda h: float(smap.get(int(h), np.nan)))\n",
        "        elif \"head\" in _df_slot.columns and \"switches\" in _df_slot.columns:\n",
        "            smap = _df_slot.set_index(\"head\")[\"switches\"].to_dict()\n",
        "            df[\"switches\"] = df[\"head\"].map(lambda h: float(smap.get(int(h), np.nan)))\n",
        "        else:\n",
        "            df[\"switches\"] = np.nan\n",
        "    else:\n",
        "        df[\"switches\"] = np.nan\n",
        "\n",
        "    if df[\"switches\"].notna().any():\n",
        "        mn = float(df[\"switches\"].min())\n",
        "        mx = float(df[\"switches\"].max())\n",
        "        df[\"size\"] = 30 + 18 * (df[\"switches\"] - mn) / (mx - mn + 1e-8)\n",
        "    else:\n",
        "        df[\"size\"] = 60.0\n",
        "\n",
        "    return df\n",
        "\n",
        "# Prefer existing globals if present AND has required cols\n",
        "df_hl = _first_df_from_globals([\"df_hl\", \"df_hp\", \"df_points\", \"df_head_layer\"])\n",
        "\n",
        "if df_hl is None or not set([\"layer\",\"head\",\"M\",\"P\",\"C\",\"locked\"]).issubset(df_hl.columns):\n",
        "    if (\"feat_by_layer\" in globals()) and (\"feature_names\" in globals()):\n",
        "        df_hl = build_df_hl_from_feat(\n",
        "            feat_by_layer=feat_by_layer,\n",
        "            feature_names=feature_names,\n",
        "            lock_threshold=taxonomy_cfg[\"LOCK_THRESHOLD\"],\n",
        "        )\n",
        "        print(\"Rebuilt df_hl from feat_by_layer:\", df_hl.shape)\n",
        "    else:\n",
        "        raise RuntimeError(\"Missing head\u00d7layer table and cannot rebuild: need feat_by_layer + feature_names.\")\n",
        "\n",
        "_require(df_hl, [\"layer\",\"head\",\"M\",\"P\",\"C\",\"locked\"], \"df_hl\")\n",
        "\n",
        "# ----------------------------\n",
        "# 2) Resolve df_env (layer fields) and canonicalize to E_slot/E_gamma\n",
        "# ----------------------------\n",
        "df_env = _first_df_from_globals([\"df_env\", \"df_environment\", \"df_fields\", \"df_layer_env\"])\n",
        "\n",
        "if df_env is None:\n",
        "    # last resort: compute layer env from df_hl if it already has E_slot/E_gamma\n",
        "    if set([\"E_slot\",\"E_gamma\"]).issubset(df_hl.columns):\n",
        "        df_env = df_hl.groupby(\"layer\", as_index=False).agg(E_slot=(\"E_slot\",\"mean\"), E_gamma=(\"E_gamma\",\"mean\"))\n",
        "    else:\n",
        "        raise RuntimeError(\n",
        "            \"Could not find a non-empty df_env-like table in globals \"\n",
        "            \"and df_hl does not already include E_slot/E_gamma.\"\n",
        "        )\n",
        "\n",
        "df_env = df_env.copy()\n",
        "if \"layer\" not in df_env.columns:\n",
        "    raise RuntimeError(f\"df_env missing 'layer'. Have columns: {list(df_env.columns)}\")\n",
        "\n",
        "# Canonicalize E_slot\n",
        "if \"E_slot\" not in df_env.columns:\n",
        "    if \"E_slot_raw\" in df_env.columns:\n",
        "        df_env[\"E_slot\"] = z(df_env[\"E_slot_raw\"].to_numpy())\n",
        "    elif set([\"slotspace_gate\",\"slotspace_delta_norm\"]).issubset(df_env.columns):\n",
        "        df_env[\"E_slot\"] = z((df_env[\"slotspace_gate\"] * df_env[\"slotspace_delta_norm\"]).to_numpy())\n",
        "    elif \"slotspace_gate\" in df_env.columns:\n",
        "        df_env[\"E_slot\"] = z(df_env[\"slotspace_gate\"].to_numpy())\n",
        "    else:\n",
        "        raise RuntimeError(f\"Can't construct E_slot from df_env columns: {list(df_env.columns)}\")\n",
        "\n",
        "# Canonicalize E_gamma\n",
        "if \"E_gamma\" not in df_env.columns:\n",
        "    if \"content_read_gamma\" in df_env.columns:\n",
        "        df_env[\"E_gamma\"] = z(df_env[\"content_read_gamma\"].to_numpy())\n",
        "    elif \"content_read_gamma_mean\" in df_env.columns:\n",
        "        df_env[\"E_gamma\"] = z(df_env[\"content_read_gamma_mean\"].to_numpy())\n",
        "    else:\n",
        "        raise RuntimeError(f\"Can't construct E_gamma from df_env columns: {list(df_env.columns)}\")\n",
        "\n",
        "df_env = df_env[[\"layer\",\"E_slot\",\"E_gamma\"]].copy()\n",
        "_require(df_env, [\"layer\",\"E_slot\",\"E_gamma\"], \"df_env (canonical)\")\n",
        "\n",
        "# Attach fields to df_hl (in case missing)\n",
        "if not set([\"E_slot\",\"E_gamma\"]).issubset(df_hl.columns):\n",
        "    df_hl = df_hl.merge(df_env, on=\"layer\", how=\"left\")\n",
        "\n",
        "_require(df_hl, [\"E_slot\",\"E_gamma\"], \"df_hl (+fields)\")\n",
        "\n",
        "# ----------------------------\n",
        "# 3) Resolve drift per layer (df_layer_drift) or compute\n",
        "# ----------------------------\n",
        "df_layer_drift = _first_df_from_globals([\"df_layer_drift\", \"df_drift_layer\", \"df_layer_transitions\"])\n",
        "\n",
        "use_existing = (\n",
        "    isinstance(df_layer_drift, pd.DataFrame)\n",
        "    and len(df_layer_drift) > 0\n",
        "    and (\"layer\" in df_layer_drift.columns)\n",
        "    and (\"drift_mean\" in df_layer_drift.columns)\n",
        ")\n",
        "\n",
        "if not use_existing:\n",
        "    # compute drift_mean from df_hl by head contiguous diffs\n",
        "    dtmp = df_hl.sort_values([\"head\",\"layer\"]).copy()\n",
        "    dtmp[\"M_next\"] = dtmp.groupby(\"head\")[\"M\"].shift(-1)\n",
        "    dtmp[\"P_next\"] = dtmp.groupby(\"head\")[\"P\"].shift(-1)\n",
        "    dtmp[\"C_next\"] = dtmp.groupby(\"head\")[\"C\"].shift(-1)\n",
        "    dtmp[\"layer_next\"] = dtmp.groupby(\"head\")[\"layer\"].shift(-1)\n",
        "    contig = (dtmp[\"layer_next\"] == (dtmp[\"layer\"] + 1))\n",
        "    dtmp[\"drift_norm\"] = np.where(\n",
        "        contig,\n",
        "        np.sqrt((dtmp[\"M_next\"]-dtmp[\"M\"])**2 + (dtmp[\"P_next\"]-dtmp[\"P\"])**2 + (dtmp[\"C_next\"]-dtmp[\"C\"])**2),\n",
        "        np.nan\n",
        "    )\n",
        "    df_layer_drift = (dtmp.dropna(subset=[\"drift_norm\"])\n",
        "                      .groupby(\"layer\", as_index=False)\n",
        "                      .agg(drift_mean=(\"drift_norm\",\"mean\")))\n",
        "else:\n",
        "    df_layer_drift = df_layer_drift[[\"layer\",\"drift_mean\"]].copy()\n",
        "\n",
        "# ----------------------------\n",
        "# 4) Build canonical df_layer\n",
        "# ----------------------------\n",
        "df_layer = (df_env.merge(df_layer_drift, on=\"layer\", how=\"left\")\n",
        "            .sort_values(\"layer\").reset_index(drop=True))\n",
        "df_layer[\"drift_mean\"] = df_layer[\"drift_mean\"].fillna(0.0)\n",
        "print(\"Resolved inputs:  head\u00d7layer: df_hl\", df_hl.shape, \"  env(layer):\", df_env.shape, \"  drift(layer):\", df_layer_drift.shape)\n",
        "display(df_layer.head())\n",
        "\n",
        "# ----------------------------\n",
        "# 5) Phase model + substrate classes\n",
        "# ----------------------------\n",
        "def classify_phases(df_layer, t_field=0.5, drift_q_lo=0.3, drift_q_hi=0.7):\n",
        "    df = df_layer.copy()\n",
        "\n",
        "    # FIX: thresholds robust to NaN drift_mean\n",
        "    if \"drift_mean\" not in df.columns:\n",
        "        df[\"drift_mean\"] = np.nan\n",
        "    D_LO, D_HI = _drift_thresholds(df, drift_q_lo, drift_q_hi)\n",
        "\n",
        "    df[\"slot_state\"]  = _field_state(df[\"E_slot\"].to_numpy(),  t_field)\n",
        "    df[\"gamma_state\"] = _field_state(df[\"E_gamma\"].to_numpy(), t_field)\n",
        "\n",
        "    dm = df[\"drift_mean\"].to_numpy()\n",
        "    df[\"drift_regime\"] = np.where(dm >= D_HI, \"HIGH\", np.where(dm <= D_LO, \"LOW\", \"MID\"))\n",
        "\n",
        "    def _field_regime(row):\n",
        "        ss, gs = row[\"slot_state\"], row[\"gamma_state\"]\n",
        "        if ss == \"HIGH\" and gs == \"HIGH\":\n",
        "            return \"HANDOFF\"\n",
        "        if ss == \"HIGH\" and gs != \"HIGH\":\n",
        "            return \"SLOT_DOM\"\n",
        "        if gs == \"HIGH\" and ss != \"HIGH\":\n",
        "            return \"GAMMA_DOM\"\n",
        "        if ss == \"LOW\" and gs == \"LOW\":\n",
        "            return \"LOW_FIELD\"\n",
        "        return \"MIXED_MID\"\n",
        "\n",
        "    df[\"field_regime\"] = df.apply(_field_regime, axis=1)\n",
        "\n",
        "    phase = []\n",
        "    for _, r in df.iterrows():\n",
        "        fr = r[\"field_regime\"]\n",
        "        dr = r[\"drift_regime\"]\n",
        "        if fr == \"LOW_FIELD\" and dr == \"HIGH\":\n",
        "            phase.append(\"PHASE_FORMATION\")\n",
        "        elif fr == \"HANDOFF\":\n",
        "            phase.append(\"PHASE_HANDOFF\")\n",
        "        elif fr == \"GAMMA_DOM\" and dr != \"LOW\":\n",
        "            phase.append(\"PHASE_REMODELING\")\n",
        "        elif fr == \"SLOT_DOM\" and dr == \"LOW\":\n",
        "            phase.append(\"PHASE_CONSOLIDATION\")\n",
        "        elif fr == \"SLOT_DOM\" and dr != \"LOW\":\n",
        "            phase.append(\"PHASE_TERMINAL_TUNING\")\n",
        "        else:\n",
        "            phase.append(\"PHASE_MIXED\")\n",
        "\n",
        "    df[\"phase\"] = phase\n",
        "    return df, {\"D_LO\": float(D_LO), \"D_HI\": float(D_HI)}\n",
        "\n",
        "def classify_substrate_points(df_points, axis_thr):\n",
        "    df = df_points.copy()\n",
        "    df[\"M_bin\"] = _bin3(df[\"M\"].to_numpy(), axis_thr[\"M\"][\"lo\"], axis_thr[\"M\"][\"hi\"])\n",
        "    df[\"P_bin\"] = _bin3(df[\"P\"].to_numpy(), axis_thr[\"P\"][\"lo\"], axis_thr[\"P\"][\"hi\"])\n",
        "    df[\"C_bin\"] = _bin3(df[\"C\"].to_numpy(), axis_thr[\"C\"][\"lo\"], axis_thr[\"C\"][\"hi\"])\n",
        "    locked = df[\"locked\"].to_numpy().astype(int)\n",
        "\n",
        "    cls = np.array([\"OTHER\"] * len(df), dtype=object)\n",
        "\n",
        "    # Locked archetypes\n",
        "    cls[(locked==1) & (df[\"M_bin\"]==\"HI\") & (df[\"P_bin\"]==\"HI\")] = \"FROZEN_HIGH_CAPACITY_ANCHOR\"\n",
        "    cls[(locked==1) & (df[\"C_bin\"]==\"HI\")]                       = \"BRITTLE_LOCKER\"\n",
        "    cls[(locked==1) & (cls==\"OTHER\")]                            = \"LOCKED_MISC\"\n",
        "\n",
        "    # Adaptive archetypes\n",
        "    cls[(locked==0) & (df[\"M_bin\"]==\"HI\") & (df[\"P_bin\"]!=\"LO\") & (df[\"C_bin\"]!=\"HI\")] = \"HIGH_CAPACITY_INTEGRATOR\"\n",
        "    cls[(locked==0) & (df[\"C_bin\"]==\"HI\") & (df[\"P_bin\"]!=\"LO\")]                       = \"SEMANTIC_SPECIALIST\"\n",
        "    cls[(locked==0) & (df[\"P_bin\"]==\"LO\") & (df[\"C_bin\"]==\"LO\")]                       = \"EXPLORATORY_ROUTER\"\n",
        "    cls[(locked==0) & (df[\"M_bin\"]==\"MID\") & (df[\"P_bin\"]==\"MID\") & (df[\"C_bin\"]==\"MID\")] = \"BALANCED_GENERALIST\"\n",
        "    cls[(locked==0) & (df[\"P_bin\"]==\"HI\") & (df[\"M_bin\"]!=\"HI\") & (df[\"C_bin\"]!=\"HI\")]    = \"ROUTING_SPECIALIST\"\n",
        "\n",
        "    df[\"substrate_class\"] = cls\n",
        "    return df\n",
        "\n",
        "def build_outputs(cfg):\n",
        "    t_field = float(cfg[\"FIELD_POS\"])\n",
        "    qlo = float(cfg[\"AXIS_Q_LO\"]); qhi = float(cfg[\"AXIS_Q_HI\"])\n",
        "    dqlo = float(cfg[\"DRIFT_Q_LO\"]); dqhi = float(cfg[\"DRIFT_Q_HI\"])\n",
        "\n",
        "    axis_thr = _axis_thresholds(df_hl, q_lo=qlo, q_hi=qhi)\n",
        "    df_pts = classify_substrate_points(df_hl, axis_thr=axis_thr)\n",
        "\n",
        "    df_layer_reg, dthr = classify_phases(df_layer, t_field=t_field, drift_q_lo=dqlo, drift_q_hi=dqhi)\n",
        "    phase_map = df_layer_reg.set_index(\"layer\")[\"phase\"].to_dict()\n",
        "    df_pts[\"phase\"] = df_pts[\"layer\"].map(phase_map)\n",
        "\n",
        "    return df_pts, df_layer_reg, axis_thr, dthr\n",
        "\n",
        "# ----------------------------\n",
        "# 6) Sweep + BEFORE/AFTER\n",
        "# ----------------------------\n",
        "def _score_row(r):\n",
        "    phase_cols = [\"PHASE_FORMATION\",\"PHASE_REMODELING\",\"PHASE_HANDOFF\",\"PHASE_CONSOLIDATION\",\"PHASE_TERMINAL_TUNING\",\"PHASE_MIXED\"]\n",
        "    nonzero = sum(int(r[c] > 0) for c in phase_cols)\n",
        "    penalty = 0.0 if nonzero >= 4 else (4 - nonzero) * 5.0\n",
        "    return float(r[\"OTHER_pct\"] + penalty)\n",
        "\n",
        "best_cfg = None\n",
        "\n",
        "if DO_SWEEP:\n",
        "    sweep_rows = []\n",
        "    for t_field in FIELD_T_LIST:\n",
        "        for (qlo,qhi) in AXIS_Q_LIST:\n",
        "            axis_thr = _axis_thresholds(df_hl, q_lo=qlo, q_hi=qhi)\n",
        "            pts = classify_substrate_points(df_hl, axis_thr=axis_thr)\n",
        "            other_pct = 100.0 * float((pts[\"substrate_class\"] == \"OTHER\").mean())\n",
        "\n",
        "            for (dqlo,dqhi) in DRIFT_Q_LIST:\n",
        "                layer_reg, dthr = classify_phases(df_layer, t_field=t_field, drift_q_lo=dqlo, drift_q_hi=dqhi)\n",
        "                phase_counts = layer_reg[\"phase\"].value_counts().to_dict()\n",
        "\n",
        "                sweep_rows.append({\n",
        "                    \"FIELD_POS\": t_field,\n",
        "                    \"AXIS_Q_LO\": qlo,\n",
        "                    \"AXIS_Q_HI\": qhi,\n",
        "                    \"DRIFT_Q_LO\": dqlo,\n",
        "                    \"DRIFT_Q_HI\": dqhi,\n",
        "                    \"D_LO\": dthr[\"D_LO\"],\n",
        "                    \"D_HI\": dthr[\"D_HI\"],\n",
        "                    \"OTHER_pct\": other_pct,\n",
        "                    \"PHASE_FORMATION\": int(phase_counts.get(\"PHASE_FORMATION\", 0)),\n",
        "                    \"PHASE_REMODELING\": int(phase_counts.get(\"PHASE_REMODELING\", 0)),\n",
        "                    \"PHASE_HANDOFF\": int(phase_counts.get(\"PHASE_HANDOFF\", 0)),\n",
        "                    \"PHASE_CONSOLIDATION\": int(phase_counts.get(\"PHASE_CONSOLIDATION\", 0)),\n",
        "                    \"PHASE_TERMINAL_TUNING\": int(phase_counts.get(\"PHASE_TERMINAL_TUNING\", 0)),\n",
        "                    \"PHASE_MIXED\": int(phase_counts.get(\"PHASE_MIXED\", 0)),\n",
        "                })\n",
        "\n",
        "    df_sweep = pd.DataFrame(sweep_rows)\n",
        "    df_sweep[\"score\"] = df_sweep.apply(_score_row, axis=1)\n",
        "    df_sweep = df_sweep.sort_values([\"score\",\"OTHER_pct\"]).reset_index(drop=True)\n",
        "\n",
        "    print(\"\\n=== taxonomy sweep (top 12) ===\")\n",
        "    display(df_sweep.head(12))\n",
        "\n",
        "    best = df_sweep.iloc[0].to_dict()\n",
        "    best_cfg = dict(\n",
        "        LOCK_THRESHOLD=taxonomy_cfg[\"LOCK_THRESHOLD\"],\n",
        "        AXIS_Q_LO=float(best[\"AXIS_Q_LO\"]),\n",
        "        AXIS_Q_HI=float(best[\"AXIS_Q_HI\"]),\n",
        "        FIELD_POS=float(best[\"FIELD_POS\"]),\n",
        "        FIELD_NEG=-float(best[\"FIELD_POS\"]),\n",
        "        DRIFT_Q_LO=float(best[\"DRIFT_Q_LO\"]),\n",
        "        DRIFT_Q_HI=float(best[\"DRIFT_Q_HI\"]),\n",
        "    )\n",
        "    print(\"\\n=== recommended taxonomy_cfg ===\")\n",
        "    display(pd.DataFrame([best_cfg]))\n",
        "\n",
        "df_pts_before, df_layer_regimes_before, axis_thresholds_before, drift_thresholds_before = build_outputs(taxonomy_cfg)\n",
        "df_pts_after = df_layer_regimes_after = axis_thresholds_after = drift_thresholds_after = None\n",
        "\n",
        "if best_cfg is not None:\n",
        "    df_pts_after, df_layer_regimes_after, axis_thresholds_after, drift_thresholds_after = build_outputs(best_cfg)\n",
        "\n",
        "def summarize_points(df_pts, df_layer_reg, title):\n",
        "    print(f\"\\n=== {title} ===\")\n",
        "    print(\"Substrate class counts:\")\n",
        "    display(df_pts[\"substrate_class\"].value_counts().to_frame(\"count\"))\n",
        "\n",
        "    print(\"Substrate classes by phase (counts):\")\n",
        "    display(pd.crosstab(df_pts[\"phase\"], df_pts[\"substrate_class\"]))\n",
        "\n",
        "    print(\"Locked vs adaptive by class:\")\n",
        "    display(pd.crosstab(df_pts[\"substrate_class\"], df_pts[\"locked\"]))\n",
        "\n",
        "    print(\"Layer regimes:\")\n",
        "    display(df_layer_reg[[\"layer\",\"E_slot\",\"E_gamma\",\"field_regime\",\"drift_mean\",\"drift_regime\",\"phase\"]])\n",
        "\n",
        "summarize_points(df_pts_before, df_layer_regimes_before, \"BEFORE (taxonomy_cfg)\")\n",
        "if df_pts_after is not None:\n",
        "    summarize_points(df_pts_after, df_layer_regimes_after, \"AFTER (recommended cfg)\")\n",
        "\n",
        "def plot_phase_timeline(df_layer_reg, title):\n",
        "    phase_order = [\n",
        "        \"PHASE_FORMATION\",\"PHASE_REMODELING\",\"PHASE_HANDOFF\",\n",
        "        \"PHASE_CONSOLIDATION\",\"PHASE_TERMINAL_TUNING\",\"PHASE_MIXED\"\n",
        "    ]\n",
        "    phase_to_y = {p:i for i,p in enumerate(phase_order)}\n",
        "    y = df_layer_reg[\"phase\"].map(phase_to_y).to_numpy()\n",
        "\n",
        "    plt.figure(figsize=(10.5, 2.8))\n",
        "    plt.scatter(df_layer_reg[\"layer\"], y, s=120, edgecolor=\"k\", alpha=0.9)\n",
        "    plt.yticks(list(phase_to_y.values()), list(phase_to_y.keys()))\n",
        "    plt.xlabel(\"Layer\")\n",
        "    plt.title(title)\n",
        "    plt.grid(True, axis=\"x\", alpha=0.25)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_phase_timeline(df_layer_regimes_before, \"Phase timeline (BEFORE)\")\n",
        "if df_layer_regimes_after is not None:\n",
        "    plot_phase_timeline(df_layer_regimes_after, \"Phase timeline (AFTER)\")\n",
        "\n",
        "# Exports (canonical)\n",
        "df_layer_regimes = df_layer_regimes_after if df_layer_regimes_after is not None else df_layer_regimes_before\n",
        "df_tax = df_pts_after if df_pts_after is not None else df_pts_before\n",
        "axis_thresholds = axis_thresholds_after if axis_thresholds_after is not None else axis_thresholds_before\n",
        "drift_thresholds = drift_thresholds_after if drift_thresholds_after is not None else drift_thresholds_before\n",
        "\n",
        "print(\"\\nExports:\")\n",
        "print(\"  df_tax: head\u00d7layer points + bins + substrate_class + phase\")\n",
        "print(\"  df_layer_regimes: layer table with field_regime/drift_regime/phase\")\n",
        "print(\"  axis_thresholds, drift_thresholds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "cellView": "form",
        "id": "L7FaWm_QWRm0",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769533880179,
          "user_tz": 300,
          "elapsed": 3491,
          "user": {
            "displayName": "Secondscapes",
            "userId": "04150343613140054698"
          }
        },
        "outputId": "e1827e04-fc73-4b09-9cc6-e794fcf437fe"
      },
      "id": "L7FaWm_QWRm0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualization Suite (Narrative Order)\nProduce the figure suite in narrative order, saving to artifacts/."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title Slot Embeddings \u2014 compact, rigorous, low-overload (HF-loaded + ckpt_obj available)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# =============================================================================\n",
        "# State dict resolution (HF-loaded model + ckpt_obj)\n",
        "# =============================================================================\n",
        "\n",
        "def resolve_state_dict(*, model=None, ckpt_obj=None):\n",
        "    \"\"\"\n",
        "    Resolve a PyTorch state_dict from either:\n",
        "      - ckpt_obj (already loaded object; dict or state_dict-like)\n",
        "      - model (HF-loaded model; uses model.state_dict())\n",
        "\n",
        "    Priority: ckpt_obj > model\n",
        "    \"\"\"\n",
        "    if ckpt_obj is not None:\n",
        "        # common checkpoint wrappers\n",
        "        if isinstance(ckpt_obj, dict):\n",
        "            if \"model_state_dict\" in ckpt_obj:\n",
        "                return ckpt_obj[\"model_state_dict\"]\n",
        "            if \"state_dict\" in ckpt_obj:\n",
        "                return ckpt_obj[\"state_dict\"]\n",
        "            if \"model\" in ckpt_obj and isinstance(ckpt_obj[\"model\"], dict):\n",
        "                return ckpt_obj[\"model\"]\n",
        "        # might already be a plain state_dict\n",
        "        return ckpt_obj\n",
        "\n",
        "    if model is None:\n",
        "        raise ValueError(\"Need either ckpt_obj or model to resolve state_dict.\")\n",
        "    return model.state_dict()\n",
        "\n",
        "def extract_slot_keys_by_layer(state_dict):\n",
        "    \"\"\"\n",
        "    Returns dict: layer -> tensor [H,K,d] for keys that contain 'slot_keys'.\n",
        "    Expected key pattern: blocks.{layer}.attn.slot_keys\n",
        "    \"\"\"\n",
        "    slot = {}\n",
        "    for k, v in state_dict.items():\n",
        "        if \"slot_keys\" not in k:\n",
        "            continue\n",
        "        try:\n",
        "            li = int(k.split(\".\")[1])\n",
        "        except Exception:\n",
        "            continue\n",
        "        slot[li] = v.detach().cpu()\n",
        "    return slot\n",
        "\n",
        "# =============================================================================\n",
        "# Metrics\n",
        "# =============================================================================\n",
        "\n",
        "def _cos_sim(a, b, eps=1e-12):\n",
        "    a = a / (np.linalg.norm(a) + eps)\n",
        "    b = b / (np.linalg.norm(b) + eps)\n",
        "    return float(np.dot(a, b))\n",
        "\n",
        "def _pairwise_cosine_stats(X, max_pairs=4000, seed=0):\n",
        "    \"\"\"\n",
        "    X: [N,d], returns mean/std cosine similarity over sampled random pairs.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    N = X.shape[0]\n",
        "    if N < 2:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "    P = min(max_pairs, N * (N - 1) // 2)\n",
        "    sims = np.empty((P,), dtype=np.float32)\n",
        "    for p in range(P):\n",
        "        i = rng.integers(0, N)\n",
        "        j = rng.integers(0, N - 1)\n",
        "        if j >= i:\n",
        "            j += 1\n",
        "        sims[p] = _cos_sim(X[i], X[j])\n",
        "    return float(sims.mean()), float(sims.std())\n",
        "\n",
        "def compute_slot_metrics(slot_by_layer, pair_samples=5000, seed=0, dead_thr=0.10):\n",
        "    \"\"\"\n",
        "    Per-layer metrics summarizing slot embeddings:\n",
        "      - norms mean/std\n",
        "      - intra-head diversity (mean cosine distance within head)\n",
        "      - inter-head similarity for same slot index across heads (mean cosine sim)\n",
        "      - global pairwise cosine similarity within layer (sampled)\n",
        "      - dead slot fraction (norm < dead_thr)\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    layers = sorted(slot_by_layer.keys())\n",
        "    for l in layers:\n",
        "        S = slot_by_layer[l].numpy()  # [H,K,d]\n",
        "        H, K, d = S.shape\n",
        "        X = S.reshape(-1, d).astype(np.float32)\n",
        "\n",
        "        norms = np.linalg.norm(X, axis=1)\n",
        "        norm_mean = float(norms.mean())\n",
        "        norm_std  = float(norms.std())\n",
        "        dead_frac = float((norms < dead_thr).mean())\n",
        "\n",
        "        # Intra-head diversity = mean(1 - cos) within each head, then mean across heads\n",
        "        intra_means = []\n",
        "        intra_stds  = []\n",
        "        for h in range(H):\n",
        "            head = S[h].astype(np.float32)  # [K,d]\n",
        "            # all pairs (K=16 in your config; fine)\n",
        "            sims = []\n",
        "            for i in range(K):\n",
        "                for j in range(i + 1, K):\n",
        "                    sims.append(_cos_sim(head[i], head[j]))\n",
        "            sims = np.asarray(sims, dtype=np.float32)\n",
        "            div = 1.0 - sims\n",
        "            intra_means.append(float(div.mean()))\n",
        "            intra_stds.append(float(div.std()))\n",
        "\n",
        "        intra_div_mean = float(np.mean(intra_means))\n",
        "        intra_div_std  = float(np.mean(intra_stds))\n",
        "\n",
        "        # Inter-head similarity for same slot index across heads\n",
        "        inter_means = []\n",
        "        inter_stds  = []\n",
        "        for k in range(K):\n",
        "            across = S[:, k, :].astype(np.float32)  # [H,d]\n",
        "            sims = []\n",
        "            for i in range(H):\n",
        "                for j in range(i + 1, H):\n",
        "                    sims.append(_cos_sim(across[i], across[j]))\n",
        "            sims = np.asarray(sims, dtype=np.float32)\n",
        "            inter_means.append(float(sims.mean()))\n",
        "            inter_stds.append(float(sims.std()))\n",
        "\n",
        "        inter_same_slot_mean = float(np.mean(inter_means))\n",
        "        inter_same_slot_std  = float(np.mean(inter_stds))\n",
        "\n",
        "        global_cos_mean, global_cos_std = _pairwise_cosine_stats(X, max_pairs=pair_samples, seed=seed + 101*l)\n",
        "\n",
        "        rows.append(dict(\n",
        "            layer=int(l), H=int(H), K=int(K), d=int(d),\n",
        "            norm_mean=norm_mean, norm_std=norm_std,\n",
        "            intra_div_mean=intra_div_mean, intra_div_std=intra_div_std,\n",
        "            inter_same_slot_mean=inter_same_slot_mean, inter_same_slot_std=inter_same_slot_std,\n",
        "            global_cos_mean=global_cos_mean, global_cos_std=global_cos_std,\n",
        "            dead_frac=dead_frac,\n",
        "        ))\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# =============================================================================\n",
        "# Plotting (one overview dashboard + optional representative layers)\n",
        "# =============================================================================\n",
        "\n",
        "def plot_slot_overview(df, title=\"Slot Embedding Overview\"):\n",
        "    layers = df[\"layer\"].to_numpy()\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 9))\n",
        "\n",
        "    ax = axes[0, 0]\n",
        "    ax.plot(layers, df[\"norm_mean\"], \"o-\", lw=2)\n",
        "    ax.fill_between(layers, df[\"norm_mean\"] - df[\"norm_std\"], df[\"norm_mean\"] + df[\"norm_std\"], alpha=0.25)\n",
        "    ax.set_title(\"Slot embedding norms (mean \u00b1 std)\")\n",
        "    ax.set_xlabel(\"Layer\"); ax.set_ylabel(\"L2 norm\")\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    ax = axes[0, 1]\n",
        "    ax.plot(layers, df[\"intra_div_mean\"], \"o-\", lw=2)\n",
        "    ax.fill_between(layers, df[\"intra_div_mean\"] - df[\"intra_div_std\"], df[\"intra_div_mean\"] + df[\"intra_div_std\"], alpha=0.25)\n",
        "    ax.set_title(\"Intra-head diversity (mean cosine distance)\")\n",
        "    ax.set_xlabel(\"Layer\"); ax.set_ylabel(\"mean(1 - cos)\")\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    ax = axes[1, 0]\n",
        "    ax.plot(layers, df[\"inter_same_slot_mean\"], \"o-\", lw=2)\n",
        "    ax.fill_between(layers, df[\"inter_same_slot_mean\"] - df[\"inter_same_slot_std\"], df[\"inter_same_slot_mean\"] + df[\"inter_same_slot_std\"], alpha=0.25)\n",
        "    ax.set_title(\"Inter-head similarity for same slot index (mean cos)\")\n",
        "    ax.set_xlabel(\"Layer\"); ax.set_ylabel(\"mean cos\")\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    ax = axes[1, 1]\n",
        "    ax.plot(layers, df[\"global_cos_mean\"], \"o-\", lw=2)\n",
        "    ax.fill_between(layers, df[\"global_cos_mean\"] - df[\"global_cos_std\"], df[\"global_cos_mean\"] + df[\"global_cos_std\"], alpha=0.25)\n",
        "    ax.set_title(\"Global within-layer similarity (sampled cos)\")\n",
        "    ax.set_xlabel(\"Layer\"); ax.set_ylabel(\"mean cos\")\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    fig.suptitle(title, y=1.02, fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(14, 2.8))\n",
        "    plt.plot(layers, df[\"dead_frac\"], \"o-\", lw=2)\n",
        "    plt.ylim(-0.01, max(0.05, float(df[\"dead_frac\"].max()) + 0.01))\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.xlabel(\"Layer\"); plt.ylabel(\"fraction\")\n",
        "    plt.title(\"Dead/low-norm slots (norm < threshold)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def visualize_single_layer_compact(slots_tensor, layer_idx, show_similarity=False):\n",
        "    \"\"\"\n",
        "    Compact layer view:\n",
        "      - PCA (SVD) scatter colored by head\n",
        "      - norms heatmap (H x K)\n",
        "      - optional HKxHK cosine similarity matrix (heavy; off by default)\n",
        "    \"\"\"\n",
        "    S = slots_tensor.detach().cpu().numpy().astype(np.float32)  # [H,K,d]\n",
        "    H, K, d = S.shape\n",
        "    X = S.reshape(-1, d)\n",
        "\n",
        "    # PCA via SVD\n",
        "    Xc = X - X.mean(axis=0, keepdims=True)\n",
        "    U, s, Vt = np.linalg.svd(Xc, full_matrices=False)\n",
        "    Z = Xc @ Vt[:2].T\n",
        "    var2 = float((s[:2]**2).sum() / (s**2).sum())\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 4.5))\n",
        "\n",
        "    ax = axes[0]\n",
        "    for h in range(H):\n",
        "        i0, i1 = h*K, (h+1)*K\n",
        "        ax.scatter(Z[i0:i1, 0], Z[i0:i1, 1], s=40, alpha=0.75, label=(f\"H{h}\" if H <= 12 else None))\n",
        "    ax.set_title(f\"Layer {layer_idx}: PCA (SVD) \u2022 var\u2248{var2:.1%}\")\n",
        "    ax.set_xlabel(\"PC1\"); ax.set_ylabel(\"PC2\")\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    if H <= 12:\n",
        "        ax.legend(ncol=2, fontsize=8, frameon=True)\n",
        "\n",
        "    ax = axes[1]\n",
        "    norms = np.linalg.norm(X, axis=1).reshape(H, K)\n",
        "    im = ax.imshow(norms, aspect=\"auto\")\n",
        "    ax.set_title(\"Slot norms (H \u00d7 K)\")\n",
        "    ax.set_xlabel(\"slot\"); ax.set_ylabel(\"head\")\n",
        "    plt.colorbar(im, ax=ax, label=\"L2 norm\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    if show_similarity:\n",
        "        Xn = X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-12)\n",
        "        sim = Xn @ Xn.T  # [HK,HK]\n",
        "        plt.figure(figsize=(7, 6))\n",
        "        plt.imshow(sim, vmin=-1, vmax=1, aspect=\"auto\", cmap=\"RdBu_r\")\n",
        "        for h in range(1, H):\n",
        "            plt.axhline(h*K - 0.5, color=\"white\", lw=1.5, alpha=0.9)\n",
        "            plt.axvline(h*K - 0.5, color=\"white\", lw=1.5, alpha=0.9)\n",
        "        plt.title(f\"Layer {layer_idx}: cosine similarity (HK\u00d7HK)\")\n",
        "        plt.xlabel(\"slot index\"); plt.ylabel(\"slot index\")\n",
        "        plt.colorbar(label=\"cos sim\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "def pick_representative_layers(df, n=5):\n",
        "    layers = df[\"layer\"].to_numpy()\n",
        "    picks = {int(layers[0]), int(layers[len(layers)//2]), int(layers[-1])}\n",
        "    picks.add(int(df.loc[df[\"intra_div_mean\"].idxmax(), \"layer\"]))\n",
        "    picks.add(int(df.loc[df[\"intra_div_mean\"].idxmin(), \"layer\"]))\n",
        "    picks = list(dict.fromkeys(sorted(picks)))  # unique, sorted\n",
        "    return picks[:n]\n",
        "\n",
        "# =============================================================================\n",
        "# Run (assumes: HF model is loaded and ckpt_obj is available)\n",
        "# =============================================================================\n",
        "\n",
        "# You said: assume ckpt_obj is available; also model is HF-loaded.\n",
        "# We won't force either name; try common names.\n",
        "_model = globals().get(\"model\", None)\n",
        "_ckpt_obj = globals().get(\"ckpt_obj\", None)\n",
        "\n",
        "state_dict = resolve_state_dict(model=_model, ckpt_obj=_ckpt_obj)\n",
        "slot_by_layer = extract_slot_keys_by_layer(state_dict)\n",
        "\n",
        "layers = sorted(slot_by_layer.keys())\n",
        "print(f\"Found slot_keys in {len(layers)} layers: {layers[:5]} ... {layers[-5:]}\")\n",
        "\n",
        "df_slots = compute_slot_metrics(slot_by_layer, pair_samples=5000, seed=0, dead_thr=0.10)\n",
        "\n",
        "print(\"\\n=== Slot embedding summary (per layer) ===\")\n",
        "display(df_slots)\n",
        "\n",
        "plot_slot_overview(df_slots, title=f\"Slot Embeddings \u2022 {len(df_slots)} layers\")\n",
        "\n",
        "cols_show = [\"layer\", \"norm_mean\", \"intra_div_mean\", \"inter_same_slot_mean\", \"global_cos_mean\", \"dead_frac\"]\n",
        "df_rank = df_slots[cols_show].copy()\n",
        "\n",
        "print(\"\\n=== Most specialized layers (highest intra-head diversity) ===\")\n",
        "display(df_rank.sort_values(\"intra_div_mean\", ascending=False).head(5))\n",
        "\n",
        "print(\"\\n=== Most collapsed layers (highest global cosine similarity) ===\")\n",
        "display(df_rank.sort_values(\"global_cos_mean\", ascending=False).head(5))\n",
        "\n",
        "print(\"\\n=== Least diverse within-head (lowest intra-head diversity) ===\")\n",
        "display(df_rank.sort_values(\"intra_div_mean\", ascending=True).head(5))\n",
        "\n",
        "REP_LAYERS = pick_representative_layers(df_slots, n=5)\n",
        "print(f\"\\nRepresentative layers (compact views): {REP_LAYERS}\")\n",
        "\n",
        "for l in REP_LAYERS:\n",
        "    visualize_single_layer_compact(slot_by_layer[l], l, show_similarity=False)\n",
        "\n",
        "# If you want ONE heavy similarity matrix:\n",
        "# visualize_single_layer_compact(slot_by_layer[REP_LAYERS[0]], REP_LAYERS[0], show_similarity=True)"
      ],
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "i0lzgkHFjQMe",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769532751553,
          "user_tz": 300,
          "elapsed": 9294,
          "user": {
            "displayName": "Secondscapes",
            "userId": "04150343613140054698"
          }
        },
        "outputId": "8d62abb9-e8c8-4fd8-e4d3-01851b2a7014"
      },
      "id": "i0lzgkHFjQMe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title slotkeys embedding density plot\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import umap\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.neighbors import KernelDensity\n",
        "from scipy.interpolate import griddata\n",
        "\n",
        "#from asa.load_pretrained import load_pretrained  # Your loader\n",
        "\n",
        "# Load model (returns model, report, config or similar)\n",
        "#model, *_ = load_pretrained()  # Add args like device='cpu', variant='default' if needed\n",
        "#device = torch.device('cpu')   # Or 'cuda' if you have GPU\n",
        "#model.to(device)\n",
        "model.eval()\n",
        "\n",
        "#print(\"Model loaded successfully.\")\n",
        "\n",
        "# Collect slot_keys from all layers\n",
        "slot_keys_list = []\n",
        "for layer_idx in range(21):\n",
        "    attn = model.blocks[layer_idx].attn  # Direct access per your state_dict\n",
        "    keys = attn.slot_keys.detach().cpu().numpy()  # [8 heads, 16 slots, 48 dim]\n",
        "    slot_keys_list.append(keys)\n",
        "\n",
        "slot_keys = np.stack(slot_keys_list)  # [21 layers, 8, 16, 48]\n",
        "\n",
        "# Flatten into dataset of points: each (layer, head, slot) \u2192 one 48-dim vector\n",
        "# Total points: 21 \u00d7 8 \u00d7 16 = 2688\n",
        "data = slot_keys.reshape(-1, 48)\n",
        "\n",
        "# Optional: Normalize per dimension (helps UMAP in high-dim)\n",
        "data_mean = data.mean(axis=0)\n",
        "data_std = data.std(axis=0) + 1e-8  # Avoid div-by-zero\n",
        "data = (data - data_mean) / data_std\n",
        "\n",
        "print(f\"Extracted {data.shape[0]} slot key vectors in 48-dim space.\")\n",
        "\n",
        "# Step 1: UMAP projection to 2D (good for capturing clusters/manifold structure)\n",
        "reducer = umap.UMAP(\n",
        "    n_components=2,\n",
        "    n_neighbors=15,       # Tune for local vs global structure\n",
        "    min_dist=0.1,\n",
        "    metric='cosine',      # Cosine often better for embeddings\n",
        "    random_state=42\n",
        ")\n",
        "proj_2d = reducer.fit_transform(data)\n",
        "\n",
        "# Step 2: Density-based height map via KDE\n",
        "kde = KernelDensity(kernel='gaussian', bandwidth=0.3).fit(proj_2d)  # Bandwidth ~0.2-0.5; tune for smoothness\n",
        "x_min, x_max = proj_2d[:, 0].min() - 2, proj_2d[:, 0].max() + 2\n",
        "y_min, y_max = proj_2d[:, 1].min() - 2, proj_2d[:, 1].max() + 2\n",
        "x_grid, y_grid = np.mgrid[x_min:x_max:150j, y_min:y_max:150j]  # Higher res for detail\n",
        "grid_points = np.c_[x_grid.ravel(), y_grid.ravel()]\n",
        "log_dens = kde.score_samples(grid_points)\n",
        "heights = np.exp(log_dens).reshape(x_grid.shape)\n",
        "\n",
        "# Step 3: 3D Surface Plot\n",
        "fig = plt.figure(figsize=(12, 9))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Surface\n",
        "surf = ax.plot_surface(x_grid, y_grid, heights, cmap='viridis', alpha=0.85, rstride=1, cstride=1)\n",
        "fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5, label='Density (Height)')\n",
        "\n",
        "# Optional: Scatter original points colored by layer (to see progression/switches)\n",
        "layer_colors = np.repeat(np.arange(21), 8 * 16)  # [2688] with layer index\n",
        "ax.scatter(proj_2d[:, 0], proj_2d[:, 1], np.zeros_like(proj_2d[:, 0]),\n",
        "           c=layer_colors, cmap='plasma', s=8, alpha=0.6, label='Slot points by layer')\n",
        "\n",
        "ax.set_title('Routing Manifold: 3D Height Map of Slot Keys (Density as Height)')\n",
        "ax.set_xlabel('UMAP Dimension 1')\n",
        "ax.set_ylabel('UMAP Dimension 2')\n",
        "ax.set_zlabel('Density')\n",
        "ax.view_init(elev=30, azim=120)  # Nice viewing angle; rotate interactively in plt\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save if desired\n",
        "# fig.savefig('routing_manifold_3d.png', dpi=300)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "cellView": "form",
        "id": "QEwiFB6ByMuu",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769532777610,
          "user_tz": 300,
          "elapsed": 26053,
          "user": {
            "displayName": "Secondscapes",
            "userId": "04150343613140054698"
          }
        },
        "outputId": "a5791329-0cfa-4c60-a41f-55661dd9b667"
      },
      "id": "QEwiFB6ByMuu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title Substrate visualization suite (FINAL, fixed second-order + true depth slices)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
        "\n",
        "# ============================================================\n",
        "# 0) CONFIG\n",
        "# ============================================================\n",
        "\n",
        "LOCK_THRESHOLD = 8.0  # inertia_half_lag \u2265 this \u2192 \"locked\"\n",
        "\n",
        "DEPTH_BINS = {\n",
        "    \"early\": list(range(0, 7)),    # L0\u2013L6\n",
        "    \"mid\":   list(range(7, 15)),   # L7\u2013L14\n",
        "    \"late\":  list(range(15, 21)),  # L15\u2013L20\n",
        "}\n",
        "\n",
        "def z(x):\n",
        "    x = np.asarray(x, dtype=np.float64)\n",
        "    return (x - x.mean()) / (x.std() + 1e-8)\n",
        "\n",
        "# ============================================================\n",
        "# 1) PER-HEAD TABLE: (M,P,C,L,size)\n",
        "# ============================================================\n",
        "\n",
        "dfh = df_slot.copy()\n",
        "\n",
        "# reduced coordinates\n",
        "dfh[\"M\"] = z(dfh[\"mean_write_ess\"])\n",
        "dfh[\"P\"] = z(dfh[\"mean_inertia_mean\"])\n",
        "dfh[\"C\"] = z(dfh[\"mean_top4_mass\"])\n",
        "\n",
        "# lock flag + size\n",
        "dfh[\"locked\"] = (dfh[\"mean_inertia_half_lag\"] >= LOCK_THRESHOLD).astype(np.int32)\n",
        "dfh[\"size\"] = dfh[\"switches\"].astype(np.float32)\n",
        "\n",
        "# ============================================================\n",
        "# 2) 3D SCATTER: (M, P, C), color by locked, size by switches\n",
        "# ============================================================\n",
        "\n",
        "def plot_3d_scatter(df, title):\n",
        "    fig = plt.figure(figsize=(9, 7))\n",
        "    ax = fig.add_subplot(111, projection=\"3d\")\n",
        "\n",
        "    sc = ax.scatter(\n",
        "        df[\"M\"], df[\"P\"], df[\"C\"],\n",
        "        c=df[\"locked\"],\n",
        "        cmap=\"coolwarm\",\n",
        "        s=30 + 18 * df[\"size\"],\n",
        "        alpha=0.85\n",
        "    )\n",
        "\n",
        "    ax.set_xlabel(\"M: memory capacity (z(write_ess))\")\n",
        "    ax.set_ylabel(\"P: routing smoothness (z(inertia_mean))\")\n",
        "    ax.set_zlabel(\"C: commitment (z(top4_mass))\")\n",
        "    ax.set_title(title)\n",
        "\n",
        "    cbar = fig.colorbar(sc, ax=ax, shrink=0.6)\n",
        "    cbar.set_label(f\"locked (1 if inertia_half_lag \u2265 {LOCK_THRESHOLD:g})\")\n",
        "\n",
        "    ax.view_init(elev=25, azim=135)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_3d_scatter(dfh, \"Head substrate space (anchor slot basis): (M, P, C)\\ncolor=lock flag, size=switches\")\n",
        "\n",
        "# ============================================================\n",
        "# 3) DEPTH SLICES (FACET BY LAYER GROUPS)\n",
        "#    We need per-point layer membership. We can reconstruct it from results['aligned']['cluster_by_layer']\n",
        "#    because that map is [layer -> cluster_id per head] in ANCHOR basis.\n",
        "# ============================================================\n",
        "\n",
        "if (\"results\" not in globals()) or (\"aligned\" not in results) or (\"cluster_by_layer\" not in results[\"aligned\"]):\n",
        "    raise RuntimeError(\n",
        "        \"Need results['aligned']['cluster_by_layer'] to facet by depth.\\n\"\n",
        "        \"Run the clustering/alignment cell (anchor mode) before this visualization cell.\"\n",
        "    )\n",
        "\n",
        "cluster_by_layer = results[\"aligned\"][\"cluster_by_layer\"]\n",
        "layers_sorted = sorted(cluster_by_layer.keys())\n",
        "H = len(cluster_by_layer[layers_sorted[0]])\n",
        "\n",
        "# Build (layer, slot) point table and join to dfh (which is per-slot aggregates)\n",
        "rows = []\n",
        "for l in layers_sorted:\n",
        "    for s in range(H):\n",
        "        rows.append({\"layer\": int(l), \"slot\": int(s)})\n",
        "\n",
        "df_points = pd.DataFrame(rows).merge(dfh, left_on=\"slot\", right_on=\"slot\", how=\"left\")\n",
        "\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "for i, (name, layer_list) in enumerate(DEPTH_BINS.items(), start=1):\n",
        "    sub = df_points[df_points[\"layer\"].isin(layer_list)].copy()\n",
        "    ax = fig.add_subplot(1, 3, i, projection=\"3d\")\n",
        "\n",
        "    ax.scatter(\n",
        "        sub[\"M\"], sub[\"P\"], sub[\"C\"],\n",
        "        c=sub[\"locked\"],\n",
        "        cmap=\"coolwarm\",\n",
        "        s=18 + 10 * sub[\"size\"],\n",
        "        alpha=0.80\n",
        "    )\n",
        "\n",
        "    ax.set_title(f\"{name.upper()} (layers {layer_list[0]}\u2013{layer_list[-1]})\")\n",
        "    ax.set_xlabel(\"M\")\n",
        "    ax.set_ylabel(\"P\")\n",
        "    ax.set_zlabel(\"C\")\n",
        "    ax.view_init(elev=25, azim=135)\n",
        "\n",
        "plt.suptitle(\"Substrate geometry by depth regime (points replicated across layers; size/lock from slot aggregates)\", y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# 4) PHASE PORTRAIT ACROSS DEPTH (PER LAYER MEANS)\n",
        "#    Use df_feat (layer means of head features) + df_so (second-order params)\n",
        "# ============================================================\n",
        "\n",
        "# Build df_feat if missing\n",
        "if \"df_feat\" not in globals() or df_feat is None or len(df_feat) == 0:\n",
        "    if (\"feat_by_layer\" not in globals()) or (\"feature_names\" not in globals()):\n",
        "        raise RuntimeError(\"Need df_feat OR (feat_by_layer + feature_names) to compute per-layer means.\")\n",
        "    rows = []\n",
        "    for l, X in enumerate(feat_by_layer):\n",
        "        if X is None:\n",
        "            continue\n",
        "        X = np.asarray(X)\n",
        "        rec = {\"layer\": int(l)}\n",
        "        for j, nm in enumerate(feature_names):\n",
        "            rec[f\"mean_{nm}\"] = float(X[:, j].mean())\n",
        "        rows.append(rec)\n",
        "    df_feat = pd.DataFrame(rows)\n",
        "\n",
        "# Merge per-layer features with second-order params\n",
        "if \"df_so\" not in globals() or df_so is None or len(df_so) == 0:\n",
        "    raise RuntimeError(\"Need df_so from your Second-Order Slotspace Attention cell.\")\n",
        "\n",
        "df_layer = df_feat.merge(df_so[[\"layer\", \"slotspace_gate\", \"content_read_gamma\"]], on=\"layer\", how=\"left\").sort_values(\"layer\")\n",
        "\n",
        "# Reduced per-layer axes\n",
        "M_layer = z(df_layer[\"mean_write_ess\"])\n",
        "P_layer = z(df_layer[\"mean_inertia_mean\"])\n",
        "\n",
        "# Choose second-order strength for arrow magnitude\n",
        "second_order = df_layer[\"slotspace_gate\"].to_numpy()  # canonical\n",
        "\n",
        "plt.figure(figsize=(8.5, 6.5))\n",
        "plt.scatter(\n",
        "    M_layer,\n",
        "    P_layer,\n",
        "    c=df_layer[\"layer\"],\n",
        "    cmap=\"viridis\",\n",
        "    s=140,\n",
        "    alpha=0.92,\n",
        "    edgecolor=\"k\"\n",
        ")\n",
        "\n",
        "# Arrow overlay: direction is +P (up); magnitude scaled by z(second_order)\n",
        "so_z = z(second_order)\n",
        "for i in range(len(df_layer)):\n",
        "    plt.arrow(\n",
        "        float(M_layer[i]),\n",
        "        float(P_layer[i]),\n",
        "        0.0,\n",
        "        0.18 * float(so_z[i]),\n",
        "        head_width=0.03,\n",
        "        alpha=0.55,\n",
        "        color=\"black\",\n",
        "        length_includes_head=True\n",
        "    )\n",
        "\n",
        "plt.xlabel(\"M (layer mean): z(mean_write_ess)\")\n",
        "plt.ylabel(\"P (layer mean): z(mean_inertia_mean)\")\n",
        "plt.title(\"Phase portrait across depth\\narrows = z(slotspace_gate) (second-order strength)\")\n",
        "plt.colorbar(label=\"Layer index\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Optional: show gamma overlay as a second plot (often peaks earlier than gate)\n",
        "gamma = df_layer[\"content_read_gamma\"].to_numpy()\n",
        "plt.figure(figsize=(8.5, 6.5))\n",
        "plt.scatter(\n",
        "    M_layer,\n",
        "    P_layer,\n",
        "    c=df_layer[\"layer\"],\n",
        "    cmap=\"viridis\",\n",
        "    s=140,\n",
        "    alpha=0.92,\n",
        "    edgecolor=\"k\"\n",
        ")\n",
        "gz = z(gamma)\n",
        "for i in range(len(df_layer)):\n",
        "    plt.arrow(\n",
        "        float(M_layer[i]),\n",
        "        float(P_layer[i]),\n",
        "        0.0,\n",
        "        0.18 * float(gz[i]),\n",
        "        head_width=0.03,\n",
        "        alpha=0.55,\n",
        "        color=\"black\",\n",
        "        length_includes_head=True\n",
        "    )\n",
        "plt.xlabel(\"M (layer mean): z(mean_write_ess)\")\n",
        "plt.ylabel(\"P (layer mean): z(mean_inertia_mean)\")\n",
        "plt.title(\"Phase portrait across depth\\narrows = z(content_read_gamma)\")\n",
        "plt.colorbar(label=\"Layer index\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 844
        },
        "cellView": "form",
        "id": "ODDQUrlC888J",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769532779355,
          "user_tz": 300,
          "elapsed": 1726,
          "user": {
            "displayName": "Secondscapes",
            "userId": "04150343613140054698"
          }
        },
        "outputId": "ef63548b-8cca-4fbc-ac71-b53da7cb9e14"
      },
      "id": "ODDQUrlC888J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title Substrate visualization suite \u2014 PER-HEAD story (true head\u00d7layer points + trajectories)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
        "\n",
        "# ============================================================\n",
        "# 0) CONFIG\n",
        "# ============================================================\n",
        "\n",
        "LOCK_THRESHOLD = 8.0  # inertia_half_lag \u2265 this \u2192 locked (per-head, per-layer)\n",
        "\n",
        "DEPTH_BINS = {\n",
        "    \"early\": list(range(0, 7)),    # L0\u2013L6\n",
        "    \"mid\":   list(range(7, 15)),   # L7\u2013L14\n",
        "    \"late\":  list(range(15, 21)),  # L15\u2013L20\n",
        "}\n",
        "\n",
        "def z(x):\n",
        "    x = np.asarray(x, dtype=np.float64)\n",
        "    return (x - x.mean()) / (x.std() + 1e-8)\n",
        "\n",
        "# ============================================================\n",
        "# 1) BUILD PER-HEAD\u00d7LAYER TABLE from feat_by_layer\n",
        "# ============================================================\n",
        "\n",
        "if (\"feat_by_layer\" not in globals()) or (\"feature_names\" not in globals()):\n",
        "    raise RuntimeError(\"Need feat_by_layer + feature_names (your head feature bank) to build per-head\u00d7layer points.\")\n",
        "\n",
        "# Feature columns we need to define M/P/C/L\n",
        "need = {\n",
        "    \"mean_write_ess\": \"M\",\n",
        "    \"mean_inertia_mean\": \"P\",\n",
        "    \"mean_top4_mass\": \"C\",\n",
        "    \"mean_inertia_half_lag\": \"L\",  # for lock flag\n",
        "}\n",
        "for k in need:\n",
        "    if k not in [f\"mean_{nm}\" for nm in feature_names] and k not in feature_names:\n",
        "        # Your feature_names already look like ['inertia_lag1','inertia_mean',...,'write_ess'] (without mean_ prefix)\n",
        "        pass\n",
        "\n",
        "# Map your feature bank names -> indices\n",
        "# (Your feature_names list is like: inertia_lag1, inertia_mean, inertia_slope, inertia_half_lag, tok_ent, eff_slots, top4_mass, write_tail_half_life, write_ess)\n",
        "name_to_j = {nm: j for j, nm in enumerate(feature_names)}\n",
        "\n",
        "req = [\"write_ess\", \"inertia_mean\", \"top4_mass\", \"inertia_half_lag\"]\n",
        "missing = [r for r in req if r not in name_to_j]\n",
        "if missing:\n",
        "    raise RuntimeError(f\"Missing required features in feature_names: {missing}\\nfeature_names={feature_names}\")\n",
        "\n",
        "rows = []\n",
        "for layer, X in enumerate(feat_by_layer):\n",
        "    if X is None:\n",
        "        continue\n",
        "    X = np.asarray(X)  # shape [H, D]\n",
        "    H = X.shape[0]\n",
        "    for h in range(H):\n",
        "        rows.append({\n",
        "            \"layer\": int(layer),\n",
        "            \"head\": int(h),\n",
        "            \"write_ess\": float(X[h, name_to_j[\"write_ess\"]]),\n",
        "            \"inertia_mean\": float(X[h, name_to_j[\"inertia_mean\"]]),\n",
        "            \"top4_mass\": float(X[h, name_to_j[\"top4_mass\"]]),\n",
        "            \"inertia_half_lag\": float(X[h, name_to_j[\"inertia_half_lag\"]]),\n",
        "        })\n",
        "\n",
        "df_hp = pd.DataFrame(rows)\n",
        "if len(df_hp) == 0:\n",
        "    raise RuntimeError(\"Built empty df_hp; feat_by_layer had no usable entries.\")\n",
        "\n",
        "# Global z-scoring over ALL head\u00d7layer points (so geometry is comparable across depth)\n",
        "df_hp[\"M\"] = z(df_hp[\"write_ess\"])\n",
        "df_hp[\"P\"] = z(df_hp[\"inertia_mean\"])\n",
        "df_hp[\"C\"] = z(df_hp[\"top4_mass\"])\n",
        "df_hp[\"locked\"] = (df_hp[\"inertia_half_lag\"] >= LOCK_THRESHOLD).astype(np.int32)\n",
        "\n",
        "# Join in per-head aggregate switches (optional sizing)\n",
        "if \"df_slot\" in globals():\n",
        "    if \"slot\" in df_slot.columns:\n",
        "        # You were calling heads \"slot\" in df_slot\n",
        "        df_switch = df_slot[[\"slot\", \"switches\"]].rename(columns={\"slot\": \"head\"})\n",
        "        df_hp = df_hp.merge(df_switch, on=\"head\", how=\"left\")\n",
        "    elif \"head\" in df_slot.columns:\n",
        "        df_switch = df_slot[[\"head\", \"switches\"]]\n",
        "        df_hp = df_hp.merge(df_switch, on=\"head\", how=\"left\")\n",
        "\n",
        "df_hp[\"switches\"] = df_hp[\"switches\"].fillna(0.0)\n",
        "df_hp[\"size\"] = df_hp[\"switches\"].astype(np.float32)\n",
        "\n",
        "# Join in cluster id per layer per head (optional role label)\n",
        "if (\"results\" in globals()) and (\"aligned\" in results) and (\"cluster_by_layer\" in results[\"aligned\"]):\n",
        "    cluster_by_layer = results[\"aligned\"][\"cluster_by_layer\"]  # dict layer -> list/array len H\n",
        "    cid = []\n",
        "    for _, r in df_hp.iterrows():\n",
        "        l = int(r[\"layer\"]); h = int(r[\"head\"])\n",
        "        if l in cluster_by_layer:\n",
        "            cid.append(int(cluster_by_layer[l][h]))\n",
        "        else:\n",
        "            cid.append(-1)\n",
        "    df_hp[\"cluster\"] = cid\n",
        "else:\n",
        "    df_hp[\"cluster\"] = -1\n",
        "\n",
        "print(\"df_hp (per-head\u00d7layer) sample:\")\n",
        "display(df_hp.head(10))\n",
        "\n",
        "# ============================================================\n",
        "# 2) 3D SCATTER (true head\u00d7layer points)\n",
        "#    Color: layer (depth) OR cluster; Size: switches; Marker: lock\n",
        "# ============================================================\n",
        "\n",
        "def plot_3d_points(df, title, color_by=\"layer\"):\n",
        "    fig = plt.figure(figsize=(9.5, 7.5))\n",
        "    ax = fig.add_subplot(111, projection=\"3d\")\n",
        "\n",
        "    cval = df[color_by].to_numpy()\n",
        "    s = 18 + 8.0 * df[\"size\"].to_numpy()\n",
        "\n",
        "    # two scatters so locked vs adaptive are visually separable\n",
        "    d0 = df[df[\"locked\"] == 0]\n",
        "    d1 = df[df[\"locked\"] == 1]\n",
        "\n",
        "    sc0 = ax.scatter(d0[\"M\"], d0[\"P\"], d0[\"C\"], c=d0[color_by], cmap=\"viridis\",\n",
        "                     s=(18 + 8.0*d0[\"size\"]), alpha=0.78, marker=\"o\", label=\"adaptive\")\n",
        "    sc1 = ax.scatter(d1[\"M\"], d1[\"P\"], d1[\"C\"], c=d1[color_by], cmap=\"viridis\",\n",
        "                     s=(18 + 8.0*d1[\"size\"]), alpha=0.90, marker=\"^\", label=\"locked\")\n",
        "\n",
        "    ax.set_xlabel(\"M: z(write_ess)\")\n",
        "    ax.set_ylabel(\"P: z(inertia_mean)\")\n",
        "    ax.set_zlabel(\"C: z(top4_mass)\")\n",
        "    ax.set_title(title)\n",
        "    ax.legend(loc=\"best\", frameon=True)\n",
        "\n",
        "    cbar = fig.colorbar(sc0, ax=ax, shrink=0.6)\n",
        "    cbar.set_label(color_by)\n",
        "\n",
        "    ax.view_init(elev=25, azim=135)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_3d_points(df_hp, \"Head\u00d7Layer substrate space (M,P,C)\\nmarker=lock, size=switches, color=layer\", color_by=\"layer\")\n",
        "\n",
        "# Optional: color by cluster instead (role space)\n",
        "if (df_hp[\"cluster\"] >= 0).any():\n",
        "    plot_3d_points(df_hp, \"Head\u00d7Layer substrate space (M,P,C)\\nmarker=lock, size=switches, color=cluster\", color_by=\"cluster\")\n",
        "\n",
        "# ============================================================\n",
        "# 3) TRUE DEPTH SLICES (early/mid/late)\n",
        "# ============================================================\n",
        "\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "for i, (name, layer_list) in enumerate(DEPTH_BINS.items(), start=1):\n",
        "    sub = df_hp[df_hp[\"layer\"].isin(layer_list)]\n",
        "    ax = fig.add_subplot(1, 3, i, projection=\"3d\")\n",
        "\n",
        "    d0 = sub[sub[\"locked\"] == 0]\n",
        "    d1 = sub[sub[\"locked\"] == 1]\n",
        "\n",
        "    ax.scatter(d0[\"M\"], d0[\"P\"], d0[\"C\"], c=d0[\"layer\"], cmap=\"viridis\",\n",
        "               s=(14 + 6.0*d0[\"size\"]), alpha=0.70, marker=\"o\")\n",
        "    ax.scatter(d1[\"M\"], d1[\"P\"], d1[\"C\"], c=d1[\"layer\"], cmap=\"viridis\",\n",
        "               s=(14 + 6.0*d1[\"size\"]), alpha=0.90, marker=\"^\")\n",
        "\n",
        "    ax.set_title(f\"{name.upper()} layers {layer_list[0]}\u2013{layer_list[-1]}\")\n",
        "    ax.set_xlabel(\"M\"); ax.set_ylabel(\"P\"); ax.set_zlabel(\"C\")\n",
        "    ax.view_init(elev=25, azim=135)\n",
        "\n",
        "plt.suptitle(\"Substrate geometry by depth regime (TRUE head\u00d7layer points)\", y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# 4) PER-HEAD TRAJECTORIES across depth (M vs P)\n",
        "#    - one panel per head; color by layer\n",
        "#    - optionally annotate cluster id and lock state\n",
        "# ============================================================\n",
        "\n",
        "H = int(df_hp[\"head\"].max() + 1)\n",
        "ncols = 4\n",
        "nrows = int(np.ceil(H / ncols))\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(4.3*ncols, 3.6*nrows), sharex=True, sharey=True)\n",
        "axes = np.array(axes).reshape(-1)\n",
        "\n",
        "for h in range(H):\n",
        "    ax = axes[h]\n",
        "    dh = df_hp[df_hp[\"head\"] == h].sort_values(\"layer\")\n",
        "    ax.scatter(dh[\"M\"], dh[\"P\"], c=dh[\"layer\"], cmap=\"viridis\", s=60, edgecolor=\"k\", alpha=0.9)\n",
        "    ax.plot(dh[\"M\"], dh[\"P\"], \"-\", alpha=0.35)\n",
        "    # mark locked points\n",
        "    dhl = dh[dh[\"locked\"] == 1]\n",
        "    if len(dhl) > 0:\n",
        "        ax.scatter(dhl[\"M\"], dhl[\"P\"], facecolors=\"none\", edgecolors=\"red\", s=140, linewidths=1.8, alpha=0.9)\n",
        "\n",
        "    # annotate cluster transitions if available (light)\n",
        "    if (dh[\"cluster\"] >= 0).any():\n",
        "        for _, r in dh.iterrows():\n",
        "            if int(r[\"layer\"]) in (0, 10, 20):  # keep sparse so it's readable\n",
        "                ax.text(r[\"M\"], r[\"P\"], str(int(r[\"cluster\"])), fontsize=9, alpha=0.8)\n",
        "\n",
        "    ax.set_title(f\"Head {h}\")\n",
        "    ax.grid(True, alpha=0.25)\n",
        "\n",
        "for k in range(H, len(axes)):\n",
        "    axes[k].axis(\"off\")\n",
        "\n",
        "fig.suptitle(\"Per-head trajectories in (M,P) across depth\\ncolor=layer, red ring=locked, text=cluster@{0,10,20}\", y=1.02)\n",
        "fig.supxlabel(\"M: z(write_ess)\")\n",
        "fig.supylabel(\"P: z(inertia_mean)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# 5) PER-HEAD 3D TRAJECTORIES (optional: one head at a time)\n",
        "# ============================================================\n",
        "\n",
        "def plot_head_3d_trajectory(head_id: int):\n",
        "    dh = df_hp[df_hp[\"head\"] == head_id].sort_values(\"layer\")\n",
        "    fig = plt.figure(figsize=(8.5, 6.8))\n",
        "    ax = fig.add_subplot(111, projection=\"3d\")\n",
        "    ax.plot(dh[\"M\"], dh[\"P\"], dh[\"C\"], \"-\", alpha=0.6)\n",
        "    sc = ax.scatter(dh[\"M\"], dh[\"P\"], dh[\"C\"], c=dh[\"layer\"], cmap=\"viridis\", s=80, edgecolor=\"k\", alpha=0.95)\n",
        "    # lock marker overlay\n",
        "    dhl = dh[dh[\"locked\"] == 1]\n",
        "    if len(dhl) > 0:\n",
        "        ax.scatter(dhl[\"M\"], dhl[\"P\"], dhl[\"C\"], facecolors=\"none\", edgecolors=\"red\", s=200, linewidths=2.0)\n",
        "\n",
        "    ax.set_xlabel(\"M\"); ax.set_ylabel(\"P\"); ax.set_zlabel(\"C\")\n",
        "    ax.set_title(f\"3D trajectory: head {head_id} (color=layer, red ring=locked)\")\n",
        "    fig.colorbar(sc, ax=ax, shrink=0.6).set_label(\"layer\")\n",
        "    ax.view_init(elev=25, azim=135)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example: plot a couple of heads (edit these indices)\n",
        "for hid in [0, 6]:\n",
        "    plot_head_3d_trajectory(hid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "cellView": "form",
        "id": "GWITAbw5MWUg",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769532783141,
          "user_tz": 300,
          "elapsed": 3781,
          "user": {
            "displayName": "Secondscapes",
            "userId": "04150343613140054698"
          }
        },
        "outputId": "9692a13f-5f4a-41e3-a0f5-d85d8010ee95"
      },
      "id": "GWITAbw5MWUg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Validation & Export\nConfirm canonical tables, export artifacts, and summarize findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Appendix: Narrative Notes\nThe narrative scaffolding and framing preserved from the original notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evolving Story"
      ],
      "metadata": {
        "id": "9QZNqy99WgQe"
      },
      "id": "9QZNqy99WgQe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Core framing\n",
        "We model attention heads as participating in a depth-evolving substrate composed of three orthogonal capacities: memory capacity (M), routing smoothness (P), and routing commitment (C), modulated by a layer-wise second-order control field. Across depth, the model undergoes a phase transition in which memory-bearing substrates emerge, specialize, and consolidate, while routing anchors stabilize policy continuity.\n",
        "Everything below refines this statement.\n",
        "2. Canonical axes (reduced, final)\n",
        "These are the only axes we treat as fundamental.\n",
        "Axis M \u2014 Memory Capacity\n",
        "Definition\n",
        "How much temporal state a head can maintain across tokens.\n",
        "Operationalization\n",
        "Primary: write_ess\n",
        "Secondary (supporting): write_tail_half_life\n",
        "Interpretation\n",
        "High M \u21d2 diffuse, long-horizon write distributions\n",
        "Low M \u21d2 short-lived or sharply localized memory\n",
        "Empirical facts\n",
        "Strongly increases with depth\n",
        "Strongest predictor of role switching\n",
        "Drives cluster lifecycle changes\n",
        "Status\n",
        "Primary adaptive axis\n",
        "Everything else modulates how M is used.\n",
        "Axis P \u2014 Routing Smoothness\n",
        "Definition\n",
        "How smoothly a head\u2019s routing policy evolves over time.\n",
        "Operationalization\n",
        "inertia_mean\n",
        "inertia_slope\n",
        "Interpretation\n",
        "High P \u21d2 routing changes gradually (low curvature in policy space)\n",
        "Low P \u21d2 abrupt routing changes\n",
        "Empirical facts\n",
        "Positively correlated with regime switching\n",
        "Peaks in the same depth window as second-order strength\n",
        "Distinct from routing lock\n",
        "Status\n",
        "Adaptive modulation axis\n",
        "Controls how heads reconfigure, not whether they do.\n",
        "Axis C \u2014 Routing Commitment\n",
        "Definition\n",
        "How decisively a head selects a small subset of slots at each step.\n",
        "Operationalization\n",
        "top4_mass (preferred)\n",
        "tok_ent, eff_slots (redundant)\n",
        "Interpretation\n",
        "High C \u21d2 sharp, decisive routing\n",
        "Low C \u21d2 exploratory or diffuse routing\n",
        "Empirical facts\n",
        "Weak\u2013moderate correlation with switching\n",
        "Largely orthogonal to M\n",
        "Modulates expression, not capacity\n",
        "Status\n",
        "Behavioral style axis\n",
        "Shapes local dynamics without defining substrate role.\n",
        "3. Flags (not axes)\n",
        "These are categorical regime indicators, not continuous dimensions.\n",
        "Flag L \u2014 Routing Lock\n",
        "Definition\n",
        "Whether a head\u2019s routing policy is effectively frozen.\n",
        "Operationalization\n",
        "inertia_half_lag \u2265 threshold (e.g. \u2265 8)\n",
        "Interpretation\n",
        "Locked heads preserve policy identity\n",
        "Unlocked heads can reconfigure\n",
        "Empirical facts\n",
        "Negatively correlated with switching\n",
        "Distinguishes anchors from adaptive carriers\n",
        "Saturates (nonlinear, thresholded)\n",
        "Status\n",
        "Role flag, not a coordinate\n",
        "Never treat as a continuous axis.\n",
        "4. Second-order control field (environment, not a head feature)\n",
        "These are layer-level control signals that shape the substrate.\n",
        "Slotspace Gate\n",
        "Definition\n",
        "Strength of second-order slot\u2013slot interaction.\n",
        "Interpretation\n",
        "High values enable nonlocal coordination across heads\n",
        "Opens the \u201csubstrate formation window\u201d\n",
        "Empirical facts\n",
        "Peaks sharply at L14\u2013L16\n",
        "Coincides with memory expansion and role diversification\n",
        "Content Read Gamma\n",
        "Definition\n",
        "Strength of content-based attention relative to slotspace routing.\n",
        "Interpretation\n",
        "Bias toward content-driven reads\n",
        "Typically peaks earlier than slotspace gate\n",
        "Slotspace Delta Norm\n",
        "Definition\n",
        "Magnitude of second-order correction applied during routing.\n",
        "Interpretation\n",
        "Direct measure of non-first-order dynamics\n",
        "Tracks active substrate reshaping\n",
        "Status of second-order metrics\n",
        "Control field / phase driver\n",
        "These do not define head identity; they define what kinds of identities can exist at a given depth.\n",
        "5. Taxonomy of head roles (final)\n",
        "This is now stable and evidence-backed.\n",
        "1. Adaptive Memory Carriers\n",
        "Signature\n",
        "High M\n",
        "Moderate\u2013high P\n",
        "Low L (unlocked)\n",
        "High switching\n",
        "Function\n",
        "Carry and reshape long-range state\n",
        "Absorb depth-dependent responsibilities\n",
        "Where\n",
        "Emerge in mid-depth\n",
        "Persist into late depth in reduced number\n",
        "2. Routing Anchors\n",
        "Signature\n",
        "Low\u2013moderate M\n",
        "High L (locked)\n",
        "High C\n",
        "Low switching\n",
        "Function\n",
        "Preserve routing semantics\n",
        "Stabilize computation across depth\n",
        "Where\n",
        "Present throughout\n",
        "Become dominant late\n",
        "3. Peripheral / Local Processors\n",
        "Signature\n",
        "Low M\n",
        "Low P\n",
        "Low\u2013moderate C\n",
        "Variable switching\n",
        "Function\n",
        "Token-local or shallow features\n",
        "Opportunistic participation\n",
        "Where\n",
        "Early and mid layers\n",
        "Mostly disappear or get absorbed\n",
        "6. Depth-wise mechanistic narrative (locked)\n",
        "This is the story across layers, grounded in your plots.\n",
        "Phase I \u2014 Local Processing (L0\u2013L3)\n",
        "Low M, weak second-order control\n",
        "Heads largely interchangeable\n",
        "Routing shallow, memory short\n",
        "Function\n",
        "Feature extraction, local patterning\n",
        "Phase II \u2014 Substrate Formation (L8\u2013L15)\n",
        "Second-order gate and gamma peak\n",
        "Rapid expansion of M\n",
        "Head roles diverge sharply\n",
        "High switching and cluster churn\n",
        "Function\n",
        "State construction and abstraction\n",
        "This is where substrates form.\n",
        "Phase III \u2014 Consolidation (L16\u2013L20)\n",
        "Memory concentrates into fewer heads\n",
        "Routing locks engage\n",
        "Switching declines\n",
        "Cluster lifecycles stabilize\n",
        "Function\n",
        "State refinement and decision stabilization\n",
        "7. What is orthogonal vs coupled (final word)\n",
        "Strongly coupled\n",
        "write_ess \u2194 write_tail_half_life\n",
        "inertia_mean \u2194 inertia_slope\n",
        "Weakly coupled\n",
        "M \u2194 P (only during second-order-active layers)\n",
        "Orthogonal\n",
        "M \u27c2 C\n",
        "M \u27c2 L\n",
        "C \u27c2 L\n",
        "Antagonistic\n",
        "L \u2194 switching\n",
        "inertia_half_lag \u2194 adaptivity\n",
        "8. What you should never conflate again\n",
        "Routing smoothness \u2260 routing lock\n",
        "Memory capacity \u2260 routing decisiveness\n",
        "Second-order strength \u2260 head feature\n",
        "Anchor alignment \u2260 early-layer stability\n",
        "These distinctions are now empirically justified.\n",
        "9. Compact glossary (for future reference)\n",
        "Substrate: A stable, functionally coherent head population defined in (M,P,C) space.\n",
        "Adaptive: High-M, unlocked heads that change role across depth.\n",
        "Locked: Heads with saturated routing inertia.\n",
        "Second-order window: Depth region where slotspace gate is high and substrates can reorganize.\n",
        "Anchor basis: Slot index system aligned via Hungarian matching to a reference layer."
      ],
      "metadata": {
        "id": "ykWRywfuAAw3"
      },
      "id": "ykWRywfuAAw3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1) Formal class names\n",
        "\n",
        "Class A \u2014 Adaptive Memory Carriers (AMC)\n",
        "Mechanism: write-memory capacity + unlocked routing + smooth policy evolution\n",
        "Role: carry/reshape long-range state; most depth-plastic\n",
        "Class R \u2014 Locked Routing Anchors (LRA)\n",
        "Mechanism: routing lock + high commitment (sharp routing)\n",
        "Role: preserve routing semantics across depth; stabilizers\n",
        "Class P \u2014 Diffuse Local Processors (DLP)\n",
        "Mechanism: low memory + diffuse routing (low commitment)\n",
        "Role: token-local or \u201cexploratory\u201d routing; auxiliary computation\n",
        "Optional 4th (if you want to distinguish a real subtype that may appear in your data):\n",
        "Class M \u2014 Committed Memory Specialists (CMS)\n",
        "Mechanism: high memory + high commitment but unlocked\n",
        "Role: memory heads that route sharply (often emerges late)\n",
        "You can start with A/R/P, and only introduce CMS if it\u2019s clearly present as a separable population.\n",
        "2) Standardize the variables (so thresholds mean something)\n",
        "You already z-score M/P/C in the visualization cell. Keep that as the formal basis:\n",
        "\ufffd\n",
        "M = z(\\text{write_ess})\n",
        "\ufffd  (or an agreed blend of mean+slope)\n",
        "P = z(\\text{inertia_mean})\n",
        "\ufffd\n",
        "C = z(\\text{top4_mass})\n",
        "\ufffd where \ufffd is your lock threshold (e.g. 8)\n",
        "L = \\mathbb{1}[\\text{inertia_half_lag} \\ge \\tau_L]\n",
        "All conditions below assume M/P/C are z-scored over heads (anchor-basis population).\n",
        "3) Conditions (necessary vs sufficient)\n",
        "There are different ways to do this. The cleanest is:\n",
        "Necessary conditions: must be true, otherwise the class assignment is invalid.\n",
        "Sufficient conditions: if true, you can safely assign the class without needing anything else.\n",
        "Anything in-between = \u201ccandidate\u201d or \u201cborderline\u201d.\n",
        "I\u2019ll give both.\n",
        "A) Adaptive Memory Carriers (AMC)\n",
        "Necessary conditions (AMC)\n",
        "Unlocked: \ufffd\n",
        "High memory: \ufffd\n",
        "Recommended starting threshold:\n",
        "\ufffd (upper ~30% of heads)\n",
        "Sufficient conditions (AMC)\n",
        "\ufffd\n",
        "\ufffd\n",
        "\ufffd\n",
        "Recommended:\n",
        "\ufffd\n",
        "\ufffd\n",
        "Interpretation: strongly memory-capable and policy-smooth without being locked.\n",
        "Optional strengthening (if you want a more \u201cfunctional\u201d definition):\n",
        "Add a behavioral criterion: switches >= s_low (e.g. \u2265 median).\n",
        "But if you want purely mechanistic criteria, keep it in M/P/L only.\n",
        "R) Locked Routing Anchors (LRA)\n",
        "Necessary conditions (LRA)\n",
        "Locked: \ufffd\n",
        "That\u2019s the defining property, because half-lag behaves categorical.\n",
        "Sufficient conditions (LRA)\n",
        "\ufffd\n",
        "\ufffd\n",
        "Recommended:\n",
        "\ufffd (above-average commitment)\n",
        "Optional exclusion (to prevent \u201clocked memory anchors\u201d from being misfiled):\n",
        "\ufffd with \ufffd\n",
        "So a stricter sufficient set is:\n",
        "\ufffd and \ufffd and \ufffd\n",
        "Interpretation: routing is frozen and routing is sharp \u21d2 stable anchor.\n",
        "P) Diffuse Local Processors (DLP)\n",
        "Necessary conditions (DLP)\n",
        "Low memory: \ufffd\n",
        "Diffuse routing: \ufffd\n",
        "Recommended:\n",
        "\ufffd\n",
        "\ufffd\n",
        "Sufficient conditions (DLP)\n",
        "\ufffd\n",
        "\ufffd\n",
        "\ufffd (usually)\n",
        "Recommended:\n",
        "\ufffd\n",
        "\ufffd\n",
        "Interpretation: low memory + low commitment \u21d2 local, non-specialized processing.\n",
        "(Optional) M) Committed Memory Specialists (CMS)\n",
        "This is useful if you see a \u201chigh M, high C, unlocked\u201d pocket.\n",
        "Necessary conditions (CMS)\n",
        "\ufffd\n",
        "\ufffd\n",
        "\ufffd\n",
        "Recommended:\n",
        "\ufffd\n",
        "\ufffd\n",
        "Sufficient conditions (CMS)\n",
        "Same as necessary, plus optionally:\n",
        "\ufffd (not especially smooth) or \ufffd free\n",
        "Interpretation: \u201cbig memory, decisive routing\u201d.\n",
        "4) Tie-break rules (so classification is deterministic)\n",
        "Because real points will sit on boundaries, define a priority order.\n",
        "I recommend:\n",
        "If \ufffd \u2192 assign LRA unless it meets CMS-like memory criteria you explicitly care about.\n",
        "Else if \ufffd is high \u2192 AMC (or CMS if commitment is also high).\n",
        "Else if \ufffd low and \ufffd low \u2192 DLP.\n",
        "Else \u2192 Unclassified / Mixed (this is allowed and honest).\n",
        "Mixed points are actually informative; they often mark the transition window.\n",
        "5) How to pick thresholds without arbitrariness\n",
        "To avoid \u201cwe eyeballed 0.5\u201d, choose thresholds by quantiles:\n",
        "\ufffd = 70th or 75th percentile of M\n",
        "\ufffd  = 60th percentile\n",
        "\ufffd = 60th percentile of C\n",
        "\ufffd  = 40th percentile\n",
        "\ufffd  = 50th percentile of P\n",
        "And lock threshold:\n",
        "\ufffd chosen by inspecting bimodality of inertia_half_lag (you already saw saturation). A good formal rule is: \ufffd = smallest value where the tail mass drops sharply (knee).\n",
        "That makes the taxonomy data-driven but still interpretable.\n",
        "6) Compact formal definition sheet (copy/paste)\n",
        "AMC: \ufffd and \ufffd and \ufffd\n",
        "LRA: \ufffd and \ufffd (and optionally \ufffd)\n",
        "DLP: \ufffd and \ufffd and \ufffd\n",
        "CMS (optional): \ufffd and \ufffd and \ufffd"
      ],
      "metadata": {
        "id": "I0TMt51HBGtz"
      },
      "id": "I0TMt51HBGtz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Canonical \u201cone sequence\u201d ASA story diagram (Stage \u2192 tensors \u2192 metrics \u2192 interpretation)\n",
        "Input\n",
        "x[B,T,C]\n",
        "Token stream entering layer.\n",
        "1) Projections\n",
        "k_write = Wk_write(x) \u2192 [B,H,T,d]\n",
        "v_write = Wv_write(x) \u2192 [B,H,T,d]\n",
        "q_read  = Wq_read(x)  \u2192 [B,H,T,d]\n",
        "(optional) k_write = RoPE(k_write) and/or normalize_k(k_write)\n",
        "Enables: geometry probes (norms, cosine structure, head anisotropy)\n",
        "Meaning: \u201cwhat the token offers to memory\u201d (k/v) and \u201cwhat it asks\u201d (q)\n",
        "2) Addressing for writes\n",
        "slot_keys[H,K,d] (learned)\n",
        "write_logits_raw[b,h,k,t] = <slot_keys[h,k], k_write[b,h,t]> / sqrt(d)\n",
        "write_logits = write_logits_raw / write_temperature\n",
        "(optional) write_logits += ALiBi(h,t) (+ mask)\n",
        "Enables: write distribution p_write(t|h,k) \u2192 M metrics (ESS, tail half-life)\n",
        "Meaning: how much each past token is \u201celigible\u201d to live in each slot\n",
        "3) Online addressed-state scan (slot memory formation) State per head-slot:\n",
        "m_state[B,H,K], denom_state[B,H,K], numer_state[B,H,K,d] Outputs:\n",
        "slot_state_t[B,H,t,K,d] (implicit/explicit)\n",
        "slot_state_norm[B,H,K,t] (if logged)\n",
        "Enables: slot-state stability/drift, per-slot energy, write-time influence traces\n",
        "Meaning: \u201cwhat each slot contains\u201d at each time\n",
        "4) Read logits: policy formation\n",
        "read_logits_key[b,h,t,k] = <q_read[b,h,t], slot_keys[h,k]> / sqrt(d)\n",
        "(optional) read_logits_content[b,h,t,k] = <q_read[b,h,t], slot_state_t[b,h,t,k]> / sqrt(d)\n",
        "read_logits = read_logits_key + gamma * read_logits_content\n",
        "gamma = softplus(_content_read_gamma_raw) (clamped)\n",
        "Enables: policy decomposition key-vs-content dominance, gating diagnostics\n",
        "Meaning: \u201cwhere should I look?\u201d and optionally \u201cwhat matches the current slot contents?\u201d\n",
        "5) Routing policy (read weights)\n",
        "read_weights[b,h,t,k] = softmax(read_logits / read_temperature)\n",
        "Enables:\n",
        "C commitment: topk_mass, entropy, eff_slots\n",
        "P/L persistence: inertia curve vs lag \u2192 inertia_mean/slope/half_lag (+ lock flag)\n",
        "Meaning: the actual policy over slots per token\n",
        "6) First-order readout\n",
        "out1[b,h,t,d] = \u03a3_k read_weights[b,h,t,k] * slot_state_t[b,h,t,k,d]\n",
        "out = out_proj( concat_heads(out1) )\n",
        "Enables: contribution analysis, head-wise functional outputs, residual comparisons\n",
        "Meaning: addressed memory read (baseline ASA behavior)\n",
        "7) Second-order slotspace refine (control-field injection) Inputs:\n",
        "read_weights lifted into slotspace:\n",
        "u = slot_in(read_weights) \u2192 [B,H,T,M]\n",
        "q_s,k_s,v_s = slot_q/k/v(u) \u2192 [B,H,T,M]\n",
        "(optional) RoPE_slotspace(q_s,k_s)\n",
        "qf = phi(q_s), kf = phi(k_s) Online scan:\n",
        "S_state[B,H,M,M], Z_state[B,H,M]\n",
        "u2[B,H,T,M] (slotspace aggregated representation) Project back to slots:\n",
        "slot_w = slot_out(u2) \u2192 [B,H,T,K]\n",
        "slot_w_eff = tanh(slot_w) (signed) or softmax(slot_w) (unsigned) Correction:\n",
        "delta[b,h,t,d] = \u03a3_k slot_w_eff[b,h,t,k] * slot_state_t[b,h,t,k,d]\n",
        "gate = softplus(_slotspace_gate_raw)\n",
        "out_h = out1 + gate * delta Logged:\n",
        "slotspace_delta_norm (mean \u2016delta\u2016)\n",
        "Enables: \u201csecond-order strength\u201d vs behavior correlations; where substrate emerges\n",
        "Meaning: policy-conditioned cross-slot mixing added on top of first-order read\n",
        "Part 2 \u2014 Exhaustive tensor/intermediate inventory + the metrics they enable\n",
        "Below is a forward-pass tensor ledger in three tiers:\n",
        "Tier A: core primitives (already heavily studied)\n",
        "Tier B: under-used but high-value tensors you\u2019re already logging or can log cheaply\n",
        "Tier C: \u201cmissed\u201d tensors that are not always logged but unlock new mechanistic metrics\n",
        "I\u2019ll include: shape, where it lives, metrics enabled (existing + suggested), and why it matters.\n",
        "Tier A \u2014 Core ASA primitives (you already use)\n",
        "Parameters\n",
        "slot_keys[H,K,d]\n",
        "Metrics\n",
        "slot-key geometry: pairwise cosine, isotropy, clustering, drift across depth\n",
        "key \u201cspecialization\u201d: nearest-neighbor stability across layers (anchor basis)\n",
        "Interpretation: slot identity basis / addressing vocabulary.\n",
        "Wk_write, Wv_write, Wq_read (linear maps)\n",
        "Metrics\n",
        "projection norm / singular spectrum per layer (capacity + conditioning)\n",
        "head-wise q/k/v scale (calibration)\n",
        "Interpretation: defines what information routes/writes.\n",
        "Activations\n",
        "k_write[B,H,T,d] (after RoPE/normalize_k)\n",
        "Existing metrics: (mostly implicit)\n",
        "New metrics\n",
        "key norm distribution vs depth (saturation / collapse)\n",
        "token-to-token key autocorrelation (temporal smoothness before routing)\n",
        "alignment with slot keys: max cosine per token \u2192 \u201caddressability\u201d\n",
        "Interpretation: token\u2019s address features.\n",
        "write_logits[B,H,K,T] (+ write_logits_raw)\n",
        "Existing metrics\n",
        "write_tail_half_life (tail mass HL)\n",
        "write_ess\n",
        "New metrics\n",
        "write entropy over time (per slot/head)\n",
        "write \u201crecency bias index\u201d: corr(logit, t) or avg(ALiBi effect / logit std)\n",
        "per-head slot competition: Gini over slots of total write mass\n",
        "Interpretation: which past tokens accumulate into which slots.\n",
        "slot_state_t[B,H,T,K,d] (implicit, but you compute it)\n",
        "Existing metrics\n",
        "slot_state_norm\n",
        "New metrics\n",
        "slot-state drift: \u2016s(t)-s(t-1)\u2016 (per slot/head)\n",
        "slot-state staleness: similarity of s(t) to s(t-\u03c4)\n",
        "slot \u201cwrite responsiveness\u201d: corr(\u2016\u0394s\u2016, peak write mass)\n",
        "Interpretation: actual memory contents and their dynamics.\n",
        "read_logits_key[B,H,T,K], read_logits_content[B,H,T,K], read_logits[B,H,T,K]\n",
        "Existing metrics: not explicitly separated in your substrate axes\n",
        "New metrics\n",
        "key/content dominance: Var(key) vs Var(gamma*content)\n",
        "policy sensitivity: \u2202read_logits/\u2202gamma approximations via finite differences\n",
        "\u201ccontent-use fraction\u201d: mean |gammacontent| / (|key|+|gammacontent|)\n",
        "Interpretation: whether policy is address-driven vs memory-content-driven.\n",
        "read_weights[B,H,T,K]\n",
        "Existing metrics\n",
        "tok_ent, eff_slots, top4_mass\n",
        "routing inertia curve \u2192 inertia_lag1/mean/slope/half_lag\n",
        "New metrics\n",
        "policy volatility: mean TV distance between p(t) and p(t-1)\n",
        "slot \u201cstickiness\u201d: P(argmax(t)==argmax(t-1))\n",
        "exploration bursts: frequency of entropy spikes\n",
        "Interpretation: the routing policy itself (C/P/L live here).\n",
        "out1[B,H,T,d] (first-order output)\n",
        "Existing metrics: not yet central in substrate axes\n",
        "New metrics\n",
        "per-head output energy; contribution to final residual stream\n",
        "stability of out1 across depth in anchor head basis\n",
        "Interpretation: what memory read contributes pre-refine.\n",
        "Tier B \u2014 Already present gates/strengths (low-hanging mechanistic anchors)\n",
        "content_read_gamma (scalar per layer)\n",
        "Existing: plotted in your second-order cell\n",
        "New metrics\n",
        "cross-layer coupling: gamma vs (P, C, M) slopes\n",
        "\u201ccontent reliance index\u201d: gamma * mean(|read_logits_content|)\n",
        "Interpretation: how much memory content influences routing.\n",
        "alibi_strength (scalar per layer if learnable)\n",
        "New metrics\n",
        "effective recency: alibi_strength * slope statistics of write logits vs t\n",
        "explain HL/ESS changes via alibi strength\n",
        "Interpretation: explicit imposed recency bias on memory.\n",
        "slotspace_gate (scalar per layer)\n",
        "Existing: your \u201csecond-order strength\u201d\n",
        "New metrics\n",
        "gate * delta_norm (\u201capplied second-order energy\u201d)\n",
        "gate vs cluster instability / switching rates\n",
        "Interpretation: how strongly slotspace correction is injected.\n",
        "slotspace_delta_norm (scalar per layer if logged)\n",
        "Existing: optional logging\n",
        "New metrics\n",
        "second-order dominance: delta_norm / out1_norm\n",
        "per-head delta_norm if you log per-head (recommended)\n",
        "Interpretation: how large the refine correction actually is.\n",
        "Tier C \u2014 \u201cMissed\u201d tensors that unlock the next round of metrics\n",
        "These are the ones I\u2019d prioritize to log (even temporarily) because they bridge mechanism \u2192 substrate.\n",
        "Write-side decomposition\n",
        "alibi_bias_applied[1,H,1,T]\n",
        "Metrics\n",
        "per-head bias magnitude relative to write_logits std\n",
        "HL/ESS predicted from bias alone (control)\n",
        "Why: separates learned addressing from imposed time bias.\n",
        "p_write[b,h,k,t] explicitly (from log_softmax(write_logits, dim=-1))\n",
        "Metrics\n",
        "full temporal entropy, KL between slots, temporal concentration profiles\n",
        "Why: HL/ESS are summaries; the whole distribution reveals regimes (bi/tri-modal time focus).\n",
        "Policy decomposition\n",
        "read_logits_key vs gamma*read_logits_content (already available)\n",
        "Metrics\n",
        "per-head \u201cpolicy source\u201d: % decisions explained by key term alone\n",
        "disagreement rate: argmax(key) \u2260 argmax(total)\n",
        "Why: tells you whether \u201cpolicy inertia\u201d is geometric (keys) or semantic (content).\n",
        "Slotspace internals (refine mechanism)\n",
        "slot_w[B,H,T,K] (pre-tanh)\n",
        "Metrics\n",
        "signed vs unsigned behavior; saturation of tanh\n",
        "slotspace-induced mixing entropy (over K) distinct from read policy entropy\n",
        "\u201ccontrol authority\u201d: variance of slot_w over time\n",
        "Why: read_weights is policy; slot_w is control.\n",
        "u, q_s, k_s, v_s (slotspace lifted signals)\n",
        "Metrics\n",
        "slotspace feature anisotropy, RoPE effects, stability across depth\n",
        "Why: explains when/why second-order kicks in.\n",
        "u2[B,H,T,M] (slotspace aggregated)\n",
        "Metrics\n",
        "effective memory length in slotspace (HL/ESS analog but in M-dim)\n",
        "second-order inertia (compare u2(t) similarity across lag)\n",
        "Why: might be the \u201csubstrate manifold\u201d carrier more than M/P/C alone.\n",
        "Output comparison\n",
        "out_h before vs after refine (or delta per head/time)\n",
        "Metrics\n",
        "refinement angle: cosine(out1, delta)\n",
        "orth/par decomposition (you already have in Intervene variant)\n",
        "per-layer fraction of heads where delta is mostly orthogonal\n",
        "Why: this directly links to \u201ccontrol vs content\u201d and regime switching.\n",
        "Metrics ledger (existing vs \u201cleft to get\u201d) aligned to your M/P/C/L vocabulary\n",
        "M \u2014 Memory capacity / temporal support (write-side)\n",
        "Existing\n",
        "write_ess (primary)\n",
        "write_tail_half_life (secondary)\n",
        "Left to get (recommended)\n",
        "write entropy over time (per head/slot)\n",
        "write bimodality / multimodality score (peaks in p_write(t))\n",
        "per-slot \u201cage profile\u201d (expected age E[T - t] per slot)\n",
        "P \u2014 Policy smoothness (routing dynamics)\n",
        "Existing\n",
        "inertia_mean, inertia_slope, inertia_half_lag (good)\n",
        "Left to get\n",
        "policy volatility: mean TV(p(t), p(t-1))\n",
        "policy stickiness: P(argmax stable)\n",
        "inertia conditional on high-confidence tokens (filter by top1 prob)\n",
        "C \u2014 Commitment / decisiveness (policy concentration)\n",
        "Existing\n",
        "top4_mass, tok_ent, eff_slots\n",
        "Left to get\n",
        "margin: top1 - top2\n",
        "\u201chardness\u201d index: E[max_k p(k)]\n",
        "entropy conditioned on depth phase (early/mid/late tokens)\n",
        "L \u2014 Lock flag (categorical regime marker)\n",
        "Existing\n",
        "locked = inertia_half_lag \u2265 threshold\n",
        "Left to get\n",
        "two-threshold hysteresis lock (avoid boundary jitter):\n",
        "lock if \u2265 \u03c4_hi, unlock if \u2264 \u03c4_lo\n",
        "lock explained by source:\n",
        "\u201ckey-locked\u201d if key-only inertia high\n",
        "\u201ccontent-locked\u201d if content-driven inertia high\n",
        "What I would log next (minimal set, maximal leverage)\n",
        "If you want the smallest set of new tensors to log to close the mechanistic loop:\n",
        "read_logits_key and read_logits_content (you already store them in full mode)\n",
        "slot_w (pre-tanh) and/or slot_w_eff (post-tanh)\n",
        "delta (or at least per-head delta_norm, and cosine(out1, delta))\n",
        "alibi_bias_applied (to separate time prior from learned addressing)\n",
        "That gives you direct tests of:\n",
        "whether locking is key-driven vs content-driven,\n",
        "whether second-order acts as \u201ccontrol authority,\u201d\n",
        "and whether memory capacity is caused by (learned) slot assignment vs (imposed) ALiBi."
      ],
      "metadata": {
        "id": "XFPsSBBSFkkp"
      },
      "id": "XFPsSBBSFkkp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "ASA Forward-Pass Ledger\n",
        "(Tensor \u2192 Mechanism \u2192 Metrics \u2192 Substrate role)\n",
        "Legend:\n",
        "Tier A = foundational / already central\n",
        "Tier B = logged or cheap to log, high explanatory power\n",
        "Tier C = not always logged, but unlocks missing mechanisms\n",
        "0. Input & Projections\n",
        "Tensor\n",
        "Shape\n",
        "Tier\n",
        "Meaning\n",
        "Enables Metrics\n",
        "x\n",
        "[B,T,C]\n",
        "A\n",
        "Incoming token stream\n",
        "token-conditioned routing, depth trends\n",
        "k_write\n",
        "[B,H,T,d]\n",
        "A\n",
        "What token offers to memory\n",
        "key norm, temporal smoothness\n",
        "v_write\n",
        "[B,H,T,d]\n",
        "A\n",
        "What token stores\n",
        "value energy, slot contribution\n",
        "q_read\n",
        "[B,H,T,d]\n",
        "A\n",
        "What token asks for\n",
        "query sharpness, policy sensitivity\n",
        "Notes\n",
        "All memory and routing behavior ultimately conditions on the geometry here.\n",
        "Any collapse or anisotropy here propagates downstream.\n",
        "1. Write Addressing (Memory Formation)\n",
        "Tensor\n",
        "Shape\n",
        "Tier\n",
        "Meaning\n",
        "Enables Metrics\n",
        "slot_keys\n",
        "[H,K,d]\n",
        "A\n",
        "Slot identity basis\n",
        "slot specialization, clustering\n",
        "write_logits_raw\n",
        "[B,H,K,T]\n",
        "A\n",
        "Raw eligibility of tokens per slot\n",
        "temporal preference\n",
        "write_logits\n",
        "[B,H,K,T]\n",
        "A\n",
        "Temperature + bias-adjusted writes\n",
        "write entropy\n",
        "alibi_bias_applied\n",
        "[1,H,1,T]\n",
        "C\n",
        "Explicit recency prior\n",
        "explain HL / ESS\n",
        "p_write (derived)\n",
        "[B,H,K,T]\n",
        "C\n",
        "Write distribution over time\n",
        "multimodality, age profile\n",
        "Primary M-metrics\n",
        "write_ess\n",
        "write_tail_half_life\n",
        "Secondary (recommended)\n",
        "write entropy vs time\n",
        "bimodality / peak count in p_write(t)\n",
        "expected slot age E[T\u2212t]\n",
        "2. Slot State (Online Memory Scan)\n",
        "Tensor\n",
        "Shape\n",
        "Tier\n",
        "Meaning\n",
        "Enables Metrics\n",
        "slot_state_t\n",
        "[B,H,T,K,d]\n",
        "A\n",
        "Slot contents over time\n",
        "memory drift\n",
        "slot_state_norm\n",
        "[B,H,K,T]\n",
        "A\n",
        "Slot energy\n",
        "stability, saturation\n",
        "\u0394slot_state (derived)\n",
        "same\n",
        "C\n",
        "Update magnitude\n",
        "write responsiveness\n",
        "Interpretation\n",
        "This is memory. Everything else is policy or control.\n",
        "Slow drift + long write HL \u21d2 persistent memory substrate.\n",
        "3. Read Logits (Policy Formation)\n",
        "Tensor\n",
        "Shape\n",
        "Tier\n",
        "Meaning\n",
        "Enables Metrics\n",
        "read_logits_key\n",
        "[B,H,T,K]\n",
        "A\n",
        "Address-based routing\n",
        "key dominance\n",
        "read_logits_content\n",
        "[B,H,T,K]\n",
        "A\n",
        "Content-based routing\n",
        "semantic pull\n",
        "content_read_gamma\n",
        "scalar\n",
        "B\n",
        "Content gating strength\n",
        "content reliance\n",
        "read_logits\n",
        "[B,H,T,K]\n",
        "A\n",
        "Final routing scores\n",
        "policy margin\n",
        "Key missing but crucial metrics\n",
        "key vs content variance ratio\n",
        "argmax disagreement: argmax(key) \u2260 argmax(total)\n",
        "effective policy source (% content-driven)\n",
        "4. Read Weights (Routing Policy)\n",
        "Tensor\n",
        "Shape\n",
        "Tier\n",
        "Meaning\n",
        "Enables Metrics\n",
        "read_weights\n",
        "[B,H,T,K]\n",
        "A\n",
        "Policy distribution over slots\n",
        "C / P / L\n",
        "entropy(p)\n",
        "[B,H,T]\n",
        "A\n",
        "Policy uncertainty\n",
        "commitment\n",
        "topk_mass\n",
        "[B,H,T]\n",
        "A\n",
        "Concentration\n",
        "decisiveness\n",
        "\u0394p(t) (derived)\n",
        "[B,H,T]\n",
        "B\n",
        "Policy change\n",
        "volatility\n",
        "Primary metrics\n",
        "C: tok_ent, eff_slots, top4_mass\n",
        "P: inertia curve, inertia_half_lag\n",
        "L: locked flag (derived from P)\n",
        "Recommended additions\n",
        "TV distance: \u00bd\u2016p(t)\u2212p(t\u22121)\u2016\u2081\n",
        "argmax stability probability\n",
        "entropy spikes (exploration bursts)\n",
        "5. First-Order Readout (Baseline ASA)\n",
        "Tensor\n",
        "Shape\n",
        "Tier\n",
        "Meaning\n",
        "Enables Metrics\n",
        "out1\n",
        "[B,H,T,d]\n",
        "B\n",
        "Memory read before refine\n",
        "contribution analysis\n",
        "out\n",
        "[B,T,C]\n",
        "A\n",
        "Layer output\n",
        "residual influence\n",
        "Interpretation\n",
        "This is pure addressed memory with no higher-order control.\n",
        "Serves as the reference vector for refine geometry.\n",
        "6. Slotspace Refine (Second-Order Control)\n",
        "Lift + Aggregate\n",
        "Tensor\n",
        "Shape\n",
        "Tier\n",
        "Meaning\n",
        "u = slot_in(read_weights)\n",
        "[B,H,T,M]\n",
        "B\n",
        "Policy lifted into slotspace\n",
        "q_s,k_s,v_s\n",
        "[B,H,T,M]\n",
        "C\n",
        "Slotspace features\n",
        "u2\n",
        "[B,H,T,M]\n",
        "C\n",
        "Aggregated control state\n",
        "Project Back\n",
        "Tensor\n",
        "Shape\n",
        "Tier\n",
        "Meaning\n",
        "Enables Metrics\n",
        "slot_w\n",
        "[B,H,T,K]\n",
        "B\n",
        "Slot mixing weights\n",
        "control entropy\n",
        "slot_w_eff\n",
        "[B,H,T,K]\n",
        "B\n",
        "Signed / normalized mixing\n",
        "cross-slot coupling\n",
        "delta\n",
        "[B,H,T,d]\n",
        "B\n",
        "Correction vector\n",
        "control strength\n",
        "slotspace_gate\n",
        "scalar\n",
        "B\n",
        "Control gain\n",
        "second-order energy\n",
        "Key metrics\n",
        "slotspace_delta_norm\n",
        "delta_norm / out1_norm\n",
        "cosine(out1, delta)\n",
        "per-head delta dominance\n",
        "7. Final Composition\n",
        "Quantity\n",
        "Meaning\n",
        "out_h = out1 + gate * delta\n",
        "Memory + control synthesis\n",
        "out_proj(out_h)\n",
        "Returns to model space\n",
        "Interpretation\n",
        "If delta \u27c2 out1 \u2192 true control\n",
        "If delta \u2225 out1 \u2192 amplification / sharpening\n",
        "Gate \u2248 0 \u21d2 pure ASA\n",
        "Gate large + stable policy \u21d2 substrate formation\n",
        "Mapping to Your Substrate Axes\n",
        "Axis\n",
        "Primary Tensors\n",
        "Secondary Tensors\n",
        "M (memory span)\n",
        "write_logits, p_write, slot_state\n",
        "alibi_bias\n",
        "P (policy persistence)\n",
        "read_weights\n",
        "read_logits_key/content\n",
        "C (commitment)\n",
        "read_weights\n",
        "slot_w\n",
        "L (lock)\n",
        "inertia metrics\n",
        "key/content dominance\n",
        "2nd-order strength\n",
        "delta, slotspace_gate\n",
        "u2, slot_w\n",
        "Minimal \u201cnext logging set\u201d (high ROI)\n",
        "If you add only four things, make it these:\n",
        "read_logits_key vs read_logits_content\n",
        "slot_w (pre-tanh)\n",
        "delta (or per-head delta_norm + cosine(out1,delta))\n",
        "alibi_bias_applied"
      ],
      "metadata": {
        "id": "jWNTDI7gG54D"
      },
      "id": "jWNTDI7gG54D"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "0) Canonical inputs (head\u00d7layer table + environment table)\n",
        "Per head\u00d7layer rows (df_hl, shape ~ [L*H, \u2026]):\n",
        "layer (int)\n",
        "head (int)\n",
        "M, P, C  (already z-scored in your table)\n",
        "inertia_half_lag (raw)\n",
        "locked (optional; otherwise computed)\n",
        "switches (optional; for size only)\n",
        "Per layer rows (env table, shape ~ [L, \u2026]):\n",
        "layer (int)\n",
        "E_slot (z-scored)  # derived from gate * delta_norm (or gate if delta missing)\n",
        "E_gamma (z-scored) # derived from content_read_gamma\n",
        "1) Thresholds (final, explicit)\n",
        "1.1 Lock threshold (head-local, raw metric)\n",
        "LOCK_THRESHOLD = 8.0\n",
        "locked = (inertia_half_lag >= LOCK_THRESHOLD)\n",
        "Rationale: you already use 8.0 and it cleanly separates adaptive vs locked heads in your plots.\n",
        "1.2 Substrate region thresholds (using z-scored M/P/C)\n",
        "Use fixed quantiles computed globally over all head\u00d7layer points in df_hl (not per-layer):\n",
        "q_lo = 0.30\n",
        "q_hi = 0.70\n",
        "For each axis X in {M,P,C}:\n",
        "X_lo = quantile(df_hl[X], q_lo)\n",
        "X_hi = quantile(df_hl[X], q_hi)\n",
        "Then define:\n",
        "X is LOW  if X <= X_lo\n",
        "X is HIGH if X >= X_hi\n",
        "X is MID  otherwise\n",
        "Notes:\n",
        "This avoids arbitrary \u201c\u00b11\u201d z-score cutoffs and adapts to whatever the actual spread is.\n",
        "Using 30/70 gives stable, non-fragile bins with enough points.\n",
        "(If you want slightly sharper categories, use 0.25/0.75. But 0.30/0.70 is usually better for small models.)\n",
        "1.3 Field regime thresholds (layer fields, already z-scored)\n",
        "Use fixed z-score bands (simple and interpretable):\n",
        "FIELD_POS = +0.5\n",
        "FIELD_NEG = -0.5\n",
        "For each layer l:\n",
        "gamma_state:\n",
        "gamma_high if E_gamma >= +0.5\n",
        "gamma_low  if E_gamma <= -0.5\n",
        "gamma_mid  otherwise\n",
        "slot_state:\n",
        "slot_high if E_slot >= +0.5\n",
        "slot_low  if E_slot <= -0.5\n",
        "slot_mid  otherwise\n",
        "Then define a single \u201cfield regime\u201d label per layer:\n",
        "LOW_FIELD:    slot_low and gamma_low\n",
        "GAMMA_DOM:    gamma_high and not slot_high   (slot_mid or slot_low)\n",
        "HANDOFF:      gamma_high and slot_high\n",
        "SLOT_DOM:     slot_high and not gamma_high   (gamma_mid or gamma_low)\n",
        "MIXED_MID:    everything else (mostly mid bands)\n",
        "Notes:\n",
        "This is robust to noise and keeps \u201chandoff\u201d crisp.\n",
        "If you want exactly one regime always, keep MIXED_MID; don\u2019t drop it.\n",
        "2) Depth phases (derived from field regimes + drift)\n",
        "These are layer-level labels (apply to each head\u00d7layer point by join on layer).\n",
        "You already compute drift_mean per layer; use that too.\n",
        "2.1 Compute drift regime (layer-level)\n",
        "Let drift_mean(l) = mean over heads of ||\u0394(M,P,C)|| from layer l-1 to l (you already have this).\n",
        "Define drift thresholds by quantiles across layers:\n",
        "D_hi = quantile(drift_mean, 0.70)\n",
        "D_lo = quantile(drift_mean, 0.30)\n",
        "Then:\n",
        "drift_high if drift_mean >= D_hi\n",
        "drift_low  if drift_mean <= D_lo\n",
        "drift_mid  otherwise\n",
        "2.2 Phase labels (layer-level)\n",
        "Assign per layer in priority order:\n",
        "PHASE_FORMATION\n",
        "Condition: field regime is LOW_FIELD AND drift_high\n",
        "PHASE_REMODELING\n",
        "Condition: field regime is GAMMA_DOM AND not drift_low\n",
        "PHASE_HANDOFF\n",
        "Condition: field regime is HANDOFF\n",
        "PHASE_CONSOLIDATION\n",
        "Condition: field regime is SLOT_DOM AND drift_low\n",
        "PHASE_TERMINAL_TUNING\n",
        "Condition: field regime is SLOT_DOM AND not drift_low (or simply: late layers after consolidation begins, if you want a depth override)\n",
        "Fallback:\n",
        "PHASE_MIXED for MIXED_MID or anything not matched above.\n",
        "This matches your story: early drift high, gamma peaks mid, slot ramps late, drift suppressed during consolidation.\n",
        "3) Substrate classes (head\u00d7layer labels)\n",
        "Each class is defined as:\n",
        "A region in M/P/C bins (LOW/MID/HIGH)\n",
        "a field regime (layer-level)\n",
        "optionally locked vs adaptive\n",
        "Use these exact names so the taxonomy is stable.\n",
        "Class A: EXPLORATORY_ROUTER\n",
        "Conditions:\n",
        "P is LOW\n",
        "C is LOW\n",
        "locked == 0\n",
        "field regime in {LOW_FIELD, GAMMA_DOM, MIXED_MID}\n",
        "Interpretation: adaptive, low inertia + low commitment\n",
        "Class B: SEMANTIC_SPECIALIST\n",
        "Conditions:\n",
        "C is HIGH\n",
        "P is not HIGH (P is LOW or MID)\n",
        "locked == 0\n",
        "field regime in {GAMMA_DOM, HANDOFF}\n",
        "Interpretation: high commitment under content shaping\n",
        "Class C: HIGH_CAPACITY_INTEGRATOR\n",
        "Conditions:\n",
        "M is HIGH\n",
        "locked == 0\n",
        "field regime in {LOW_FIELD, GAMMA_DOM, MIXED_MID}\n",
        "Interpretation: broad memory capacity, still adaptive\n",
        "Class D: POLICY_ANCHOR\n",
        "Conditions:\n",
        "P is HIGH\n",
        "C is HIGH\n",
        "locked == 1\n",
        "field regime in {SLOT_DOM, HANDOFF}\n",
        "Interpretation: stable policy carrier\n",
        "Class E: FROZEN_HIGH_CAPACITY_ANCHOR\n",
        "Conditions:\n",
        "M is HIGH\n",
        "P is HIGH\n",
        "locked == 1\n",
        "field regime in {SLOT_DOM, HANDOFF}\n",
        "Interpretation: high capacity but stabilized/locked\n",
        "Class F: BRITTLE_LOCKER\n",
        "Conditions:\n",
        "M is LOW\n",
        "P is HIGH\n",
        "C is HIGH\n",
        "locked == 1\n",
        "field regime in {SLOT_DOM, HANDOFF}\n",
        "Interpretation: high commitment + high inertia but low capacity\n",
        "Class G: TRANSITIONER\n",
        "Conditions:\n",
        "field regime == HANDOFF\n",
        "and not already classified as D/E/F\n",
        "Interpretation: heads undergoing handoff but not yet in anchor basins\n",
        "Fallback:\n",
        "CLASS_OTHER\n",
        "Tie-break order (important)\n",
        "Assign in this order so classes are mutually exclusive:\n",
        "E (Frozen high-capacity anchor)\n",
        "F (Brittle locker)\n",
        "D (Policy anchor)\n",
        "G (Transitioner)\n",
        "C (High-capacity integrator)\n",
        "B (Semantic specialist)\n",
        "A (Exploratory router)\n",
        "OTHER\n",
        "Reason: locked classes should dominate; handoff is a regime, not a geometry, so it\u2019s later; capacity integrator should beat specialist/router.\n",
        "4) Minimal \u201cregime table\u201d you can print in logs\n",
        "For each layer l, print:\n",
        "layer\n",
        "E_slot, E_gamma\n",
        "field_regime\n",
        "drift_mean, drift_regime\n",
        "phase\n",
        "For each head\u00d7layer point, print:\n",
        "layer, head\n",
        "M,P,C bins\n",
        "locked\n",
        "class"
      ],
      "metadata": {
        "id": "HM8i8GzAR5SF"
      },
      "id": "HM8i8GzAR5SF"
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "9QZNqy99WgQe"
      ],
      "provenance": [
        {
          "file_id": "https://github.com/digitaldaimyo/ASA/blob/main/notebooks/ASA_HF_Checkpoint_CanonProbes_and_MiniFinetune.ipynb",
          "timestamp": 1769473209559
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}