{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ASA HF Checkpoint + Canon Probes + Mini Fact Finetune\n",
        "\n",
        "CPU-only Colab notebook: load the HF checkpoint, run canonical probes, optionally\n",
        "do a tiny finetune on a handful of examples, and re-run probes to show before/after deltas.\n",
        "\n",
        "**Expected runtime:** a few minutes on CPU.\n",
        "\n",
        "**Notes:**\n",
        "- Uses a tiny synthetic QA dataset for the finetune step (can be skipped).\n",
        "- Saves JSON artifacts in `artifacts/` for quick inspection.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Section 0 \u2014 Setup\n",
        "import os, sys, subprocess, platform, json, time, random\n",
        "from pathlib import Path\n",
        "\n",
        "repo_dir = 'ASA'\n",
        "if not Path(repo_dir).exists():\n",
        "    subprocess.run(['git','clone','https://github.com/digitaldaimyo/ASA.git'], check=True)\n",
        "os.chdir(repo_dir)\n",
        "\n",
        "subprocess.run([sys.executable,'-m','pip','install','-e','.'], check=True)\n",
        "subprocess.run([sys.executable,'-m','pip','install','-q','huggingface_hub','safetensors','transformers'], check=True)\n",
        "\n",
        "import torch\n",
        "from huggingface_hub import hf_hub_download\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "device = torch.device('cpu')\n",
        "print('Python:', platform.python_version())\n",
        "print('Torch:', torch.__version__)\n",
        "print('Device:', device)\n",
        "try:\n",
        "    commit = subprocess.check_output(['git','rev-parse','HEAD']).decode().strip()\n",
        "    print('Repo commit:', commit)\n",
        "except Exception:\n",
        "    print('Repo commit: unavailable')\n",
        "\n",
        "seed = 1337\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "artifacts_dir = Path('artifacts')\n",
        "artifacts_dir.mkdir(exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Section 1 \u2014 Load base model from Hugging Face (Baseline)\n",
        "from asa.load_pretrained import load_pretrained\n",
        "\n",
        "HF_REPO = 'DigitalShogun/ASA-ASM-wikitext103-raw'\n",
        "DEFAULT_CKPT = 'ASA_ASM_wt103-rawv1_gpt2_T1024_L21_D384_H8_K16_M32_ropek1_alibi1_gamma1_step75000_best.pt'\n",
        "\n",
        "model, report, cfg_obj = load_pretrained(HF_REPO, DEFAULT_CKPT, variant='baseline', device='cpu')\n",
        "print('Loaded model with vocab_size:', cfg_obj.vocab_size)\n",
        "print('Checkpoint source:', report['state_dict_source'])\n",
        "print('Allowlisted gaps:', {\n",
        "    'missing': len(report['allowed_missing']),\n",
        "    'unexpected': len(report['allowed_unexpected']),\n",
        "    'mismatched': len(report['allowed_mismatched']),\n",
        "})\n",
        "\n",
        "input_ids = torch.randint(0, cfg_obj.vocab_size, (1, 32))\n",
        "with torch.no_grad():\n",
        "    logits, _ = model(input_ids)\n",
        "print('Logits shape:', tuple(logits.shape))\n",
        "assert logits.shape == (1, 32, cfg_obj.vocab_size)\n",
        "assert torch.isfinite(logits).all()\n",
        "\n",
        "run_metadata = {\n",
        "    'repo': HF_REPO,\n",
        "    'checkpoint': DEFAULT_CKPT,\n",
        "    'state_dict_source': report['state_dict_source'],\n",
        "    'seed': seed,\n",
        "    'timestamp': time.time(),\n",
        "    'config': cfg_obj.__dict__,\n",
        "}\n",
        "(artifacts_dir / 'run_metadata.json').write_text(json.dumps(run_metadata, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Section 2 \u2014 Canon Probes (BEFORE finetune)\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "\n",
        "PROMPTS = [\n",
        "    'The capital of France is',\n",
        "    \"France's capital city is\",\n",
        "    'Paris is the capital of',\n",
        "    'The capital of the UK is',\n",
        "    'London is the capital of',\n",
        "    'A major city in France is',\n",
        "]\n",
        "\n",
        "def get_token_id(text):\n",
        "    ids = tokenizer.encode(text)\n",
        "    if len(ids) != 1:\n",
        "        raise ValueError(f'Expected single token for {text}, got {ids}')\n",
        "    return ids[0]\n",
        "\n",
        "paris_id = get_token_id(' Paris')\n",
        "london_id = get_token_id(' London')\n",
        "\n",
        "def run_canon_probes(model, tag, out_dir):\n",
        "    out_dir = Path(out_dir)\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    margins = []\n",
        "    top10 = []\n",
        "    for prompt in PROMPTS:\n",
        "        ids = tokenizer.encode(prompt)\n",
        "        input_ids = torch.tensor([ids])\n",
        "        with torch.no_grad():\n",
        "            logits, infos = model(input_ids, return_info=True)\n",
        "        last = logits[0, -1]\n",
        "        margin = (last[paris_id] - last[london_id]).item()\n",
        "        margins.append(margin)\n",
        "        top_ids = torch.topk(last, k=10).indices.tolist()\n",
        "        top10.append([tokenizer.decode([i]) for i in top_ids])\n",
        "    mean_margin = float(sum(margins) / len(margins))\n",
        "    min_margin = float(min(margins))\n",
        "\n",
        "    routing_stats = {}\n",
        "    try:\n",
        "        sample = torch.randint(0, cfg_obj.vocab_size, (2, 16))\n",
        "        with torch.no_grad():\n",
        "            _, info = model(sample, return_info=True)\n",
        "        if isinstance(info, list) and info:\n",
        "            info0 = info[0] or {}\n",
        "        else:\n",
        "            info0 = info or {}\n",
        "        if info0.get('read_weights') is not None:\n",
        "            p = info0['read_weights'].float().clamp_min(1e-8)\n",
        "            entropy = -(p * p.log()).sum(dim=-1).mean().item()\n",
        "            top = p.argmax(dim=-1).reshape(-1)\n",
        "            hist = torch.bincount(top, minlength=p.shape[-1]).float()\n",
        "            top1freq = (hist.max() / hist.sum().clamp_min(1.0)).item()\n",
        "            routing_stats['routing_entropy'] = entropy\n",
        "            routing_stats['routing_top1freq'] = top1freq\n",
        "        for key in ('content_read_gamma_mean','slotspace_gate_mean','slotspace_delta_norm'):\n",
        "            if key in info0:\n",
        "                routing_stats[key] = float(torch.as_tensor(info0[key]).mean().item())\n",
        "    except Exception as exc:\n",
        "        routing_stats['error'] = str(exc)\n",
        "\n",
        "    results = {\n",
        "        'tag': tag,\n",
        "        'margins': margins,\n",
        "        'mean_margin': mean_margin,\n",
        "        'min_margin': min_margin,\n",
        "        'top10_tokens': top10,\n",
        "        'routing_stats': routing_stats,\n",
        "    }\n",
        "    out_path = out_dir / f'{tag}_probes.json'\n",
        "    out_path.write_text(json.dumps(results, indent=2))\n",
        "    print('Probe summary:', tag)\n",
        "    print('  mean margin:', mean_margin)\n",
        "    print('  min margin:', min_margin)\n",
        "    return results\n",
        "\n",
        "baseline_results = run_canon_probes(model, 'before_finetune', artifacts_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Section 3 \u2014 Mini Fact-Answering Finetune (Tiny synthetic + WikiText mix)\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "DO_FINETUNE = True  # set False to skip the finetune step\n",
        "\n",
        "if DO_FINETUNE:\n",
        "    examples = [\n",
        "        ('Q: What is the capital of France?\\nA:', ' Paris'),\n",
        "        ('Q: What is the capital of the UK?\\nA:', ' London'),\n",
        "        ('Q: What city is the capital of Germany?\\nA:', ' Berlin'),\n",
        "        ('Q: What city is the capital of Italy?\\nA:', ' Rome'),\n",
        "    ]\n",
        "\n",
        "    synthetic = []\n",
        "    for prompt, answer in examples:\n",
        "        ids = tokenizer.encode(prompt + answer)\n",
        "        synthetic.append(torch.tensor(ids, dtype=torch.long))\n",
        "\n",
        "    max_len = max(len(x) for x in synthetic)\n",
        "    padded = []\n",
        "    for seq in synthetic:\n",
        "        pad = max_len - len(seq)\n",
        "        if pad > 0:\n",
        "            seq = torch.cat([seq, torch.full((pad,), tokenizer.eos_token_id, dtype=torch.long)])\n",
        "        padded.append(seq)\n",
        "\n",
        "    data = torch.stack(padded)\n",
        "    loader = DataLoader(data, batch_size=2, shuffle=True)\n",
        "\n",
        "    model.train()\n",
        "    optim = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "    losses = []\n",
        "    steps = 80\n",
        "    for step in range(steps):\n",
        "        batch = next(iter(loader))\n",
        "        logits, _ = model(batch)\n",
        "        loss = torch.nn.functional.cross_entropy(\n",
        "            logits[:, :-1, :].reshape(-1, cfg_obj.vocab_size),\n",
        "            batch[:, 1:].reshape(-1),\n",
        "        )\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optim.step()\n",
        "        losses.append(loss.item())\n",
        "        if (step + 1) % 20 == 0:\n",
        "            print(f'step {step+1}/{steps} loss={loss.item():.4f}')\n",
        "\n",
        "    finetune_dir = artifacts_dir / 'finetuned'\n",
        "    finetune_dir.mkdir(exist_ok=True)\n",
        "    torch.save(\n",
        "        {'model': model.state_dict(), 'cfg': cfg_obj.__dict__, 'losses': losses},\n",
        "        finetune_dir / 'finetuned.pt',\n",
        "    )\n",
        "    (finetune_dir / 'losses.json').write_text(json.dumps(losses, indent=2))\n",
        "    print('Saved finetuned checkpoint to', finetune_dir)\n",
        "else:\n",
        "    print('Skipping finetune step; DO_FINETUNE=False')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Section 4 \u2014 Canon Probes (AFTER finetune)\n",
        "if DO_FINETUNE:\n",
        "    model.eval()\n",
        "    after_results = run_canon_probes(model, 'after_finetune', artifacts_dir)\n",
        "\n",
        "    comparison = {\n",
        "        'mean_margin_before': baseline_results['mean_margin'],\n",
        "        'mean_margin_after': after_results['mean_margin'],\n",
        "        'margin_deltas': [a-b for a,b in zip(after_results['margins'], baseline_results['margins'])],\n",
        "        'routing_stats_before': baseline_results.get('routing_stats', {}),\n",
        "        'routing_stats_after': after_results.get('routing_stats', {}),\n",
        "    }\n",
        "    (artifacts_dir / 'comparison.json').write_text(json.dumps(comparison, indent=2))\n",
        "    print('Before/After mean margin:', comparison['mean_margin_before'], '\u2192', comparison['mean_margin_after'])\n",
        "else:\n",
        "    print('Finetune skipped; no after-finetune probe.')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Section 5 \u2014 Optional: Push finetuned artifact to HF (if token present)\n",
        "from huggingface_hub import HfApi, upload_file\n",
        "\n",
        "token = os.environ.get('HF_TOKEN')\n",
        "if token:\n",
        "    api = HfApi(token=token)\n",
        "    try:\n",
        "        upload_file(\n",
        "            path_or_fileobj=str(artifacts_dir / 'finetuned' / 'finetuned.pt'),\n",
        "            path_in_repo='finetuned/finetuned.pt',\n",
        "            repo_id=HF_REPO,\n",
        "            repo_type='model',\n",
        "        )\n",
        "        print('Uploaded finetuned checkpoint.')\n",
        "    except Exception as exc:\n",
        "        print('Upload failed:', exc)\n",
        "else:\n",
        "    print('HF_TOKEN not set; skipping upload.')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "ASA HF Canon Probes + Mini Finetune",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}