{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# ASA HF Checkpoint + Canon Probes + Mini Fact Finetune\n\nCPU-only Colab notebook: load the HF checkpoint, run canonical probes, generate samples, run a mini finetune (synthetic + optional WikiText mix), and re-run probes/generations.\n\n**Expected runtime:** a few minutes on CPU for probes and a short finetune.\n\n**Notes:**\n- Uses the ASA/ASM generation helpers and mini-alignment loop from `building_blocks/working _example.py`.\n- Saves JSON artifacts in `artifacts/` for quick inspection.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Section 0 \u2014 Setup\nThis installs the repo and minimal dependencies, seeds the run, and creates an artifacts folder."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import os, sys, subprocess, platform, json, time, random\nfrom pathlib import Path\n\nrepo_dir = 'ASA'\nif not Path(repo_dir).exists():\n    subprocess.run(['git','clone','https://github.com/digitaldaimyo/ASA.git'], check=True)\nos.chdir(repo_dir)\n\nsubprocess.run([sys.executable,'-m','pip','install','-e','.'], check=True)\nsubprocess.run([sys.executable,'-m','pip','install','-q','huggingface_hub','safetensors','transformers','datasets'], check=True)\n\nimport torch\nfrom huggingface_hub import hf_hub_download\nfrom transformers import AutoTokenizer\n\ndevice = torch.device('cpu')\nprint('Python:', platform.python_version())\nprint('Torch:', torch.__version__)\nprint('Device:', device)\ntry:\n    commit = subprocess.check_output(['git','rev-parse','HEAD']).decode().strip()\n    print('Repo commit:', commit)\nexcept Exception:\n    print('Repo commit: unavailable')\n\nseed = 1337\nrandom.seed(seed)\ntorch.manual_seed(seed)\n\nartifacts_dir = Path('artifacts')\nartifacts_dir.mkdir(exist_ok=True)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Section 1 \u2014 Load base model from Hugging Face (Baseline)\nLoads the public checkpoint and verifies a forward pass."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "from asa.load_pretrained import load_pretrained\n\nHF_REPO = 'DigitalShogun/ASA-ASM-wikitext103-raw'\nDEFAULT_CKPT = 'ASA_ASM_wt103-rawv1_gpt2_T1024_L21_D384_H8_K16_M32_ropek1_alibi1_gamma1_step75000_best.pt'\n\nmodel, report, cfg_obj = load_pretrained(HF_REPO, DEFAULT_CKPT, variant='baseline', device='cpu')\nprint('Loaded model with vocab_size:', cfg_obj.vocab_size)\nprint('Checkpoint source:', report['state_dict_source'])\nprint('Allowlisted gaps:', {\n    'missing': len(report['allowed_missing']),\n    'unexpected': len(report['allowed_unexpected']),\n    'mismatched': len(report['allowed_mismatched']),\n})\n\ninput_ids = torch.randint(0, cfg_obj.vocab_size, (1, 32))\nwith torch.no_grad():\n    logits, _ = model(input_ids)\nprint('Logits shape:', tuple(logits.shape))\nassert logits.shape == (1, 32, cfg_obj.vocab_size)\nassert torch.isfinite(logits).all()\n\nrun_metadata = {\n    'repo': HF_REPO,\n    'checkpoint': DEFAULT_CKPT,\n    'state_dict_source': report['state_dict_source'],\n    'seed': seed,\n    'timestamp': time.time(),\n    'config': cfg_obj.__dict__,\n}\n(artifacts_dir / 'run_metadata.json').write_text(json.dumps(run_metadata, indent=2))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Section 2 \u2014 Canon Probes (BEFORE finetune)\nRuns a small set of Paris/London margin probes and captures routing stats when available."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "tokenizer = AutoTokenizer.from_pretrained('gpt2', use_fast=True)\n\nPROMPTS = [\n    'The capital of France is',\n    \"France's capital city is\",\n    'Paris is the capital of',\n    'The capital of the UK is',\n    'London is the capital of',\n    'A major city in France is',\n]\n\ndef get_token_id(text):\n    ids = tokenizer.encode(text)\n    if len(ids) != 1:\n        raise ValueError(f'Expected single token for {text}, got {ids}')\n    return ids[0]\n\nparis_id = get_token_id(' Paris')\nlondon_id = get_token_id(' London')\n\ndef run_canon_probes(model, tag, out_dir):\n    out_dir = Path(out_dir)\n    out_dir.mkdir(parents=True, exist_ok=True)\n    margins = []\n    top10 = []\n    for prompt in PROMPTS:\n        ids = tokenizer.encode(prompt)\n        input_ids = torch.tensor([ids])\n        with torch.no_grad():\n            logits, infos = model(input_ids, return_info=True)\n        last = logits[0, -1]\n        margin = (last[paris_id] - last[london_id]).item()\n        margins.append(margin)\n        top_ids = torch.topk(last, k=10).indices.tolist()\n        top10.append([tokenizer.decode([i]) for i in top_ids])\n    mean_margin = float(sum(margins) / len(margins))\n    min_margin = float(min(margins))\n\n    routing_stats = {}\n    try:\n        sample = torch.randint(0, cfg_obj.vocab_size, (2, 16))\n        with torch.no_grad():\n            _, info = model(sample, return_info=True)\n        if isinstance(info, list) and info:\n            info0 = info[0] or {}\n        else:\n            info0 = info or {}\n        if info0.get('read_weights') is not None:\n            p = info0['read_weights'].float().clamp_min(1e-8)\n            entropy = -(p * p.log()).sum(dim=-1).mean().item()\n            top = p.argmax(dim=-1).reshape(-1)\n            hist = torch.bincount(top, minlength=p.shape[-1]).float()\n            top1freq = (hist.max() / hist.sum().clamp_min(1.0)).item()\n            routing_stats['routing_entropy'] = entropy\n            routing_stats['routing_top1freq'] = top1freq\n        for key in ('content_read_gamma_mean','slotspace_gate_mean','slotspace_delta_norm'):\n            if key in info0:\n                routing_stats[key] = float(torch.as_tensor(info0[key]).mean().item())\n    except Exception as exc:\n        routing_stats['error'] = str(exc)\n\n    results = {\n        'tag': tag,\n        'margins': margins,\n        'mean_margin': mean_margin,\n        'min_margin': min_margin,\n        'top10_tokens': top10,\n        'routing_stats': routing_stats,\n    }\n    out_path = out_dir / f'{tag}_probes.json'\n    out_path.write_text(json.dumps(results, indent=2))\n    print('Probe summary:', tag)\n    print('  mean margin:', mean_margin)\n    print('  min margin:', min_margin)\n    return results\n\nbaseline_results = run_canon_probes(model, 'before_finetune', artifacts_dir)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Section 3 \u2014 Generation helpers (ASA-aware)\nDefines the ASA/ASM generation utilities from the working example, including router-aware resampling and repetition controls."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import math\n\nfrom typing import Any, Dict, Optional, Tuple, Union, List\n\n\nimport torch\n\nimport torch.nn.functional as F\n\n\n# Helpers\n\ndef _top_k_top_p_filtering(\n    logits: torch.Tensor,\n    top_k: int = 0,\n    top_p: float = 1.0,\n    min_tokens_to_keep: int = 1,\n) -> torch.Tensor:\n    \"\"\"\n    Filter a distribution of logits using top-k and/or nucleus (top-p).\n    logits: [V]\n    \"\"\"\n    if top_k > 0:\n        top_k = min(max(top_k, min_tokens_to_keep), logits.size(-1))\n        kth = torch.topk(logits, top_k).values[-1]\n        logits = logits.masked_fill(logits < kth, float(\"-inf\"))\n\n    if top_p < 1.0:\n        sorted_logits, sorted_idx = torch.sort(logits, descending=True)\n        probs = F.softmax(sorted_logits, dim=-1)\n        cumprobs = probs.cumsum(dim=-1)\n\n        # Remove tokens with cumulative prob above threshold\n        cutoff = cumprobs > top_p\n        # Keep at least min_tokens_to_keep\n        cutoff[:min_tokens_to_keep] = False\n\n        sorted_logits = sorted_logits.masked_fill(cutoff, float(\"-inf\"))\n        logits = logits.scatter(0, sorted_idx, sorted_logits)\n\n    return logits\n\n\ndef _apply_repetition_penalty(\n    logits: torch.Tensor,\n    generated_ids: torch.Tensor,\n    penalty: float,\n) -> torch.Tensor:\n    \"\"\"\n    Classic repetition penalty (GPT-2 style): penalize logits of previously generated tokens.\n    logits: [V], generated_ids: [t]\n    \"\"\"\n    if penalty is None or penalty == 1.0 or generated_ids.numel() == 0:\n        return logits\n    uniq = torch.unique(generated_ids)\n    # If logit > 0: divide by penalty; else multiply by penalty\n    l = logits[uniq]\n    logits[uniq] = torch.where(l > 0, l / penalty, l * penalty)\n    return logits\n\n\n\ndef _no_repeat_ngram_ban(\n    logits: torch.Tensor,\n    generated_ids: torch.Tensor,\n    no_repeat_ngram_size: int,\n) -> torch.Tensor:\n    \"\"\"\n    Ban tokens that would create a repeated n-gram of size N in the generated sequence.\n    logits: [V], generated_ids: [t]\n    \"\"\"\n    n = int(no_repeat_ngram_size or 0)\n    if n <= 1 or generated_ids.numel() < n - 1:\n        return logits\n\n    seq = generated_ids.tolist()\n    prefix = seq[-(n - 1):]  # length n-1\n    # Build set of next tokens seen after this prefix in the past\n    banned = set()\n    for i in range(len(seq) - n + 1):\n        if seq[i:i + n - 1] == prefix:\n            banned.add(seq[i + n - 1])\n\n    if banned:\n        banned = torch.tensor(list(banned), device=logits.device, dtype=torch.long)\n        logits[banned] = float(\"-inf\")\n    return logits\n\n\n# -----------------------------------------------------------------------------\n# ASA/ASM-specific generation\n# -----------------------------------------------------------------------------\n@torch.no_grad()\ndef asa_generate(\n    prompt: Union[str, List[int], torch.Tensor],\n    model: torch.nn.Module,\n    gen: Dict[str, Any],\n) -> Dict[str, Any]:\n    \"\"\"\n    Generation crafted for ASA/ASM models:\n      - Uses soft sampling by default (hard routing variants are unstable per your ablations).\n      - Optionally uses ASA internal telemetry (return_info=True) to perform *router-aware fallback*\n        when EOS-risk is high early, or when routing is pathologically branchy.\n      - Supports standard sampling controls + mild anti-repetition.\n      - Keeps inference dropout off.\n\n    Args\n    ----\n    prompt:\n        - str: requires gen[\"tokenizer\"] providing encode/decode\n        - List[int] / 1D torch.Tensor: token ids\n    model:\n        ASMLanguageModel (or compatible) returning logits or (logits, infos) if return_info=True\n    gen params (dict):\n        Required (if prompt is str):\n          tokenizer: a HF tokenizer with encode/decode\n        Common:\n          max_new_tokens: int (default 128)\n          temperature: float (default 0.8)\n          top_p: float (default 0.9)\n          top_k: int (default 50)\n          min_new_tokens: int (default 0)\n          eos_token_id: int (default tokenizer.eos_token_id if available)\n          pad_token_id: int (optional)\n          do_sample: bool (default True)\n          repetition_penalty: float (default 1.05)\n          no_repeat_ngram_size: int (default 3)\n          device: torch.device or str (default model device)\n        ASA-aware controls:\n          asa_info: bool (default True) -> request return_info and use it\n          eos_risk_threshold: float (default 0.25)\n          early_steps: int (default 24) -> window in which to apply EOS-risk mitigations\n          branchy_entropy_threshold: float (default None) -> if set, triggers extra sharpening\n          rescue_mode: str in {\"none\",\"scaffold\",\"resample\"} (default \"resample\")\n              - \"resample\": if EOS risk triggers, resample with lower temp / higher top_k keep\n              - \"scaffold\": if tokenizer provided and prompt looks like a known template,\n                            inject a short scaffold (see below) once at the start\n          rescue_temp: float (default 0.65)\n          rescue_top_p: float (default 0.85)\n          rescue_top_k: int (default 80)\n          max_resample_tries: int (default 4)\n        Return:\n          return_text: bool (default True if tokenizer present else False)\n\n    Returns\n    -------\n    dict with:\n      \"input_ids\": [1, T+new]\n      \"generated_ids\": [new]\n      \"text\": optional\n      \"info_trace\": optional list of per-step ASA stats (if asa_info=True)\n    \"\"\"\n    model.eval()\n\n    tokenizer = gen.get(\"tokenizer\", None)\n    device = gen.get(\"device\", None)\n    if device is None:\n        device = next(model.parameters()).device\n\n    # --- tokenize prompt ---\n    if isinstance(prompt, str):\n        if tokenizer is None:\n            raise ValueError(\"prompt is str but gen['tokenizer'] was not provided.\")\n        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n    elif isinstance(prompt, list):\n        input_ids = torch.tensor(prompt, device=device, dtype=torch.long).unsqueeze(0)\n    elif isinstance(prompt, torch.Tensor):\n        if prompt.dim() == 1:\n            input_ids = prompt.to(device=device, dtype=torch.long).unsqueeze(0)\n        elif prompt.dim() == 2:\n            input_ids = prompt.to(device=device, dtype=torch.long)\n        else:\n            raise ValueError(\"prompt tensor must be 1D or 2D token ids.\")\n    else:\n        raise TypeError(\"prompt must be str, List[int], or torch.Tensor of token ids.\")\n\n    max_new = int(gen.get(\"max_new_tokens\", 128))\n    min_new = int(gen.get(\"min_new_tokens\", 0))\n    do_sample = bool(gen.get(\"do_sample\", True))\n\n    temperature = float(gen.get(\"temperature\", 0.8))\n    top_p = float(gen.get(\"top_p\", 0.9))\n    top_k = int(gen.get(\"top_k\", 50))\n\n    repetition_penalty = float(gen.get(\"repetition_penalty\", 1.05))\n    no_repeat_ngram_size = int(gen.get(\"no_repeat_ngram_size\", 3))\n\n    eos_token_id = gen.get(\"eos_token_id\", None)\n    if eos_token_id is None and tokenizer is not None:\n        eos_token_id = tokenizer.eos_token_id\n    if eos_token_id is None:\n        eos_token_id = -1  # disable EOS logic if unknown\n\n    asa_info = bool(gen.get(\"asa_info\", True))\n    eos_risk_threshold = float(gen.get(\"eos_risk_threshold\", 0.25))\n    early_steps = int(gen.get(\"early_steps\", 24))\n    branchy_entropy_threshold = gen.get(\"branchy_entropy_threshold\", None)\n    rescue_mode = str(gen.get(\"rescue_mode\", \"resample\")).lower()\n    rescue_temp = float(gen.get(\"rescue_temp\", 0.65))\n    rescue_top_p = float(gen.get(\"rescue_top_p\", 0.85))\n    rescue_top_k = int(gen.get(\"rescue_top_k\", 80))\n    max_resample_tries = int(gen.get(\"max_resample_tries\", 4))\n\n    # Optional scaffold injection (architecture-aware: helps route trajectory)\n    if rescue_mode == \"scaffold\" and tokenizer is not None and isinstance(prompt, str):\n        # Very small, conservative scaffold set\u2014extend as you like\n        scaffolds = [\n            (\"The capital of\", \" the city of\"),\n            (\"Albert Einstein was born\", \" in\"),\n            (\"The scientific method involves\", \" the process of\"),\n            (\"The algorithm proceeds as follows\", \" 1.\"),\n        ]\n        for k, s in scaffolds:\n            if prompt.strip().startswith(k) and not prompt.strip().endswith(s.strip()):\n                input_ids = tokenizer.encode(prompt + s, return_tensors=\"pt\").to(device)\n                break\n\n    info_trace: List[Dict[str, float]] = []\n\n    # Generation loop\n    cur_ids = input_ids\n    for step in range(max_new):\n        # Model forward\n        if asa_info:\n            out = model(cur_ids, return_info=True)\n            logits, infos = out\n            # infos is list per layer; take last block's light stats if present\n            last = infos[-1] if isinstance(infos, list) and len(infos) > 0 else None\n            stat = {}\n            if isinstance(last, dict):\n                # these are CPU tensors in your module; cast to float if present\n                for k in [\"entropy_mean\", \"top1freq_mean\", \"content_read_gamma_mean\", \"slotspace_gate_mean\", \"slotspace_delta_norm\"]:\n                    if k in last and last[k] is not None:\n                        try:\n                            stat[k] = float(last[k].item())\n                        except Exception:\n                            pass\n            # Store later for debugging\n        else:\n            logits = model(cur_ids, return_info=False)\n            stat = None\n\n        next_logits = logits[0, -1, :]  # [V]\n\n        # Basic constraints\n        if step < min_new and eos_token_id >= 0:\n            next_logits = next_logits.clone()\n            next_logits[eos_token_id] = float(\"-inf\")\n\n        # Anti-repetition (mild, usually good for ASA because content-read is self-referential)\n        gen_so_far = cur_ids[0, input_ids.shape[1]:]  # only newly generated, if any\n        next_logits = _apply_repetition_penalty(next_logits, gen_so_far, repetition_penalty)\n        next_logits = _no_repeat_ngram_ban(next_logits, cur_ids[0], no_repeat_ngram_size)\n\n        # Router-aware rescue (early EOS / excessive branchiness)\n        # Use next-token EOS risk; optionally sharpen if branchy.\n        tries = 0\n        used_temp, used_top_p, used_top_k = temperature, top_p, top_k\n        while True:\n            l = next_logits\n            if used_temp and used_temp > 0:\n                l = l / used_temp\n\n            l = _top_k_top_p_filtering(l, top_k=used_top_k, top_p=used_top_p)\n\n            probs = F.softmax(l, dim=-1)\n            p_eos = float(probs[eos_token_id].item()) if eos_token_id >= 0 else 0.0\n            ent = float(-(probs.clamp_min(1e-12) * probs.clamp_min(1e-12).log()).sum().item())\n\n            # Condition: early EOS risk is too high\n            eos_risky = (eos_token_id >= 0) and (step < early_steps) and (p_eos > eos_risk_threshold)\n\n            # Condition: branchy token distribution (optional) -> reduce temperature a bit\n            branchy = False\n            if branchy_entropy_threshold is not None and step < early_steps:\n                branchy = ent > float(branchy_entropy_threshold)\n\n            if (eos_risky or branchy) and rescue_mode == \"resample\" and tries < max_resample_tries:\n                used_temp = min(used_temp, rescue_temp)\n                used_top_p = min(used_top_p, rescue_top_p)\n                used_top_k = max(used_top_k, rescue_top_k)\n                tries += 1\n                continue\n\n            # Choose token\n            if do_sample:\n                next_id = torch.multinomial(probs, num_samples=1)\n            else:\n                next_id = torch.argmax(probs, dim=-1, keepdim=True)\n\n            break\n\n        # Log trace\n        if asa_info:\n            rec = {\"step\": float(step), \"token_entropy\": float(ent), \"p_eos\": float(p_eos)}\n            if stat:\n                for k, v in stat.items():\n                    rec[k] = float(v)\n            # record rescue adjustments\n            rec[\"temp_used\"] = float(used_temp)\n            rec[\"top_p_used\"] = float(used_top_p)\n            rec[\"top_k_used\"] = float(used_top_k)\n            info_trace.append(rec)\n\n        # Append token\n        cur_ids = torch.cat([cur_ids, next_id.view(1, 1)], dim=1)\n\n        # Stop on EOS\n        if eos_token_id >= 0 and int(next_id.item()) == int(eos_token_id) and step >= min_new:\n            break\n\n    generated_ids = cur_ids[:, input_ids.shape[1]:]\n\n    out: Dict[str, Any] = {\n        \"input_ids\": cur_ids,\n        \"generated_ids\": generated_ids,\n    }\n    if asa_info:\n        out[\"info_trace\"] = info_trace\n\n    return_text = bool(gen.get(\"return_text\", tokenizer is not None))\n    if return_text and tokenizer is not None:\n        out[\"text\"] = tokenizer.decode(cur_ids[0].tolist(), skip_special_tokens=False)\n\n    return out\n\n\n# =========================\n# PATCH 1: wrappers for your crafted asa_generate\n# =========================\n\n@torch.no_grad()\ndef asa_greedy_suffix(\n    prompt: str,\n    model: torch.nn.Module,\n    gen: dict,\n    max_new_tokens: int = 8,\n    strip: bool = True,\n) -> str:\n    \"\"\"\n    Runs your asa_generate in greedy mode and returns ONLY the suffix after `prompt`.\n    This is what you want for exact-match checks / scoring.\n    \"\"\"\n    # Copy gen so we can override safely\n    g = dict(gen)\n    g[\"do_sample\"] = False\n    g[\"max_new_tokens\"] = int(max_new_tokens)\n\n    out = asa_generate(prompt, model, g)\n    text = out.get(\"text\", None)\n    if text is None:\n        # Fallback: decode manually\n        tok = g.get(\"tokenizer\", None)\n        if tok is None:\n            raise ValueError(\"No decoded text available; provide gen['tokenizer'].\")\n        text = tok.decode(out[\"input_ids\"][0].tolist(), skip_special_tokens=False)\n\n    # Suffix (best-effort): if prompt string matches prefix of decoded text\n    if text.startswith(prompt):\n        suf = text[len(prompt):]\n    else:\n        # Robust fallback: try to locate the prompt inside the decoded text\n        idx = text.find(prompt)\n        suf = text[idx + len(prompt):] if idx >= 0 else text\n\n    if strip:\n        suf = suf.replace(\"\\n\", \" \").strip()\n    return suf\n\n\n@torch.no_grad()\ndef asa_generate_many(\n    prompts: list,\n    model: torch.nn.Module,\n    gen: dict,\n    do_sample: bool = False,\n    max_new_tokens: int = 8,\n) -> list:\n    \"\"\"\n    Convenience wrapper: runs asa_generate per prompt (loop) and returns decoded texts.\n    \"\"\"\n    g = dict(gen)\n    g[\"do_sample\"] = bool(do_sample)\n    g[\"max_new_tokens\"] = int(max_new_tokens)\n\n    outs = []\n    for p in prompts:\n        out = asa_generate(p, model, g)\n        text = out.get(\"text\", None)\n        if text is None:\n            tok = g.get(\"tokenizer\", None)\n            if tok is None:\n                raise ValueError(\"No decoded text available; provide gen['tokenizer'].\")\n            text = tok.decode(out[\"input_ids\"][0].tolist(), skip_special_tokens=False)\n        outs.append(text)\n    return outs\n\n@torch.no_grad()\ndef score_next_token_rank(\n    prompt: str,\n    target_token: str,\n    model: torch.nn.Module,\n    gen: dict,\n) -> dict:\n    \"\"\"\n    Computes P(target) and rank for the *next token* only, matching your printed diagnostics.\n    \"\"\"\n    tok = gen[\"tokenizer\"]\n    device = next(model.parameters()).device\n\n    # encode prompt\n    input_ids = tok.encode(prompt, return_tensors=\"pt\").to(device)\n\n    # encode target token as a single token (best-effort)\n    target_ids = tok.encode(target_token, add_special_tokens=False)\n    if len(target_ids) != 1:\n        return {\"ok\": False, \"reason\": f\"target_token maps to {len(target_ids)} tokens\", \"target_ids\": target_ids}\n\n    target_id = target_ids[0]\n\n    model.eval()\n    logits = model(input_ids)  # if your model needs return_info=False default\n    if isinstance(logits, (tuple, list)):\n        logits = logits[0]\n    next_logits = logits[0, -1, :]\n\n    probs = torch.softmax(next_logits, dim=-1)\n    p_t = float(probs[target_id].item())\n\n    # rank: 1 = best\n    sorted_idx = torch.argsort(next_logits, descending=True)\n    rank = int((sorted_idx == target_id).nonzero(as_tuple=False).item()) + 1\n\n    return {\"ok\": True, \"p_target\": p_t, \"rank\": rank, \"target_id\": target_id}\n\n#\n\n\n\n#@title multigen\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)\n\ngener = dict(\n    tokenizer=tokenizer,\n    max_new_tokens=32,\n    #min_new_tokens=4,\n    temperature=0.1,\n    top_p=0.95,\n    top_k=80,\n    repetition_penalty=1.03,\n    no_repeat_ngram_size=3, # 3\n    asa_info=False,\n    rescue_mode=None, # \"resample\", None\n    #eos_risk_threshold=0.25,\n    #early_steps=24,\n    #branchy_entropy_threshold=7.5,   # optional; depends on vocab size and filtering\n)\n\nprint(\"#\"*5, \"Countries\", \"#\"*5)\nfinishers = [\"is\", \"sounds like\", \"consists of\", \"is a form of\", \"all changed when\"]\nqualities = [\"capital\", \"language\", \"geography\", \"government\", \"history\"]\ncountries = [\"France\", \"Spain\", \"Russia\", \"Italy\", \"Japan\", \"Egypt\", \"Germany\", \"Brazil\"]\nfor country in countries:\n    for quality, finisher in zip(qualities, finishers):\n        out = asa_generate(f\"The {quality} of {country} {finisher}\", model, gener)\n        print(out[\"text\"])\n\nprint(\"#\"*5, \"People\", \"#\"*5)\npeople = [\"Albert Einstein\", \"George Patton\", \"Charles Darwin\", \"George Washington\", \"Winston Churchill\"]\nfactoids = [\"was born\", \"contributed\", \"accomplished\", \"had a strong opinion about\", \"died\"]\nfor person in people:\n    for factoid in factoids:\n        out = asa_generate(f\"{person} {factoid}\", model, gener)\n        print(out[\"text\"])\n\n\n# Optionally inspect router-aware trace:\n# out[\"info_trace\"][:5]\n\n\n#@title Prepare Generator\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)\n\ngener = dict(\n    tokenizer=tokenizer,\n    max_new_tokens=32,\n    #min_new_tokens=4,\n    temperature=0.1,\n    top_p=0.95,\n    top_k=80,\n    repetition_penalty=1.03,\n    no_repeat_ngram_size=3, # 3\n    asa_info=False,\n    rescue_mode=None, # \"resample\", None\n    #eos_risk_threshold=0.25,\n    #early_steps=24,\n    #branchy_entropy_threshold=7.5,   # optional; depends on vocab size and filtering\n)\n\n\n\n# ==========================================\n#@title Expanded Mini-alignment dataset + WikiText mix + optional slot-attn-only finetune + rerun generations\n# (Aligned to ASMLanguageModel + your asa_generate)\n# ==========================================\n\nimport random\nimport math\nimport re\nimport itertools\nimport torch\nimport torch.nn.functional as F\n\nfrom torch.utils.data import Dataset, DataLoader\n\n# -----------------------\n# 0) Repro & device\n# -----------------------\nSEED = 1337\nrandom.seed(SEED)\ntorch.manual_seed(SEED)\ndevice = next(model.parameters()).device\n\n\nfrom datasets import load_dataset\n# Use a community-hosted mirror\n#dataset = load_dataset('segyges/wikitext-103', name='wikitext-103-raw-v1')\n\n\n# -----------------------\n# 0.5) Config knobs (NEW)\n# -----------------------\nCFG = {\n    # mix in WikiText\n    \"use_wiki\": False,\n    \"wiki_dataset_name\": \"wikitext\",\n    \"wiki_config_candidates\": [\"wikitext-103-raw-v1\", \"wikitext-2-raw-v1\"],  # fallback\n    \"wiki_num_samples\": 1536,         # number of wiki chunks (not lines)\n    \"wiki_chunk_chars_min\": 400,      # filter small chunks\n    \"wiki_chunk_chars_max\": 1200,     # chunk size (chars) before tokenization\n\n    # training\n    \"max_len\": 128,                   # increased since wiki chunks are longer\n    \"batch_size\": 16,\n    \"steps\": 77,\n    \"lr\": 7e-6,\n    \"weight_decay\": 0.007,\n    \"grad_clip\": 1.0,\n\n    # finetune mode\n    # \"all\" trains everything; \"slot_attn_only\" freezes everything except slot-space attention op\n    \"finetune_mode\": \"slot_attn_only\",  # or \"all\" or  \"slot_attn_only\"\n\n    # which params count as \"slot attention\" (adjust to your module names)\n    #\"slot_train_name_regex\": r\"(slot|slots).*(attn|attention)|((attn|attention).*(slot|slots))\",\n\n    \"slot_train_name_regex\":r\"(^|\\.)(slot_in|slot_q|slot_k|slot_v|slot_out)\\.weight$|(^|\\.)(_slotspace_gate_raw)$\",\n\n\n}\n\n# -----------------------\n# 1) Utilities (aligned to your model call style)\n# -----------------------\n@torch.no_grad()\ndef next_token_stats(prompt: str, target_token_str: str, model, tokenizer):\n    model.eval()\n    inp = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n\n    tgt_ids = tokenizer.encode(target_token_str, add_special_tokens=False)\n    if len(tgt_ids) != 1:\n        return {\"ok\": False, \"reason\": f\"target string maps to {len(tgt_ids)} tokens\", \"target_ids\": tgt_ids}\n\n    tgt = tgt_ids[0]\n\n    out = model(inp)\n    logits = out[0] if isinstance(out, (tuple, list)) else out\n    last = logits[0, -1, :]\n    probs = torch.softmax(last, dim=-1)\n\n    p = float(probs[tgt].item())\n    rank = int((torch.argsort(last, descending=True) == tgt).nonzero(as_tuple=False).item()) + 1\n    top1_id = int(torch.argmax(last).item())\n    top1 = tokenizer.decode([top1_id])\n\n    return {\"ok\": True, \"p_target\": p, \"rank\": rank, \"top1\": top1, \"target_id\": tgt}\n\n@torch.no_grad()\ndef greedy_suffix(prompt: str, model, gen, max_new_tokens=8):\n    g = dict(gen)\n    g[\"do_sample\"] = False\n    g[\"max_new_tokens\"] = int(max_new_tokens)\n    out = asa_generate(prompt, model, g)\n    text = out[\"text\"]\n    if text.startswith(prompt):\n        return text[len(prompt):].replace(\"\\n\", \" \").strip()\n    idx = text.find(prompt)\n    if idx >= 0:\n        return text[idx+len(prompt):].replace(\"\\n\", \" \").strip()\n    return text.replace(\"\\n\", \" \").strip()\n\n@torch.no_grad()\ndef eval_exact_match(examples, model, gen, max_new_tokens=8):\n    model.eval()\n    ok = 0\n    for ex in examples:\n        pred = greedy_suffix(ex[\"prompt\"], model, gen, max_new_tokens=max_new_tokens)\n        gold = ex[\"completion\"].replace(\"\\n\", \" \").strip()\n        ok += int(pred.startswith(gold))\n    return ok / max(1, len(examples))\n\n# -----------------------\n# 2) Dataset builders\n# -----------------------\n\n\ndef load_wikitext_chunks(tokenizer, num_samples=2048, chunk_chars_min=400, chunk_chars_max=1200):\n    \"\"\"\n    Produces wiki training examples as plain LM text chunks:\n      ex = {\"prompt\": \"\", \"completion\": \"<wiki chunk>\", \"tag\": \"wiki\"}\n    We chunk by chars first, then token-truncate later in dataset.\n    \"\"\"\n    try:\n        from datasets import load_dataset\n    except Exception as e:\n        print(\"[WikiText] datasets not available; skipping WikiText mix.\")\n        return []\n\n    ds = None\n    used_cfg = None\n    for cfg in CFG[\"wiki_config_candidates\"]:\n        try:\n            ds = load_dataset(CFG[\"wiki_dataset_name\"], cfg, split=\"train\")\n            used_cfg = cfg\n            break\n        except Exception:\n            ds = None\n\n    if ds is None:\n        print(\"[WikiText] Could not load WikiText (tried configs:\", CFG[\"wiki_config_candidates\"], \"). Skipping.\")\n        return []\n\n    print(f\"[WikiText] Loaded {CFG['wiki_dataset_name']} / {used_cfg} train split with {len(ds)} rows.\")\n\n    # Pull raw text field (wikitext uses 'text')\n    texts = [t for t in ds[\"text\"] if isinstance(t, str) and len(t.strip()) > 0]\n\n    # Make chunks: concatenate consecutive lines until size bound, filter small chunks\n    chunks = []\n    buf = []\n    buf_len = 0\n\n    # shuffle deterministically\n    rng = random.Random(SEED)\n    rng.shuffle(texts)\n\n    for line in texts:\n        line = line.strip()\n        # skip headings markup lines; keep normal prose\n        if line.startswith(\"=\") and line.endswith(\"=\"):\n            continue\n        if not line:\n            continue\n\n        # add line to buffer\n        if buf_len + len(line) + 1 <= chunk_chars_max:\n            buf.append(line)\n            buf_len += len(line) + 1\n        else:\n            chunk = \" \".join(buf).strip()\n            if len(chunk) >= chunk_chars_min:\n                chunks.append(chunk)\n            buf = [line]\n            buf_len = len(line) + 1\n\n        if len(chunks) >= num_samples:\n            break\n\n    # flush\n    if len(chunks) < num_samples:\n        chunk = \" \".join(buf).strip()\n        if len(chunk) >= chunk_chars_min:\n            chunks.append(chunk)\n\n    # create examples\n    wiki_examples = [{\"prompt\": \"\", \"completion\": c, \"tag\": \"wiki\"} for c in chunks[:num_samples]]\n    print(f\"[WikiText] Prepared {len(wiki_examples)} wiki chunks.\")\n\n    return wiki_examples\n\n# -----------------------\n# 3) Split synthetic (entity-holdout) + build mixed train set\n# -----------------------\n#pairs = build_pairs_expanded(tokenizer)\n\nfrom collections import Counter\n\nholdout_capitals = {\n    \"Spain\", \"Canada\", \"Poland\", \"Portugal\", \"Greece\", \"Austria\",\n    \"Norway\", \"Ireland\", \"Romania\", \"Croatia\", \"Argentina\", \"Chile\"\n}\nholdout_languages = {\"Brazil\", \"Mexico\", \"Netherlands\", \"Sweden\", \"Finland\"}\nholdout_currencies = {\"Japan\", \"Switzerland\", \"South Africa\", \"Thailand\"}\nholdout_continents = {\"Kenya\", \"Vietnam\", \"Peru\", \"New Zealand\"}\n\ntrain_examples, holdout_examples = [], []\nfor ex in pairs:\n    task = ex[\"tag\"].split(\":\")[0]\n\n    if task == \"capital\":\n        country = ex[\"tag\"].split(\":\", 1)[1]\n        (holdout_examples if country in holdout_capitals else train_examples).append(ex)\n    elif task == \"language\":\n        country = ex[\"tag\"].split(\":\", 1)[1]\n        (holdout_examples if country in holdout_languages else train_examples).append(ex)\n    elif task == \"currency\":\n        country = ex[\"tag\"].split(\":\", 1)[1]\n        (holdout_examples if country in holdout_currencies else train_examples).append(ex)\n    elif task == \"continent\":\n        country = ex[\"tag\"].split(\":\", 1)[1]\n        (holdout_examples if country in holdout_continents else train_examples).append(ex)\n    else:\n        if random.random() < 0.2:\n            holdout_examples.append(ex)\n        else:\n            train_examples.append(ex)\n\nprint(f\"\\n[Synthetic] Total kept pairs: {len(pairs)} | Train: {len(train_examples)} | Holdout: {len(holdout_examples)}\")\nprint(f\"[Synthetic] Split: {len(train_examples)/len(pairs)*100:.1f}% / {len(holdout_examples)/len(pairs)*100:.1f}%\")\nholdout_by_cat = Counter(ex[\"tag\"].split(\":\")[0] for ex in holdout_examples)\nprint(\"[Synthetic] Holdout by category:\", dict(holdout_by_cat))\n\n# NEW: load wiki and mix into TRAIN ONLY\nwiki_examples = []\nif CFG[\"use_wiki\"]:\n    wiki_examples = load_wikitext_chunks(\n        tokenizer,\n        num_samples=CFG[\"wiki_num_samples\"],\n        chunk_chars_min=CFG[\"wiki_chunk_chars_min\"],\n        chunk_chars_max=CFG[\"wiki_chunk_chars_max\"],\n    )\n\nmixed_train_examples = train_examples + wiki_examples\nprint(f\"\\n[Mix] Train synthetic={len(train_examples)} + wiki={len(wiki_examples)} => mixed_train={len(mixed_train_examples)}\")\nprint(f\"[Mix] Holdout (synthetic only) = {len(holdout_examples)}\")\n\n# -----------------------\n# 4) Tiny finetune dataset (teacher forcing)\n#    Works for BOTH: prompt+completion pairs and raw wiki chunks (prompt=\"\")\n# -----------------------\nclass PromptCompletionDataset(Dataset):\n    def __init__(self, examples, tokenizer, max_len=128):\n        self.examples = examples\n        self.tok = tokenizer\n        self.max_len = int(max_len)\n\n    def __len__(self): return len(self.examples)\n\n    def __getitem__(self, idx):\n        ex = self.examples[idx]\n        text = ex[\"prompt\"] + ex[\"completion\"]\n        ids = self.tok.encode(text)\n\n        # keep the tail; for wiki, this acts like \"random suffix LM\"\n        ids = ids[-self.max_len:]\n\n        x = torch.tensor(ids[:-1], dtype=torch.long)\n        y = torch.tensor(ids[1:], dtype=torch.long)\n        return x, y, ex\n\ndef collate_pad(batch):\n    xs, ys, exs = zip(*batch)\n    maxT = max(x.size(0) for x in xs)\n    pad_id = tokenizer.eos_token_id  # GPT-2 no pad token\n\n    X = torch.full((len(xs), maxT), pad_id, dtype=torch.long)\n    Y = torch.full((len(xs), maxT), -100, dtype=torch.long)\n\n    for i, (x, y) in enumerate(zip(xs, ys)):\n        T = x.size(0)\n        X[i, :T] = x\n        Y[i, :T] = y\n    return X.to(device), Y.to(device), exs\n\ntrain_ds = PromptCompletionDataset(mixed_train_examples, tokenizer, max_len=CFG[\"max_len\"])\ntrain_dl = DataLoader(\n    train_ds,\n    batch_size=min(CFG[\"batch_size\"], len(train_ds)),\n    shuffle=True,\n    collate_fn=collate_pad\n)\n\n# -----------------------\n# 5) Optional: freeze everything except slot-space attention (NEW)\n# -----------------------\ndef configure_finetune_mode(model, mode: str, name_regex: str):\n    \"\"\"\n    mode:\n      - \"all\": train everything\n      - \"slot_attn_only\": only train parameters whose full name matches `name_regex`\n    \"\"\"\n    if mode == \"all\":\n        for p in model.parameters():\n            p.requires_grad = True\n        trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n        total = sum(p.numel() for p in model.parameters())\n        print(f\"[Finetune] mode=all trainable={trainable}/{total} ({trainable/total*100:.2f}%)\")\n        return\n\n    if mode != \"slot_attn_only\":\n        raise ValueError(f\"Unknown finetune_mode={mode}\")\n\n    rx = re.compile(name_regex, flags=re.IGNORECASE)\n\n    # freeze everything\n    for _, p in model.named_parameters():\n        p.requires_grad = False\n\n    # unfreeze matching params\n    matched = []\n    for n, p in model.named_parameters():\n        if rx.search(n) is not None:\n            p.requires_grad = True\n            matched.append((n, p.numel()))\n\n    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    total = sum(p.numel() for p in model.parameters())\n    print(f\"[Finetune] mode=slot_attn_only regex={name_regex!r}\")\n    print(f\"[Finetune] trainable={trainable}/{total} ({trainable/total*100:.4f}%) matched_tensors={len(matched)}\")\n\n    # show top matches by size\n    matched.sort(key=lambda x: -x[1])\n    for n, k in matched[:25]:\n        print(f\"  [trainable] {k:>10}  {n}\")\n\nconfigure_finetune_mode(model, CFG[\"finetune_mode\"], CFG[\"slot_train_name_regex\"])\n\n# -----------------------\n# 6) Pre-eval (synthetic only, as before)\n# -----------------------\nprint(\"\\n\" + \"=\"*80)\nprint(\"PRE-TRAINING EVALUATION (synthetic only)\")\nprint(\"=\"*80)\n\n# FIX: Enable asa_info to handle model's tuple return type correctly\ngener['asa_info'] = True\n\npre_acc_train = eval_exact_match(train_examples, model, gener, max_new_tokens=8)\npre_acc_hold  = eval_exact_match(holdout_examples, model, gener, max_new_tokens=8)\n\nprint(f\"\\n[PRE] Exact-match accuracy:\")\nprint(f\"  Train:   {pre_acc_train:.3f} ({int(pre_acc_train*len(train_examples))}/{len(train_examples)})\")\nprint(f\"  Holdout: {pre_acc_hold:.3f} ({int(pre_acc_hold*len(holdout_examples))}/{len(holdout_examples)})\")\n\nprint(\"\\n[PRE] Next-token stats for sample of single-token targets (synthetic only):\")\nsample_for_stats = random.sample(pairs, min(30, len(pairs)))\nfor ex in sample_for_stats:\n    stats = next_token_stats(ex[\"prompt\"], ex[\"completion\"], model, tokenizer)\n    if stats[\"ok\"]:\n        print(f\"  {ex['tag']:<25} P={stats['p_target']:.4f} rank={stats['rank']:>5} top1={stats['top1']!r}\")\n    else:\n        print(f\"  {ex['tag']:<25} (skip) {stats['reason']}\")\n\n# -----------------------\n# 7) Light training (mixed: synthetic + wiki)\n# -----------------------\nprint(\"\\n\" + \"=\"*80)\nprint(\"TRAINING (mixed synthetic + wiki)\")\nprint(\"=\"*80)\n\nmodel.train()\n\n# IMPORTANT: optimizer must only see trainable params (esp for slot_attn_only)\ntrainable_params = [p for p in model.parameters() if p.requires_grad]\nif len(trainable_params) == 0:\n    raise RuntimeError(\"No trainable parameters. Check CFG['finetune_mode'] and regex.\")\n\nopt = torch.optim.AdamW(\n    trainable_params,\n    lr=CFG[\"lr\"],\n    betas=(0.9, 0.95),\n    weight_decay=CFG[\"weight_decay\"]\n)\n\nsteps = int(CFG[\"steps\"])\ngrad_clip = float(CFG[\"grad_clip\"])\n\nprint(f\"Training for {steps} steps with batch_size={train_dl.batch_size}\")\nprint(f\"Total mixed_train_examples={len(mixed_train_examples)} | synthetic={len(train_examples)} | wiki={len(wiki_examples)}\\n\")\n\n# stable batch stream (avoid re-instantiating iter(train_dl) each step)\nbatch_iter = itertools.cycle(train_dl)\n\nfor step in range(steps):\n    X, Y, _ = next(batch_iter)\n    opt.zero_grad(set_to_none=True)\n\n    logits = model(X)\n    logits = logits[0] if isinstance(logits, (tuple, list)) else logits\n\n    loss = F.cross_entropy(\n        logits.view(-1, logits.size(-1)),\n        Y.view(-1),\n        ignore_index=-100\n    )\n    loss.backward()\n    torch.nn.utils.clip_grad_norm_(trainable_params, grad_clip)\n    opt.step()\n\n    if (step + 1) % 50 == 0:\n        print(f\"  [train] step {step+1:>4}/{steps} loss={float(loss.item()):.4f}\")\n\nmodel.eval()\n\n# -----------------------\n# 8) Post-eval (synthetic only, as before)\n# -----------------------\nprint(\"\\n\" + \"=\"*80)\nprint(\"POST-TRAINING EVALUATION (synthetic only)\")\nprint(\"=\"*80)\n\npost_acc_train = eval_exact_match(train_examples, model, gener, max_new_tokens=8)\npost_acc_hold  = eval_exact_match(holdout_examples, model, gener, max_new_tokens=8)\n\nprint(f\"\\n[POST] Exact-match accuracy:\")\nprint(f\"  Train:   {post_acc_train:.3f} ({int(post_acc_train*len(train_examples))}/{len(train_examples)})\")\nprint(f\"  Holdout: {post_acc_hold:.3f} ({int(post_acc_hold*len(holdout_examples))}/{len(holdout_examples)})\")\n\nprint(f\"\\n[DELTA] Accuracy change:\")\nprint(f\"  Train:   {pre_acc_train:.3f} -> {post_acc_train:.3f} (\u0394={post_acc_train-pre_acc_train:+.3f})\")\nprint(f\"  Holdout: {pre_acc_hold:.3f} -> {post_acc_hold:.3f} (\u0394={post_acc_hold-pre_acc_hold:+.3f})\")\n\nprint(\"\\n[POST] Next-token stats for same sample (synthetic only):\")\nfor ex in sample_for_stats:\n    stats = next_token_stats(ex[\"prompt\"], ex[\"completion\"], model, tokenizer)\n    if stats[\"ok\"]:\n        print(f\"  {ex['tag']:<25} P={stats['p_target']:.4f} rank={stats['rank']:>5} top1={stats['top1']!r}\")\n\n# -----------------------\n# 9) Generations (synthetic categories only)\n# -----------------------\nprint(\"\\n\" + \"=\"*80)\nprint(\"GENERATION SAMPLES (greedy decoding) (synthetic only)\")\nprint(\"=\"*80)\n\ngeneration_samples = []\nby_category = {}\nfor ex in pairs:\n    cat = ex[\"tag\"].split(\":\")[0]\n    by_category.setdefault(cat, []).append(ex)\n\nfor cat, exs in sorted(by_category.items()):\n    generation_samples.extend(exs[:2])\n\ngeneration_samples = generation_samples[:25]\n\nfor ex in generation_samples:\n    raw = greedy_suffix(ex[\"prompt\"], model, gener, max_new_tokens=12)\n\n    tag_base = ex[\"tag\"].split(\":\")[0]\n    if tag_base == \"capital\":\n        scaffold_prompt = ex[\"prompt\"] + \" the city of\"\n    elif tag_base == \"language\":\n        scaffold_prompt = ex[\"prompt\"] + \" primarily\"\n    elif tag_base == \"currency\":\n        scaffold_prompt = ex[\"prompt\"]\n    else:\n        scaffold_prompt = ex[\"prompt\"]\n\n    sca = greedy_suffix(scaffold_prompt, model, gener, max_new_tokens=12)\n\n    print(f\"\\n{'\u2500'*80}\")\n    print(f\"CATEGORY: {ex['tag']:<25} TARGET: {ex['completion']!r}\")\n    print(f\"PROMPT:   {ex['prompt']!r}\")\n    print(f\"RAW:      {raw[:100]}\")\n    if scaffold_prompt != ex[\"prompt\"]:\n        print(f\"SCAFFOLD: {sca[:100]}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"EVALUATION COMPLETE\")\nprint(\"=\"*80)\n\n\n# ASA/ASM-specific generation\n\ndef asa_generate(\n    prompt: Union[str, List[int], torch.Tensor],\n    model: torch.nn.Module,\n    gen: Dict[str, Any],\n) -> Dict[str, Any]:\n    \"\"\"\n    Generation crafted for ASA/ASM models:\n      - Uses soft sampling by default (hard routing variants are unstable per your ablations).\n      - Optionally uses ASA internal telemetry (return_info=True) to perform *router-aware fallback*\n        when EOS-risk is high early, or when routing is pathologically branchy.\n      - Supports standard sampling controls + mild anti-repetition.\n      - Keeps inference dropout off.\n\n    Args\n    ----\n    prompt:\n        - str: requires gen[\"tokenizer\"] providing encode/decode\n        - List[int] / 1D torch.Tensor: token ids\n    model:\n        ASMLanguageModel (or compatible) returning logits or (logits, infos) if return_info=True\n    gen params (dict):\n        Required (if prompt is str):\n          tokenizer: a HF tokenizer with encode/decode\n        Common:\n          max_new_tokens: int (default 128)\n          temperature: float (default 0.8)\n          top_p: float (default 0.9)\n          top_k: int (default 50)\n          min_new_tokens: int (default 0)\n          eos_token_id: int (default tokenizer.eos_token_id if available)\n          pad_token_id: int (optional)\n          do_sample: bool (default True)\n          repetition_penalty: float (default 1.05)\n          no_repeat_ngram_size: int (default 3)\n          device: torch.device or str (default model device)\n        ASA-aware controls:\n          asa_info: bool (default True) -> request return_info and use it\n          eos_risk_threshold: float (default 0.25)\n          early_steps: int (default 24) -> window in which to apply EOS-risk mitigations\n          branchy_entropy_threshold: float (default None) -> if set, triggers extra sharpening\n          rescue_mode: str in {\"none\",\"scaffold\",\"resample\"} (default \"resample\")\n              - \"resample\": if EOS risk triggers, resample with lower temp / higher top_k keep\n              - \"scaffold\": if tokenizer provided and prompt looks like a known template,\n                            inject a short scaffold (see below) once at the start\n          rescue_temp: float (default 0.65)\n          rescue_top_p: float (default 0.85)\n          rescue_top_k: int (default 80)\n          max_resample_tries: int (default 4)\n        Return:\n          return_text: bool (default True if tokenizer present else False)\n\n    Returns\n    -------\n    dict with:\n      \"input_ids\": [1, T+new]\n      \"generated_ids\": [new]\n      \"text\": optional\n      \"info_trace\": optional list of per-step ASA stats (if asa_info=True)\n    \"\"\"\n    model.eval()\n\n    tokenizer = gen.get(\"tokenizer\", None)\n    device = gen.get(\"device\", None)\n    if device is None:\n        device = next(model.parameters()).device\n\n    # --- tokenize prompt ---\n    if isinstance(prompt, str):\n        if tokenizer is None:\n            raise ValueError(\"prompt is str but gen['tokenizer'] was not provided.\")\n        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n    elif isinstance(prompt, list):\n        input_ids = torch.tensor(prompt, device=device, dtype=torch.long).unsqueeze(0)\n    elif isinstance(prompt, torch.Tensor):\n        if prompt.dim() == 1:\n            input_ids = prompt.to(device=device, dtype=torch.long).unsqueeze(0)\n        elif prompt.dim() == 2:\n            input_ids = prompt.to(device=device, dtype=torch.long)\n        else:\n            raise ValueError(\"prompt tensor must be 1D or 2D token ids.\")\n    else:\n        raise TypeError(\"prompt must be str, List[int], or torch.Tensor of token ids.\")\n\n    max_new = int(gen.get(\"max_new_tokens\", 128))\n    min_new = int(gen.get(\"min_new_tokens\", 0))\n    do_sample = bool(gen.get(\"do_sample\", True))\n\n    temperature = float(gen.get(\"temperature\", 0.8))\n    top_p = float(gen.get(\"top_p\", 0.9))\n    top_k = int(gen.get(\"top_k\", 50))\n\n    repetition_penalty = float(gen.get(\"repetition_penalty\", 1.05))\n    no_repeat_ngram_size = int(gen.get(\"no_repeat_ngram_size\", 3))\n\n    eos_token_id = gen.get(\"eos_token_id\", None)\n    if eos_token_id is None and tokenizer is not None:\n        eos_token_id = tokenizer.eos_token_id\n    if eos_token_id is None:\n        eos_token_id = -1  # disable EOS logic if unknown\n\n    asa_info = bool(gen.get(\"asa_info\", True))\n    eos_risk_threshold = float(gen.get(\"eos_risk_threshold\", 0.25))\n    early_steps = int(gen.get(\"early_steps\", 24))\n    branchy_entropy_threshold = gen.get(\"branchy_entropy_threshold\", None)\n    rescue_mode = str(gen.get(\"rescue_mode\", \"resample\")).lower()\n    rescue_temp = float(gen.get(\"rescue_temp\", 0.65))\n    rescue_top_p = float(gen.get(\"rescue_top_p\", 0.85))\n    rescue_top_k = int(gen.get(\"rescue_top_k\", 80))\n    max_resample_tries = int(gen.get(\"max_resample_tries\", 4))\n\n    # Optional scaffold injection (architecture-aware: helps route trajectory)\n    if rescue_mode == \"scaffold\" and tokenizer is not None and isinstance(prompt, str):\n        # Very small, conservative scaffold set\u2014extend as you like\n        scaffolds = [\n            (\"The capital of\", \" the city of\"),\n            (\"Albert Einstein was born\", \" in\"),\n            (\"The scientific method involves\", \" the process of\"),\n            (\"The algorithm proceeds as follows\", \" 1.\"),\n        ]\n        for k, s in scaffolds:\n            if prompt.strip().startswith(k) and not prompt.strip().endswith(s.strip()):\n                input_ids = tokenizer.encode(prompt + s, return_tensors=\"pt\").to(device)\n                break\n\n    info_trace: List[Dict[str, float]] = []\n\n    # Generation loop\n    cur_ids = input_ids\n    for step in range(max_new):\n        # Model forward\n        if asa_info:\n            out = model(cur_ids, return_info=True)\n            logits, infos = out\n            # infos is list per layer; take last block's light stats if present\n            last = infos[-1] if isinstance(infos, list) and len(infos) > 0 else None\n            stat = {}\n            if isinstance(last, dict):\n                # these are CPU tensors in your module; cast to float if present\n                for k in [\"entropy_mean\", \"top1freq_mean\", \"content_read_gamma_mean\", \"slotspace_gate_mean\", \"slotspace_delta_norm\"]:\n                    if k in last and last[k] is not None:\n                        try:\n                            stat[k] = float(last[k].item())\n                        except Exception:\n                            pass\n            # Store later for debugging\n        else:\n            logits = model(cur_ids, return_info=False)\n            stat = None\n\n        next_logits = logits[0, -1, :]  # [V]\n\n        # Basic constraints\n        if step < min_new and eos_token_id >= 0:\n            next_logits = next_logits.clone()\n            next_logits[eos_token_id] = float(\"-inf\")\n\n        # Anti-repetition (mild, usually good for ASA because content-read is self-referential)\n        gen_so_far = cur_ids[0, input_ids.shape[1]:]  # only newly generated, if any\n        next_logits = _apply_repetition_penalty(next_logits, gen_so_far, repetition_penalty)\n        next_logits = _no_repeat_ngram_ban(next_logits, cur_ids[0], no_repeat_ngram_size)\n\n        # Router-aware rescue (early EOS / excessive branchiness)\n        # Use next-token EOS risk; optionally sharpen if branchy.\n        tries = 0\n        used_temp, used_top_p, used_top_k = temperature, top_p, top_k\n        while True:\n            l = next_logits\n            if used_temp and used_temp > 0:\n                l = l / used_temp\n\n            l = _top_k_top_p_filtering(l, top_k=used_top_k, top_p=used_top_p)\n\n            probs = F.softmax(l, dim=-1)\n            p_eos = float(probs[eos_token_id].item()) if eos_token_id >= 0 else 0.0\n            ent = float(-(probs.clamp_min(1e-12) * probs.clamp_min(1e-12).log()).sum().item())\n\n            # Condition: early EOS risk is too high\n            eos_risky = (eos_token_id >= 0) and (step < early_steps) and (p_eos > eos_risk_threshold)\n\n            # Condition: branchy token distribution (optional) -> reduce temperature a bit\n            branchy = False\n            if branchy_entropy_threshold is not None and step < early_steps:\n                branchy = ent > float(branchy_entropy_threshold)\n\n            if (eos_risky or branchy) and rescue_mode == \"resample\" and tries < max_resample_tries:\n                used_temp = min(used_temp, rescue_temp)\n                used_top_p = min(used_top_p, rescue_top_p)\n                used_top_k = max(used_top_k, rescue_top_k)\n                tries += 1\n                continue\n\n            # Choose token\n            if do_sample:\n                next_id = torch.multinomial(probs, num_samples=1)\n            else:\n                next_id = torch.argmax(probs, dim=-1, keepdim=True)\n\n            break\n\n        # Log trace\n        if asa_info:\n            rec = {\"step\": float(step), \"token_entropy\": float(ent), \"p_eos\": float(p_eos)}\n            if stat:\n                for k, v in stat.items():\n                    rec[k] = float(v)\n            # record rescue adjustments\n            rec[\"temp_used\"] = float(used_temp)\n            rec[\"top_p_used\"] = float(used_top_p)\n            rec[\"top_k_used\"] = float(used_top_k)\n            info_trace.append(rec)\n\n        # Append token\n        cur_ids = torch.cat([cur_ids, next_id.view(1, 1)], dim=1)\n\n        # Stop on EOS\n        if eos_token_id >= 0 and int(next_id.item()) == int(eos_token_id) and step >= min_new:\n            break\n\n    generated_ids = cur_ids[:, input_ids.shape[1]:]\n\n    out: Dict[str, Any] = {\n        \"input_ids\": cur_ids,\n        \"generated_ids\": generated_ids,\n    }\n    if asa_info:\n        out[\"info_trace\"] = info_trace\n\n    return_text = bool(gen.get(\"return_text\", tokenizer is not None))\n    if return_text and tokenizer is not None:\n        out[\"text\"] = tokenizer.decode(cur_ids[0].tolist(), skip_special_tokens=False)\n\n    return out\n\n\n# =========================\n# PATCH 1: wrappers for your crafted asa_generate\n# =========================\n\n@torch.no_grad()\n\ndef asa_greedy_suffix(\n    prompt: str,\n    model: torch.nn.Module,\n    gen: dict,\n    max_new_tokens: int = 8,\n    strip: bool = True,\n) -> str:\n    \"\"\"\n    Runs your asa_generate in greedy mode and returns ONLY the suffix after `prompt`.\n    This is what you want for exact-match checks / scoring.\n    \"\"\"\n    # Copy gen so we can override safely\n    g = dict(gen)\n    g[\"do_sample\"] = False\n    g[\"max_new_tokens\"] = int(max_new_tokens)\n\n    out = asa_generate(prompt, model, g)\n    text = out.get(\"text\", None)\n    if text is None:\n        # Fallback: decode manually\n        tok = g.get(\"tokenizer\", None)\n        if tok is None:\n            raise ValueError(\"No decoded text available; provide gen['tokenizer'].\")\n        text = tok.decode(out[\"input_ids\"][0].tolist(), skip_special_tokens=False)\n\n    # Suffix (best-effort): if prompt string matches prefix of decoded text\n    if text.startswith(prompt):\n        suf = text[len(prompt):]\n    else:\n        # Robust fallback: try to locate the prompt inside the decoded text\n        idx = text.find(prompt)\n        suf = text[idx + len(prompt):] if idx >= 0 else text\n\n    if strip:\n        suf = suf.replace(\"\\n\", \" \").strip()\n    return suf\n\n\n@torch.no_grad()\n\ndef asa_generate_many(\n    prompts: list,\n    model: torch.nn.Module,\n    gen: dict,\n    do_sample: bool = False,\n    max_new_tokens: int = 8,\n) -> list:\n    \"\"\"\n    Convenience wrapper: runs asa_generate per prompt (loop) and returns decoded texts.\n    \"\"\"\n    g = dict(gen)\n    g[\"do_sample\"] = bool(do_sample)\n    g[\"max_new_tokens\"] = int(max_new_tokens)\n\n    outs = []\n    for p in prompts:\n        out = asa_generate(p, model, g)\n        text = out.get(\"text\", None)\n        if text is None:\n            tok = g.get(\"tokenizer\", None)\n            if tok is None:\n                raise ValueError(\"No decoded text available; provide gen['tokenizer'].\")\n            text = tok.decode(out[\"input_ids\"][0].tolist(), skip_special_tokens=False)\n        outs.append(text)\n    return outs\n\n@torch.no_grad()\n\ndef score_next_token_rank(\n    prompt: str,\n    target_token: str,\n    model: torch.nn.Module,\n    gen: dict,\n) -> dict:\n    \"\"\"\n    Computes P(target) and rank for the *next token* only, matching your printed diagnostics.\n    \"\"\"\n    tok = gen[\"tokenizer\"]\n    device = next(model.parameters()).device\n\n    # encode prompt\n    input_ids = tok.encode(prompt, return_tensors=\"pt\").to(device)\n\n    # encode target token as a single token (best-effort)\n    target_ids = tok.encode(target_token, add_special_tokens=False)\n    if len(target_ids) != 1:\n        return {\"ok\": False, \"reason\": f\"target_token maps to {len(target_ids)} tokens\", \"target_ids\": target_ids}\n\n    target_id = target_ids[0]\n\n    model.eval()\n    logits = model(input_ids)  # if your model needs return_info=False default\n    if isinstance(logits, (tuple, list)):\n        logits = logits[0]\n    next_logits = logits[0, -1, :]\n\n    probs = torch.softmax(next_logits, dim=-1)\n    p_t = float(probs[target_id].item())\n\n    # rank: 1 = best\n    sorted_idx = torch.argsort(next_logits, descending=True)\n    rank = int((sorted_idx == target_id).nonzero(as_tuple=False).item()) + 1\n\n    return {\"ok\": True, \"p_target\": p_t, \"rank\": rank, \"target_id\": target_id}\n\n#\n\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Section 4 \u2014 Sample generations (pre-finetune)\nRuns a small batch of prompts to show baseline text generation behavior."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "gener = dict(\n    tokenizer=tokenizer,\n    max_new_tokens=32,\n    temperature=0.1,\n    top_p=0.95,\n    top_k=80,\n    repetition_penalty=1.03,\n    no_repeat_ngram_size=3,\n    asa_info=False,\n    rescue_mode=None,\n)\n\nprint('#' * 5, 'Countries', '#' * 5)\nfinishers = ['is', 'sounds like', 'consists of', 'is a form of', 'all changed when']\nqualities = ['capital', 'language', 'geography', 'government', 'history']\ncountries = ['France', 'Spain', 'Russia', 'Italy', 'Japan', 'Egypt', 'Germany', 'Brazil']\nfor country in countries:\n    for quality, finisher in zip(qualities, finishers):\n        out = asa_generate(f'The {quality} of {country} {finisher}', model, gener)\n        print(out['text'])\n\nprint('#' * 5, 'People', '#' * 5)\npeople = ['Albert Einstein', 'George Patton', 'Charles Darwin', 'George Washington', 'Winston Churchill']\nfactoids = ['was born', 'contributed', 'accomplished', 'had a strong opinion about', 'died']\nfor person in people:\n    for factoid in factoids:\n        out = asa_generate(f'{person} {factoid}', model, gener)\n        print(out['text'])\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Section 5 \u2014 Synthetic mini-alignment dataset\nBuilds a large prompt/completion set (capitals, languages, currencies, etc.) used for the mini finetune loop."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def is_single_token(s: str, tokenizer) -> bool:\n    ids = tokenizer.encode(s, add_special_tokens=False)\n    return len(ids) == 1\n\ndef build_pairs_expanded(tokenizer):\n    \"\"\"\n    Massively expanded dataset generation with WikiText-103 style templates.\n    Includes geographical, historical, scientific, cultural, and biographical facts.\n    \"\"\"\n    pairs = []\n\n    # ========================================\n    # GEOGRAPHY SECTION (Massively Expanded)\n    # ========================================\n\n    # ---- Capitals (comprehensive list)\n    capitals = {\n        # Europe\n        \"France\": \" Paris\",\n        \"Germany\": \" Berlin\",\n        \"Italy\": \" Rome\",\n        \"Spain\": \" Madrid\",\n        \"Portugal\": \" Lisbon\",\n        \"Greece\": \" Athens\",\n        \"Austria\": \" Vienna\",\n        \"Poland\": \" Warsaw\",\n        \"Norway\": \" Oslo\",\n        \"Sweden\": \" Stockholm\",\n        \"Finland\": \" Helsinki\",\n        \"Denmark\": \" Copenhagen\",\n        \"Ireland\": \" Dublin\",\n        \"Belgium\": \" Brussels\",\n        \"Netherlands\": \" Amsterdam\",\n        \"Switzerland\": \" Bern\",\n        \"Czech Republic\": \" Prague\",\n        \"Hungary\": \" Budapest\",\n        \"Romania\": \" Bucharest\",\n        \"Bulgaria\": \" Sofia\",\n        \"Croatia\": \" Zagreb\",\n        \"Serbia\": \" Belgrade\",\n        \"Slovakia\": \" Bratislava\",\n        \"Slovenia\": \" Ljubljana\",\n        \"Lithuania\": \" Vilnius\",\n        \"Latvia\": \" Riga\",\n        \"Estonia\": \" Tallinn\",\n        \"Iceland\": \" Reykjavik\",\n        \"Luxembourg\": \" Luxembourg\",\n        \"Malta\": \" Valletta\",\n        \"Cyprus\": \" Nicosia\",\n\n        # Asia\n        \"Japan\": \" Tokyo\",\n        \"China\": \" Beijing\",\n        \"India\": \" Delhi\",\n        \"South Korea\": \" Seoul\",\n        \"North Korea\": \" Pyongyang\",\n        \"Thailand\": \" Bangkok\",\n        \"Vietnam\": \" Hanoi\",\n        \"Indonesia\": \" Jakarta\",\n        \"Philippines\": \" Manila\",\n        \"Malaysia\": \" Kuala\",\n        \"Singapore\": \" Singapore\",\n        \"Myanmar\": \" Naypyidaw\",\n        \"Cambodia\": \" Phnom\",\n        \"Laos\": \" Vientiane\",\n        \"Bangladesh\": \" Dhaka\",\n        \"Pakistan\": \" Islamabad\",\n        \"Afghanistan\": \" Kabul\",\n        \"Iran\": \" Tehran\",\n        \"Iraq\": \" Baghdad\",\n        \"Saudi Arabia\": \" Riyadh\",\n        \"Turkey\": \" Ankara\",\n        \"Israel\": \" Jerusalem\",\n        \"Jordan\": \" Amman\",\n        \"Lebanon\": \" Beirut\",\n        \"Syria\": \" Damascus\",\n        \"Yemen\": \" Sanaa\",\n        \"Oman\": \" Muscat\",\n        \"Kuwait\": \" Kuwait\",\n        \"Qatar\": \" Doha\",\n        \"Bahrain\": \" Manama\",\n        \"United Arab Emirates\": \" Abu\",\n        \"Nepal\": \" Kathmandu\",\n        \"Sri Lanka\": \" Colombo\",\n        \"Mongolia\": \" Ulaanbaatar\",\n        \"Kazakhstan\": \" Astana\",\n        \"Uzbekistan\": \" Tashkent\",\n\n        # Africa\n        \"Egypt\": \" Cairo\",\n        \"South Africa\": \" Pretoria\",\n        \"Nigeria\": \" Abuja\",\n        \"Kenya\": \" Nairobi\",\n        \"Ethiopia\": \" Addis\",\n        \"Morocco\": \" Rabat\",\n        \"Algeria\": \" Algiers\",\n        \"Tunisia\": \" Tunis\",\n        \"Libya\": \" Tripoli\",\n        \"Sudan\": \" Khartoum\",\n        \"Ghana\": \" Accra\",\n        \"Tanzania\": \" Dodoma\",\n        \"Uganda\": \" Kampala\",\n        \"Angola\": \" Luanda\",\n        \"Mozambique\": \" Maputo\",\n        \"Zimbabwe\": \" Harare\",\n        \"Zambia\": \" Lusaka\",\n        \"Senegal\": \" Dakar\",\n        \"Ivory Coast\": \" Yamoussoukro\",\n        \"Cameroon\": \" Yaounde\",\n\n        # Americas\n        \"United States\": \" Washington\",\n        \"Canada\": \" Ottawa\",\n        \"Mexico\": \" Mexico\",\n        \"Brazil\": \" Brasilia\",\n        \"Argentina\": \" Buenos\",\n        \"Chile\": \" Santiago\",\n        \"Colombia\": \" Bogota\",\n        \"Peru\": \" Lima\",\n        \"Venezuela\": \" Caracas\",\n        \"Ecuador\": \" Quito\",\n        \"Bolivia\": \" La\",\n        \"Paraguay\": \" Asuncion\",\n        \"Uruguay\": \" Montevideo\",\n        \"Cuba\": \" Havana\",\n        \"Jamaica\": \" Kingston\",\n        \"Costa Rica\": \" San\",\n        \"Panama\": \" Panama\",\n        \"Guatemala\": \" Guatemala\",\n        \"Honduras\": \" Tegucigalpa\",\n        \"Nicaragua\": \" Managua\",\n\n        # Oceania\n        \"Australia\": \" Canberra\",\n        \"New Zealand\": \" Wellington\",\n        \"Papua New Guinea\": \" Port\",\n        \"Fiji\": \" Suva\",\n\n        # Former USSR\n        \"Russia\": \" Moscow\",\n        \"Ukraine\": \" Kyiv\",\n        \"Belarus\": \" Minsk\",\n        \"Georgia\": \" Tbilisi\",\n        \"Armenia\": \" Yerevan\",\n        \"Azerbaijan\": \" Baku\",\n    }\n\n    for c, cap in capitals.items():\n        pairs.append({\"prompt\": f\"The capital of {c} is\", \"completion\": cap, \"tag\": f\"capital:{c}\"})\n        pairs.append({\"prompt\": f\"{c}'s capital city is\", \"completion\": cap, \"tag\": f\"capital:{c}\"})\n        pairs.append({\"prompt\": f\"{c} has its capital in\", \"completion\": cap, \"tag\": f\"capital:{c}\"})\n\n    # ---- Languages (comprehensive)\n    languages = {\n        \"France\": \" French\",\n        \"Germany\": \" German\",\n        \"Italy\": \" Italian\",\n        \"Japan\": \" Japanese\",\n        \"Spain\": \" Spanish\",\n        \"Russia\": \" Russian\",\n        \"Brazil\": \" Portuguese\",\n        \"Portugal\": \" Portuguese\",\n        \"Egypt\": \" Arabic\",\n        \"China\": \" Chinese\",\n        \"India\": \" Hindi\",\n        \"Mexico\": \" Spanish\",\n        \"Argentina\": \" Spanish\",\n        \"Netherlands\": \" Dutch\",\n        \"Greece\": \" Greek\",\n        \"Poland\": \" Polish\",\n        \"Turkey\": \" Turkish\",\n        \"Iran\": \" Persian\",\n        \"Israel\": \" Hebrew\",\n        \"Sweden\": \" Swedish\",\n        \"Norway\": \" Norwegian\",\n        \"Denmark\": \" Danish\",\n        \"Finland\": \" Finnish\",\n        \"Czech Republic\": \" Czech\",\n        \"Hungary\": \" Hungarian\",\n        \"Romania\": \" Romanian\",\n        \"Thailand\": \" Thai\",\n        \"Vietnam\": \" Vietnamese\",\n        \"South Korea\": \" Korean\",\n    }\n    for c, lang in languages.items():\n        pairs.append({\"prompt\": f\"The language of {c} is\", \"completion\": lang, \"tag\": f\"language:{c}\"})\n        pairs.append({\"prompt\": f\"The official language of {c} is\", \"completion\": lang, \"tag\": f\"language:{c}\"})\n        pairs.append({\"prompt\": f\"People in {c} speak\", \"completion\": lang, \"tag\": f\"language:{c}\"})\n\n    # ---- Currencies (comprehensive)\n    currencies = {\n        \"Japan\": \" yen\",\n        \"Russia\": \" ruble\",\n        \"India\": \" rupee\",\n        \"Mexico\": \" peso\",\n        \"China\": \" yuan\",\n        \"United Kingdom\": \" pound\",\n        \"United States\": \" dollar\",\n        \"Canada\": \" dollar\",\n        \"Australia\": \" dollar\",\n        \"Germany\": \" euro\",\n        \"France\": \" euro\",\n        \"Italy\": \" euro\",\n        \"Spain\": \" euro\",\n        \"Portugal\": \" euro\",\n        \"Greece\": \" euro\",\n        \"Austria\": \" euro\",\n        \"Netherlands\": \" euro\",\n        \"Belgium\": \" euro\",\n        \"Poland\": \" zloty\",\n        \"Czech Republic\": \" koruna\",\n        \"Sweden\": \" krona\",\n        \"Norway\": \" krone\",\n        \"Denmark\": \" krone\",\n        \"Switzerland\": \" franc\",\n        \"Brazil\": \" real\",\n        \"South Africa\": \" rand\",\n        \"Turkey\": \" lira\",\n        \"Thailand\": \" baht\",\n        \"Indonesia\": \" rupiah\",\n    }\n    for c, cur in currencies.items():\n        pairs.append({\"prompt\": f\"The currency of {c} is the\", \"completion\": cur, \"tag\": f\"currency:{c}\"})\n        pairs.append({\"prompt\": f\"{c} uses the\", \"completion\": cur, \"tag\": f\"currency:{c}\"})\n\n    # ---- Continents (expanded with variations)\n    continents = {\n        \"France\": \" Europe\",\n        \"Germany\": \" Europe\",\n        \"Italy\": \" Europe\",\n        \"Spain\": \" Europe\",\n        \"Poland\": \" Europe\",\n        \"Greece\": \" Europe\",\n        \"Sweden\": \" Europe\",\n        \"Norway\": \" Europe\",\n        \"Russia\": \" Europe\",\n        \"Egypt\": \" Africa\",\n        \"Nigeria\": \" Africa\",\n        \"Kenya\": \" Africa\",\n        \"South Africa\": \" Africa\",\n        \"Morocco\": \" Africa\",\n        \"Japan\": \" Asia\",\n        \"China\": \" Asia\",\n        \"India\": \" Asia\",\n        \"Thailand\": \" Asia\",\n        \"Vietnam\": \" Asia\",\n        \"Indonesia\": \" Asia\",\n        \"Brazil\": \" South\",\n        \"Argentina\": \" South\",\n        \"Chile\": \" South\",\n        \"Peru\": \" South\",\n        \"Colombia\": \" South\",\n        \"Canada\": \" North\",\n        \"United States\": \" North\",\n        \"Mexico\": \" North\",\n        \"Australia\": \" Oceania\",\n        \"New Zealand\": \" Oceania\",\n    }\n    for c, cont in continents.items():\n        pairs.append({\"prompt\": f\"{c} is in\", \"completion\": cont, \"tag\": f\"continent:{c}\"})\n        pairs.append({\"prompt\": f\"{c} is located in\", \"completion\": cont, \"tag\": f\"continent:{c}\"})\n\n    # ---- Major Rivers\n    rivers = {\n        \"The Nile flows through\": \" Egypt\",\n        \"The Amazon flows through\": \" Brazil\",\n        \"The Thames flows through\": \" London\",\n        \"The Seine flows through\": \" Paris\",\n        \"The Danube flows through\": \" Europe\",\n        \"The Rhine flows through\": \" Germany\",\n        \"The Ganges flows through\": \" India\",\n        \"The Yangtze flows through\": \" China\",\n        \"The Mississippi flows through\": \" America\",\n        \"The Nile is located in\": \" Africa\",\n        \"The Amazon is in\": \" South\",\n        \"The Rhine is in\": \" Europe\",\n    }\n    for p, comp in rivers.items():\n        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"rivers\"})\n\n    # ---- Mountain ranges and peaks\n    mountains = {\n        \"Mount Everest is in\": \" Nepal\",\n        \"The Alps are in\": \" Europe\",\n        \"The Himalayas are in\": \" Asia\",\n        \"The Andes are in\": \" South\",\n        \"The Rocky Mountains are in\": \" North\",\n        \"Mount Fuji is in\": \" Japan\",\n        \"The Pyrenees are between France and\": \" Spain\",\n    }\n    for p, comp in mountains.items():\n        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"mountains\"})\n\n    # ---- Oceans and seas\n    oceans = {\n        \"The Pacific Ocean is the\": \" largest\",\n        \"The Atlantic Ocean is the\": \" second\",\n        \"The Mediterranean Sea is in\": \" Europe\",\n        \"The Caribbean Sea is in\": \" Central\",\n        \"The Baltic Sea is in\": \" Northern\",\n    }\n    for p, comp in oceans.items():\n        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"oceans\"})\n\n    # ========================================\n    # HISTORICAL FACTS (Massively Expanded)\n    # ========================================\n\n    # ---- Birth locations (expanded)\n    born_in = {\n        \"Albert Einstein was born in\": \" Germany\",\n        \"Charles Darwin was born in\": \" England\",\n        \"George Washington was born in\": \" Virginia\",\n        \"Winston Churchill was born in\": \" England\",\n        \"Napoleon Bonaparte was born in\": \" Corsica\",\n        \"Leonardo da Vinci was born in\": \" Italy\",\n        \"William Shakespeare was born in\": \" England\",\n        \"Isaac Newton was born in\": \" England\",\n        \"Marie Curie was born in\": \" Poland\",\n        \"Galileo Galilei was born in\": \" Italy\",\n        \"Aristotle was born in\": \" Greece\",\n        \"Plato was born in\": \" Athens\",\n        \"Confucius was born in\": \" China\",\n        \"Buddha was born in\": \" Nepal\",\n        \"Muhammad Ali was born in\": \" Kentucky\",\n        \"Martin Luther King was born in\": \" Georgia\",\n        \"Abraham Lincoln was born in\": \" Kentucky\",\n        \"Thomas Edison was born in\": \" Ohio\",\n        \"Nikola Tesla was born in\": \" Croatia\",\n        \"Sigmund Freud was born in\": \" Czech\",\n    }\n    for p, comp in born_in.items():\n        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"born_in\"})\n\n    # ---- Death years (single token years)\n    death_years = {\n        \"Albert Einstein died in\": \" 1955\",\n        \"Isaac Newton died in\": \" 1727\",\n        \"Charles Darwin died in\": \" 1882\",\n        \"Leonardo da Vinci died in\": \" 1519\",\n        \"William Shakespeare died in\": \" 1616\",\n        \"George Washington died in\": \" 1799\",\n        \"Napoleon Bonaparte died in\": \" 1821\",\n        \"Abraham Lincoln died in\": \" 1865\",\n        \"Marie Curie died in\": \" 1934\",\n        \"Nikola Tesla died in\": \" 1943\",\n    }\n    for p, comp in death_years.items():\n        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"death_year\"})\n\n    # ---- Historical events and dates\n    historical_events = {\n        \"World War I began in\": \" 1914\",\n        \"World War II began in\": \" 1939\",\n        \"World War II ended in\": \" 1945\",\n        \"The American Revolution began in\": \" 1775\",\n        \"The French Revolution began in\": \" 1789\",\n        \"The Russian Revolution was in\": \" 1917\",\n        \"The fall of the Berlin Wall was in\": \" 1989\",\n        \"The September 11 attacks occurred in\": \" 2001\",\n        \"The moon landing was in\": \" 1969\",\n        \"Christopher Columbus sailed in\": \" 1492\",\n        \"The Declaration of Independence was signed in\": \" 1776\",\n        \"The Civil War began in\": \" 1861\",\n        \"The Great Depression began in\": \" 1929\",\n        \"The Cold War began after\": \" 1945\",\n    }\n    for p, comp in historical_events.items():\n        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"historical_event\"})\n\n    # ---- Century associations\n    centuries = {\n        \"The Renaissance occurred in the\": \" 15th\",\n        \"The Industrial Revolution began in the\": \" 18th\",\n        \"The Enlightenment was in the\": \" 18th\",\n        \"The Victorian Era was in the\": \" 19th\",\n        \"World War I was in the\": \" 20th\",\n    }\n    for p, comp in centuries.items():\n        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"century\"})\n\n    # ---- Leaders and rulers\n    leaders = {\n        \"Julius Caesar was a\": \" Roman\",\n        \"Alexander the Great was a\": \" Macedonian\",\n        \"Cleopatra was the queen of\": \" Egypt\",\n        \"Queen Victoria ruled\": \" Britain\",\n        \"Napoleon was the emperor of\": \" France\",\n        \"Peter the Great ruled\": \" Russia\",\n        \"Elizabeth I was queen of\": \" England\",\n        \"Henry VIII was king of\": \" England\",\n        \"Louis XIV was king of\": \" France\",\n    }\n    for p, comp in leaders.items():\n        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"leader\"})\n\n    # ========================================\n    # SCIENTIFIC FACTS (Massively Expanded)\n    # ========================================\n\n    # ---- Physics facts\n    physics = {\n        \"The speed of light is approximately\": \" 300\",\n        \"Gravity was discovered by\": \" Newton\",\n        \"Einstein developed the theory of\": \" relativity\",\n        \"The atomic bomb was developed during\": \" World\",\n        \"Newton's laws describe\": \" motion\",\n        \"Electrons have a\": \" negative\",\n        \"Protons have a\": \" positive\",\n        \"The Earth orbits the\": \" Sun\",\n        \"The Moon orbits the\": \" Earth\",\n    }\n    for p, comp in physics.items():\n        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"physics\"})\n\n    # ---- Chemistry facts\n    chemistry = {\n        \"Water is composed of hydrogen and\": \" oxygen\",\n        \"The symbol for gold is\": \" Au\",\n        \"The symbol for silver is\": \" Ag\",\n        \"The symbol for iron is\": \" Fe\",\n        \"The periodic table was created by\": \" Mendeleev\",\n        \"Oxygen has atomic number\": \" 8\",\n        \"Carbon has atomic number\": \" 6\",\n        \"Hydrogen has atomic number\": \" 1\",\n        \"Salt is composed of sodium and\": \" chloride\",\n    }\n    for p, comp in chemistry.items():\n        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"chemistry\"})\n\n    # ---- Biology facts\n    biology = {\n        \"DNA stands for deoxyribonucleic\": \" acid\",\n        \"Photosynthesis occurs in\": \" plants\",\n        \"The heart pumps\": \" blood\",\n        \"Humans have\": \" 46\",\n        \"Evolution was proposed by\": \" Darwin\",\n        \"Cells are the basic unit of\": \" life\",\n        \"Mitochondria produce\": \" energy\",\n        \"The largest organ is the\": \" skin\",\n    }\n    for p, comp in biology.items():\n        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"biology\"})\n\n    # ---- Mathematics facts\n    mathematics = {\n        \"Pi is approximately\": \" 3\",\n        \"A triangle has\": \" three\",\n        \"A square has\": \" four\",\n        \"A circle has\": \" 360\",\n        \"The Pythagorean theorem relates\": \" triangles\",\n        \"Calculus was invented by\": \" Newton\",\n        \"Algebra originated in\": \" ancient\",\n    }\n    for p, comp in mathematics.items():\n        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"mathematics\"})\n\n    # ---- Astronomy facts\n    astronomy = {\n        \"The Sun is a\": \" star\",\n        \"Jupiter is a\": \" gas\",\n        \"Mars is the\": \" red\",\n        \"Saturn has\": \" rings\",\n        \"The Solar System has\": \" eight\",\n        \"The Milky Way is a\": \" galaxy\",\n        \"A light year measures\": \" distance\",\n        \"The nearest star to Earth is the\": \" Sun\",\n        \"Pluto was reclassified as a\": \" dwarf\",\n    }\n    for p, comp in astronomy.items():\n        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"astronomy\"})\n\n    # ========================================\n    # CULTURAL FACTS (Massively Expanded)\n    # ========================================\n\n    # ---- Literature and authors\n    literature = {\n        \"Shakespeare wrote\": \" Hamlet\",\n        \"Homer wrote the\": \" Odyssey\",\n        \"Tolkien wrote The Lord of the\": \" Rings\",\n        \"George Orwell wrote\": \" 1984\",\n        \"Jane Austen wrote Pride and\": \" Prejudice\",\n        \"Mark Twain wrote The Adventures of\": \" Tom\",\n        \"Charles Dickens wrote A Tale of\": \" Two\",\n        \"Ernest Hemingway wrote The Old Man and the\": \" Sea\",\n        \"F. Scott Fitzgerald wrote The Great\": \" Gatsby\",\n        \"Leo Tolstoy wrote War and\": \" Peace\",\n        \"Fyodor Dostoevsky wrote Crime and\": \" Punishment\",\n        \"Victor Hugo wrote Les\": \" Miserables\",\n        \"Miguel de Cervantes wrote Don\": \" Quixote\",\n        \"Dante wrote The Divine\": \" Comedy\",\n        \"Virgil wrote the\": \" Aeneid\",\n    }\n    for p, comp in literature.items():\n        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"literature\"})\n\n    # ---- Art and artists\n    art = {\n        \"Leonardo da Vinci painted the Mona\": \" Lisa\",\n        \"Vincent van Gogh painted Starry\": \" Night\",\n        \"Pablo Picasso was a\": \" Spanish\",\n        \"Michelangelo painted the Sistine\": \" Chapel\",\n        \"Claude Monet was an\": \" Impressionist\",\n        \"Salvador Dali was a\": \" Surrealist\",\n        \"Rembrandt was a\": \" Dutch\",\n        \"Andy Warhol was a\": \" Pop\",\n    }\n    for p, comp in art.items():\n        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"art\"})\n\n    # ---- Music and composers\n    music = {\n        \"Mozart was a\": \" composer\",\n        \"Beethoven wrote\": \" symphonies\",\n        \"Bach was a\": \" Baroque\",\n        \"Chopin was a\": \" Polish\",\n        \"Tchaikovsky was a\": \" Russian\",\n        \"Wagner was a\": \" German\",\n        \"Vivaldi wrote The Four\": \" Seasons\",\n        \"Handel wrote\": \" Messiah\",\n    }\n    for p, comp in music.items():\n        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"music\"})\n\n    # ---- Sports facts\n    sports = {\n        \"The Olympics originated in\": \" Greece\",\n        \"Soccer is called football in\": \" Europe\",\n        \"Basketball was invented in\": \" America\",\n        \"Baseball is popular in\": \" America\",\n        \"Cricket is popular in\": \" India\",\n        \"The World Cup is held every\": \" four\",\n        \"Tennis is played on a\": \" court\",\n        \"Golf is played on a\": \" course\",\n    }\n    for p, comp in sports.items():\n        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"sports\"})\n\n    # ========================================\n    # TECHNOLOGY AND INVENTIONS\n    # ========================================\n\n    technology = {\n        \"The telephone was invented by\": \" Bell\",\n        \"The light bulb was invented by\": \" Edison\",\n        \"The airplane was invented by the Wright\": \" Brothers\",\n        \"The printing press was invented by\": \" Gutenberg\",\n        \"The steam engine was invented by\": \" Watt\",\n        \"The radio was invented by\": \" Marconi\",\n        \"The computer was invented in the\": \" 20th\",\n        \"The internet was developed in\": \" America\",\n        \"Apple was founded by Steve\": \" Jobs\",\n        \"Microsoft was founded by Bill\": \" Gates\",\n        \"Facebook was founded by Mark\": \" Zuckerberg\",\n    }\n    for p, comp in technology.items():\n        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"technology\"})\n\n    # ========================================\n    # ARCHITECTURE AND LANDMARKS\n    # ========================================\n\n    landmarks = {\n        \"The Eiffel Tower is in\": \" Paris\",\n        \"The Colosseum is in\": \" Rome\",\n        \"The Taj Mahal is in\": \" India\",\n        \"The Great Wall is in\": \" China\",\n        \"The Statue of Liberty is in\": \" New\",\n        \"Big Ben is in\": \" London\",\n        \"The Pyramids are in\": \" Egypt\",\n        \"The Parthenon is in\": \" Athens\",\n        \"The Kremlin is in\": \" Moscow\",\n        \"Machu Picchu is in\": \" Peru\",\n        \"Petra is in\": \" Jordan\",\n        \"Angkor Wat is in\": \" Cambodia\",\n        \"The Sydney Opera House is in\": \" Australia\",\n    }\n    for p, comp in landmarks.items():\n        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"landmarks\"})\n\n    # ========================================\n    # ANIMALS AND NATURE\n    # ========================================\n\n    animals = {\n        \"The largest animal is the blue\": \" whale\",\n        \"The fastest land animal is the\": \" cheetah\",\n        \"The tallest animal is the\": \" giraffe\",\n        \"Lions are found in\": \" Africa\",\n        \"Pandas are native to\": \" China\",\n        \"Kangaroos are native to\": \" Australia\",\n        \"Penguins live in\": \" Antarctica\",\n        \"Tigers are native to\": \" Asia\",\n        \"Elephants are found in\": \" Africa\",\n        \"Polar bears live in the\": \" Arctic\",\n    }\n    for p, comp in animals.items():\n        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"animals\"})\n\n    # ========================================\n    # RELIGIONS AND MYTHOLOGY\n    # ========================================\n\n    religions = {\n        \"Christianity originated in\": \" Israel\",\n        \"Islam originated in\": \" Saudi\",\n        \"Buddhism originated in\": \" India\",\n        \"Hinduism originated in\": \" India\",\n        \"Judaism originated in\": \" Israel\",\n        \"The Bible is the holy book of\": \" Christianity\",\n        \"The Quran is the holy book of\": \" Islam\",\n        \"Zeus was the king of the\": \" Greek\",\n        \"Thor was a\": \" Norse\",\n        \"Ra was an\": \" Egyptian\",\n    }\n    for p, comp in religions.items():\n        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"religion\"})\n\n    # ========================================\n    # FOOD AND CUISINE\n    # ========================================\n\n    cuisine = {\n        \"Pizza originated in\": \" Italy\",\n        \"Sushi originated in\": \" Japan\",\n        \"Tacos originated in\": \" Mexico\",\n        \"Hamburgers are popular in\": \" America\",\n        \"Pasta is from\": \" Italy\",\n        \"Croissants are from\": \" France\",\n        \"Curry is from\": \" India\",\n        \"Paella is from\": \" Spain\",\n        \"Kimchi is from\": \" Korea\",\n    }\n    for p, comp in cuisine.items():\n        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"cuisine\"})\n\n    # ========================================\n    # ECONOMIC AND POLITICAL FACTS\n    # ========================================\n\n    economics = {\n        \"The largest economy is\": \" America\",\n        \"The European Union uses the\": \" euro\",\n        \"OPEC stands for Organization of\": \" Petroleum\",\n        \"The World Bank is headquartered in\": \" Washington\",\n        \"The United Nations is headquartered in\": \" New\",\n        \"NATO stands for North Atlantic\": \" Treaty\",\n        \"GDP stands for Gross Domestic\": \" Product\",\n    }\n    for p, comp in economics.items():\n        pairs.append({\"prompt\": p, \"completion\": comp, \"tag\": \"economics\"})\n\n    # ---- Filter to single-token completions\n    kept = []\n    dropped = []\n    for ex in pairs:\n        if is_single_token(ex[\"completion\"], tokenizer):\n            kept.append(ex)\n        else:\n            dropped.append(ex)\n\n    # ---- Basic reporting\n    from collections import Counter\n    counts = Counter(ex[\"tag\"].split(\":\")[0] for ex in kept)\n    print(\"Kept counts by task:\", dict(counts))\n    print(f\"\\nTotal generated pairs: {len(pairs)}\")\n    print(f\"Single-token completions: {len(kept)}\")\n    print(f\"Multi-token completions (dropped): {len(dropped)}\")\n\n    if dropped:\n        print(f\"\\nShowing first 20 dropped (multi-token) examples:\")\n        for ex in dropped[:20]:\n            ids = tokenizer.encode(ex[\"completion\"], add_special_tokens=False)\n            print(f\"  {ex['tag']:<20} {ex['prompt']:<50} -> {repr(ex['completion']):<20} token_ids={ids}\")\n\n    return kept\n\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)\npairs = build_pairs_expanded(tokenizer)\n\n\n\npairs = build_pairs_expanded(tokenizer)\nprint('Synthetic pairs:', len(pairs))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Section 6 \u2014 Finetune configuration and utilities\nConfigures the synthetic/WikiText mix, builds dataloaders, and defines evaluation helpers."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "DO_FINETUNE = True  # set False if you want to skip this section\n\nCFG = {\n    # mix in WikiText\n    \"use_wiki\": False,\n    \"wiki_dataset_name\": \"wikitext\",\n    \"wiki_config_candidates\": [\"wikitext-103-raw-v1\", \"wikitext-2-raw-v1\"],  # fallback\n    \"wiki_num_samples\": 1536,         # number of wiki chunks (not lines)\n    \"wiki_chunk_chars_min\": 400,      # filter small chunks\n    \"wiki_chunk_chars_max\": 1200,     # chunk size (chars) before tokenization\n\n    # training\n    \"max_len\": 128,                   # increased since wiki chunks are longer\n    \"batch_size\": 16,\n    \"steps\": 77,\n    \"lr\": 7e-6,\n    \"weight_decay\": 0.007,\n    \"grad_clip\": 1.0,\n\n    # finetune mode\n    # \"all\" trains everything; \"slot_attn_only\" freezes everything except slot-space attention op\n    \"finetune_mode\": \"slot_attn_only\",  # or \"all\" or  \"slot_attn_only\"\n\n    # which params count as \"slot attention\" (adjust to your module names)\n    #\"slot_train_name_regex\": r\"(slot|slots).*(attn|attention)|((attn|attention).*(slot|slots))\",\n\n    \"slot_train_name_regex\":r\"(^|\\.)(slot_in|slot_q|slot_k|slot_v|slot_out)\\.weight$|(^|\\.)(_slotspace_gate_raw)$\",\n\n\n}\n\n# -----------------------\n# 1) Utilities (aligned to your model call style)\n# -----------------------\n@torch.no_grad()\ndef next_token_stats(prompt: str, target_token_str: str, model, tokenizer):\n    model.eval()\n    inp = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n\n    tgt_ids = tokenizer.encode(target_token_str, add_special_tokens=False)\n    if len(tgt_ids) != 1:\n        return {\"ok\": False, \"reason\": f\"target string maps to {len(tgt_ids)} tokens\", \"target_ids\": tgt_ids}\n\n    tgt = tgt_ids[0]\n\n    out = model(inp)\n    logits = out[0] if isinstance(out, (tuple, list)) else out\n    last = logits[0, -1, :]\n    probs = torch.softmax(last, dim=-1)\n\n    p = float(probs[tgt].item())\n    rank = int((torch.argsort(last, descending=True) == tgt).nonzero(as_tuple=False).item()) + 1\n    top1_id = int(torch.argmax(last).item())\n    top1 = tokenizer.decode([top1_id])\n\n    return {\"ok\": True, \"p_target\": p, \"rank\": rank, \"top1\": top1, \"target_id\": tgt}\n\n@torch.no_grad()\ndef greedy_suffix(prompt: str, model, gen, max_new_tokens=8):\n    g = dict(gen)\n    g[\"do_sample\"] = False\n    g[\"max_new_tokens\"] = int(max_new_tokens)\n    out = asa_generate(prompt, model, g)\n    text = out[\"text\"]\n    if text.startswith(prompt):\n        return text[len(prompt):].replace(\"\\n\", \" \").strip()\n    idx = text.find(prompt)\n    if idx >= 0:\n        return text[idx+len(prompt):].replace(\"\\n\", \" \").strip()\n    return text.replace(\"\\n\", \" \").strip()\n\n@torch.no_grad()\ndef eval_exact_match(examples, model, gen, max_new_tokens=8):\n    model.eval()\n    ok = 0\n    for ex in examples:\n        pred = greedy_suffix(ex[\"prompt\"], model, gen, max_new_tokens=max_new_tokens)\n        gold = ex[\"completion\"].replace(\"\\n\", \" \").strip()\n        ok += int(pred.startswith(gold))\n    return ok / max(1, len(examples))\n\n# -----------------------\n# 2) Dataset builders\n# -----------------------\n\n\ndef load_wikitext_chunks(tokenizer, num_samples=2048, chunk_chars_min=400, chunk_chars_max=1200):\n    \"\"\"\n    Produces wiki training examples as plain LM text chunks:\n      ex = {\"prompt\": \"\", \"completion\": \"<wiki chunk>\", \"tag\": \"wiki\"}\n    We chunk by chars first, then token-truncate later in dataset.\n    \"\"\"\n    try:\n        from datasets import load_dataset\n    except Exception as e:\n        print(\"[WikiText] datasets not available; skipping WikiText mix.\")\n        return []\n\n    ds = None\n    used_cfg = None\n    for cfg in CFG[\"wiki_config_candidates\"]:\n        try:\n            ds = load_dataset(CFG[\"wiki_dataset_name\"], cfg, split=\"train\")\n            used_cfg = cfg\n            break\n        except Exception:\n            ds = None\n\n    if ds is None:\n        print(\"[WikiText] Could not load WikiText (tried configs:\", CFG[\"wiki_config_candidates\"], \"). Skipping.\")\n        return []\n\n    print(f\"[WikiText] Loaded {CFG['wiki_dataset_name']} / {used_cfg} train split with {len(ds)} rows.\")\n\n    # Pull raw text field (wikitext uses 'text')\n    texts = [t for t in ds[\"text\"] if isinstance(t, str) and len(t.strip()) > 0]\n\n    # Make chunks: concatenate consecutive lines until size bound, filter small chunks\n    chunks = []\n    buf = []\n    buf_len = 0\n\n    # shuffle deterministically\n    rng = random.Random(SEED)\n    rng.shuffle(texts)\n\n    for line in texts:\n        line = line.strip()\n        # skip headings markup lines; keep normal prose\n        if line.startswith(\"=\") and line.endswith(\"=\"):\n            continue\n        if not line:\n            continue\n\n        # add line to buffer\n        if buf_len + len(line) + 1 <= chunk_chars_max:\n            buf.append(line)\n            buf_len += len(line) + 1\n        else:\n            chunk = \" \".join(buf).strip()\n            if len(chunk) >= chunk_chars_min:\n                chunks.append(chunk)\n            buf = [line]\n            buf_len = len(line) + 1\n\n        if len(chunks) >= num_samples:\n            break\n\n    # flush\n    if len(chunks) < num_samples:\n        chunk = \" \".join(buf).strip()\n        if len(chunk) >= chunk_chars_min:\n            chunks.append(chunk)\n\n    # create examples\n    wiki_examples = [{\"prompt\": \"\", \"completion\": c, \"tag\": \"wiki\"} for c in chunks[:num_samples]]\n    print(f\"[WikiText] Prepared {len(wiki_examples)} wiki chunks.\")\n\n    return wiki_examples\n\n# -----------------------\n# 3) Split synthetic (entity-holdout) + build mixed train set\n# -----------------------\n#pairs = build_pairs_expanded(tokenizer)\n\nfrom collections import Counter\n\nholdout_capitals = {\n    \"Spain\", \"Canada\", \"Poland\", \"Portugal\", \"Greece\", \"Austria\",\n    \"Norway\", \"Ireland\", \"Romania\", \"Croatia\", \"Argentina\", \"Chile\"\n}\nholdout_languages = {\"Brazil\", \"Mexico\", \"Netherlands\", \"Sweden\", \"Finland\"}\nholdout_currencies = {\"Japan\", \"Switzerland\", \"South Africa\", \"Thailand\"}\nholdout_continents = {\"Kenya\", \"Vietnam\", \"Peru\", \"New Zealand\"}\n\ntrain_examples, holdout_examples = [], []\nfor ex in pairs:\n    task = ex[\"tag\"].split(\":\")[0]\n\n    if task == \"capital\":\n        country = ex[\"tag\"].split(\":\", 1)[1]\n        (holdout_examples if country in holdout_capitals else train_examples).append(ex)\n    elif task == \"language\":\n        country = ex[\"tag\"].split(\":\", 1)[1]\n        (holdout_examples if country in holdout_languages else train_examples).append(ex)\n    elif task == \"currency\":\n        country = ex[\"tag\"].split(\":\", 1)[1]\n        (holdout_examples if country in holdout_currencies else train_examples).append(ex)\n    elif task == \"continent\":\n        country = ex[\"tag\"].split(\":\", 1)[1]\n        (holdout_examples if country in holdout_continents else train_examples).append(ex)\n    else:\n        if random.random() < 0.2:\n            holdout_examples.append(ex)\n        else:\n            train_examples.append(ex)\n\nprint(f\"\\n[Synthetic] Total kept pairs: {len(pairs)} | Train: {len(train_examples)} | Holdout: {len(holdout_examples)}\")\nprint(f\"[Synthetic] Split: {len(train_examples)/len(pairs)*100:.1f}% / {len(holdout_examples)/len(pairs)*100:.1f}%\")\nholdout_by_cat = Counter(ex[\"tag\"].split(\":\")[0] for ex in holdout_examples)\nprint(\"[Synthetic] Holdout by category:\", dict(holdout_by_cat))\n\n# NEW: load wiki and mix into TRAIN ONLY\nwiki_examples = []\nif CFG[\"use_wiki\"]:\n    wiki_examples = load_wikitext_chunks(\n        tokenizer,\n        num_samples=CFG[\"wiki_num_samples\"],\n        chunk_chars_min=CFG[\"wiki_chunk_chars_min\"],\n        chunk_chars_max=CFG[\"wiki_chunk_chars_max\"],\n    )\n\nmixed_train_examples = train_examples + wiki_examples\nprint(f\"\\n[Mix] Train synthetic={len(train_examples)} + wiki={len(wiki_examples)} => mixed_train={len(mixed_train_examples)}\")\nprint(f\"[Mix] Holdout (synthetic only) = {len(holdout_examples)}\")\n\n# -----------------------\n# 4) Tiny finetune dataset (teacher forcing)\n#    Works for BOTH: prompt+completion pairs and raw wiki chunks (prompt=\"\")\n# -----------------------\nclass PromptCompletionDataset(Dataset):\n    def __init__(self, examples, tokenizer, max_len=128):\n        self.examples = examples\n        self.tok = tokenizer\n        self.max_len = int(max_len)\n\n    def __len__(self): return len(self.examples)\n\n    def __getitem__(self, idx):\n        ex = self.examples[idx]\n        text = ex[\"prompt\"] + ex[\"completion\"]\n        ids = self.tok.encode(text)\n\n        # keep the tail; for wiki, this acts like \"random suffix LM\"\n        ids = ids[-self.max_len:]\n\n        x = torch.tensor(ids[:-1], dtype=torch.long)\n        y = torch.tensor(ids[1:], dtype=torch.long)\n        return x, y, ex\n\ndef collate_pad(batch):\n    xs, ys, exs = zip(*batch)\n    maxT = max(x.size(0) for x in xs)\n    pad_id = tokenizer.eos_token_id  # GPT-2 no pad token\n\n    X = torch.full((len(xs), maxT), pad_id, dtype=torch.long)\n    Y = torch.full((len(xs), maxT), -100, dtype=torch.long)\n\n    for i, (x, y) in enumerate(zip(xs, ys)):\n        T = x.size(0)\n        X[i, :T] = x\n        Y[i, :T] = y\n    return X.to(device), Y.to(device), exs\n\ntrain_ds = PromptCompletionDataset(mixed_train_examples, tokenizer, max_len=CFG[\"max_len\"])\ntrain_dl = DataLoader(\n    train_ds,\n    batch_size=min(CFG[\"batch_size\"], len(train_ds)),\n    shuffle=True,\n    collate_fn=collate_pad\n)\n\n# -----------------------\n# 5) Optional: freeze everything except slot-space attention (NEW)\n# -----------------------\ndef configure_finetune_mode(model, mode: str, name_regex: str):\n    \"\"\"\n    mode:\n      - \"all\": train everything\n      - \"slot_attn_only\": only train parameters whose full name matches `name_regex`\n    \"\"\"\n    if mode == \"all\":\n        for p in model.parameters():\n            p.requires_grad = True\n        trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n        total = sum(p.numel() for p in model.parameters())\n        print(f\"[Finetune] mode=all trainable={trainable}/{total} ({trainable/total*100:.2f}%)\")\n        return\n\n    if mode != \"slot_attn_only\":\n        raise ValueError(f\"Unknown finetune_mode={mode}\")\n\n    rx = re.compile(name_regex, flags=re.IGNORECASE)\n\n    # freeze everything\n    for _, p in model.named_parameters():\n        p.requires_grad = False\n\n    # unfreeze matching params\n    matched = []\n    for n, p in model.named_parameters():\n        if rx.search(n) is not None:\n            p.requires_grad = True\n            matched.append((n, p.numel()))\n\n    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    total = sum(p.numel() for p in model.parameters())\n    print(f\"[Finetune] mode=slot_attn_only regex={name_regex!r}\")\n    print(f\"[Finetune] trainable={trainable}/{total} ({trainable/total*100:.4f}%) matched_tensors={len(matched)}\")\n\n    # show top matches by size\n    matched.sort(key=lambda x: -x[1])\n    for n, k in matched[:25]:\n        print(f\"  [trainable] {k:>10}  {n}\")\n\nconfigure_finetune_mode(model, CFG[\"finetune_mode\"], CFG[\"slot_train_name_regex\"])\n\n# -----------------------\n# 6) Pre-eval (synthetic only, as before)\n# -----------------------\nprint(\"\\n\" + \"=\"*80)\nprint(\"PRE-TRAINING EVALUATION (synthetic only)\")\nprint(\"=\"*80)\n\n# FIX: Enable asa_info to handle model's tuple return type correctly\ngener['asa_info'] = True\n\npre_acc_train = eval_exact_match(train_examples, model, gener, max_new_tokens=8)\npre_acc_hold  = eval_exact_match(holdout_examples, model, gener, max_new_tokens=8)\n\nprint(f\"\\n[PRE] Exact-match accuracy:\")\nprint(f\"  Train:   {pre_acc_train:.3f} ({int(pre_acc_train*len(train_examples))}/{len(train_examples)})\")\nprint(f\"  Holdout: {pre_acc_hold:.3f} ({int(pre_acc_hold*len(holdout_examples))}/{len(holdout_examples)})\")\n\nprint(\"\\n[PRE] Next-token stats for sample of single-token targets (synthetic only):\")\nsample_for_stats = random.sample(pairs, min(30, len(pairs)))\nfor ex in sample_for_stats:\n    stats = next_token_stats(ex[\"prompt\"], ex[\"completion\"], model, tokenizer)\n    if stats[\"ok\"]:\n        print(f\"  {ex['tag']:<25} P={stats['p_target']:.4f} rank={stats['rank']:>5} top1={stats['top1']!r}\")\n    else:\n        print(f\"  {ex['tag']:<25} (skip) {stats['reason']}\")\n\n# -----------------------\n# 7) Light training (mixed: synthetic + wiki)\n# -----------------------\nprint(\"\\n\" + \"=\"*80)\nprint(\"TRAINING (mixed synthetic + wiki)\")\nprint(\"=\"*80)\n\nmodel.train()\n\n# IMPORTANT: optimizer must only see trainable params (esp for slot_attn_only)\ntrainable_params = [p for p in model.parameters() if p.requires_grad]\nif len(trainable_params) == 0:\n    raise RuntimeError(\"No trainable parameters. Check CFG['finetune_mode'] and regex.\")\n\nopt = torch.optim.AdamW(\n    trainable_params,\n    lr=CFG[\"lr\"],\n    betas=(0.9, 0.95),\n    weight_decay=CFG[\"weight_decay\"]\n)\n\nsteps = int(CFG[\"steps\"])\ngrad_clip = float(CFG[\"grad_clip\"])\n\nprint(f\"Training for {steps} steps with batch_size={train_dl.batch_size}\")\nprint(f\"Total mixed_train_examples={len(mixed_train_examples)} | synthetic={len(train_examples)} | wiki={len(wiki_examples)}\\n\")\n\n# stable batch stream (avoid re-instantiating iter(train_dl) each step)\nbatch_iter = itertools.cycle(train_dl)\n\nfor step in range(steps):\n    X, Y, _ = next(batch_iter)\n    opt.zero_grad(set_to_none=True)\n\n    logits = model(X)\n    logits = logits[0] if isinstance(logits, (tuple, list)) else logits\n\n    loss = F.cross_entropy(\n        logits.view(-1, logits.size(-1)),\n        Y.view(-1),\n        ignore_index=-100\n    )\n    loss.backward()\n    torch.nn.utils.clip_grad_norm_(trainable_params, grad_clip)\n    opt.step()\n\n    if (step + 1) % 50 == 0:\n        print(f\"  [train] step {step+1:>4}/{steps} loss={float(loss.item()):.4f}\")\n\nmodel.eval()\n\n# -----------------------\n# 8) Post-eval (synthetic only, as before)\n# -----------------------\nprint(\"\\n\" + \"=\"*80)\nprint(\"POST-TRAINING EVALUATION (synthetic only)\")\nprint(\"=\"*80)\n\npost_acc_train = eval_exact_match(train_examples, model, gener, max_new_tokens=8)\npost_acc_hold  = eval_exact_match(holdout_examples, model, gener, max_new_tokens=8)\n\nprint(f\"\\n[POST] Exact-match accuracy:\")\nprint(f\"  Train:   {post_acc_train:.3f} ({int(post_acc_train*len(train_examples))}/{len(train_examples)})\")\nprint(f\"  Holdout: {post_acc_hold:.3f} ({int(post_acc_hold*len(holdout_examples))}/{len(holdout_examples)})\")\n\nprint(f\"\\n[DELTA] Accuracy change:\")\nprint(f\"  Train:   {pre_acc_train:.3f} -> {post_acc_train:.3f} (\u0394={post_acc_train-pre_acc_train:+.3f})\")\nprint(f\"  Holdout: {pre_acc_hold:.3f} -> {post_acc_hold:.3f} (\u0394={post_acc_hold-pre_acc_hold:+.3f})\")\n\nprint(\"\\n[POST] Next-token stats for same sample (synthetic only):\")\nfor ex in sample_for_stats:\n    stats = next_token_stats(ex[\"prompt\"], ex[\"completion\"], model, tokenizer)\n    if stats[\"ok\"]:\n        print(f\"  {ex['tag']:<25} P={stats['p_target']:.4f} rank={stats['rank']:>5} top1={stats['top1']!r}\")\n\n# -----------------------\n# 9) Generations (synthetic categories only)\n# -----------------------\nprint(\"\\n\" + \"=\"*80)\nprint(\"GENERATION SAMPLES (greedy decoding) (synthetic only)\")\nprint(\"=\"*80)\n\ngeneration_samples = []\nby_category = {}\nfor ex in pairs:\n    cat = ex[\"tag\"].split(\":\")[0]\n    by_category.setdefault(cat, []).append(ex)\n\nfor cat, exs in sorted(by_category.items()):\n    generation_samples.extend(exs[:2])\n\ngeneration_samples = generation_samples[:25]\n\nfor ex in generation_samples:\n    raw = greedy_suffix(ex[\"prompt\"], model, gener, max_new_tokens=12)\n\n    tag_base = ex[\"tag\"].split(\":\")[0]\n    if tag_base == \"capital\":\n        scaffold_prompt = ex[\"prompt\"] + \" the city of\"\n    elif tag_base == \"language\":\n        scaffold_prompt = ex[\"prompt\"] + \" primarily\"\n    elif tag_base == \"currency\":\n        scaffold_prompt = ex[\"prompt\"]\n    else:\n        scaffold_prompt = ex[\"prompt\"]\n\n    sca = greedy_suffix(scaffold_prompt, model, gener, max_new_tokens=12)\n\n    print(f\"\\n{'\u2500'*80}\")\n    print(f\"CATEGORY: {ex['tag']:<25} TARGET: {ex['completion']!r}\")\n    print(f\"PROMPT:   {ex['prompt']!r}\")\n    print(f\"RAW:      {raw[:100]}\")\n    if scaffold_prompt != ex[\"prompt\"]:\n        print(f\"SCAFFOLD: {sca[:100]}\")\n\nprint(\"\\n\" + \"=\"*80)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Section 7 \u2014 Canon Probes (AFTER finetune)\nRe-runs the canon probes after the finetune loop so you can compare margins and routing stats."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "if 'DO_FINETUNE' in globals() and DO_FINETUNE:\n    model.eval()\n    after_results = run_canon_probes(model, 'after_finetune', artifacts_dir)\n\n    comparison = {\n        'mean_margin_before': baseline_results['mean_margin'],\n        'mean_margin_after': after_results['mean_margin'],\n        'margin_deltas': [a-b for a,b in zip(after_results['margins'], baseline_results['margins'])],\n        'routing_stats_before': baseline_results.get('routing_stats', {}),\n        'routing_stats_after': after_results.get('routing_stats', {}),\n    }\n    (artifacts_dir / 'comparison.json').write_text(json.dumps(comparison, indent=2))\n    print('Before/After mean margin:', comparison['mean_margin_before'], '\u2192', comparison['mean_margin_after'])\nelse:\n    print('Finetune skipped; no after-finetune probe.')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Section 8 \u2014 Optional: Push finetuned artifact to HF\nUploads the finetuned weights if `HF_TOKEN` is set in the environment."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "from huggingface_hub import HfApi, upload_file\n\ntoken = os.environ.get('HF_TOKEN')\nif token:\n    api = HfApi(token=token)\n    try:\n        upload_file(\n            path_or_fileobj=str(artifacts_dir / 'finetuned' / 'finetuned.pt'),\n            path_in_repo='finetuned/finetuned.pt',\n            repo_id=HF_REPO,\n            repo_type='model',\n        )\n        print('Uploaded finetuned checkpoint.')\n    except Exception as exc:\n        print('Upload failed:', exc)\nelse:\n    print('HF_TOKEN not set; skipping upload.')\n"
    }
  ],
  "metadata": {
    "colab": {
      "name": "ASA HF Canon Probes + Mini Finetune",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}